<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[AWS certification]]></title>
    <url>%2F2019-08-25-AWS-Certificate%2F</url>
    <content type="text"><![CDATA[ConceptingCloud computing is the on-demand delivery of IT resources and applications via the Internet with pay-as-you-go pricing. Whether you run applications that share photos to millions of mobile users or deliver services that support the critical operations of your business, the cloud provides rapid access to flexible and low-cost IT resources. In its simplest form, cloud computing provides an easy way to access servers, storage, databases, and a broad set of application services over the Internet. Benefits of AWSThere are six advantages for AWS clouding Global in minutes Variable vs capital expense Economies of scale Stop guessing capacity Focus on business differentaiors Increate speed and agility Cost savingOne of the key benefits of cloud computing is the opportunity to replace up-front capital infrastructure expenses with low variable costs that scale with your business. With the cloud, businesses no longer need to plan for and procure servers and other IT infrastructure weeks or months in advance. Instead, they can instantly spin up hundres or thousands of servers in minutes and deliver results faster. With pay-per-use billing, AWS clouding services become an operational expense instead of a capital expense. MetadataMetadata, known as tags, that you can create and assign to your Amazon EC2 resources Amazon Web Services. Amazon Elastic Compute Cloud (Kindle Location 152). Amazon Web Services. Kindle Edition. AZ (Available Zones) Each availability zone is a physical data center in the region, but separate from the other ones (so that they’re isolated from disasters) AWS Consoles are region scoped (except IAM, S3 &amp; Route53) EC2Here you need to create an AMI, but because AMI are bounded in the regions they are created, they need to be copied across regions for disaster recovery purposes Placement group Placements groups are the answer here, where “cluster” guarantees high network performance (correct answer), whereas “spread” would guarantee independent failures between instances. When you launch a new EC2 instance, the EC2 service attempts to place the instance in such a way that all of your instances are spread out across underlying hardware to minimize correlated failures. You can use placement groups to influence the placement of a group of interdependent instances to meet the needs of your workload. Depending on the type of workload, you can create a placement group using one of the following placement strategies: Cluster – packs instances close together inside an Availability Zone. This strategy enables workloads to achieve the low-latency network performance necessary for tightly-coupled node-to-node communication that is typical of HPC applications. Partition – spreads your instances across logical partitions such that groups of instances in one partition do not share the underlying hardware with groups of instances in different partitions. This strategy is typically used by large distributed and replicated workloads, such as Hadoop, Cassandra, and Kafka. Spread – strictly places a small group of instances across distinct underlying hardware to reduce correlated failures. There is no charge for creating a placement group. Cluster Cluster Placement Groups A cluster placement group is a logical grouping of instances within a single Availability Zone. A placement group can span peered VPCs in the same Region. The chief benefit of a cluster placement group, in addition to a 10 Gbps flow limit, is the non-blocking, non-oversubscribed, fully bi-sectional nature of the connectivity. In other words, all nodes within the placement group can talk to all other nodes within the placement group at the full line rate of 10 Gbps flows and 100 Gbps aggregate without any slowing due to over-subscription. ASGASG Lauch configurationLaunch configurations are immutable meaning they cannot be updated. You have to create a new launch configuration, attach it to the ASG and then terminate old instances / launch new instances ASG terminationAZs will be balanced first, then the instance with the oldest launch configuration within that AZ will be terminated. For a reference to the default termination policy logic, have a look at this link: https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-termination.html IAMYour whole AWS security is there:• Users• Groups• Roles Policies are written in JSON (JavaScript Object Notation) IAM has a global view Permissions let you specify access to AWS resources. Permissions are granted to IAM entities (users, groups, and roles) and by default these entities start with no permissions. In other words, IAM entities can do nothing in AWS until you grant them your desired permissions. To give entities permissions, you can attach a policy that specifies the type of access, the actions that can be performed, and the resources on which the actions can be performed. In addition, you can specify any conditions that must be set for access to be allowed or denied. IAM policiesA permissions policy describes who has access to what. Policies attached to an IAM identity are identity-based policies (IAM policies) and policies attached to a resource are resource-based policies. Amazon RDS supports only identity-based policies (IAM policies). DB authenticatioin via IAMMySQL and PostgreSQL both support IAM database authentication. To protect the confidential data of your customers, you have to ensure that your RDS database can only be accessed using the profile credentials specific to your EC2 instances via an authentication token. You can authenticate to your DB instance using AWS Identity and Access Management (IAM) database authentication. IAM database authentication works with MySQL and PostgreSQL. With this authentication method, you don’t need to use a password when you connect to a DB instance. Instead, you use an authentication token. An authentication token is a unique string of characters that Amazon RDS generates on request. Authentication tokens are generated using AWS Signature Version 4. Each token has a lifetime of 15 minutes. You don’t need to store user credentials in the database, because authentication is managed externally using IAM. You can also still use standard database authentication. IAM Federation• Big enterprises usually integrate their own repository of users with IAM• This way, one can login into AWS using their company credentials• Identity Federation uses the SAML standard (Active Directory) • One IAM User per PHYSICAL PERSON• One IAM Role per Application STSTemporary Security CredentialsYou can use the AWS Security Token Service (AWS STS) to create and provide trusted users with temporary security credentials that can control access to your AWS resources. Temporary security credentials work almost identically to the long-term access key credentials that your IAM users can use, with the following differences: Temporary security credentials are short-term, as the name implies. They can be configured to last for anywhere from a few minutes to several hours. After the credentials expire, AWS no longer recognizes them or allows any kind of access from API requests made with them. Temporary security credentials are not stored with the user but are generated dynamically and provided to the user when requested. When (or even before) the temporary security credentials expire, the user can request new credentials, as long as the user requesting them still has permissions to do so. StoragePerformanceInstance Store will have the highest disk performance but comes with the storage being wiped if the instance is terminated, which is acceptable in this case. EBS volumes would provide good performance as far as disk goes, but not as good as Instance Store. EBS data survives instance termination or reboots. EFS is a network drive, and finally S3 cannot be mounted as a local disk (natively). Need to define two terms:• RPO: Recovery Point Objective• RTO: Recovery Time Objective S3Generating S3 pre-signed URLs would bypass CloudFront, therefore we should use CloudFront signed URL. To generate that URL we must code, and Lambda is the perfect tool for running that code on the fly. As the file is greater than 5GB in size, you must use Multi Part upload to upload that file to S3. OAIDon’t make the S3 bucket public. You cannot attach IAM roles to the CloudFront distribution. S3 buckets don’t have security groups. Here you need to use an OAI. Read more here: https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html Restricting Access to Amazon S3 Content by Using an Origin Access IdentityTo restrict access to content that you serve from Amazon S3 buckets, you create CloudFront signed URLs or signed cookies to limit access to files in your Amazon S3 bucket, and then you create a special CloudFront user called an origin access identity (OAI) and associate it with your distribution. Then you configure permissions so that CloudFront can use the OAI to access and serve files to your users, but users can’t use a direct URL to the S3 bucket to access a file there. Taking these steps help you maintain secure access to the files that you serve through CloudFront. In general, if you’re using an Amazon S3 bucket as the origin for a CloudFront distribution, you can either allow everyone to have access to the files there, or you can restrict access. If you limit access by using, for example, CloudFront signed URLs or signed cookies, you also won’t want people to be able to view files by simply using the direct URL for the file. Instead, you want them to only access the files by using the CloudFront URL, so your protections work. For more information about using signed URLs and signed cookies, see Serving Private Content with Signed URLs and Signed Cookies EncryptionWith SSE-C, your company can still provide the encryption key but let AWS do the encryption EBS (Elastic Block Storage)EBS is already redundant storage (replicated within an AZ)But what if you want to increase IOPS to say 100 000 IOPS? RAIDRAID 0 (increase performance) EC2 instanceOne logical volumeeitherEBS Volume 1• Combining 2 or more volumes and getting the total disk space and I/O• But one disk fails, all the data is failed RAID 1 (increase fault tolerance) EC2 instanceOne logical volumeboth• RAID 1 = Mirroring a volume to another• If one disk fails, our logical volume is still working• We have to send the data to two EBS volume at the same time (2x network) EBS types keeping as io1 but reducing the iops may interfere with the burst of performance we need. The EC2 instance type changes won’t affect the 90% of the costs that are incurred to us. CloudFormation is a free service to use. Therefore, gp2 is the right choice, allowing us to save on cost while keeping a burst in performance when needed You can now choose between three Amazon EBS volume types to best meet the needs of your workloads: General Purpose (SSD), Provisioned IOPS (SSD), and Magnetic volumes. General Purpose (SSD)GP2 volumes are suitable for a broad range of workloads, including small to medium-sized databases, development and test environments, and boot volumes. Provisioned IOPS (SSD)Such volumes offer storage with consistent and low-latency performance, are designed for I/O-intensive applications such as large relational or NoSQL databases, and allow you to choose the level of performance you need. Magnetic volumesformerly known as Standard volumes, provide the lowest cost per gigabyte of all Amazon EBS volume types and are ideal for workloads where data is accessed infrequently and applications where the lowest storage cost is important. Backed by Solid-State Drives (SSDs), General Purpose (SSD) volumes provide the ability to burst to 3,000 IOPS per volume, independent of volume size, to meet the performance needs of most applications and also deliver a consistent baseline of 3 IOPS/GB. General Purpose (SSD) volumes offer the same five nines of availability and durable snapshot capabilities as other volume types. Pricing and performance for General Purpose (SSD) volumes are simple and predictable. You pay for each GB of storage you provision, and there are no additional charges for I/O performed on a volume. Prices start as low as $0.10/GB. EBS snapshotWhile it is completing, an in-progress snapshot is not affected by ongoing reads and writes to the volume. You can take a snapshot of an attached volume that is in use. However, snapshots only capture data that has been written to your Amazon EBS volume at the time the snapshot command is issued. This might exclude any data that has been cached by any applications or the operating system. If you can pause any file writes to the volume long enough to take a snapshot, your snapshot should be complete. However, if you can’t pause all file writes to the volume, you should unmount the volume from within the instance, issue the snapshot command, and then remount the volume to ensure a consistent and complete snapshot. You can remount and use your volume while the snapshot status is pending. Save network cost S3 would imply changing the application code, Glacier is not applicable as the files are frequently requested, Storage Gateway isn’t for distributing files to end users. CloudFront is the right answer, because we can put it in front of our ASG and leverage a Global Caching feature that will help us distribute the content reliably with dramatically reduced costs (the ASG won’t need to scale as much) EFSInstance Stores or EBS volumes are local disks and cannot be shared across instances. Here, we need a network file system (NFS), which is exactly what EFS is designed for. RedshiftCreating a smaller cluster with the cold data would not decrease the storage cost of Redshift, which will increase as we keep on creating data. Moving the data to S3 glacier will prevent us from being able to query it. Redshift’s internal storage does not have “tiers”. Therefore, we should migrate the data to S3 IA and use Athena (serverless SQL query engine on top of S3) to analyze the cold data. CloundFrontOriginUntil now, CloudFront could serve up content from Amazon S3. In content-distribution lingo, S3 was the only supported origin server. You would store your web objects (web pages, style sheets, images, JavaScript, and so forth) in S3, and then create a CloudFront distribution. Here is the basic flow: Effective today we are opening up CloudFront and giving you the ability to use the origin server of your choice. You can now create a CloudFront distribution using a custom origin. Each distribution will can point to an S3 or to a custom origin. This could be another storage service, or it could be something more interesting and more dynamic, such as an EC2 instance or even an Elastic Load Balancer: CloudFormationCloudFormation vs Elastic BeanstalkElastic Beanstalk provides an environment to easily deploy and run applications in the cloud.CloudFormation is a convenient provisioning mechanism for a broad range of AWS resources. VPCyou can optionally connect to your own network, known as virtual private clouds (VPCs) Amazon Web Services. Amazon Elastic Compute Cloud (Kindle Locations 153-154). Amazon Web Services. Kindle Edition. Amazon VPC lets you provision a logically isolated section of the Amazon Web Services (AWS) cloud where you can launch AWS resources in a virtual network that you define. You have complete control over your virtual networking environment, including selection of your own IP address ranges, creation of subnets, and configuration of route tables and network gateways. You can also create a hardware Virtual Private Network (VPN) connection between your corporate datacenter and your VPC and leverage the AWS cloud as an extension of your corporate datacenter. SubnetA subnet is a range of IP addresses in your VPC. You can launch AWS resources into a specified subnet. Use a public subnet for resources that must be connected to the internet, and a private subnet for resources that won’t be connected to the internet.To protect the AWS resources in each subnet, use security groups and network access control lists (ACL). VPC EndpointYou must remember that the two services that use a VPC Endpoint Gateway are Amazon S3 and DynamoDB. The rest are VPC Endpoint Interface NACL (Network ACL)NACL is stateless. • NACL are like a firewall which control traffic from and to subnet• Default NACL allows everything outbound and everything inbound• One NACL per Subnet, new Subnets are assigned the Default NACL• Define NACL rules:• Rules have a number (1-32766) and higher precedence with a lower number• E.g. If you define #100 ALLOW and #200 DENY , IP will be allowed • Last rule is an asterisk (*) and denies a request in case of no rule match• AWS recommends adding rules by increment of 100• Newly created NACL will deny everything• NACL are a great way of blocking a specific IP at the subnet level NACL noteworhty pointsNetwork ACL Basics Your VPC automatically comes with a modifiable default network ACL. By default, it allows all inbound and outbound IPv4 traffic and, if applicable, IPv6 traffic. IGW (Internet GateWay)After creating an IGW, make sure the route tables are updated. Additionally, ensure the security group allow the ICMP protocol for ping requests NATNAT Instances would work but won’t scale and you would have to manage them (as they’re EC2 instances). Egress-Only Internet Gateways are for IPv6, not IPv4. Internet Gateways must be deployed in a public subnet. Therefore you must use a NAT Gateway in your public subnet in order to provide internet access to your instances in your private subnets. How to prevent DDoSAWS provides flexible infrastructure and services that help customers implement strong DDoS mitigations and create highly available application architectures that follow AWS Best Practices for DDoS Resiliency. These include services such as Amazon Route 53, Amazon CloudFront, Elastic Load Balancing, and AWS WAF to control and absorb traffic, and deflect unwanted requests. These services integrate with AWS Shield, a managed DDoS protection service that provides always-on detection and automatic inline mitigations to safeguard web applications running on AWS. AWS ShieldAWS Shield is a managed DDoS protection service that is available in two tiers: Standard and Advanced. AWS Shield Standard applies always-on detection and inline mitigation techniques, such as deterministic packet filtering and priority-based traffic shaping, to minimize application downtime and latency. AWS Shield StandardIt is included automatically and transparently to your Elastic Load Balancing load balancers, Amazon CloudFront distributions, and Amazon Route 53 resources at no additional cost. When you use these services that include AWS Shield Standard, you receive comprehensive availability protection against all known infrastructure layer attacks. Customers who have the technical expertise to manage their own monitoring and mitigation of application layer attacks can use AWS Shield together with AWS WAF rules to create a comprehensive DDoS attack mitigation strategy. AWS Shield AdvancedIt provides enhanced DDoS attack detection and monitoring for application-layer traffic to your Elastic Load Balancing load balancers, CloudFront distributions, Amazon Route 53 hosted zones and resources attached to an Elastic IP address, such Amazon EC2 instances. AWS Shield Advanced uses additional techniques to provide granular detection of DDoS attacks, such as resource-specific traffic monitoring to detect HTTP floods or DNS query floods. AWS Shield Advanced includes 24x7 access to the AWS DDoS Response Team (DRT), support experts who apply manual mitigations for more complex and sophisticated DDoS attacks, directly create or update AWS WAF rules, and can recommend improvements to your AWS architectures. AWS WAF is included at no additional cost for resources that you protect with AWS Shield Advanced. AWS Shield Advanced includes access to near real-time metrics and reports, for extensive visibility into infrastructure layer and application layer DDoS attacks. You can combine AWS Shield Advanced metrics with additional, fine-tuned AWS WAF metrics for a more comprehensive CloudWatch monitoring and alarming strategy. Customers subscribed to AWS Shield Advanced can also apply for a credit for charges that result from scaling during a DDoS attack on protected Amazon EC2, Amazon CloudFront, Elastic Load Balancing, or Amazon Route 53 resources. See the AWS Shield Developer Guide for a detailed comparison of the two AWS Shield offerings. CIDRTo add a CIDR block to your VPC, the following rules apply: -The allowed block size is between a /28 netmask and /16 netmask.-The CIDR block must not overlap with any existing CIDR block that’s associated with the VPC.-You cannot increase or decrease the size of an existing CIDR block.-You have a limit on the number of CIDR blocks you can associate with a VPC and the number of routes you can add to a route table. You cannot associate a CIDR block if this results in you exceeding your limits.-The CIDR block must not be the same or larger than the CIDR range of a route in any of the VPC route tables. For example, if you have a route with a destination of 10.0.0.0/24 to a virtual private gateway, you cannot associate a CIDR block of the same range or larger. However, you can associate a CIDR block of 10.0.0.0/25 or smaller.-The first four IP addresses and the last IP address in each subnet CIDR block are not available for you to use, and cannot be assigned to an instance. AWS WAFAWS WAF is a web application firewall that helps protect web applications from common web exploits that could affect application availability, compromise security, or consume excessive resources. You can use AWS WAF to define customizable web security rules that control which traffic accesses your web applications. If you use AWS Shield Advanced, you can use AWS WAF at no extra cost for those protected resources and can engage the DRT to create WAF rules. AWS WAF rules use conditions to target specific requests and trigger an action, allowing you to identify and block common DDoS request patterns and effectively mitigate a DDoS attack. These include size constraint conditions to block a web request based on the length of its query string or request body, and geographic match conditions to implement geo restriction (also known as geoblocking) on requests that originate from specific countries. AWS SSM (Simple System Manager)AWS SSM is parameter store. ELB: Elastic Load BalancingTo automatically distribute incoming application traffic across multiple instances, use Elastic Load Balancing. For HA, even though our ASG is deployed across 3 AZ, the minimum capacity to be highly available is 2. Finally, we can save costs by reserving these two instances as we know they’ll be up and running at any time Application Load Balancer vs Network load balancerPath based routing and host based routing are only available for the Application Load Balancer (ALB). Deploying an NGINX load balancer on EC2 would work but would suffer management and scaling issues. Read more here: https://aws.amazon.com/blogs/aws/new-host-based-routing-support-for-aws-application-load-balancers/ ALB &amp; ASGAdding the entire CIDR of the ALB would work, but wouldn’t guarantee that only the ALB can access the EC2 instances that are part of the ASG. Here, the right solution is to add a rule on the ASG security group to allow incoming traffic from the security group configured for the ALB. SNIsupport for multiple TLS/SSL certificates on Application Load Balancers (ALB) using Server Name Indication (SNI). You can now host multiple TLS secured applications, each with its own TLS certificate, behind a single load balancer. In order to use SNI, all you need to do is bind multiple certificates to the same secure listener on your load balancer. ALB will automatically choose the optimal TLS certificate for each client. These new features are provided at no additional charge. One of our most frequent requests on forums, reddit, and in my e-mail inbox has been to use the Server Name Indication (SNI) extension of TLS to choose a certificate for a client. Since TLS operates at the transport layer, below HTTP, it doesn’t see the hostname requested by a client. SNI works by having the client tell the server “This is the domain I expect to get a certificate for” when it first connects. The server can then choose the correct certificate to respond to the client. All modern web browsers and a large majority of other clients support SNI. In fact, today we see SNI supported by over 99.5% of clients connecting to CloudFront. RDSRDS stands for Relational Database Service Read ReplicsRDS Read Replicas for read scalability Up to 5 Read Replicas Within AZ, Cross AZ or Cross Region Replication is ASYNC, so reads are eventually consistent Replicas can be promoted to their own DB Applications must update the connection string to leverage read replicas Amazon RDS Multi-AZ DeploymentsAmazon RDS Multi-AZ deployments provide enhanced availability and durability for Database (DB) Instances, making them a natural fit for production database workloads. When you provision a Multi-AZ DB Instance, Amazon RDS automatically creates a primary DB Instance and synchronously replicates the data to a standby instance in a different Availability Zone (AZ). Each AZ runs on its own physically distinct, independent infrastructure, and is engineered to be highly reliable. In case of an infrastructure failure, Amazon RDS performs an automatic failover to the standby (or to a read replica in the case of Amazon Aurora), so that you can resume database operations as soon as the failover is complete. Since the endpoint for your DB Instance remains the same after a failover, your application can resume database operation without the need for manual administrative intervention. SecurityUse security groups to control what IP addresses or Amazon EC2 instances can connect to your databases on a DB instance.Run your DB instance in an Amazon Virtual Private Cloud (VPC) for the greatest possible network access control. Working with a DB Instance in a VPCYour VPC must have at least two subnets. These subnets must be in two different Availability Zones in the region where you want to deploy your DB instance.If you want your DB instance in the VPC to be publicly accessible, you must enable the VPC attributes DNS hostnames and DNS resolution. Elastic CacheIAM Auth is not supported by ElastiCache Amazon CloudWatchTo monitor basic statistics for your instances and Amazon EBS volumes, use Amazon CloudWatch. Amazon Web Services. Amazon Elastic Compute Cloud (Kindle Locations 180-184). Amazon Web Services. Kindle Edition. Disabling the Termination from the ASG would prevent our ASG to be Elastic and impact our costs. Making a snapshot of the EC2 instance before it gets terminated could work but it’s tedious, not elastic and very expensive, as all we’re interested about are log files. Using AWS Lambda would be extremely hard to use for this task. Here, the natural and by far easiest solution would be to use the CloudWatch Logs agents on the EC2 instances to automatically send log files into CloudWatch, so we can analyze them in the future easily should any problems arise. API GatewayQ: What API types are supported by Amazon API Gateway? Amazon API Gateway offers two options to create RESTful APIs, HTTP APIs (Preview) and REST APIs, as well as an option to create WebSocket APIs. HTTP API: HTTP APIs, currently available in Preview, are optimized for building APIs that proxy to AWS Lambda functions or HTTP backends, making them ideal for serverless workloads. They do not currently offer API management functionality. REST API: REST APIs offer API proxy functionality and API management features in a single solution. REST APIs offer API management features such as usage plans, API keys, publishing, and monetizing APIs. WebSocket API: WebSocket APIs maintain a persistent connection between connected clients to enable real-time message communication. With WebSocket APIs in API Gateway, you can define backend integrations with AWS Lambda functions, Amazon Kinesis, or any HTTP endpoint to be invoked when messages are received from the connected clients. CloudTrailTo monitor the calls made to the Amazon EC2 API for your account, including calls made by the AWS Management Console, command line tools, and other services, use AWS CloudTrail. In general, to analyze any API calls made within your AWS account, you should use CloudTrail ​ Set up a new CloudTrail trail in a new S3 bucket using the AWS CLI and also pass both the –is-multi-region-trail and –include-global-service-events parameters then encrypt log files using KMS encryption. Apply Multi Factor Authentication (MFA) Delete on the S3 bucket and ensure that only authorized users can access the logs by configuring the bucket policies. Charge the first copy of management events is free. Cloud Trail retention daysManagement event activity is recorded by AWS CloudTrail for the last 90 days, and can be viewed and searched free of charge from the AWS CloudTrail console, or by using the AWS CLI. AWS ECSSummary: AWS ECS – Elastic Container Service• ECS is a container orchestration service• ECS helps you run Docker containers on EC2 machines• ECS is complicated, and made of:• “ECS Core”: Running ECS on user-provisioned EC2 instances• Fargate: Running ECS tasks on AWS-provisioned compute (serverless)• EKS: Running ECS on AWS-powered Kubernetes (running on EC2)• ECR: Docker Container Registry hosted by AWS• ECS &amp; Docker are very popular for microservices• For now, for the exam, only “ECS Core” &amp; ECR is in scope• IAM security and roles at the ECS task level AWS ECS – Concepts• ECS cluster: set of EC2 instances• ECS service: applications definitions running on ECS cluster• ECS tasks + definition: containers running to create the application• ECS IAM roles: roles assigned to tasks to interact with AWS AWS ECS – ALB integration• Application Load Balancer (ALB) has a direct integration feature with ECS called “port mapping”, This allows you to run multiple instances of the same application on the same EC2 machine Dynamic mappingDynamic Port Mapping is available for the Application Load Balancer. A reverse proxy solution would work but would be too much work to manage. Here the ALB has a feature that provides a direct dynamic port mapping feature and integration with the ECS service so we will leverage that. Read more here: https://aws.amazon.com/premiumsupport/knowledge-center/dynamic-port-mapping-ecs/ AWS ECS – ECS Setup &amp; Config file• Run an EC2 instance, install the ECS agent with ECS config file• Or use an ECS-ready Linux AMI (still need to modify config file) • ECS Config file is at /etc/ecs/ecs.config AWS ECR – Elastic Container Registry• Store, managed and deploy your containers on AWS• Fully integrated with IAM &amp; ECS• Sent over HTTPS (encryption in flight) and encrypted at rest Specifying Sensitive DataAmazon ECS enables you to inject sensitive data into your containers by storing your sensitive data in either AWS Secrets Manager secrets or AWS Systems Manager Parameter Store parameters and then referencing them in your container definition. This feature is supported by tasks using both the EC2 and Fargate launch types. Required IAM Permissions for Amazon ECS SecretsTo use this feature, you must have the Amazon ECS task execution role and reference it in your task definition. This allows the container agent to pull the necessary AWS Systems Manager or Secrets Manager resources. For more information, see Amazon ECS Task Execution IAM Role. To provide access to the AWS Systems Manager Parameter Store parameters that you create, manually add the following permissions as an inline policy to the task execution role. For more information, see Adding and Removing IAM Policies. ssm:GetParameters—Required if you are referencing a Systems Manager Parameter Store parameter in a task definition. secretsmanager:GetSecretValue—Required if you are referencing a Secrets Manager secret either directly or if your Systems Manager Parameter Store parameter is referencing a Secrets Manager secret in a task definition. kms:Decrypt—Required only if your secret uses a custom KMS key and not the default key. The ARN for your custom key should be added as a resource. LambdaAWS Lambda functions time out after 15 minutes, and are not usually meant for long running jobs. Lambda parameters EncryptionAlthough Lambda encrypts the environment variables in your function by default, the sensitive information would still be visible to other users who have access to the Lambda console. This is because Lambda uses a default KMS key to encrypt the variables, which is usually accessible by other users. The best option in this scenario is to use encryption helpers to secure your environment variables. Enabling SSL would encrypt data only when in-transit. Your other teams would still be able to view the plaintext at-rest. Use AWS KMS instead. Option 3 is incorrect since, as mentioned, Lambda does provide encryption functionality of environment variables. Lambda functionsYou upload your application code in the form of one or more Lambda functions. Lambda stores code in Amazon S3 and encrypts it at rest. layerA layer is a ZIP archive that contains libraries, a custom runtime, or other dependencies. Use layers to manage your function’s dependencies independently and keep your deployment package small. Invoking FunctionsLambda supports synchronous and asynchronous invocation of a Lambda function. You can control the invocation type only when you invoke a Lambda function (referred to as on-demand invocation).An event source is the entity that publishes events, and a Lambda function is the custom code that processes the events.Event source mapping maps an event source to a Lambda function. It enables automatic invocation of your Lambda function when events occur. Disaster RecoveryOverviewNeed to define two terms:• RPO: Recovery Point Objective• RTO: Recover y Time Objective Disaster Recovery Strategies• Backup and Restore• Pilot Light• Warm Standby• Hot Site / Multi Site Approach Route53Simple Records do not have health checks, here the most likely issue is that the TTL is still in effect so you have to wait until it expires for the new users to perform another DNS query and get the value for your new Load Balancer. DatabaseElastiCache / RDS / Neptune are not serverless databases. DynamoDB is serverless, single digit latency and horizontally scales. DynamoDBDynamoDB Streams will contain a stream of all the changes that happen to a DynamoDB table. It can be chained with a Lambda function that will be triggered to react to these changes, one of which being a developer’s milestone. DAX is a caching layer DAXDAX will be transparent and won’t require an application refactoring, and will cache the “hot keys”. ElastiCache could also be a solution, but it will require a lot of refactoring work on the AWS Lambda side. DynamoDB is horizontally scalable, has a DynamoDB streams capability and is multi AZ by default. On top of it, we can adjust the RCU and WCU automatically using Auto Scaling. DynamoDB’s partition keyDynamoDB supports two types of primary keys: Partition key: A simple primary key, composed of one attribute known as the partition key. Attributes in DynamoDB are similar in many ways to fields or columns in other database systems.Partition key and sort key: Referred to as a composite primary key, this type of key is composed of two attributes. The first attribute is the partition key, and the second attribute is the sort key. Following is an example. Recommendations for partition keysUse high-cardinality attributes. These are attributes that have distinct values for each item, like e-mailid, employee_no, customerid, sessionid, orderid, and so on. Use composite attributes. Try to combine more than one attribute to form a unique key, if that meets your access pattern. For example, consider an orders table with customerid+productid+countrycode as the partition key and order_date as the sort key. Cache the popular items when there is a high volume of read traffic using Amazon DynamoDB Accelerator (DAX). The cache acts as a low-pass filter, preventing reads of unusually popular items from swamping partitions. For example, consider a table that has deals information for products. Some deals are expected to be more popular than others during major sale events like Black Friday or Cyber Monday. DAX is a fully managed, in-memory cache for DynamoDB that doesn’t require developers to manage cache invalidation, data population, or cluster management. DAX also is compatible with DynamoDB API calls, so developers can incorporate it more easily into existing applications. Add random numbers or digits from a predetermined range for write-heavy use cases. Suppose that you expect a large volume of writes for a partition key (for example, greater than 1000 1 K writes per second). In this case, use an additional prefix or suffix (a fixed number from predetermined range, say 1–10) and add it to the partition key. AuroraAurora Read Replicas can be deployed globally Aurora endpointsAmazon Aurora typically involves a cluster of DB instances instead of a single instance. Each connection is handled by a specific DB instance. When you connect to an Aurora cluster, the host name and port that you specify point to an intermediate handler called an endpoint. Aurora uses the endpoint mechanism to abstract these connections. Thus, you don’t have to hardcode all the hostnames or write your own logic for load-balancing and rerouting connections when some DB instances aren’t available. For certain Aurora tasks, different instances or groups of instances perform different roles. For example, the primary instance handles all data definition language (DDL) and data manipulation language (DML) statements. Up to 15 Aurora Replicas handle read-only query traffic. Using endpoints, you can map each connection to the appropriate instance or group of instances based on your use case. For example, to perform DDL statements you can connect to whichever instance is the primary instance. To perform queries, you can connect to the reader endpoint, with Aurora automatically performing load-balancing among all the Aurora Replicas. For clusters with DB instances of different capacities or configurations, you can connect to custom endpoints associated with different subsets of DB instances. For diagnosis or tuning, you can connect to a specific instance endpoint to examine details about a specific DB instance. Types of Aurora EndpointsAn endpoint is represented as an Aurora-specific URL that contains a host address and a port. The following types of endpoints are available from an Aurora DB cluster. Cluster endpointA cluster endpoint for an Aurora DB cluster that connects to the current primary DB instance for that DB cluster. This endpoint is the only one that can perform write operations such as DDL statements. Because of this, the cluster endpoint is the one that you connect to when you first set up a cluster or when your cluster only contains a single DB instance.Each Aurora DB cluster has one cluster endpoint and one primary DB instance. You use the cluster endpoint for all write operations on the DB cluster, including inserts, updates, deletes, and DDL changes. You can also use the cluster endpoint for read operations, such as queries. The cluster endpoint provides failover support for read/write connections to the DB cluster. If the current primary DB instance of a DB cluster fails, Aurora automatically fails over to a new primary DB instance. During a failover, the DB cluster continues to serve connection requests to the cluster endpoint from the new primary DB instance, with minimal interruption of service. Reader endpointA reader endpoint for an Aurora DB cluster connects to one of the available Aurora Replicas for that DB cluster. Each Aurora DB cluster has one reader endpoint. If there is more than one Aurora Replica, the reader endpoint directs each connection request to one of the Aurora Replicas. The reader endpoint provides load-balancing support for read-only connections to the DB cluster. Use the reader endpoint for read operations, such as queries. You can’t use the reader endpoint for write operations. Custom endpointA custom endpoint for an Aurora cluster represents a set of DB instances that you choose. When you connect to the endpoint, Aurora performs load balancing and chooses one of the instances in the group to handle the connection. You define which instances this endpoint refers to, and you decide what purpose the endpoint serves. An Aurora DB cluster has no custom endpoints until you create one. You can create up to five custom endpoints for each provisioned Aurora cluster. You can’t use custom endpoints for Aurora Serverless clusters. The custom endpoint provides load-balanced database connections based on criteria other than the read-only or read-write capability of the DB instances. For example, you might define a custom endpoint to connect to instances that use a particular AWS instance class or a particular DB parameter group. Then you might tell particular groups of users about this custom endpoint. For example, you might direct internal users to low-capacity instances for report generation or ad hoc (one-time) querying, and direct production traffic to high-capacity instances. Instead of using one DB instance for each specialized purpose and connecting to its instance endpoint, you can have multiple groups of specialized DB instances. In this case, each group has its own custom endpoint. This way, Aurora can perform load balancing among all the instances dedicated to tasks such as reporting or handling production or internal queries. The custom endpoints provide load balancing and high availability for each group of DB instances within your cluster. If one of the DB instances within a group becomes unavailable, Aurora directs subsequent custom endpoint connections to one of the other DB instances associated with the same endpoint. Instance endpointAn instance endpoint connects to a specific DB instance within an Aurora cluster. Each DB instance in a DB cluster has its own unique instance endpoint. So there is one instance endpoint for the current primary DB instance of the DB cluster, and there is one instance endpoint for each of the Aurora Replicas in the DB cluster. The instance endpoint provides direct control over connections to the DB cluster, for scenarios where using the cluster endpoint or reader endpoint might not be appropriate. For example, your client application might require more fine-grained load balancing based on workload type. In this case, you can configure multiple clients to connect to different Aurora Replicas in a DB cluster to distribute read workloads. KinesisSQS FIFO will not work here as they cannot sustain thousands of messages per second. SNS cannot be used for data streaming. Lambda isn’t meant to retain data. Kinesis is the right answer here, with providing a partition key in our message we can guarantee ordering for a specific sensor, even if our stream is sharded BeanStalkWhen you create an AWS Elastic Beanstalk environment, you can specify an Amazon Machine Image (AMI) to use instead of the standard Elastic Beanstalk AMI included in your platform version. A custom AMI can improve provisioning times when instances are launched in your environment if you need to install a lot of software that isn’t included in the standard AMIs. Read more here: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.customenv.html MQSNS, SQS and Kinesis are AWS’ proprietary technologies and do not come with MQTT compatibility. Here the only possible answer is Amazon MQ X Ray AWS X-RayAnalyze and debug production, distributed applications AWS X-Ray helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture. With X-Ray, you can understand how your application and its underlying services are performing to identify and troubleshoot the root cause of performance issues and errors. X-Ray provides an end-to-end view of requests as they travel through your application, and shows a map of your application’s underlying components. You can use X-Ray to analyze both applications in development and in production, from simple three-tier applications to complex microservices applications consisting of thousands of services. ElasticCacheRedisWhat are Amazon ElastiCache for Redis nodes, clusters, and replications groups?An ElastiCache for Redis node is the smallest building block of an Amazon ElastiCache for Redis deployment. Each ElastiCache for Redis node supports the Redis protocol and has its own DNS name and port. Multiple types of ElastiCache for Redis nodes are supported, each with varying amount of CPU capability, and associated memory. An ElastiCache for Redis node may take on a primary or a read replica role. A primary node can be replicated to multiple read replica nodes. An ElastiCache for Redis cluster is a collection of one or more ElastiCache for Redis nodes of the same role; the primary node will be in the primary cluster and the read replica node will be in a read replica cluster. At this time a cluster can only have one node. In the future, we will increase this limit. A cluster manages a logical key space, where each node is responsible for a part of the key space. Most of your management operations will be performed at the cluster level. An ElastiCache for Redis replication group encapsulates the primary and read replica clusters for a Redis installation. A replication group will have only one primary cluster and zero or many read replica clusters. All nodes within a replication group (and consequently cluster) will be of the same node type, and have the same parameter and security group settings. Does Amazon ElastiCache for Redis support Multi-AZ operation?Yes, with Amazon ElastiCache for Redis you can create a read replica in another AWS Availability Zone. Upon a failure of the primary node, we will provision a new primary node. In scenarios where the primary node cannot be provisioned, you can decide which read replica to promote to be the new primary. What is Multi-AZ for an ElastiCache for Redis replication group?An ElastiCache for Redis replication group consists of a primary and up to five read replicas. Redis asynchronously replicates the data from the primary to the read replicas. During certain types of planned maintenance, or in the unlikely event of ElastiCache node failure or Availability Zone failure, Amazon ElastiCache will automatically detect the failure of a primary, select a read replica, and promote it to become the new primary. ElastiCache also propagates the DNS changes of the promoted read replica, so if your application is writing to the primary node endpoint, no endpoint change will be needed. How can I use encryption in-transit, at-rest, and Redis AUTH?Encryption in-transit, encryption at-rest, and Redis AUTH are all opt-in features. At the time of Redis cluster creation via the console or command line interface, you can specify if you want to enable encryption and Redis AUTH and can proceed to provide an authentication token for communication with the Redis cluster. Once the cluster is setup with encryption enabled, ElastiCache seamlessly manages certificate expiration and renewal without requiring any additional action from the application. Additionally, the Redis clients need to support TLS to avail of the encrypted in-transit traffic. Is Redis password functionality supported in Amazon ElastiCache for Redis?Yes, Amazon ElastiCache for Redis supports Redis passwords via Redis AUTH feature. It is an opt-in feature available in ElastiCache for Redis version 3.2.6 onwards. You must enable encryption in-transit to use Redis AUTH on your ElastiCache for Redis cluster. encryptionElastiCache for Redis at-rest encryption is an optional feature to increase data security by encrypting on-disk data. When enabled on a replication group, it encrypts the following aspects: Disk during sync, backup and swap operations Backups stored in Amazon S3 ElastiCache for Redis offers default (service managed) encryption at rest, as well as ability to use your own symetric customer managed customer master keys in AWS Key Management Service (KMS). KMSManage keys used for encrypted DB instances using the AWS KMS. KMS encryption keys are specific to the region that they are created in. Q&amp;A Lambda would time out after 15 minutes (2000*3=6000 seconds = 100 minutes). Glue is for performing ETL, but cannot run custom Python scripts. Kinesis Streams is for real time data (here we are in a batch setup), RDS could be used to run SQL queries on the data, but no Python script. The correct answer is EC2 Elastic Beanstalk cannot manage AWS Lambda functions, OpsWorks is for Chef / Puppet, and Trusted Advisor is to get recommendations regarding the 5 pillars of the well architected framework. Kinesis cannot scale infinitely and we may have the same throughput issues. SNS won’t keep our data if it cannot be delivered, and DAX is used for caching reads, not to help with writes. Here, using SQS as a middleware will help us sustain the write throughput during write peaks CloudFront is not a good solution here as the content is highly dynamic, and CloudFront will cache things. Dynamic applications cannot be deployed to S3, and Route53 does not help in scaling your application. ASG is the correct answer here Network Load Balancers expose a fixed IP to the public web, therefore allowing your application to be predictably reached using these IPs, while allowing you to scale your application behind the Network Load Balancer using an ASG. Application and Classic Load Balancers expose a fixed DNS (=URL). Finally, the ASG does not have a dynamic Elastic IPs attachment feature Hosting the master pack into S3 will require an application code refactor. Upgrading the EC2 instances won’t help save network cost and ELB does not have any caching capability. Here you need to create a CloudFront distribution to add a caching layer in front of your ELB. That caching layer will be very effective as the image pack is a static file, and tremendously save in network cost. Adding Aurora Read Replicas would greatly increase the cost, switching to a Load Balancer would not improve the problems, and AWS Lambda has no native in memory caching capability. Here, using API Gateway Caching feature is the answer, as we can accept to serve stale data to our users SQS will allow you to buffer the image compression requests and process them asynchronously. It also has a direct built-in mechanism for retries and scales seamlessly. To process these jobs, due to the unpredictable nature of their volume, and the desire to save on costs, Spot Instances are recommended. Distributing the static content through S3 allows to offload most of the network usage to S3 and free up our applications running on ECS. EFS will not change anything as static content on EFS would still have to be distributed by our ECS instances S3 does not work as overwrites are eventually consistent so the latest data will not always be read. Neptune is a graph database so it’s not a good fit, ElastiCache could work but it’s a better fit as a caching technology to enhance reads. Here, the best fit is RDS. RDS Multi AZ helps with disaster recovery in case of an AZ failure. ElastiCache would definitely help with the read load, but would require a refactor of the application’s core logic. DynamoDB with DAX would also probably help with the read load, but once again it would require a refactor of the application’s core logic. Here, our only option to scale reads is to use RDS Read Replicas Resources RDS cheatsheet: https://tutorialsdojo.com/aws-cheat-sheet-amazon-relational-database-service-amazon-rds/ VPC cheatsheet: https://tutorialsdojo.com/aws-cheat-sheet-amazon-vpc/]]></content>
      <tags>
        <tag>AWS</tag>
        <tag>Cloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Conversations with God]]></title>
    <url>%2F2019-09-12-Conversations-with-God%2F</url>
    <content type="text"><![CDATA[Feelings is the language of the soul.If you want to know what’s true for you about something, look to how your’re feeling about.]]></content>
  </entry>
  <entry>
    <title><![CDATA[Kafka]]></title>
    <url>%2F2019-07-07-Kafka%2F</url>
    <content type="text"><![CDATA[KafkaKafka is fast. A single node can handle hundreds of read/writes from thousands of clients in real time. Kafka is also distributed and scalable. It creates and takes down nodes in an elastic manner, without incurring any downtime. Data streams are split into partitions and spread over different brokers for capability and redundancy. History of KafkaThe result was a publish/subscribe messaging system that had an interface typical of messaging systems but a storage layer more like a log-aggregation system. Combined with the adoption of Apache Avro for message serialization, Kafka was effective for handling both metrics and user-activity tracking at a scale of billions of messages per day. Kafka features Language AgnosticProducers and consumers use binary protocol to talk to a Kafka cluster. DurabilityKafka does not track which messages were read by each consumer. Kafka keeps all messages for a finite amount of time, and it is consumers’ responsibility to track their location per topic, i.e. offsets. Terminology:Topic: a feed of messages or packagesPartition: group of topics split for scalability and redundancyProducer: process that introduces messages into the queueConsumer: process that subscribes to various topics and processes from a feed of published messagesBroker: a node that is part of the Kafka cluster Topics and PartitionsMessages in Kafka are categorized into topics. The closest analogies for a topic are a database table or a folder in a filesystem. Topics are additionally broken down into a number of partitions. Going back to the “commit log” description, a partition is a sin‐ gle log. Messages are written to it in an append-only fashion, and are read in order from beginning to end. Note that as a topic typically has multiple partitions, there is no guarantee of message time-ordering across the entire topic, just within a single partition. Partitions are also the way that Kafka provides redundancy and scalability. Each partition can be hosted on a different server, which means that a single topic can be scaled horizontally across multiple servers to provide performance far beyond the ability of a single server. Producers and consumersKafka clients are users of the system, and there are two basic types: producers and consumers. There are also advanced client APIs—Kafka Connect API for data inte‐ gration and Kafka Streams for stream processing. The advanced clients use producers and consumers as building blocks and provide higher-level functionality on top. producersProducers create new messages. In other publish/subscribe systems, these may be called publishers or writers. In general, a message will be produced to a specific topic. By default, the producer does not care what partition a specific message is written to and will balance messages over all partitions of a topic evenly. In some cases, the pro‐ ducer will direct messages to specific partitions. This is typically done using the mes‐ sage key and a partitioner that will generate a hash of the key and map it to a specific partition. This assures that all messages produced with a given key will get written to the same partition. The producer could also use a custom partitioner that follows other business rules for mapping messages to partitions. ConsumersConsumers read messages. In other publish/subscribe systems, these clients may be called subscribers or readers. The consumer subscribes to one or more topics and reads the messages in the order in which they were produced. The consumer keeps track of which messages it has already consumed by keeping track of the offset of messages. The offset is another bit of metadata—an integer value that continually increases—that Kafka adds to each message as it is produced. Each message in a given partition has a unique offset. By storing the offset of the last consumed message for each partition, either in Zookeeper or in Kafka itself, a consumer can stop and restart without losing its place. Consumers work as part of a consumer group, which is one or more consumers that work together to consume a topic. The group assures that each partition is only con‐ sumed by one member. there are three consumers in a single group consuming a topic. Two of the consumers are working from one partition each, while the third consumer is working from two partitions. The mapping of a consumer to a partition is often called ownership of the partition by the consumer. Consumer groupConsumers may be grouped in a consumer group with multiple consumers. Each consumer in a consumer group will read messages from a unique subset of partitions in each topic they subscribe to. Each message is delivered to one consumer in the group, and all messages with the same key arrive at the same consumer. Brokers and ClustersA single Kafka server is called a broker. The broker receives messages from producers, assigns offsets to them, and commits the messages to storage on disk. It also services consumers, responding to fetch requests for partitions and responding with the mes‐ sages that have been committed to disk. Depending on the specific hardware and its performance characteristics, a single broker can easily handle thousands of partitions and millions of messages per second.Kafka brokers are designed to operate as part of a cluster. Within a cluster of brokers, one broker will also function as the cluster controller (elected automatically from the live members of the cluster). The controller is responsible for administrative operations, including assigning partitions to brokers and monitoring for broker failures. A partition is owned by a single broker in the cluster, and that broker is called the leader of the partition. A partition may be assigned to multiple brokers, which will result in the partition being replicated This provides redundancy of messages in the partition, such that another broker can take over leadership if there is a broker failure. However, all consumers and producers operating on that partition must connect to the leader. retentionsA key feature of Apache Kafka is that of retention, which is the durable storage of messages for some period of time. Kafka brokers are configured with a default reten‐ tion setting for topics, either retaining messages for some period of time (e.g., 7 days) or until the topic reaches a certain size in bytes (e.g., 1 GB). Once these limits are reached, messages are expired and deleted so that the retention configuration is a minimum amount of data available at any time. Individual topics can also be config‐ ured with their own retention settings so that messages are stored for only as long as they are useful. For example, a tracking topic might be retained for several days, whereas application metrics might be retained for only a few hours. Topics can also be configured as log compacted, which means that Kafka will retain only the last mes‐ sage produced with a specific key. This can be useful for changelog-type data, where only the last update is interesting. mirror makerThe Kafka project includes a tool called MirrorMaker, used for this purpose. At its core, MirrorMaker is simply a Kafka consumer and producer, linked together with a queue. Messages are consumed from one Kafka cluster and produced for another. Why Kafka?Multiple ProducersKafka is able to seamlessly handle multiple producers, whether those clients are using many topics or the same topic. Multiple ConsumersIn addition to multiple producers, Kafka is designed for multiple consumers to read any single stream of messages without interfering with each other. This is in contrast to many queuing systems where once a message is consumed by one client, it is not available to any other. Multiple Kafka consumers can choose to operate as part of a group and share a stream, assuring that the entire group processes a given message only once. ##Disk-Based RetentionNot only can Kafka handle multiple consumers, but durable message retention means that consumers do not always need to work in real time. Messages are committed to disk, and will be stored with configurable retention rules. ScalableKafka’s flexible scalability makes it easy to handle any amount of data. Users can start with a single broker as a proof of concept, expand to a small development cluster of three brokers, and move into production with a larger cluster of tens or even hun‐ dreds of brokers that grows over time as the data scales up. High PerformanceAll of these features come together to make Apache Kafka a publish/subscribe mes‐ saging system with excellent performance under high load. Producers, consumers, and brokers can all be scaled out to handle very large message streams with ease. This can be done while still providing subsecond message latency from producing a mes‐ sage to availability to consumers. Process of producing messageWe start producing messages to Kafka by creating a ProducerRecord, which must include the topic we want to send the record to and a value. Optionally, we can also specify a key and/or a partition. Once we send the ProducerRecord, the first thing the producer will do is serialize the key and value objects to ByteArrays so they can be sent over the network.Next, the data is sent to a partitioner. If we specified a partition in the ProducerRecord, the partitioner doesn’t do anything and simply returns the partition we specified. If we didn’t, the partitioner will choose a partition for us, usually based on the ProducerRecord key. Once a partition is selected, the producer knows which topic and partition the record will go to. It then adds the record to a batch of records that will also be sent to the same topic and partition. A separate thread is responsible for sending those batches of records to the appropriate Kafka brokers.When the broker receives the messages, it sends back a response. If the messages were successfully written to Kafka, it will return a RecordMetadata object with the topic, partition, and the offset of the record within the partition. If the broker failed to write the messages, it will return an error. When the producer receives an error, it may retry sending the message a few more times before giving up and returning an error. Constructing a Kafka ProducerThe first step in writing messages to Kafka is to create a producer object with the properties you want to pass to the producer. A Kafka producer has three mandatory properties: bootstrap.serversList of host:port pairs of brokers that the producer will use to establish initial connection to the Kafka cluster. This list doesn’t need to include all brokers, since the producer will get more information after the initial connection. But it is rec‐ ommended to include at least two, so in case one broker goes down, the producer will still be able to connect to the cluster. key.serializerName of a class that will be used to serialize the keys of the records we will pro‐ duce to Kafka. Kafka brokers expect byte arrays as keys and values of messages. However, the producer interface allows, using parameterized types, any Java object to be sent as a key and value. This makes for very readable code, but it also means that the producer has to know how to convert these objects to byte arrays. key.serializer should be set to a name of a class that implements the org.apache.kafka.common.serialization.Serializer interface. The producer will use this class to serialize the key object to a byte array. The Kafka client pack‐ age includes ByteArraySerializer (which doesn’t do much), StringSerializer, and IntegerSerializer, so if you use common types, there is no need to implement your own serializers. Setting key.serializer is required even if you intend to send only values. value.serializerName of a class that will be used to serialize the values of the records we will pro‐ duce to Kafka. The same way you set key.serializer to a name of a class that will serialize the message key object to a byte array, you set value.serializer to a class that will serialize the message value object. Sample code to generate producer record12345private Properties kafkaProps = new Properties(); kafkaProps.put("bootstrap.servers", "broker1:9092,broker2:9092"); kafkaProps.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer"); kafkaProps.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer"); producer = new KafkaProducer&lt;String, String&gt;(kafkaProps); Deliver messageOnce we instantiate a producer, it is time to start sending messages. There are three primary methods of sending messages: Fire-and-forgetWe send a message to the server and don’t really care if it arrives succesfully or not. Most of the time, it will arrive successfully, since Kafka is highly available and the producer will retry sending messages automatically. However, some mes‐ sages will get lost using this method. Synchronous sendWe send a message, the send() method returns a Future object, and we use get() to wait on the future and see if the send() was successful or not. Asynchronous sendWe call the send() method with a callback function, which gets triggered when it receives a response from the Kafka broker. Sample code 123456789ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;&gt;("CustomerCountry", "Precision Products","France"); try &#123; producer.send(record); //fire and forget producer.send(record).get(); // synchronously, calling Future.get() &#125; catch (Exception e) &#123; e.printStackTrace();&#125; We use the producer object send() method to send the ProducerRecord. As we’ve seen in the producer architecture diagram in Figure 3-1, the message will be placed in a buffer and will be sent to the broker in a separate thread. The send() method returns a Java Future object with RecordMetadata Samle code of asynchronous12345678910private class DemoProducerCallback implements Callback &#123; @Override public void onCompletion(RecordMetadata recordMetadata, Exception e) &#123; if (e != null) &#123; e.printStackTrace(); &#125;&#125; &#125; ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;&gt;("CustomerCountry", "Biomedical Materials", "USA"); producer.send(record, new DemoProducerCallback()); RebalancingMoving partition ownership from one consumer to another is called a rebalance. Rebalances are important because they provide the consumer group with high availa‐ bility and scalability (allowing us to easily and safely add and remove consumers), but in the normal course of events they are fairly undesirable. During a rebalance, con‐ sumers can’t consume messages, so a rebalance is basically a short window of unavail‐ ability of the entire consumer group. In addition, when partitions are moved from one consumer to another, the consumer loses its current state; if it was caching any data, it will need to refresh its caches—slowing down the application until the con‐ sumer sets up its state again. Throughout this chapter we will discuss how to safely handle rebalances and how to avoid unnecessary ones. cosumberSubscribing to TopicsOnce we create a consumer, the next step is to subscribe to one or more topics. The subcribe() method takes a list of topics as a parameter, so it’s pretty simple to use: 1consumer.subscribe(Collections.singletonList("customerCountries")); Here we simply create a list with a single element: the topic name customerCountries. Sample consumer codeThe Poll LoopAt the heart of the consumer API is a simple loop for polling the server for more data. Once the consumer subscribes to topics, the poll loop handles all details of coordina‐ tion, partition rebalances, heartbeats, and data fetching, leaving the developer with a clean API that simply returns available data from the assigned partitions. The main body of a consumer will look as follows: 123456789101112131415161718192021 try &#123; while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); for (ConsumerRecord&lt;String, String&gt; record : records) &#123; log.debug("topic = %s, partition = %s, offset = %d, customer = %s, country = %s\n", record.topic(), record.partition(), record.offset(), record.key(), record.value()); int updatedCount = 1; if (custCountryMap.countainsValue(record.value())) &#123; updatedCount = custCountryMap.get(record.value()) + 1; &#125; custCountryMap.put(record.value(), updatedCount) JSONObject json = new JSONObject(custCountryMap); System.out.println(json.toString(4)) &#125;&#125;&#125; finally &#123; consumer.close(); &#125; Thread safetyThread SafetyYou can’t have multiple consumers that belong to the same group in one thread and you can’t have multiple threads safely use the same consumer. One consumer per thread is the rule. To run mul‐ tiple consumers in the same group in one application, you will need to run each in its own thread. It is useful to wrap the con‐ sumer logic in its own object and then use Java’s ExecutorService to start multiple threads each with its own consumer. CommitAs discussed before, one of Kafka’s unique characteristics is that it does not track acknowledgments from consumers the way many JMS queues do. Instead, it allows consumers to use Kafka to track their posi‐ tion (offset) in each partition.We call the action of updating the current position in the partition a commit. How does a consumer commit an offset? It produces a message to Kafka, to a special __consumer_offsets topic, with the committed offset for each partition. As long as all your consumers are up, running, and churning away, this will have no impact. However, if a consumer crashes or a new consumer joins the consumer group, this will trigger a rebalance. After a rebalance, each consumer may be assigned a new set of partitions than the one it processed before. In order to know where to pick up the work, the consumer will read the latest committed offset of each partition and con‐ tinue from there. Automatic CommitWith autocommit enabled, a call to poll will always commit the last offset returned by the previous poll. It doesn’t know which events were actually processed, so it is critical to always process all the events returned by poll() before calling poll() again. (Just like poll(), close() also commits offsets automatically.) This is usually not an issue, but pay attention when you handle exceptions or exit the poll loop prematurely. Manual commitIt is important to remember that commitSync() will commit the latest offset returned by poll(), so make sure you call commitSync() after you are done processing all the records in the collection, or you risk missing messages as described previously. When rebalance is triggered, all the messages from the beginning of the most recent batch until the time of the rebalance will be processed twice.Here is how we would use commitSync to commit offsets after we finished processing the latest batch of messages: 1234567891011121314while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); for (ConsumerRecord&lt;String, String&gt; record : records) &#123; System.out.printf("topic = %s, partition = %s, offset = %d, customer = %s, country = %s\n", record.topic(), record.partition(), record.offset(), record.key(), record.value());&#125; try &#123; consumer.commitSync(); &#125; catch (CommitFailedException e) &#123; log.error("commit failed", e) &#125;&#125; Combining Synchronous and Asynchronous CommitsNormally, occasional failures to commit without retrying are not a huge problem because if the problem is temporary, the following commit will be successful. But if we know that this is the last commit before we close the consumer, or before a reba‐ lance, we want to make extra sure that the commit succeeds.Therefore, a common pattern is to combine commitAsync() with commitSync() just before shutdown. Here is how it works (we will discuss how to commit just before rebalance when we get to the section about rebalance listeners): 1234567891011121314151617181920 try &#123; while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); for (ConsumerRecord&lt;String, String&gt; record : records) &#123; System.out.printf("topic = %s, partition = %s, offset = %d, customer = %s, country = %s\n", record.topic(), record.partition(), record.offset(), record.key(), record.value());&#125; consumer.commitAsync(); &#125; &#125; catch (Exception e) &#123; log.error("Unexpected error", e); &#125; finally &#123; try &#123; consumer.commitSync(); &#125; finally &#123; consumer.close(); &#125;&#125; ExitWhen you decide to exit the poll loop, you will need another thread to call con sumer.wakeup(). If you are running the consumer loop in the main thread, this can be done from ShutdownHook. Note that consumer.wakeup() is the only consumer method that is safe to call from a different thread. Calling wakeup will cause poll() to exit with WakeupException, or if consumer.wakeup() was called while the thread was not waiting on poll, the exception will be thrown on the next iteration when poll() is called. The ControllerThe controller is one of the Kafka brokers that, in addition to the usual broker func‐ tionality, is responsible for electing partition leaders (we’ll discuss partition leaders and what they do in the next section). The first broker that starts in the cluster becomes the controller by creating an ephemeral node in ZooKeeper called /control ler. When other brokers start, they also try to create this node, but receive a “node already exists” exception, which causes them to “realize” that the controller node already exists and that the cluster already has a controller. ReplicationReplication is at the heart of Kafka’s architecture. The very first sentence in Kafka’s documentation describes it as “a distributed, partitioned, replicated commit log ser‐ vice.” Replication is critical because it is the way Kafka guarantees availability and durability when individual nodes inevitably fail. MemeroyKafka should run entirely on RAM. JVM heap size shouldn’t be bigger than your available RAM. That is to avoid swapping. Swap usageWatch for swap usage, as it will degrade performance on Kafka and lead to operations timing out (set vm.swappiness = 0). When used swap is &gt; 128MB. Kafka Monitoring ToolsAny monitoring tools with JMX support should be able to monitor a Kafka cluster. Here are 3 monitoring tools we liked: First one is check_kafka.pl from Hari Sekhon. It performs a complete end to end test, i.e. it inserts a message in Kafka as a producer and then extracts it as a consumer. This makes our life easier when measuring service times. Another useful tool is KafkaOffsetMonitor for monitoring Kafka consumers and their position (offset) in the queue. It aids our understanding of how our queue grows and which consumers groups are lagging behind. Last but not least, the LinkedIn folks have developed what we think is the smartest tool out there: Burrow. It analyzes consumer offsets and lags over a window of time and determines the consumer status. You can retrieve this status over an HTTP endpoint and then plug it into your favourite monitoring tool (Server Density for example). Oh, and we would be amiss if we didn’t mention Yahoo’s Kafka-Manager. While it does include some basic monitoring, it is more of a management tool. If you are just looking for a Kafka management tool, check out AirBnb’s kafkat. commandsStart zookeeperbin/zookeeper-server-start.sh config/zookeeper.properties bin/kafka-server-start.sh config/server.properties ~/dev/git/kafka-demo/kafka_2.11-2.0.0/bin/kafka-topics.sh –create –zookeeper localhost:2181 –replication-factor 1 –partitions 1 –topic todtest bin/kafka-topics.sh –list –zookeeper localhost:2181bin/kafka-console-producer.sh –broker-list localhost:9092 –topic todtestbin/kafka-console-consumer.sh –bootstrap-server localhost:9092 –topic todtest –from-beginning bin/kafka-topics.sh –describe –zookeeper localhost:2181 –topic test list topics1./kafka-topics.sh --list --zookeeper localhost:2181 describe topics./kafka-topics.sh –describe –zookeeper localhost:2181 using connectorbin/connect-standalone.sh config/connect-standalone.properties config/connect-file-source.properties config/connect-file-sink.properties mvn archetype:generate -DarchetypeGroupId=org.apache.kafka -DarchetypeArtifactId=streams-quickstart-java -DarchetypeVersion=2.0.0 -DgroupId=io -DartifactId=todzhang -Dversion=0.1 -Dpackage=todzhangapp SecurityThe keystore stores each machine’s own identity. The truststore stores all the certificates that the machine should trust. Importing a certificate into one’s truststore also means trusting all certificates that are signed by that certificate. As the analogy above, trusting the government (CA) also means trusting all passports (certificates) that it has issued. This attribute is called the chain of trust, and it is particularly useful when deploying SSL on a large Kafka cluster. You can sign all certificates in the cluster with a single CA, and have all machines share the same truststore that trusts the CA. That way all machines can authenticate all other machines. To deploy SSL, the general steps are: Generate the keys and certificates Create your own Certificate Authority (CA) Sign the certificate Generate the key and the certificate for each Kafka broker in the cluster. Generate the key into a keystore called kafka.server.keystore so that you can export and sign it later with CA. The keystore file contains the private key of the certificate; therefore, it needs to be kept safely. With user promptskeytool -keystore kafka.server.keystore.jks -alias localhost -genkey Without user prompts, pass command line argumentskeytool -keystore kafka.server.keystore.jks -alias localhost -validity {validity} -genkey -storepass {keystore-pass} -keypass {key-pass} -dname {distinguished-name} -ext SAN=DNS:{hostname}Ensure that the common name (CN) exactly matches the fully qualified domain name (FQDN) of the server. The client compares the CN with the DNS domain name to ensure that it is indeed connecting to the desired server, not a malicious one. The hostname of the server can also be specified in the Subject Alternative Name (SAN). Since the distinguished name is used as the server principal when SSL is used as the inter-broker security protocol, it is useful to have hostname as a SAN rather than the CN. Create your own Certificate Authority (CA)Generate a CA that is simply a public-private key pair and certificate, and it is intended to sign other certificates. 1openssl req -new -x509 -keyout ca-key -out ca-cert -days &#123;validity&#125; Add the generated CA to the clients’ truststore so that the clients can trust this CA: 1keytool -keystore kafka.client.truststore.jks -alias CARoot -import -file ca-cert Add the generated CA to the brokers’ truststore so that the brokers can trust this CA. 1keytool -keystore kafka.server.truststore.jks -alias CARoot -import -file ca-cert Sign the certificateTo sign all certificates in the keystore with the CA that you generated: Export the certificate from the keystore: keytool -keystore kafka.server.keystore.jks -alias localhost -certreq -file cert-fileSign it with the CA: openssl x509 -req -CA ca-cert -CAkey ca-key -in cert-file -out cert-signed -days {validity} -CAcreateserial -passin pass:{ca-password}Import both the certificate of the CA and the signed certificate into the broker keystore: keytool -keystore kafka.server.keystore.jks -alias CARoot -import -file ca-certkeytool -keystore kafka.server.keystore.jks -alias localhost -import -file cert-signed SASLSimple Authentication and Security Layer (SASL) is a framework for authentication and data security in Internet protocols. It decouples authentication mechanisms from application protocols, in theory allowing any authentication mechanism supported by SASL to be used in any application protocol that uses SASL. Authentication mechanisms can also support proxy authorization, a facility allowing one user to assume the identity of another. They can also provide a data security layer offering data integrity and data confidentiality services. DIGEST-MD5 provides an example of mechanisms which can provide a data-security layer. Application protocols that support SASL typically also support Transport Layer Security (TLS) to complement the services offered by SASL. References https://blog.serverdensity.com/how-to-monitor-kafka/]]></content>
      <tags>
        <tag>2018-08-16-Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka In Spring]]></title>
    <url>%2F2019-09-02-Kafka-In-Spring%2F</url>
    <content type="text"><![CDATA[Enable Kafka listener annotated endpoints that are created under the covers by a AbstractListenerContainerFactory. To be used on Configuration classes as follows: @Configuration @EnableKafka public class AppConfig { @Bean public ConcurrentKafkaListenerContainerFactory myKafkaListenerContainerFactory() { ConcurrentKafkaListenerContainerFactory factory = new ConcurrentKafkaListenerContainerFactory(); factory.setConsumerFactory(consumerFactory()); factory.setConcurrency(4); return factory; } // other @Bean definitions } The KafkaListenerContainerFactory is responsible to create the listener container for a particular endpoint. Typical implementations, as the ConcurrentKafkaListenerContainerFactory used in the sample above, provides the necessary configuration options that are supported by the underlying MessageListenerContainer.@EnableKafka enables detection of KafkaListener annotations on any Spring-managed bean in the container. For example, given a class MyService: package com.acme.foo; public class MyService { @KafkaListener(containerFactory = “myKafkaListenerContainerFactory”, topics = “myTopic”) public void process(String msg) { // process incoming message } } The container factory to use is identified by the containerFactory attribute defining the name of the KafkaListenerContainerFactory bean to use. When none is set a KafkaListenerContainerFactory bean with name kafkaListenerContainerFactory is assumed to be present.the following configuration would ensure that every time a message is received from topic “myQueue”, MyService.process() is called with the content of the message: @Configuration @EnableKafka public class AppConfig { @Bean public MyService myService() { return new MyService(); } // Kafka infrastructure setup } Alternatively, if MyService were annotated with @Component, the following configuration would ensure that its @KafkaListener annotated method is invoked with a matching incoming message: @Configuration @EnableKafka @ComponentScan(basePackages = “com.acme.foo”) public class AppConfig { } Note that the created containers are not registered with the application context but can be easily located for management purposes using the KafkaListenerEndpointRegistry.Annotated methods can use a flexible signature; in particular, it is possible to use the Message abstraction and related annotations, see KafkaListener Javadoc for more details. For instance, the following would inject the content of the message and the kafka partition header: @KafkaListener(containerFactory = “myKafkaListenerContainerFactory”, topics = “myTopic”) public void process(String msg, @Header(“kafka_partition”) int partition) { // process incoming message } These features are abstracted by the MessageHandlerMethodFactory that is responsible to build the necessary invoker to process the annotated method. By default, DefaultMessageHandlerMethodFactory is used.When more control is desired, a @Configuration class may implement KafkaListenerConfigurer. This allows access to the underlying KafkaListenerEndpointRegistrar instance. The following example demonstrates how to specify an explicit default KafkaListenerContainerFactory { @code @Configuration @EnableKafka public class AppConfig implements KafkaListenerConfigurer { @Override public void configureKafkaListeners(KafkaListenerEndpointRegistrar registrar) { registrar.setContainerFactory(myKafkaListenerContainerFactory()); } @Bean public KafkaListenerContainerFactory&lt;?, ?&gt; myKafkaListenerContainerFactory() { // factory settings } @Bean public MyService myService() { return new MyService(); } } } It is also possible to specify a custom KafkaListenerEndpointRegistry in case you need more control on the way the containers are created and managed. The example below also demonstrates how to customize the org.springframework.messaging.handler.annotation.support.DefaultMessageHandlerMethodFactory as well as how to supply a custom Validator so that payloads annotated with Validated are first validated against a custom Validator. { @code @Configuration @EnableKafka public class AppConfig implements KafkaListenerConfigurer { @Override public void configureKafkaListeners(KafkaListenerEndpointRegistrar registrar) { registrar.setEndpointRegistry(myKafkaListenerEndpointRegistry()); registrar.setMessageHandlerMethodFactory(myMessageHandlerMethodFactory); registrar.setValidator(new MyValidator()); } @Bean public KafkaListenerEndpointRegistry myKafkaListenerEndpointRegistry() { // registry configuration } @Bean public MessageHandlerMethodFactory myMessageHandlerMethodFactory() { DefaultMessageHandlerMethodFactory factory = new DefaultMessageHandlerMethodFactory(); // factory configuration return factory; } @Bean public MyService myService() { return new MyService(); } } } Implementing KafkaListenerConfigurer also allows for fine-grained control over endpoints registration via the KafkaListenerEndpointRegistrar. For example, the following configures an extra endpoint: { @code @Configuration @EnableKafka public class AppConfig implements KafkaListenerConfigurer { @Override public void configureKafkaListeners(KafkaListenerEndpointRegistrar registrar) { SimpleKafkaListenerEndpoint myEndpoint = new SimpleKafkaListenerEndpoint(); // … configure the endpoint registrar.registerEndpoint(endpoint, anotherKafkaListenerContainerFactory()); } @Bean public MyService myService() { return new MyService(); } @Bean public KafkaListenerContainerFactory&lt;?, ?&gt; anotherKafkaListenerContainerFactory() { // ... } // Kafka infrastructure setup } } Note that all beans implementing KafkaListenerConfigurer will be detected and invoked in a similar fashion. The example above can be translated in a regular bean definition registered in the context in case you use the XML configuration.See Also:KafkaListener, KafkaListenerAnnotationBeanPostProcessor, org.springframework.kafka.config.KafkaListenerEndpointRegistrar, org.springframework.kafka.config.KafkaListenerEndpointRegistry spring-kafka-dist.spring-kafka.main flushIf you wish to block the sending thread, to await the result, you can invoke the future’s get() method. You may wish to invoke flush() before waiting or, for convenience, the template has a constructor with an autoFlush parameter which will cause the template to flush() on each send. Note, however that flushing will likely significantly reduce performance. Non Blocking (Async).12345678910public void sendToKafka(final MyOutputData data) &#123;final ProducerRecord&lt;String, String&gt; record = createRecord(data);ListenableFuture&lt;SendResult&lt;Integer, String&gt;&gt; future = template.send(record); future.addCallback(new ListenableFutureCallback&lt;SendResult&lt;Integer, String&gt;&gt;() &#123;@Overridepublic void onSuccess(SendResult&lt;Integer, String&gt; result) &#123; handleSuccess(data);&#125;@Overridepublic void onFailure(Throwable ex) &#123; handleFailure(data, record, ex);&#125;&#125;); &#125; Blocking (Sync).12345678910public void sendToKafka(final MyOutputData data) &#123;final ProducerRecord&lt;String, String&gt; record = createRecord(data);try &#123;template.send(record).get(10, TimeUnit.SECONDS); handleSuccess(data);&#125;catch (ExecutionException e) &#123; handleFailure(data, record, e.getCause()); &#125;catch (TimeoutException | InterruptedException e) &#123; handleFailure(data, record, e);&#125; &#125; KafkaTransactionManagerThe KafkaTransactionManager is an implementation of Spring Framework’s PlatformTransactionManager;]]></content>
  </entry>
  <entry>
    <title><![CDATA[SSL certificates]]></title>
    <url>%2F2019-02-26-TLS-SSL-HTTPS%2F</url>
    <content type="text"><![CDATA[What’s TLSTLS (Transport Layer Security) and its predecessor, SSL (Secure Sockets Layer), are security protocols designed to secure the communication between a server and a client, for example, a web server and a browser. Both protocols are frequently referred to as SSL. A TLS/SSL certificate (simply called SSL certificate) is required to enable SSL/TLS on your site and serve your website using the secure HTTPS protocol. We offer different types of domain-validated SSL certificates signed by globally recognized certificate authorities. CAA Certificate Authority (CA) (or Certification Authority) is an entity that issues digital certificates. The digital certificate certifies the ownership of a public key by the named subject of the certificate. This allows others (relying parties) to rely upon signatures or assertions made by the private key that corresponds to the public key that is certified. Root certificateIn the SSL ecosystem, anyone can generate a signing key and sign a new certificate with that signature. However, that certificate is not considered valid unless it has been directly or indirectly signed by a trusted CA. A trusted certificate authority is an entity that has been entitled to verify that someone is effectively who it declares to be. In order for this model to work, all the participants on the game must agree on a set of CA which they trust. All operating systems and most of web browsers ship with a set of trusted CAs. The SSL ecosystem is based on a * model of trust relationship, also called * “chain of trust” **. When a device validates a certificate, it compares the certificate issuer with the list of trusted CAs. If a match is not found, the client will then check to see if the certificate of the issuing CA was issued by a trusted CA, and so on until the end of the certificate chain. The top of the chain, the root certificate, must be issued by a trusted Certificate Authority. TipsThe root certificate is generally embedded in your connected device. In the case of web browsers, root certificates are packaged with the browser software. To install the Intermediate SSL certificates?The procedure to install the Intermediate SSL certificates depends on the web server and the environment where you install the certificate. For instance, Apache requires you to bundle the intermediate SSL certificates and assign the location of the bundle to the SSLCertificateChainFile configuration. Conversely, NGINX requires you to package the intermediate SSL certificates in a single bundle with the end-user certificate. SSL certificate chainThere are two types of certificate authorities (CAs): root CAs and intermediate CAs. In order for an SSL certificate to be trusted, that certificate must have been issued by a CA that is included in the trusted store of the device that is connecting. In this model of trust relationships, a CA is a trusted third party that is trusted by both the subject (owner) of the certificate and the party relying upon the certificate. In the context of a website, when we use the term digital certificate we often refer to SSL certificates. The CA is the authority responsible for issuing SSL certificates publicly trusted by web browsers. Anyone can issue SSL certificates, but those certificates would not be trusted automatically by web browsers. Certificates such as these are called self-signed. The CA has the responsibility to validate the entity behind an SSL certificate request and, upon successful validation, the ability to issue publicly trusted SSL certificates that will be accepted by web browsers. Essentially, the browser vendors rely on CAs to validate the entity behind a web site. How SSL work in browserThere are 3 essential elements at work in the process described above: a protocol for communications (SSL), credentials for establishing identity (the SSL certificate), and a third party that vouches for the credentials (the certificate authority). Computers use protocols to allow different systems to work together. Web servers and web browsers rely on the Secure Sockets Layer (SSL) protocol to enable encrypted communications. The browser’s request that the server identify itself is a function of the SSL protocol. Credentials for establishing identity are common to our everyday lives: a driver’s license, a passport, a company badge. An SSL certificate is a type of digital certificate that serves as a credential in the online world. Each SSL certificate uniquely identifies a specific domain (such as thawte.com) and a web server. Our trust of a credential depends on our confidence in the organization that issued it. Certificate authorities have a variety of methods to verify information provided by individuals or organizations. Established certificate authorities, such as Thawte, are well known and trusted by browser vendors. Browsers extend that trust to digital certificates that are verified by the certificate authority.PKIYou are correct that SSL uses an asymmetric key pair. One public and one private key is generated which also known as public key infrastructure (PKI). The public key is what is distributed to the world, and is used to encrypt the data. Only the private key can actually decrypt the data though. Say we both go to walmart.com and buy stuff. Each of us get a copy of Walmart’s public key to sign our transaction with. Once the transaction is signed by Walmart’s public key, only Walmart’s private key can decrypt the transaction. If I use my copy of Walmart’s public key, it will not decrypt your transaction. Walmart must keep their private key very private and secure, else anyone who gets it can decrypt transactions to Walmart. This is why the DigiNotar breach was such a big deal A sample of how browser get SSL certificateIf I get an SSL certificate from a well-known provider, what does that prove about my site and how? Here’s what I know: Assume Alice and Bob both have public and private keys If Alice encrypts something with Bob&apos;s public key, she ensures that only Bob can decrypt it (using his private key) If Alice encrypts something with her own private key, anyone can decrypt it (using her public key), but they will know that it was encrypted by her Therefore, if Alice encrypts a message first with her own private key, then with Bob&apos;s public key, she will ensure that only Bob can decrypt it and that Bob will know the message is from her.Regarding certificates, here’s what I think happens (updated): I generate a request for a certificate. In that request, I put my public key and a bunch of information about myself. The certificate issuer (in theory) checks me out to make sure it knows who I am: talks to me in person, sees my driver&apos;s license, retina scan, or whatever. If they&apos;re satisfied, the certificate issuer then encrypts my request with their private key. Anyone who decrypts it with their public key knows that they vouch for the information it contains: they agree that the public key is mine and that the information stated is true about me. This encrypted endorsement is the certificate that they issue to me. When you connect to my site via https, I send you the certificate. Your browser already knows the issuer&apos;s public key because your browser came installed with that information. Your browser uses the issuer&apos;s public key to decrypt what I sent you. The fact that the issuer&apos;s public key works to decrypt it proves that the issuer&apos;s private key was used to encrypt it, and therefore, that the issuer really did create this certificate. Inside the decrypted information is my public key, which you now know has been vouched for. You use that to encrypt some data to send to me.Your key theory: basically right, but authentication is usually done by encrypting a cryptographically secure hash of the data rather than the data itself. A CA’s signature on an SSL certificate should indicate that the CA has done a certain amount of diligence to ensure that the credentials on the certificate match the owner. That diligence varies, but the ultimate point is that they’re saying that the certificate they signed belongs to the entity named on it. See http://en.wikipedia.org/wiki/Digital_signature#Definition A public key certificate is the signed combination between a public key, identifiers, and possibly other attributes. Those who sign this document effectively assert the authenticity of the binding between the public key and the identifier and these attributes, in the same way as a passport issuing authority asserts the binding between the picture and the name in a passport, as various other pieces of information (nationality, date of birth, …). The private key is used for signing and deciphering/decrypting. The public key is used for verifying signatures and enciphering/encrypting.public key cryptography: A class of cryptographic techniques employing two-key ciphers. Messages encrypted with the public key can only be decrypted with the associated private key. Conversely, messages signed with the private key can be verified with the public key. It should be pointed out, along with all the other answers, that your private key is not always just one key that is used for both decrypting and signing messages. These should be two separate keys. This would create 4 keys for each person: Public Encryption Key - Used to encrypt data to send to me. Private Decryption Key - Used to decrypt messages that were encrypted using my Public Encryption Key. Private Signing Key - Used to sign messages that I send to other people. Public Verify Key - Used to verify that a message was, in fact, signed by me. https://en.wikipedia.org/wiki/Savvis Savvis - Wikipedia Savvis, formerly SVVS on Nasdaq and formerly known as Savvis Communications Corporation, and, later, Savvis Inc., is a subsidiary of CenturyLink, a company headquartered in Monroe, Louisiana.[1] The company sells managed hosting and colocation services with more than 50 data centers[2] (over 2 million square feet) in North America, Europe, and Asia, automated management and provisioning systems, and information technology consulting. Savvis has approximately 2,500 unique business and government customers.[3][4] The file extensions .CRT and .CER are interchangeable. If your server requires that you use the .CER file extension, you can change the extension by following the steps below: Double-click on the yourwebsite.crt file to open it into the certificate display. Select the Details tab, then select the Copy to file button. Hit Next on the Certificate Wizard. Select Base-64 encoded X.509 (.CER), then Next. Select Browse (to locate a destination) and type in the filename yourwebsite. Hit Save. You now have the file yourwebsite.cer File extensions for cryptographic certificates aren&apos;t really as standardized as you&apos;d expect. Windows by default treats double-clicking a .crt file as a request to import the certificate into the Windows Root Certificate store, but treats a .cer file as a request just to view the certificate. So, they&apos;re different in that sense, at least, that Windows has some inherent different meaning for what happens when you double click each type of file.But the way that Windows handles them when you double-click them is about the only difference between the two. Both extensions just represent that it contains a public certificate. You can rename a file or use one in place of the other in any system or configuration file that I’ve seen. And on non-Windows platforms (and even on Windows), people aren’t particularly careful about which extension they use, and treat them both interchangeably, as there’s no difference between them as long as the contents of the file are correct. *.pem, *.crt, *.ca-bundle, *.cer, *.p7b, *.p7s files contain one or more X.509 digital certificate files that use base64 (ASCII) encoding. .DER = The DER extension is used for binary DER encoded certificates. These files may also bear the CER or the CRT extension. Proper English usage would be “I have a DER encoded certificate” not “I have a DER certificate”. .PEM = The PEM extension is used for different types of X.509v3 files which contain ASCII (Base64) armored data prefixed with a “—– BEGIN …” line. .CRT = The CRT extension is used for certificates. The certificates may be encoded as binary DER or as ASCII PEM. The CER and CRT extensions are nearly synonymous. Most common among *nix systems CER = alternate form of .crt (Microsoft Convention) You can use MS to convert .crt to .cer (.both DER encoded .cer, or base64[PEM] encoded .cer) The .cer file extension is also recognized by IE as a command to run a MS cryptoAPI command (specifically rundll32.exe cryptext.dll,CryptExtOpenCER) which displays a dialogue for importing and/or viewing certificate contents. .KEY = The KEY extension is used both for public and private PKCS#8 keys. The keys may be encoded as binary DER or as ASCII PEM. The only time CRT and CER can safely be interchanged is when the encoding type can be identical. (ie PEM encoded CRT = PEM encoded CER) What is the SSL Certificate Chain? There are two types of certificate authorities (CAs): root CAs and intermediate CAs. In order for an SSL certificate to be trusted, that certificate must have been issued by a CA that is included in the trusted store of the device that is connecting. Good. I see you want to access this particular page. I need to send the page to you in a secure way. If Iencrypt it using my public key, you won’t be able to decrypt it because you don’t have my private key. And since you don’t have any public key of your own that I can use to encrypt the page for you here’s what I proposeSince you can send me encrypted messages that only me can read (you have my public key), send me an encrypted message with an encryption key in it. Just make up a random encryption key that we’ll both use to encrypt and decrypt the messages between us during this session . A simple symmetric key is enought. We’ll use the same key to encrypt and decrypt the messages. So there’s no way that anybody with your publickey can trick others to believe that he is you ? Nope. That’s the beauty of the assymetric encryption. When you send the public key to the victim’s contain your public key + a certificate that this public key belongs to you. If you are a website, then the certificate will contain the domain name of the website. Basically, a certificate says something like: the following public key “XYZ123” belongs to example.com. that’s why we have “Certificate Authorities” like Verisign, Digicert or even Symantec. It is believed that these companies have the necessary trustworthiness to deliver certificates to different •entities.Think of a CA like a registrar for public keys. Just like registrars assert that a domain name belongs to a certain person or company, CAS assert that a public key belongs to a certain domain name (or IP address) . The certificate will contain the CA that delivered it, but you don’t even have to check with them because the certificate is signed by them. That signature alone is enough proof that the certificate comes from them. A signature is simply a small message that is encrypted with their private key. Since private keys are asymetric, that means that only the associated public key can decrypt it. Asymmetric encryption works in both way. public -&gt; private and private -&gt; public.What the public key encrypts only the private key can decrypt, and what the private key can encrypt only the public key can decrypt. for PKI, we’re not looking for secrecy here, we only want to prove that we’ re the real authors of the message. Suppose I send you the message “HELLO WORLD”, encrypted with my private key. The encrypted message would be, for example, “XYZ1234”. So you receive “XYZ1234” . If I give you my public key, you would be able to decrypt “XYZ1234” into “HELLO WORLD” . And by doing so, you would have proof that that message was sent by me, because the public key you used decrypts messages that were encrypted by my private key only. And since I am the only person in the universe who has that private key, that proves that I am the author of that message. Really nice. So I don’t have to contact the CA to check the validity of the certificate, all I have to do is use their public key to decrypt the signature that’s in it. If it’s the same as err, wait, what should I compare the decrypted signature to again ? You have to find the same hash as the one you have calculated. They are sending a small hash of the whole certificate. So what you have to do is to calculate the hash of the certificate yourself, then compare it to the hash you get when you decrypt the signature. If the two are the same that means two things The CA’s public key worked, so the signature was encrypted by the associated private key, which means the certificate was really issued by the CA. Since the hash is the same, it also means that you are seeing the exact same certificate that the CA delivered to the website you are visiting. The information contained inside hasn’t been tampered with. That’s really good. So, let me recap one more time . I contact you for an HTTPS page. You send me an SSL certificate that contains your public key and a signature from the CA that delivered I make sure the certificate is valid by using the CA’s public key to decrypt the signature. In parallel, I also calculate the hash of the certificate. If my hash and the one I got from decrypting the signature are equal, that means that the certificate was really issued by the CA and that I can be sure that the public key you sent me is really yours. Because you implicitly trust the CA. Let’s continue: I generate a random key that we’ll both use as a symmetric key to encrypt and decrypt the messages we’ll be sending each other. I encrypt this symmetric key with your public key and send it to you. You decrypt my message with your private key and find my secret key. Every request or response between us will be encrypted with this shared secret symmetric key. CNThe Common Name (AKA CN) represents the server name protected by the SSL certificate. The certificate is valid only if the request hostname matches the certificate common name. To check the status, such as 1sudo openssl x509 -noout -in xxx.com.cer -text Subject: C=UK, ST=London, L=London, O=AAA Bank, OU=Product and Markets, CN=*.xxxtest.com Subject Public Key Info: commonName formatThe common name is not a URL. It doesn’t include any protocol (e.g. http:// or https://), port number, or pathname. For instance, https://example.com or example.com/path are incorrect. In both cases, the common name should be example.com Common Name vs Subject Alternative NameThe common name can only contain up to one entry: either a wildcard or non-wildcard name. It’s not possible to specify a list of names covered by an SSL certificate in the common name field. The Subject Alternative Name extension (also called Subject Alternate Name or SAN) was introduced to solve this limitation. The SAN allows issuance of multi-name SSL certificates. SHA-2 SSL CertificatesAlmost all certificates are currently signed with the SHA-2 hash algorithm. This article provides a simple overview of the SHA-1 to SHA-2 transition plans, as well additional informations on the SHA-2 hash algorithm and SSL certificates purchased with DNSimple previous than September 2014. The SHA family of hashing algorithms were developed by the National Institute of Standards and Technology (NIST) and are used by certificate authorities (CAs) when digitally signing issued certificates. Reference https://support.dnsimple.com/articles/what-is-ssl-certificate-chain/ https://www.thawte.com/resources/getting-started/how-ssl-works/]]></content>
  </entry>
  <entry>
    <title><![CDATA[Terraform]]></title>
    <url>%2F2019-07-26-Terraform%2F</url>
    <content type="text"><![CDATA[Why TerraformSoftware isn’t done when the code is working on your computer. It’s not done when the tests pass. And it’s not done when someone gives you a “ship it” on a code review. Software isn’t done until you deliver it to the user.]]></content>
      <tags>
        <tag>aws</tag>
        <tag>clouds</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux Tips]]></title>
    <url>%2F2016-06-10-Linux-Tips%2F</url>
    <content type="text"><![CDATA[Get permission denied error when sudo su (or hyphen in sudo command)bash: /home/YOURNAME/.bashrc: Permission deniedThat’s because you didn’t add “-“ hyphen in your sudo command. The difference between “-“ and “no hyphen” is that the latter keeps your existing environment (variables, etc); the former creates a new environment (with the settings of the actual user, not your own). The hyphen has two effects: 1) switches from the current directory to the home directory of the new user (e.g., to /root in the case of the root user) by logging in as that user 2) changes the environmental variables to those of the new user as dictated by their ~/.bashrc. That is, if the first argument to su is a hyphen, the current directory and environment will be changed to what would be expected if the new user had actually logged on to a new session (rather than just taking over an existing session). To delete lines in files contain pattern1sed -i '/.*167\=OPT.*/d' testdata.txt to select only only one element value of XML file :grep -oPm1 “(?&lt;=)[^&lt;]+” To check Linux release name1cat /etc/os-release How to check whether your linux is 32bit or 64 bitTo run “arch” command, this is similar to “uname -m” , it prints to the screen whether your system is running 32-bit (“i686”) or 64-bit (“x86_64”). convert line ending to unix (sometimes git submit is dos format)1dos2unix the_script_file_name To check redhat Linux version1cat /etc/redhat-release To list all users in linux1cat /etc/passwd Show IP address in Linux123$ifconfigeth0 Link encap:Ethernet HWaddr 00:50:56:9B:19:81 inet addr:133.14.16.5 Bcast:133.14.16.255 Mask:255.255.255.0 Check system resource1execute `cat /proc/cpuinfo` and `free -m` to gain information about the server’s CPU and memory. chmod commandFrom one to four octal digitsAny omitted digits are assumed to be leading zeros. The first digit = selects attributes for the set user ID (4) and set group ID (2) and save text image (1)SThe second digit = permissions for the user who owns the file: read (4), write (2), and execute (1)The third digit = permissions for other users in the file’s group: read (4), write (2), and execute (1)The fourth digit = permissions for other users NOT in the file’s group: read (4), write (2), and execute (1) The octal (0-7) value is calculated by adding up the values for each digitUser (rwx) = 4+2+1 = 7Group(rx) = 4+1 = 5World (rx) = 4+1 = 5chmode mode = 0755 Examples 1234567chmod 400 file - Read by ownerchmod 040 file - Read by groupchmod 004 file - Read by world chmod 200 file - Write by ownerchmod 020 file - Write by groupchmod 002 file - Write by world top enter u, then user id to show only user process Z: global color scheme, e.g. red or green B: global bold for all z: show color, then b to hightlight, and x highlight sort fidl, y highlight running tasks #3: show only 3 threads c: show command line F: sort, e.g. Fk sort by CPU% R: reverse order Sample config files.vimrc123456set numberset incsearchset hlsearchsyntax oncolorscheme desert ==== screenrc =https://gist.githubusercontent.com/ChrisWills/1337178/raw/8275b66c3ea86a562cdaa16f1cc6d9931d521e1b/.screenrc-main-example 123456789101112131415161718192021222324252627282930313233343536# GNU Screen - main configuration file # All other .screenrc files will source this file to inherit settings.# Author: Christian Wills - cwills.sys@gmail.com# Allow bold colors - necessary for some reasonattrcolor b ".I"# Tell screen how to set colors. AB = background, AF=foregroundtermcapinfo xterm 'Co#256:AB=\E[48;5;%dm:AF=\E[38;5;%dm'# Enables use of shift-PgUp and shift-PgDntermcapinfo xterm|xterms|xs|rxvt ti@:te@# Erase background with current bg colordefbce "on"# Enable 256 color termterm xterm-256color# Cache 30000 lines for scroll backdefscrollback 30000# New mail notification# backtick 101 30 15 $HOME/bin/mailstatus.shhardstatus alwayslastline # Very nice tabbed colored hardstatus linehardstatus string '%&#123;= Kd&#125; %&#123;= Kd&#125;%-w%&#123;= Kr&#125;[%&#123;= KW&#125;%n %t%&#123;= Kr&#125;]%&#123;= Kd&#125;%+w %-= %&#123;KG&#125; %H%&#123;KW&#125;|%&#123;KY&#125;%101`%&#123;KW&#125;|%D %M %d %Y%&#123;= Kc&#125; %C%A%&#123;-&#125;'# change command character from ctrl-a to ctrl-b (emacs users may want this)#escape ^Bb# Hide hardstatus: ctrl-a f bind f eval "hardstatus ignore"# Show hardstatus: ctrl-a Fbind F eval "hardstatus alwayslastline" ====================.bashrc ==========123456789101112131415161718192021222324252627282930313233343536373839# .bashrc# Source global definitionsif [ -f /etc/bashrc ]; then . /etc/bashrcfi# source ./promptexport PS1=''export PS1='[\e[104mLight blue \u \A\]$ 'export PS1="\[\e[32m\]\u@\h \d \t \w \[\e[m\] \\$"\e[104mLight blue# Welcome messageecho -ne "Good Morning ! It's "; date '+%A, %B %-d %Y'echo -e "And now your moment of Zen:"; fortuneechoecho "Hardware Information:"sensors # Needs: 'sudo apt-get install lm-sensors'uptime # Needs: 'sudo apt-get install lsscsi'free -m# User specific aliases and functionsPS1='\[`[ $? = 0 ] &amp;&amp; X=2 || X=1; tput setaf $X`\]\h\[`tput sgr0`\]:$PWD\n\$ '============vimrc =========set numberset incsearchset hlsearch~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~grep -v "unwanted_word" file | grep XXXXXXXX// find command exclude “permission denied”$ find . -name "java" 2&gt;/dev/null Passwordless connection in putty1231. Generate Public &amp; private key pair by keygen2. Log into Linux, nano .ssh/authorized_keys and paste the public key3. Save the private key in putty and load it in Putty session find java files older than 3 days1find . -name "*.java" -atime -3d Remove capitalization1sed -ie 's/Return /return /g' ReverseString.java replace string in files123#grep -r "pack.*me" .sed -ie 's/package.*me.*;/package com.todzhang;/g' *.javased -ie 's/package me.todzhang;/package com.todzhang;/g' ~/dev/git/algo/algoWS/src/main/java/com/todzhang/*.java create directory hierarchy via path1mkdir -p ~/abc/def/egg -p means create intermediary folders, if not exist. Those intermediary folders with permission 777 lsof to locate whether/who allocated port 8080lsof means list open files. 1lsof -n -P -i | grep 8080 To get rid of ‘’Sometimes got “403 Forbidden” error when trying to downalod file via wget, e.g. 123456$ wget http://www.xmind.net/xmind/downloads/xmind-7.5-update1-macosx.dmg--2016-09-09 23:27:29-- http://www.xmind.net/xmind/downloads/xmind-7.5-update1-macosx.dmgResolving www.xmind.net... 23.23.188.223Connecting to www.xmind.net|23.23.188.223|:80... connected.HTTP request sent, awaiting response... 403 Forbidden2016-09-09 23:27:29 ERROR 403: Forbidden. To solve this problem, using following syntax, adding -U xx 1wget -U 'Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.8.1.6) Gecko/20070802 SeaMonkey/1.1.4' http://www.xmind.net/xmind/downloads/xmind-7.5-update1-macosx.dmg case insensitive ls command in bashUpdate .bashrc or current active window 1shopt -s nocaseglob one line command to download and extract files1$cd /tmp;curl https://www.kernel.org/pub/linux/utils/util-linux/v2.24/util-linux-2.24.tar.gz | tar -zxf-;cd util-linux-2.24; Redirect request for HTTP 3xxx code1curl -LSso ~/.vim/autoload/pathogen.vim https://tpo.pe/pathogen.vim -L means redirect upon HTTP code 3xxx-Ss work together to make the curl show errors if there are-o means output to a specified file, rather than stdout search files contains keyword1grep -ri 'architect' . | awk -F ':' '&#123;print $1&#125;' Show Linux kernel and namelsb_releaselsb means Linux Standard Base , -a means print all information 1lsb_release -a -u will output 123456phray@phray-VirtualBox ~ $ lsb_release -a -uNo LSB modules are available.Distributor ID: UbuntuDescription: Ubuntu 14.04 LTSRelease: 14.04Codename: trusty /etc/os-release1cat /etc/os-release will output Following 12345678910phray@phray-VirtualBox ~ $ cat /etc/os-releaseNAME="Ubuntu"VERSION="14.04.2 LTS, Trusty Tahr"ID=ubuntuID_LIKE=debianPRETTY_NAME="Ubuntu 14.04.2 LTS"VERSION_ID="14.04"HOME_URL="http://www.ubuntu.com/"SUPPORT_URL="http://help.ubuntu.com/"BUG_REPORT_URL="http://bugs.launchpad.net/ubuntu/" Following is the command found in docker.sh 1lsb_dist=$(lsb_release -a -u 2&gt;&amp;1 | tr '[:upper:]' '[:lower:]' | grep -E 'id' | cut -d ':' -f 2 | tr -d '[[:space:]]') show the 2nd column 1lsb_release --Codename | cut -f2 gpsswdDESCRIPTIONgpasswd is used to administer the /etc/group file (and /etc/gshadow file if compiled with SHADOWGRP defined). Every group can have administrators, members and a password. System administrator can use -A option to define group administrator(s) and -M option to define members and has all rights of group administrators and members. Notes about group passwordsGroup passwords are an inherent security problem since more than one person is permitted to know the password. However, groups are a useful tool for permitting co-operation between different users. OPTIONSGroup administrator can add and delete users using -a and -d options respectively. Administrators can use -r option to remove group password. When no password is set only group members can use newgrp to join the group. Option -R disables access via a password to the group through newgrp command (however members will still be able to switch to this group). gpasswd called by a group administrator with group name only prompts for the group password. If password is set the members can still newgrp(1) without a password, non-members must supply the password. FILESTag Description/etc/group Group account information./etc/gshadow Secure group account information. 1sudo gpasswd -a USER docker Compare files difference in two folders1diff -rq ~/dev/pa ~/dev/hexo/myblog/source/_posts This used option -r (recursive) and -q quite, means only show differences To execut shell/unix command within vim1:~ls -lt To open find result with sublime12find . -name "*Linux*.md" | xargs sublime find . -name "*Linux*.md" | xargs sublime ~ # open in new Sublime window To vim/vim edit directly on file output by find command1find . -name "*tmux*" -exec vim &#123;&#125; \; Be advised you may experience following error message find: missing argument to `-exec’ actually you should add a slash in front of semi colon quite mode in apt-get apt-get will in verbose mode apt-get -q will be in less verbose , a.k.a quite mode apt-get -qq in extreme least verbose mode]]></content>
      <tags>
        <tag>DevOps</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[KDB]]></title>
    <url>%2F2018-06-12-KDB%2F</url>
    <content type="text"><![CDATA[KDBHowever kdb+ evaluates expressions right-to-left. There are no precedence rules. The reason commonly given for this behaviour is that it is a much simpler to understand system, many q-gods would agree, beginners however may not. The kdb+ built-in commands are mostly a single letter. If a system command that is not built-in is entered, that command will be passed to the underlying operating system. Created with financial institutions in mind, the database was developed as a central repository to store time series data that supports real-time analysis of billions of records. Columnar databases return answers to some queries in a more efficient way than row-based database management systems. kdb+ dictionaries, tables and nanosecond time stamps are native data types and are used to store time series data. In 1998, Kx Systems released kdb, a database built on the language K written by Arthur Whitney. In 2003, kdb+ was released as a 64-bit version of kdb. kdb/q/kdb+ is both a database (kdb) and a vector language (q). It’s used by almost every major financial institution Kdb+ is an in-memory column-oriented database based on the concept of ordered lists. In-memory means it primarily stores its data in RAM. This makes it extremely fast with a much simplified database engine but it requires a lot of RAM (Which no longer poses a problem as servers with massive amounts of RAM are now inexpensive). Column oriented database means that each column of data is stored sequentially in memory Column DBThe main reason why indexes dramatically improve performance on large datasets is that database indexes on one or more columns are typically sorted by value, which makes range queries operations (like the above “find all records with salaries between 40,000 and 50,000” example) very fast (lower time-complexity). Kdb+ the Database - Column Oriented DB allowing fast timeseries analysis Column-oriented systemsA column-oriented database serializes all of the values of a column together, then the values of the next column, and so on. For our example table, the data would be stored in this fashion:10:001,12:002,11:003,22:004;Smith:001,Jones:002,Johnson:003,Jones:004;Joe:001,Mary:002,Cathy:003,Bob:004;40000:001,50000:002,44000:003,55000:004;In this layout, any one of the columns more closely matches the structure of an index in a row-based system. This may cause confusion that can lead to the mistaken belief a column-oriented store “is really just” a row-store with an index on every column. However, it is the mapping of the data that differs dramatically. In a row-oriented indexed system, the primary key is the rowid that is mapped from indexed data. In the column-oriented system, the primary key is the data, which is mapped from rowids.[2] This may seem subtle, but the difference can be seen in this common modification to the same store: …;Smith:001;Jones:002,004;Johnson:003;… Whether or not a column-oriented system will be more efficient in operation depends heavily on the workload being automated. Operations that retrieve all the data for a given object (the entire row) are slower. A row-based system can retrieve the row in a single disk read, whereas numerous disk operations to collect data from multiple columns are required from a columnar database. However, these whole-row operations are generally rare. In the majority of cases, only a limited subset of data is retrieved. In a rolodex application, for instance, collecting the first and last names from many rows to build a list of contacts is far more common than reading all data for any single address. This is even more true for writing data into the database, especially if the data tends to be “sparse” with many optional columns. For this reason, column stores have demonstrated excellent real-world performance in spite of many theoretical disadvantages. Why are most databases row-oriented?Imagine we want to add one row somewhere in the middle of our data for 2011-02-26, on the row oriented database no problem, column oriented we will have to move almost all the data! Lucky since we mostly deal with time series new data only appends to the end of our table. Difference vs row based DB a subtle point is that unlike most standard SQL which is based on set theory, kdb+ is based on vectors of ordered lists. Where standard SQL has struggled with queries like find the top 3 stocks by price, find the bottom 3 by market cap because it has no concept of order, kdb’s ordering significantly simplifies many queries. This ordered concept allows kdb+ to provide unique timeseries joins that would be be extremely difficult in other variations of SQL and require the use of slow cursors. Language Qthe primary design objectives of q are expressiveness, speed and efficiency. In these, it is beyond compare. The design trade-off is a terseness that can be disconcerting to programmers coming from verbose traditional database programming environments – e.g., C++, Java, C# or Python – and a relational DBMS Q evolved from APL (A Programming Language), which was first invented as a mathematical notation by Kenneth Iverson at Harvard University in the 1950s. APL was introduced in the 1960s by IBM as a vector programming language, meaning that it processes a list of numbers in a single operation. It was successful in finance and other industries that required heavy number crunching. q is Kx’s proprietary language. It’s a powerful, concise and elegant array language, which means that a production system could just be a single page of code, not pages and pages of code and a nightmare to maintain. Clearly there is an initial investment in learning it, but the power it gives you to manipulate streaming, real-time and historical data makes that initial investment really worthwhile. q language - fast, interpreted vector based language Q is a interpreted vector based dynamically typed language built for speed and expressiveness. Since q is interpreted you can enter commands straight into the console there is no waiting for compilation, feedback is instantaneous. Key features of QInterpreted Q is interpreted, not compiled. During execution, data and functions live in an in-memory workspace. Iterations of the development cycle tend to be quick because all run-time information needed to test and debug is immediately available in the workspace. Q programs are stored and executed as simple text files called scripts. The interpreter’s eval and parse routines are exposed so that you can dynamically generate code in a controlled manner. Types Q is a dynamically typed language, in which type checking is mostly unobtrusive. Each variable has the type of its currently assigned value and type promotion is automatic for most numeric operations. Types are checked on operations to homogenous lists. Evaluation Order While q is entered left-to-right, expressions are evaluated right-to-left or, as the q gods prefer, left of right – meaning that a function is applied to the argument on its right. There is no operator precedence and function application can be written without brackets. Punctuation noise is significantly reduced. Null and Infinity Values In classical SQL, the value NULL represents missing data for a field of any type and takes no storage space. In q, null values are typed and take the same space as non-nulls. Numeric types also have infinity values. Infinite and null values can participate in arithmetic and other operations with (mostly) predictable results. Integrated I/O I/O is done through function handles that act as windows to the outside world. Once such a handle is initialized, passing a value to the handle is a write. Table Oriented Give up objects, ye who enter here. In contrast to traditional languages, you’ll find no classes, objects, inheritance and virtual methods in q. Instead, q has tables as first class entities. The lack of objects is not as severe as might first appear. Objects are essentially glorified records (i.e., entities with named fields), which are modeled by q dictionaries. A table can be viewed as a list of record dictionaries. Ordered Lists Because classical SQL is the algebra of sets – which are unordered with no duplicates – row order and column order are not defined, making time series processing cumbersome and slow. In q, data structures are based on ordered lists, so time series maintain the order in which they are created. Moreover, simple lists occupy contiguous storage, so processing big data is fast. Very fast. Column Oriented SQL tables are organized as rows distributed across storage and operations apply to fields within a row. Q tables are column lists in contiguous storage and operations apply on entire columns. In-Memory Database One can think of kdb+ as an in-memory database with persistent backing. Since data manipulation is performed with q, there is no separate stored procedure language. In fact, kdb+ comprises serialized q column lists written to the file system and then mapped into memory. In q, data structures are based on ordered lists, so time series maintain the order in which they are created. Moreover, simple lists occupy contiguous storage, so processing big data is fast. Very fast. Q tables are column lists in contiguous storage and operations apply on entire columns. Sample of Q code12345678910111213q)l:10 12 14 16 18 22 32 45q)sum l169q)avg l21.125q)l*10100 120 140 160 180 220 320 450q)k:til 8q)k0 1 2 3 4 5 6 7q)l+k10 13 16 19 22 27 38 52 Notice in the example code above the absence of loops, no for/while/do yet we could easily express adding one array to another. This is because the vector/list is the primary unit of data in kdb+. Operations are intended to be performed and expressed as being on an entire set of data. Dictionaries can be defined using lists, they provide a hashmap datastructure for quick lookups. Tables are constructed from dictionaries and lists. This brevity of data structures is actually one of the attributes that gives q its ability to express concisely what would take many lines in other languages. Some of the practical applications of being able to combine both streaming and real-time data together with historical, all in the same database?The most important thing is the simplicity, which translates into speed and ease of doing analysis. For example, it allows you to do complicated, time-critical analysis, such as pre-trade risk. This means that you are likely to see interesting trading opportunities before those who are using the same off-the-shelf solution as everybody else. So you’re there first and you’re there so early, you can afford to do comprehensive pre-trade risk analysis, and you’re able to look for patterns you’ve seen in the past. In addition to capturing market data, firms are using kdb+ for order-book management, algorithmic trading, and risk assessment. “They are using kdb+/q for queries being performed on both streaming or historical data — the latter easily accommodating research and back-testing,” DeltaFlow, a platform from First Derivatives that’s based on kdb+, is used by traders for high volume, low-latency algorithmic trading and by regulators for real-time detection of market abuse and unauthorized trading activity across multiple asset classes. Combined power of kdb+/qWhat’s beautiful about kdb+ is that since tables are columns of vectors, all the power of the q language can be used as easily on table data as it was on lists. Where we had sum[l],avg[l],weightedAvg[l1;l2] of lists we can write similar qSQL:select avg price, sum volume, weightedAvg[time;price] from tradeWant to apply a function to a timeseries, simply place it inline:select {a:avg x; sqrt avg (xx)-aa} price from trade Q commadnsTo obtain official console display of any q value, apply the built-in function show to it. q)show a:42 commentsAt least one whitespace character must separate / intended to begin a comment from any text to the left of it on a line. booleanBoolean values in q are stored in a single byte and are denoted as the binary values they really are with an explicit type suffix b. One way to generate boolean values is to test for equality. q)42=40+21b dateOne interesting and useful feature of q temporal values is that, as integral values under the covers, they naturally participate in arithmetic. For example, to advance a date five days, add 5. q)2000.01.01+5_ Or to advance a time by one microsecond (i.e., 1000 nanoseconds) add 1000. q)12:00:00.000000000+1000_ Or to verify that temporal values are indeed their underlying values, test for equality. q)2000.01.01=0 SymbolsSymbols are denoted by a leading back-quote (called “back tick” in q-speak) followed by characters. Symbols without embedded blanks or other special characters can be entered literally into the console.q)`aapl_ Since symbols are atoms, any two can be tested for equality. q)aapl=apl_ listThe fundamental q data structure is a list, which is an ordered collection of items sequenced from left to right. The notation for a general list encloses items with ( and ) and uses ; as separator. Spaces after the semi-colons are optional but can improve readability. q)(1; 1.2; `one)_ In the case of a homogenous list of atoms, called a simple list, q adopts a simplified format for both storage and display. The parentheses and semicolons are dropped. For example, a list of underlying numeric type separates its items with a space. q)(1; 2; 3)1 2 3 A simple list of booleans is juxtaposed with no spaces and has a trailing b type indicator. q)(1b; 0b; 1b)101bA simple list of symbols is displayed with no separating spaces. q)(one;two; three)onetwothree basic operations to construct and manipulate lists. The most fundamental is til, which takes a non-negative integer n and returns the first n integers starting at 0 (n itself is not included in the result). q)til 100 1 2 3 4 5 6 7 8 9 til list tipsBe mindful that q always evaluates expressions from right to left and that operations work on vectors whenever possible. q)1+til 101 2 3 4 5 6 7 8 9 10 Similarly, we obtain the first 10 even numbers and the first ten odd numbers. q)2til 10_ q)1+2til 10_ Finally, we obtain the first 10 even numbers starting at 42. q)42+2*til 10_ Another frequently used list primitive is join , that returns the list obtained by concatenating its right operand to its left operand. q)1 2 3,4 51 2 3 4 5 extractTo extract items from the front or back of a list, use the take operator #. Positive argument means take from the front, negative from the back. q)2#til 100 1q)-2#til 10 Applying # always results in a list. In particular, the idiom 0# returns an empty list of the same type as the first item in its argument. Using an atom argument is a succinct way to create a typed empty list of the type of the atom. q)0#1 2 3`long$() Should you extract more items than there are in the list, # restarts at the beginning and continues extracting. It does this until the specified number of items is reached. q)5#1 2 31 2 3 1 2 As with atoms, a list can be assigned to a variable. q)L:10 20 30The items of a list can be accessed via indexing, which uses square brackets and is relative to 0. q)L[0]10 FunctionConceptually, a q function is a sequence of steps that produces an output result from an input value. Since q is not purely functional, these rules can interact with the world by reaching outside the context of the function. Such actions are called side effects and should be carefully controlled. Function definition is delimited by matching curly braces { and }. Immediately after the opening brace, the formal parameters are names enclosed in square brackets [ and ] and separated by semi-colons. These parameters presumably appear in the body of the function, which follows the formal parameters and is a succession of expressions sequenced by semi-colons. Following is a simple function that returns the square of its input. On the next line we assign the same function to the variable sq. The whitespace is optional. q){[x] xx}_ q)sq:{[x] xx}_ Here is a function that takes two input values and returns the sum of their squares. q){[x;y] a:xx; b:yy; a+b}_ q)pyth:{[x;y] a:xx; b:yy; a+b}_ Here are the previous functions applied to arguments. q){[x] xx}[5]25q)sq[5]_ q){[x;y] a:xx; b:y*y; a+b}[3;4]25q)pyth[3;4]_ monadic functionIn q, as in most functional languages, we don’t need no stinkin’ brackets for application of a monadic function – i.e., with one parameter. Simply separate the function from its argument by whitespace. This is called function juxtaposition. q){xx} 5_ q)f:{xx}q)f 5_ x,y,zIt is common in mathematics to use function parameters x, y, or z. If you are content with these names (in the belief that descriptive names provide no useful information to the poor soul reading your code), you can omit their declaration and q will understand that you mean the implicit parameters x, y, and z in that order. q){xx}[5]25q){a:xx; b:y*y; a+b}[3;4]25 verbshigher order functions, or as they are called in q, adverbs. In words, we tell q to start with the initial value of 0 in the accumulator and then modify + with the adverb / so that it adds across the list. q)0 +/ 1 2 3 4 515 In this situation we don’t really need the flexibility to specify the initial value of the accumulator. It suffices to start with the first item of the list and proceed across the rest of the list. There is an even simpler form for this case. q)(+/) 1 2 3 4 5 for loopIf you are new to functional programming, you may think, “Big deal, I write for loops in my sleep.” More importantly, you can focus on what you want done without the irrelevant scaffolding of how to set up control structures. This is called declarative programming. What else can we do with our newfound adverb? Change addition to multiplication for factorial. q)(*/) 1+til 103628800 larger vs smallerThe fun isn’t limited to arithmetic primitives. We introduce |, which returns the larger of its operands and &amp;, which returns the smaller of its operands. q)42|9898q)42&amp;98 Use | or &amp; with over and you have maximum or minimum. q)(|/) 20 10 40 3040q)(&amp;/) 20 10 40 30 command ‘over’ adverbSome applications of / are so common that they have their own names. q)sum 1+til 1055q)prd 1+til 10 “o”_ q)max 20 10 40 30_ q)min 20 10 40 30 At this point the / pattern should be clear: it takes a given function and produces a new function that accumulates across the original list, producing a single result. In particular, / converts a dyadic function to a monadic aggregate function – i.e., one that collapses a list to an atom. data typelong¶ In q versions 3.0 and later, the basic integer type is a signed eight-byte integer, called long. A literal is identified as a long by the fact that it contains only numeric digits, with an optional leading minus sign, and no decimal point. It may also have an optional trailing type indicator j indicating it is a long and not another integer type. Here is a typical long integer value. q)4242Observe that the type indicator j is accepted but redundant. q)42j42 The short type represents a two-byte signed integer and requires the trailing type indicator h. For example, q)-123h_ Similarly, the int type represents a four-byte signed integer and requires the trailing type indicator i. The float type represents an IEEE standard eight-byte floating-point number, often called “double” in traditional languages. A float can hold (at least) 15 decimal digits of precision. It is denoted by optionally signed numeric digits with either a decimal point or an optional trailing type indicator f. Observe that the console shortens the display of floats with no significant digits to the right of the decimal. You can change this by using the \P command (note upper case) to specify a display width up to 16 digits. If you issue \P 0 the console will display all 17 decimal digits of the underlying binary representation, although the last digit is unreliable. boolean¶ The boolean type uses one byte to store a bit and is denoted by the bit value with the trailing type indicator b. There are no keywords for ‘true’ or ‘false’, nor are there separate logical operators for booleans. q)0b_ q)1b Text Data¶ There are two atomic text types in q. They are more akin to the SQL types CHAR and VARCHAR than the character types of traditional languages. 2.4.1 char¶ A char holds an individual ASCII or 8-bit Unicode character that is stored in one byte. It corresponds to a SQL CHAR. It is denoted by a single character enclosed in double quotes. q)”q” Some keyboard characters – e.g., the double-quote – cannot be entered directly into a char since they have special meaning in the q console. As in C, special characters are escaped with a preceding back-slash . The console display somewhat confusingly displays the escape, but the following are all actually single characters. q)”&quot;“ “&quot;“ q)”\“ _ q)”\n”_ q)”\r”_ q)”\t”_ Also as in C, you can escape any ASCII character by specifying its underlying numeric value as three octal digits. q)”\142”“b” symbol¶ A symbol is an atom holding text. It is denoted by a leading back-quote, read “back tick” in q-speak. q)q _ q)zaphod_ a symbol is not a collection of char. The symbol `a and the char “a” are not the same, as we can see by asking q if they are identical. q)`a~”a”0b listIndex Notation¶To access the item at index i in a list, follow the list immediately with [i]. This is called item indexing. For example, q)(100; 200; 300)[0] Indexed Assignment¶ Items in a list can also be assigned via item indexing. Thus, q)L:1 2 3q)L[1]:42q)L1 42 3 An omitted index returns the entire list. q)L:10 20 30 40q)L[]10 20 30 40]]></content>
  </entry>
  <entry>
    <title><![CDATA[Mac tips]]></title>
    <url>%2F2016-09-01-Mac-Tips%2F</url>
    <content type="text"><![CDATA[how to switch differnt inputThis can be configured in system proferenece-&gt;keyboard shortcuts-&gt;input sources-&gt;select next source in inputCtrl+Alt+Space How to force an app to quit on your MacPress these three keys together: Option, Command, and Esc (Escape). This is similar to pressing Control-Alt-Delete on a PC. Or choose Force Quit from the Apple () menu in the upper-left corner of your screen. To capture screenshot of MacPress Shift + Command + 5, then click an option, like Selection button to capture a still selection or Whole screen button to record your whole screen. The screenshot tools appear in a small palette, which you can drag to reposition. The palette includes options for where to save the screenshot, whether to show the pointer, and more. Save, edit, and share your shots After you take a screenshot or video, a thumbnail appears in the corner of your screen. Drag it int Forward delete (delete char next to cursor)Fn+Delete To hide and show dockJust hit Command+Option+D at any time, and the dock will glide away (or back again). WindowsOpen a file Cmd+Down arrow Cmd+O Switch between windowsWhile you have two or more documents open in your favorite word-processing software, simply press and hold the Command ⌘ key and then strike the ~ (Tilde) key on your keyboard. navigate on pagefn + ← Jump to top of documentfn + → Jump to bottom of documentfn + ↓ Advance down one pagefn + ↑ Advance up one page Enter path in open/save windowPress Ctrl+Shift+G will open GoTo window Switch running applicationsBesides Command + Tab, you can swipe up on the touchpad with three fingers to view the windows of open apps, allowing you to quickly switch between programs. This view is called Mission Control, which also has its own dedicated keyboard shortcut (F3). Capture screen shotTo capture the entire screen, press Command-Shift-3 The screen shot will be automatically saved as a PNG file on your desktop with the filename starting with “Picture” followed by a number, example Picture 1, Picture 2, and so on. To capture a portion of the screen, press Command-Shift-4TerminalPage up /Down Page up/down: Fn + up/down arrow, or Ctrl+f/bSetup command line for sublime1ln -s "/Applications/Sublime Text.app/Contents/SharedSupport/bin/subl" /usr/local/bin/sublime Open files from terminal1open abc.jpg To make changed profile settings take effect right away1source .bash_profile ApplicationsSafariSublimeSwitch different sublime windows1'command + ~' Path of customize build system1~/Library/Application Support/Sublime Text 3/Packages/User/Nodejs.sublime-build Open folders in Sublime1sublime . Open and edit file with sublime1open -a 'Sublime Text' .bash_profile Show full path in Sublime text 3With Sublime Text 3, all that’s necessary is to edit your Sublime user preferences (Preferences -&gt; Settings - User) to include: 1234&#123; // ... other settings "show_full_path": true&#125; Open files (go to anything) in sublimeCommand + P Go to matching braceCtrl + M Go to previous/next position Ctrl + “-“ Ctrl + Shift + “-“ Preview markdownCommand + Shift + P -&gt; Preview GesturesThree (3) fingures for windows switch Up: Mission control, show all running applications Left/Right: Swtich desktops Thumb + three figures Center to outside: show desktop Outside to Center: show launch pad]]></content>
      <tags>
        <tag>Mac</tag>
        <tag>shortcut</tag>
        <tag>Efficiency</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Seconds]]></title>
    <url>%2F2018-11-01-seconds%2F</url>
    <content type="text"><![CDATA[nano secondsns: 1/1,000,000,000 second, i.e. 10(-9) seconds1 ns = 1/1,000 micro second1 ns = 1/1,000,000 milis second used in telecommunications micro secondsIts symbol is μs. 微秒1 μs = 1000 ns1 μs = 1/1,000 milli Seconds 8.01 μs： light took the time to travel 1 mile in vaccum The average human eye blink takes 350,000 microseconds (just over 1/3 of one second).The average human finger snap takes 150,000 microseconds (just over 1/7 of one second).A camera flash illuminates for 1000 microseconds. milli secondms 毫秒1 ms = 1/1,000 second1 ms = 1,000 μs = 1,000,000 ns 3 ms: fly flgp its wing5 ms: bee flap wing300-400 ms: human eye to blink]]></content>
  </entry>
  <entry>
    <title><![CDATA[Java Class Loader]]></title>
    <url>%2F2016-02-25-Java-Class-Loader%2F</url>
    <content type="text"><![CDATA[Codecache The maximum size of the code cache is set via the -XX:ReservedCodeCacheSize=N flag (where N is the default just mentioned for the particular compiler). The code cache is managed like most memory in the JVM: there is an initial size (specified by -XX:InitialCodeCacheSize=N). Allocation of the code cache size starts at the initial size and increases as the cache fills up. The total of native and heap memory used by the JVM yields the total footprint of an application. The code cache is a resource with a defined maximum size that affects the total amount of compiled code the JVM can run.Tiered compilation can easily use up the entire code cache in its default configuration (particularly in Java 7); monitor the code cache and increase its size if necessary when using tiered compilation.Compilation ThresholdsThe major factor involved here is how often the code is executed; once it is executed a certain number of times, its compilation threshold is reached, and the com‐ piler deems that it has enough information to compile the code.Compilation is based on two counters in the JVM: the number of times the method has been called, and the number of times any loops in the method have branched back. Branching back can effectively be thought of as the number of times a loop has com‐ pleted execution, either because it reached the end of the loop itself or because it executed a branching statement like continue.When the JVM executes a Java method, it checks the sum of those two counters and decides whether or not the method is eligible for compilation. If it is, the method is queued for compilation So every time the loop completes an execution, the branching counter is incremented and inspected. If the branching counter has exceeded its indi‐ vidual threshold, then the loop (and not the entire method) becomes eligible for compilation.This kind of compilation is called on-stack replacement (OSR), because even if the loop is compiled, that isn’t sufficient: the JVM has to have the ability to start executing the compiled version of the loop while the loop is still running. When the code for the loop has finished compiling, the JVM replaces the code (on-stack), and the next iteration of the loop will execute the much-faster compiled version of the code. Standard compilation is triggered by the value of the -XX:CompileThreshold=N flag. The default value of N for the client compiler is 1,500; for the server compiler it is 10,000. Changing the value of the CompileThreshold flag will cause the the compiler to choose to compile the code sooner (or later) than it normally would have.Periodically (specifically, when the JVM reaches a safepoint), the value of each counter is reduced. Practically speaking, this means that the counters are a relative measure of the recent hotness of the method or loop. One side effect of this is that somewhat-frequently executed code may never be compiled, even for programs that run forever (these methods are sometimes called lukewarm [as opposed to hot]). This is one case where reducing the compilation threshold can be beneficial, and it is another reason why tiered compilation is usually slightly faster than the server compiler alone.Quick Summary Compilation occurs when the number of times a method or loop has been executed reaches a certain threshold.Changing the threshold values can cause the code to be com‐ piled sooner than it otherwise would.“Lukewarm” methods will never reach the compilation thresh‐ old (particularly for the server compiler) since the counters de‐ cay over time.that give visibility into the working of the compiler. The most important of these is -XX:+PrintCompilation (which by default is false).If PrintCompilation is enabled, every time a method (or loop) is compiled, the JVM prints out a line with information about what has just been compiled.Usually this number will simply increase monotonicallyInspecting Compilation with jstatSeeing the compilation log requires that the program be started with the -XX:+PrintCompilation flag. If the program was started without that flag, you can get some limited visibility into the working of the compiler by using jstat.jstat has two options to provide information about the compiler. The -compiler option supplies summary information about how many methods have been compiled (here 5003 is the process ID of the program to be inspected):% jstat -compiler 5003Compiled Failed Invalid Time FailedType FailedMethod206 0 0 1.97 0Note this also lists the number of methods that failed to compile and the name of the last method that failed to compile; if profiles or other information lead you to suspect that a method is slow because it hasn’t been compiled, this is an easy way to verify that hypothesis.Because jstat takes an optional argument to repeat its operation, you can see over time which methods are being compiled. In this example, jstat repeats the information for process ID 5003 every second (1,000 ms):% jstat -printcompilation 5003 1000 Compiled It’s easy to read OSR lines like this example as 25% and wonder about the other 75%, but remember that the number is the compilation ID, and the % just signifies OSR compilation. The best way to gain visibility into how code is being compiled is by enabling PrintCompilation.Output from enabling PrintCompilation can be used to make sure that compilation is proceeding as expected.If a method is compiled using standard compilation, then the next method invocation will execute the compiled method; if a loop is compiled using OSR, then the next iteration of the loop will execute the compiled code.These queues are not strictly first in, first out: methods whose invocation counters are higher have priority.this priority ordering helps to ensure that the most important code will be compiled first. (This is another reason why the compilation ID in the PrintCompilation output can appear out of order.)When the client compiler is in use, the JVM starts one compilation thread; the server compiler has two such threads. When tiered compilation is in effect, the JVM will by default start multiple client and server threads based on a somewhat complex equation involving double logs of the number of CPUs on the target platform. The number of compiler threads (for all three compiler options) can be adjusted by setting the -XX:CICompilerCount=N flag (with a default value given in the previous table).Quick Summary Compilation occurs asynchronously for methods that are placed on the compilation queue.The queue is not strictly ordered; hot methods are compiled before other methods in the queue. This is another reason why compilation IDs can appear out of order in the compilation log.InliningOne of the most important optimizations the compiler makes is to inline methods. Code that follows good object-oriented design often contains a number of attributes that are accessed via getters (and perhaps setters):public class Point { private int x, y;public void getX() { return x; } public void setX(int i) { x = i; }} The overhead for invoking a method call like this is quite high, especially relative to the amount of code in the method. In fact, in the early days of Java, performance tips often argued against this sort of encapsulation precisely because of the performance impact of all those method calls. Fortunately, JVMs now routinely perform code inlining for these kinds of methods. Hence, you can write this code:Point p = getPoint(); p.setX(p.getX() * 2);and the compiled code will essentially execute this:Point p = getPoint(); p.x = p.x * 2;Inlining is enabled by default. It can be disabled using the -XX:-Inline flag, though it is such an important performance boost that you would never actually do that (for example, disabling inlining reduces the performance of the stock batching test by over 50%). The basic decision about whether to inline a method depends on how hot it is and its size. The JVM determines if a method is hot (i.e., called frequently) based on an internal calculation; it is not directly subject to any tunable parameters. If a method is eligible for inlining because it is called frequently, then it will be inlined only if its bytecode size is less than 325 bytes (or whatever is specified as the -XX:MaxFreqInlineSize=N flag). Otherwise, it is eligible for inlining only if it is small: less than 35 bytes (or whatever is specified as the -XX:MaxInlineSize=N flag).Sometimes you will see recommendations that the value of the MaxInlineSize flag be increased so that more methods are inlined.Inlining is the most beneficial optimization the compiler can make, particularly for object-oriented code where attributes are well encapsulated. Tuning the inlining flags is rarely needed, and recommendations to do so often fail to account for the relationship between normal inlining and frequent inlining. Make sure to account for both cases when investigating the effects of inlining.Escape AnalysisThe server compiler performs some very aggressive optimizations if escape analysis is enabled (-XX:+DoEscapeAnalysis, which is true by default).Escape analysis is the most sophisticated of the optimizations the compiler can perform. This is the kind of optimization that fre¬quently causes microbenchmarks to go awry. Escape analysis can often introduce “bugs” into improperly synchronized code. Escape analysis is a technical that evaluate the scope of a Java object. In particular, if a java object allocated by some execting thread can ever be seen by a different thread, the object ‘escapes’.For example, consider this class to work with factorials: 1234567891011121314151617public class Factorial &#123;private BigInteger factorial;private int n;public Factorial(int n) &#123;this.n = n;&#125;public synchronized BigInteger getFactorial() &#123;if (factorial == null)factorial = ...;return factorial;&#125;&#125;To store the first 100 factorial values in an array, this code would be used:ArrayList&lt;BigInteger&gt; list = new ArrayList&lt;BigInteger&gt;(); for (int i = 0; i &lt; 100; i++) &#123;Factorial factorial = new Factorial(i);list.add(factorial.getFactorial());&#125; The factorial object is referenced only inside that loop; no other code can ever accessthat object. Hence, the JVM is free to perform a number of optimizations on that object:· It needn’t get a synchronization lock when calling the getFactorial() method.· It needn’t store the field n in memory; it can keep that value in a register. Similarly it can store the factorial object reference in a register.· In fact, it needn’t allocate an actual factorial object at all; it can just keep track of the individual fields of the object.Deoptimization There are two cases of deoptimization: when code is “made not entrant,” and when code is “made zombie.”This generates a deoptimization trap, and the previous optimizations are discarded. If a lot of additional calls are made with logging enabled, the JVM will quickly end up compiling that code and making new optimizations.The second thing that can cause code to be made not entrant is due to the way tiered compilation works. In tiered compilation, code is compiled by the client compiler, and then later compiled by the server compiler (and actually it’s a little more complicated than that, Deoptimizing Zombie Code When the compilation log reports that it has made zombie code, it is saying that it has reclaimed some previous code that was made not entrant. But there were still objects of the StockPriceHistoryImpl class around.Eventually all those objects were reclaimed by GC. When that happened, the compiler noticed that the methods of that class were now eligible to be marked as zombie code. The heap (usually) accounts for the largest amount of memory used by the JVM, but the JVM also uses memory for its internal operations. This nonheap memory is native memory. Native memory can also be allocated in applications (via JNI calls to malloc() and similar methods, or when using New I/O, or NIO). The total of native and heap memory used by the JVM yields the total footprint of an application. Deoptimization allows the compiler to back out previous versions of compiled code.Code is deoptimized when previous optimizations are no longer valid (e.g., because the type of the objects in question has changed). There is usually a small, momentary effect in performance when code is deoptimized, but the new code usually warms up quick‐ ly again. Under tiered compilation, code is deoptimized when it had previously been compiled by the client compiler and has now been optimized by the server compiler.Tiered Compilation LevelsIt turns out that there are five levels of execution, because the client compiler has three different levels. So the level of compilation runs from:· 0: Interpreted code· 1: Simple C1 compiled code· 2: Limited C1 compiled code· 3: Full C1 compiled code· 4: C2 compiled codeA typical compilation log shows that most methods are first compiled at level 3: full C1 compilation. (All methods start at level 0, of course.) If they run often enough, they will get compiled at level 4 (and the level 3 code will be made not entrant). This is the most frequent path: the client compiler waits to compile something until it has information about how the code is used that it can leverage to perform optimizations.If the server compiler queue is full, methods will be pulled from the server queue and compiled at level 2, which is the level at which the C1 compiler uses the invocation and back-edge counters (but doesn’t require profile feedback). That gets the method com‐ piled more quickly; the method will later be compiled at level 3 after the C1 compiler has gathered profile information, and finally compiled at level 4 when the server compiler queue is less busy. And of course when code is deoptimized, it goes to level 0.Summary:Tiered compilation can operate at five distinct levels among the two compilersChanging the path between levels is not recommended; this section just helps to explain the output of the compilation log.This chapter has provided a lot of details about how just-in-time compilation works. From a tuning perspective, the simple choice here is to use the server compiler with tiered compilation for virtually everything; this will solve 90% of compiler-related performance issues. Just make sure that the code cache is sized large enough, and the compiler will provide pretty much all the performance that is possible.If you have some experience with Java performance, you may be surprised that compilation has been discussed for an entire chapter without mentioning the final keyword. In some circles, the final keyword is thought to be an important factor in performance because it is believed to allow the JIT compiler to make better choices about inlining and other optimizations.Still, it is a persistent rumor. For the record, then, you should use the final keyword whenever it makes sense: for an immutable object or primitive value you don’t want to change, for parameters to certain inner classes, and so on. But the presence or absence of the final keyword will not affect the performance of an application.Don’t be afraid of small methods—and in particular getters and setters—because they are easily inlined. If you have a feeling that the method overhead can be ex‐ pensive, you’re correct in theory (we showed that removing inlining has a huge impact on performance). But it’s not the case in practice, since the compiler fixes that problem. Code that needs to be compiled sits in a compilation queue. The more code in the queue, the longer the program will take to achieve optimal performance. Although you can (and should) size the code cache, it is still a finite resource. The simpler the code, the more optimizations that can be performed on it. Profile feedback and escape analysis can yield much faster code, but complex loop struc‐ tures and large methods limit their effectiveness. That concept is the essential difference between committed (or allocated) memory and reserved memory (sometimes called the virtual size of a process). The JVM must tell the operating system that it might need as much as 2 GB of memory for the heap, so that memory is reserved: the operating system promises that when the JVM attempts to allocate additional memory when it increases the size of the heap, that memory will be available.Still, only 512 MB of that memory is actually allocated initially, and that 512 MB is all of the memory that actually is being used (for the heap). That (actually allocated) mem‐ ory is known as the committed memory. The amount of committed memory will fluc‐ tuate as the heap resizes; in particular, as the heap size increases, the committed memory correspondingly increases.When we look at performance, only committed memory really matters: there is never a performance problem from reserving too much memory.However, sometimes you want to make sure that the JVM does not reserve too much memory. This is particularly true for 32-bit JVMs. Since the maximum process size of a 32-bit application is 4 GB (or less, depending on the operating system), over-reserving memory can be an issue. A JVM that reserves 3.5 GB of memory for the heap is left with only 0.5 GB of native memory for its stacks, code cache, and so on. It doesn’t matter if the heap only expands to commit 1 GB of memory: because of the 3.5 GB reservation, the amount of memory for other operations is limited to 0.5 GB.64-bit JVMs aren’t limited that way by the process size, but they are limited by the total amount of virtual memory on the machine. Say that you have a small server with 4 GB of physical memory and 10 GB of virtual memory and start a JVM with a maximum One exception to this is thread stacks. Every time the JVM creates a thread, the OS allocates some native memory to hold that thread’s stack, committing more memory to the process (until the thread exits, at least). Thread stacks, though, are fully allocated when they are created. Code cacheThe code cache uses native memory to hold compiled code. As discussed in Chap‐ ter 4, this can be tuned (though performance will suffer if all the code cannot be compiled due to space limitations).Developers can allocate native memory via JNI calls, but NIO byte buffers will also allocate native memory if they are created via the allocateDirect() method. Native byte buffers are quite important from a performance perspective, since they allow native code and Java code to share data without copying it. The most common example here is buffers that are used for filesystem and socket operations. Writing data to a native NIO buffer and then sending that data to the channel or socket) requires no copying of data between the JVM and the C library used to transmit the data. If a heap byte buffer is used instead, contents of the buffer must be copied by the JVM. The allocateDirect() method call is quite expensive; direct byte buffers should be reused as much as possible. The ideal situation is when threads are independent and each can keep a direct byte buffer as a thread-local variable. That can sometimes use too much native memory if there are many threads that need buffers of variable sizes, since eventually each thread will end up with a buffer at the maximum possible size. For that kind of situation—or when thread-local buffers don’t fit the application design— an object pool of direct byte buffers may be more useful. From a tuning perspective, the footprint of the JVM can be limi¬ted in the amount of native memory it uses for direct byte buf¬fers, thread stack sizes, and the code cache (as well as the heap). Class loader A class loader in Java is simply an object whose type extends the ClassLoader class. When the virtual machine needs access to a particular class, it asks the appropriate class loader. Class loaders are organized into a tree hierarchy. At the root of this tree is the system class loader. This class loader is also called the primordial class loader or the null class loader. It is used only to load classes from the core Java API. The system class loader has one or more children. It has at least one child; the URL class loader that is used to load classes from the classpath. It may have other direct children, though typically any other class loaders are children of the URL class loader that reads the classpath. The hierarchy comes into play when it is time to load a class. Classes are loaded in one of three ways: either explicitly by calling the loadClass( ) method of a class loader, explicitly by calling the Class.forName( ) method, or implicitly when they are referenced by an already−loaded class.In any case, a class loader is asked to load the class. In the first case, the class loader is the object on which the loadClass( ) method is invoked. In the case of the forName( ) method, the class loader is either passed to that method, or it is the class loader that loaded the class that is calling the forName( ) method. The implicit case is similar: the class loader that was used to load the referencing class is also used to load the referenced class.Class loaders are responsible for asking their parent to load a class; only if that operation fails will the class loader attempt to define the class itself. The net effect of this is that system classes will always be loaded from the system class loader, classes on the class path will always be loaded by the class loader that knows about the classpath, and in general, a class will be loaded by the oldest class loader in the ancestor hierarchy that knows where to find a class. When you create a class loader, you can insert it anywhere into the hierarchy of class loaders (except at the root). Typically, when a class loader is created, its parent is the class loader of the class that is instantiating the new class loader. Implementing a Class Loader Now we’ll look at how to implement a class loader. The class loader we implement will be able to extend the normal permissions that are granted via policy files, and it will enforce certain optional security features of the class loader. The basic class that defines a class loader is the ClassLoader class (java.lang.ClassLoader):public abstract class ClassLoaderTurn a series of Java bytecodes into a class definition. This class does not define how the bytecodes are obtained but provides all other functionality needed to create the class definition. However, the preferred class to use as the basis of a class loader is the SecureClassLoader class (java.security.SecureClassLoader):public class SecureClassLoader extends ClassLoaderTurn a series of Java bytecodes into a class definition. This class adds secure functionality to the ClassLoader class, but it still does not define how bytecodes are obtained. Although this class is not abstract, you must subclass it in order to use it.The secure class loader provides additional functionality in dealing with code sources and protection domains. You should always use this class as the basis of any class loader you work with; in fact, the ClassLoader class would be private were it not for historical reasons. public class URLClassLoader extends SecureClassLoaderLoad classes securely by obtaining the bytecodes from a set of given URLs. Key Methods of the Class Loader The ClassLoader class and its subclasses have three key methods that you work with when creating your own class loader. 6.3.2.1 The loadClass( ) methodThe loadClass( ) method is the only public entry into the class loader:public Class loadClass(String name) Load the named class. A ClassNotFoundException is thrown if the class cannot be found.This is the simplest way to use a class loader directly: it requires that the class loader be instantiated and then be used via the loadClass( ) method. Once the Class object has been constructed, there are three ways in which a method in the class can be executed:The correct implementation of the loadClass( ) method is crucial to the security of the virtual machine. For instance, one operation this method performs is to call the parent class loader to see if it has already defined a particular class; this allows all thecore Java classes to be loaded by the primordial class loader. If that operation is not performed correctly, security could suffer. As a developer you should be careful when you override this method; as an administrator, this is one of the reasons to prevent untrusted code from creating a class loader. 6.3.2.2 The findClass( ) method The loadClass( ) method performs a lot of setup and bookkeeping related to defining a class, but from a developer perspective, the bulk of the work in creating a Class class object is performed by the findClass( ) method:protected Class findClass(String name) The findClass( ) method uses whatever mechanism it deems appropriate to load the class (e.g., by reading a class file from the file system or from an HTTP server). It is then responsible for creating the protection domain associated with the class and using the next method to create the Class class object. The defineClass( ) methodsThese methods all take an array of Java bytecodes and some information that specifies the permissions associated with the class represented by those bytecodes. They all return the Class class object:protected final Class defineClass(String name, byte[] b, int off, int len) Responsibilities of the Class LoaderWhen you implement a class loader, you override some or all of the methods we’ve just listed. In sum, the class loader must perform the following steps:The security manager is consulted to see if this program is allowed to access the class in question. If it is not, a security exception is thrown. This step is optional; it should be implemented at the beginning of the loadClass( ) method. This corresponds to the use of the accessClassInPackage permission.If the class loader has already loaded this class, it finds the previously defined class object and returns that object. This step is built into the loadClass( ) method. corresponds to the use of the accessClassInPackage permission.If the class loader has already loaded this class, it finds the previously defined class object and returns that object. This step is built into the loadClass( ) method. Otherwise, the class loader consults its parent to see if the parent knows how to load the class. This is a recursive operation, so the system class loader will always be asked first to load a class. This prevents programs from providing alternate definitions of classes in the core API (but a clever class loader can defeat that protection). This step is built into the loadClass( ) method.The security manager is consulted to see if this program is allowed to create the class in question. If it is not, a security exception is thrown. This step is optional; if implemented, it should appear at the beginning of the findClass( ) method. Note that this step should take place after the parent class loader is queried rather than at the beginning of the operation (as is done with the access check). No Sun−supplied class loader implements this step; it corresponds to the defineClassInPackage permission. The class file is read into an array of bytes. The mechanism by which the class loader reads the file and creates the byte array will vary depending on the class loader (which, after all, is one of the points of having different class loaders). This occurs in the findClass( ) method.The appropriate protection domain is created for the class. This can come from the default security model (i.e., from the policy files), and it can be augmented (or even replaced) by the class loader. Alternately, you can create a code source object and defer definition of the protection domain. This occurs in the findClass( ) method. Within the findClass( ) method, a Class object is constructed from the bytecodes by calling the defineClass( ) method. If you used a code source in step 6, the getPermissions( ) method will be called to find the permissions associated with the code source. The defineClass( ) method also ensures that the bytecodes are run through the bytecode verifier. Before the class can be used, it must be resolved −− which is to say that any classes that it immediately references must also be found by this class loader. The set of classes that are immediately referenced contains any classes that the class extends as well as any classes used by the static initializers of the class. Note that classes that are used only as instance variables, method parameters, or local variables are not normally loaded in this phase: they are loaded when the class actually references them (although certain compiler optimizations may require that these classes be loaded when the class is resolved). This step happens in the loadClass( ) method. If you want to use a custom class loader, the easiest route is to use the URL class loader. This limits the number of methods that you have to override.To construct an instance of this class, use one of the following constructors:public URLClassLoader(URL urls[]) 1234567891011121314151617URL urls[] = new URL[2];urls[0] = new URL("http://piccolo.East/~sdo/");urls[1] = new URL("file:/home/classes/LocalClasses.jar"); ClassLoader parent = this.getClass().getClassLoader( ); URLClassLoader ucl = new URLClassLoader(urls, parent);public final synchronized Class loadClass(String name, boolean resolve)throws ClassNotFoundException &#123;// First check if we have permission to access the package.SecurityManager sm = System.getSecurityManager( );if (sm != null) &#123;int i = name.lastIndexOf('.');if (i != −1) &#123;sm.checkPackageAccess(name.substring(0, i));&#125;&#125;return super.loadClass(name, resolve);&#125; 6.3.4.2 Step 2: Use the previously−defined class, if availableThe loadClass( ) method of the ClassLoader class performs this operation for you, which is why we’ve called the super.loadClass( ) method. 6.3.4.3 Step 3: Defer class loading to the parentThe loadClass( ) method of the ClassLoader class performs this operation. 6.3.4.4 Step 4: Optionally call the checkPackageDefinition( ) methodIn order to call the checkPackageDefinition( ) method, you must override the findClass( ) method: 1234567891011protected Class findClass(final String name)throws ClassNotFoundException &#123;// First check if we have permission to access the package. SecurityManager sm = System.getSecurityManager( );if (sm != null) &#123;int i = name.lastIndexOf('.');if (i != −1) &#123;sm.checkPackageDefinition(name.substring(0, i));&#125;&#125;return super.findClass(name);&#125; 6.3.4.5 Step 5: Read in the class bytesThe URL class loader performs this operation for you by consulting the URLs that were passed to its constructor. If you need to adjust the way in which the class bytes are read, you should use the SecureClassLoader class instead.6.3.4.6 Step 6: Create the appropriate protection domainThe URL class loader will create a code source for each class based on the URL from which the class was loaded and the signers (if any) of the class. The permissions associated with this code source will be obtained by using the getPermissions( ) method of the Policy class, which by default will return the permissions read in from the active policy files. In addition, the URL class loader will add additional permissions to that set:If the URL has a file protocol, it must specify a file permission that allows all files that descend from the URL path to be read. For example, if the URL is file:///xyz/classes/, then a file permission with a name of /xyz/classes/− and an action list of read will be added to the set of permiss ions. If the URL is a jar file (file:///xyz/MyApp.jar), the name file permission will be the URL itself.If you want to associate different permissions with the class, then you should override the getPermissions( ) method. For example, if we wanted the above rules to apply and also allow the class to exit the virtual machine, we’d use this code: 1234protected PermissionCollection getPermissions(CodeSource codesource) &#123; PermissionCollection pc = super.getPermissions(codesource);pc.add(new RuntimePermission("exitVM"));return pc;&#125; We could completely change the permissions associated with the class (bypassing the Policy class altogether) by constructing a new permission collection in this method rather than calling super.getPermissions( ). The URL class loader will use whatever permissions are returned from this getPermissions( ) method to define the protection domain that will be associated with the class.If you need to load bytes from a source that is not a URL (or from a URL for which you don’t have a protocol handler, like FTP), then you’ll need to extend the SecureClassLoader class. A subclass is required because the constructors of this class are protected, and in any case you need to override the findClass( ) The steps to use this class are exactly like the steps for the URLClassLoader class, except for step 5. To implement step 5, you must override the findClass( ) method like this: 1234567891011121314151617181920212223242526272829303132protected Class findClass(final String name) throws ClassNotFoundException &#123;// First check if we have permission to access the package.// You could remove these 7 lines to skip the optional step 4.SecurityManager sm = System.getSecurityManager( );if (sm != null) &#123;int i = name.lastIndexOf('.');if (i != −1) &#123;sm.checkPackageDefinition(name.substring(0, i));&#125;&#125;// Now read in the bytes and define the classtry &#123;return (Class)AccessController.doPrivileged(new PrivilegedExceptionAction( ) &#123;public Object run( ) throws ClassNotFoundException &#123;byte[] buf = null;try &#123;// Acutally load the class bytesbuf = readClassBytes(name);&#125; catch (Exception e) &#123;throw new ClassNotFoundException(name, e);&#125;// Create an appropriate code sourceCodeSource cs = getCodeSource(name);// Define the classreturn defineClass(name, buf,0, buf.length, cs);&#125;&#125;);&#125; catch (java.security.PrivilegedActionException pae) &#123; throw (ClassNotFoundException) pae.getException( ); &#125; The syntax of this method is complicated by the fact that we need to load the class bytes in a privileged block. Depending on your circumstances, that isn’t strictly necessary, but it’s by far the most common case for class loaders. Say that your class loader loads class A from the database; that class is given minimal permissions. When that class references class B, the class loader will be asked to load class B and class A will be on the stack. When it’s time to load the new class bytes, we need to load them with the permissions of the class loader rather than the entire stack, which is why we use a privileged block.Notwithstanding, the try block has three operations: it loads the class bytes, it defines a code source for that class, and it calls the defineClass( ) method to create the class. The first two of the opera tions are encapsulated in the readClassBytes( ) and getCodeSource( ) methods; these are methods thatyou must implement.Loading the class bytes is an operation left to the reader. The reason for providing your own class loader is that you want to read the class bytes in some special way; otherwise, you’d use the URLClassLoader class. The code source is another matter: we must determine a URL and a set of certificates that should beassociated with the class.In a signed jar file, the certificates are read from the jar file and the URL is the location of the jar file. In Chapter 12, we’ll show how to get the certificates from a standard jar file and construct the appropriate URLClassLoader class. The code source is another matter: we must determine a URL and a set of certificates that should beassociated with the class.In a signed jar file, the certificates are read from the jar file and the URL is the location of the jar file. In Chapter 12, we’ll show how to get the certificates from a standard jar file and construct the appropriate The defineClass( ) method will call back to the getPermissions( ) method in order to complete the definition of the protection domain for this class. And that’s why the URL used to construct the code source can be arbitrary: when you write the getPermissions( ) method, just make sure that you understand what the URL actually is. In default usage, the URL would be used to find entries in the policy files, but since you’re defining your own permissions anyway, the contents of the URL don’t matter. What matters is that you follow a consistent convention between the definition of your getCodeSource( ) and findClass( ) methods.Hence, possible implementations of the getPermissions( ) and getCodeSource( ) methods are as follows: 1234567891011121314protected CodeSource getCodeSource(String name) &#123;try &#123;return new CodeSource(new URL("file", "localhost", name),null);&#125; catch (MalformedURLException mue) &#123;mue.printStackTrace( );&#125;return null;&#125;protected PermissionCollection getPermissions(CodeSource codesource) &#123;PermissionCollection pc = new Permissions( );pc.add(new RuntimePermission("exitVM"));return pc;&#125; If you’re reading the class bytes from, say, a database, it would be more useful if you could pass an arbitrary string to construct the code source. That doesn’t work directly since the code source requires a URL but the file part of the URL can be any arbitrary string. In this case, we just use the class name.Note that the getPermissions( ) method of the SecureClassLoader class does not add the additional permissions that the same method of the URLClassLoader class adds. As a result, we do not call the super.getPermissions( ) DelegationAs we’ve mentioned, class loading follows a delegation model. This model permits a class loader to be instantiated with this constructor:protected ClassLoader(ClassLoader parent)Create a class loader that is associated with the given class loader. This class loader delegates all operations to the parent first: if the parent is able to fulfill the operation, this class loader takes no action. For example, when the class loader is asked to load a class via the loadClass( ) method, it first calls the loadClass( ) method of the parent. If that succeeds, the class returned by the delegate will ultimately be returned by this class. If that fails, the class loader then uses its original logic to complete its task, something like this: 1234567public Class loadClass(String name) &#123;Class cl;cl = delegate.loadClass(name);if (cl != null)return cl;// else continue with the loadClass( ) logic&#125; You may retrieve the delegate associated with a class loader with the following method public final ClassLoader getParent( )Return the class loader to which operations are being delegated.The class loader that exists at the root of the class loader hierarchy is retrieved via this method:Return the system class loader (the class loader that was used to load the base application classes). If a security manager is in place, you must have the getClassLoader runtime permission to use this method. Loading ResourcesA class loader can load not only classes, but any arbitrary resource: an audio file, an image file, or anything else. Instead of calling the loadClass( ) method, a resource is obtained by invoking one of these methods:public URL getResource(String name)public InputStream getResourceAsStream(String name)The getResource( ) method calls the getSystemResource( ) method; if it does not find a system resource, it returns the object retrieved by a call to the findResource( ) method (which by default will be null). The getResourceAsStream( ) method simply Loading LibrariesLoading classes with native methods creates a call to this method of the ClassLoader class:protected String findLibrary(String libname)Return the directory from which native libraries should be loaded.This method is used by the System.loadLibrary( ) method to determine the directory in which the native library in question should be found. If this method returns null (the default), the native library must be in one of the di 谈到常量池，在Java体系中，共用三种常量池。分别是字符串常量池、Class常量池和运行时常量池。]]></content>
      <tags>
        <tag>Java</tag>
        <tag>class loader`</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Microservices vs. SOA]]></title>
    <url>%2F2016-05-31-Microservices-Vs-SOA%2F</url>
    <content type="text"><![CDATA[Microservice Services are organized around capabilities, e.g., user interface front-end, recommendation, logistics, billing, etc. Services are small in size, messaging enabled, bounded by contexts, autonomously developed, independently deployable, decentralized and built and released with automated processes. resource-oriented computing (ROC) as a generalized form of the Web abstraction. If in the Unix abstraction “everything is a file”, in ROC, everything is a “Micro-Web-Service” PhilosophyThe philosophy of the microservices architecture essentially equates to the Unix philosophy of “Do one thing and do it well”. It is described as follows: The services are small - fine-grained to perform a single function. The organization culture must embrace automation of testing and deployment. This eases the burden on management and operations and allows for different development teams to work on independently deployable units of code. The culture and design principles must embrace failure and faults, similar to anti-fragile systems. Each service is elastic, resilient, composable, minimal, and complete. service meshIn a service mesh, each service instance is paired with an instance of a reverse proxy server, called a service proxy, sidecar proxy, or sidecar. The service instance and sidecar proxy share a container, and the containers are managed by a container orchestration tool such as Kubernetes. Differences between Microservices and SOADefinitaion of SOA Boundaries are explicit Services are autonomous Services share __ schema __ and contract, not class Service compatibility is based on policy In short, the microservice architectural style is an approach to developing a single application as a suite of small services, each running in its own process and communicating with lightweight mechanisms, often an HTTP resource API. These services are built around business capabilities and independently deployable by fully automated deployment machinery. There is a bare mininum of centralized management of these services, which may be written in different programming languages and use different data storage technologies. Related links: InfoQ Discussions StackOverFlow Microservices tag desc MartinFowler]]></content>
      <tags>
        <tag>Microservices</tag>
        <tag>SOA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Database sharding]]></title>
    <url>%2F2016-06-01-DB-Sharding%2F</url>
    <content type="text"><![CDATA[DB sharding in YHDThere are two solutions when DB becoming bottleneck in yihaodian. Scale upUpgrade Oracle DB, adding more CPU , Disk and memory to incrase I/O performance. This is for short term only, high cost. Scale outDivide the order table to multiple DBs, which is support horizontal extension, for long term purpose. Orgional Oracle is replaced by multiple MySQL DB, supporintg one master and multiple slaves, supporitng segratation of read and write. Leveraging MySQL built-in Master-slave replication (SLA&lt;1 second) sharding dimensions DB Field chosing, it should chose the filed that lead to least SQL and code change, to make the access fall in one database, instead of multiple DBs, which result in high I/O and significant logic change. Here is one practice Get all SQL Pick up top fields appear in where clause. List break down from three categories Single ID, i.e. userID=? Multiple ID. i.e. userID in (?,?,?) Not show Field Single ID Multiple ID Not show userID 120 40 330 orderID 60 80 360 shopID 15 0 485 It’s obviously we should chose userID for sharding. Hold on, this is just static analysis, we should conduct dynamic* study as well, so list most executed SQLs, e.g. top 15 SQL (account to 85% of SQL calls), if we conduct sharding by user ID, 85% of those SQL will fall in single DB and 13% fall in multiple DB, and only 2% will scan all DB, so the performance is must better than sharding on other ID fields. sharding strategyThere are two type of strategies By value range, e.g. user ID 1-9999 to DB1, and 10000-20000 to DB2. For this option, By value mod, e.g. userID mod n, when reminder is 0, go to DB1, reminder is 1, to DB2, etc. Pros and Cons: Criteria By Range By Mod number of DBs initially only require small amount of DBs, can increasse by business requests initially number based on mod number, normally a big number Accessibility initially only few DBs, perforamce cost is small, single DB performance query is poor initially big number of DBs, query acorss DBs may consume many resources, better for query on single DB DBs adjustment easy, just add new DB, and impact is limit when split existing DB not easy, change mod value may result in DB migration across DBs Data hotspot there are data hotspot issues no data hotspot issues In practice, for the sake of simplicity, mod sharding is often used. To manage further sharding, and for smooth data migration, normally new DBs are added by folds, e.g. intially 4 DBs, furhter split will be 8 DBs, then 16 DBs. This is becuase only half of data in existing DB will be migrated to new DB, while the rest half will be remain unchanged. However, there are some super IDs, e.g. one big shop with massive records than normal, if we shard DB by user ID, there will one DB will many records than others. For this case, we need to provide separate DB for those super IDs. sharding numbersFirslty, that’s depends on the ability of single DB, e.g. normally one MySQL DB can support upto 50mio records, and Oracle can support 100mio. Normally multiple DBs may leads to certain perforamnce issues, when data query across multiple DBs, if there are multithreading call, it will cost precious thread resource, while it’s single thread, the wating time will be unacceptable. Normally, the initial sharding is 4-8 DBs. Router transparencyTo certain extent, DB sharding means change of DBSChema, which inevitable result in application, however, this is irrelavent to business logic, so the DB sharding should be transparent to business logic code, therefore, DB sharding should be handled at DAL (Data Access Layer) or DDAL (Distributed Data Access Layer). For access to single DB, e.g. query by certain user id, DAL will automatically route to that DB, even further split by mod, still no applicaiton logic code change impacted. For simple across DB query, DAL in charge to aggregate results from every DB query, still transparent to upper application logic. For query involves multiple DBs with aggretation functions, e.g. groupBy, order by, min, max, avg. It’s recommended DAL consolidate request from single DB, while upper layers do further processing. That’s becuase if rely on DAL, it would be too complex, and such case is relatively rare case, so leave it to upper layer. Oracle ShardingIt’s required in Web 2.0 and high availability technologies Shardingis an application-managed scaling technique using many (hundreds /thousands of) independent databases Data is split into multiple databases (shards) Each database holds a subset (either range or hash) of the data Split the shards as data volume or access grows Shards are replicated for availability and scalability Sharding is the dominant approach for scaling massive websites Application code dispatches request to a specific database based on key value Queries are constrained -simple queries on shard-key Data isdenormalizedto avoid cross-shard operations (no joins) Each database holds all the data Request dispatched to a specific database based on read/write,key value Updates go to one database, changes are replicated to the other databases. The other databases are available for reads Provides read scalability Can be combined with horizontal sharding so that each shard is replicated to a different degree Main benefit is that you do not need to reshard Downsides of DB replica Only async log shipping which can lose data in case of failure Slaves can return inconsistent data Statement based replication has correctness issues &amp; row-based replication is immature Replication is slow (high overhead on each reader, slaves are single-threaded) No support for failover between master (primary) &amp; slaves (backup) Does not handle failure conditions such as missing or damaged logs Storage engine and replication state may become inconsistent after a crash Bringing a failed master back requires copying the database –End–]]></content>
      <tags>
        <tag>DB</tag>
        <tag>Sharding</tag>
        <tag>MobileInternet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker]]></title>
    <url>%2F2016-06-01-Docker%2F</url>
    <content type="text"><![CDATA[Dockers ConceptsOfficial Definition Docker is an open platform for developers and sysadmins to build,ship and run distributed applications Docker’s philosophy is “build-&gt;ship-&gt;run”. In contemporary IT industry, there are two major usage of Docker. Focus on Build &amp; Ship, to leverage Docker to setup a platform of “CI/CD”, for develop, test enviornment. Make use of Docker as light weight VM (virtual machine), focus on Run, apply it in large scale production environment. ACLthe access control in docker is rely on iptables, the firewall software shipped in almost all Linux release. Sample CommandsList all images 1docker images 1sudo docker version remove all exited containersdocker rm $(docker ps -a -f status=exited -q) Run12docker run IMAGE_NAME [COMMAND] # run a command in new containerdocker run -t -i f2d8ce9fa988 /bin/bash # run bash in console and interactive mode View docker details, e.g. start up script, working dir1docker inspect containerid Start bash to view files inside docker1docker -t -i imageFile /bin/bash Map hosts between host and contains12docker -P xxxdocker ps -l You’ll see 10.0.0.0:32768-&gt;5000/tcp It means host port 32768 map to port 5000 in contains To list docker containers including histories12docker ps -a # show all containers instead of only running as defaultdocker ps -l # show latest created container removes containers once return from one run1docker run --rm -name myApp1 -link db:db training/webapp env to ping other containersBe advised there is no built-in ping for containers, therefore it’s requried manually install one. As following sample: 1apt-get install -yqq inetutils-ping Links GitBook Docker —— 从入门到实践 知乎 docker Yelp Docker -中文版 Yelp Docker -英文版 Amazon Docker books]]></content>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[XA Transactions in 2PC]]></title>
    <url>%2F2016-06-01-XA-2PC%2F</url>
    <content type="text"><![CDATA[Description 2 phase commit protocol referred to as XA(eXtended Architecture) This protocol provides ACID-like properties for global transaction processing 2 phase commit protocol is an atomic commitment protocol for distributed systems. The first one is commit-request phase in which transaction manager coordinates all of the transaction resources to commit or abort. In the commit-phase, transaction manager decides to finalize operation by committing or aborting according to the votes of the each transaction resource. XA transactions need a global transaction id and local transaction id(xid) for each XA resource. Each XA Resource is enlisted to XA Manager by start(xid) method. This method tells that XA Resource is being involved in the transaction(be ready for operations). the first phase of the 2PC protocol is realized by calling prepare(xid) method. This method requests OK or ABORT vote from XA Resource. After receiving vote from each of XA Resource, XA Manager decides to execute a commit(xid) operation if all XA Resources send OK or decides to execute a rollback(xid) if an XA Resource sends ABORT. Finally, the end(xid) method is called for each of XA Resource telling that the transaction is completed. Reference links: (DZone XA 2PC)[https://dzone.com/articles/xa-transactions-2-phase-commit]]]></content>
      <tags>
        <tag>MobileInternet</tag>
        <tag>XA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CI and CD]]></title>
    <url>%2F2016-06-03-CI-CD%2F</url>
    <content type="text"><![CDATA[ConceptsCI/CD are usage of Docker, CI is continuous integration, is the process of eliciting fast, automated feedback on the correctness of your application every time there is a change to the code, while CD means continuous delivery, build upon the earlier concept by providing fast, automated feedback on the correctness and production readiness of your application every time there is a chance to code, infrastructure, or configuration. Some “best practices” Frequent commits to a common code stream Disallow commits into a “broken” build A “broken” build on CI should be attended to immediately and its resolutin should be of utmost priority Use cases]]></content>
      <tags>
        <tag>MobileInternet</tag>
        <tag>DevOps</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Setup Git in Mint Linux]]></title>
    <url>%2F2016-06-01-Setup-Git-In-Mint%2F</url>
    <content type="text"><![CDATA[How to setup Git in Mint Linux================================================= git config –global user.name “Todd Zhang” git config –global user.email phray.zhang@gmail.com git config –list git clone https://github.com/todzhanglei/todzhanglei.github.io git config –global credential.helper cache git config –global credential.helper ‘cache –timeout=36000’ To add remote1git remote add origin https://github.com/CloudsDocker/cloudsdocker.github.io.git Above command will add the remote URL with alias “origin” To pull specific branch1git pull origin blogSrc ErrorsError: The following untracked working tree files would be overwritten by checkout1git clean -d -fx ""]]></content>
      <tags>
        <tag>Git</tag>
        <tag>Mint</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python]]></title>
    <url>%2F2016-06-05-Python%2F</url>
    <content type="text"><![CDATA[(‘—–Unexpected error:’, &lt;type ‘exceptions.TypeError’&gt;) datetime.datetime.now() Python itemsTest item, MapReduce的设计灵感来自于函数式编程，这里不打算提MapReduce，就拿python中的map()函数来学习一下 Single Qutoe vs Double QuoteThere is no difference between using single quotes and double quotes in Python Generators presentation:http://www.dabeaz.com/generators/Generators.pdf 1234wwwlog=open('weblog.debug')bytecolumn=(line.rsplit(None,1)[1] for line in wwwlog)bytes=(int(x) for x in bytecolumn if x!='-')print "total:", sum(bytes) Generator as a pipelineAt each step, we declare an operation that will be applied to the entire input stream, like rsplit to all lines of the input log file. Rather than take a huge memory to process a huge file. The key is ‘think big’. Instead of focusing on the problem at a line-by-line level, you just break it down into big operations that operate on the whole file. Iteration is the gluersplitAPI docReturns a list of the words in the string, separated by the delimiter string (starting from right). 1234&gt;&gt;&gt; ' a b c '.rsplit(None, 1)[' a b', 'c']&gt;&gt;&gt; ' a b c '.rsplit(None, 2)[' a', 'b', 'c']]]></content>
      <tags>
        <tag>Coding</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Load Balancing]]></title>
    <url>%2F2016-06-07-Load-Balancing%2F</url>
    <content type="text"><![CDATA[ConceptsLVS means Linux Virtual Server, which is one Linux built-in component. Some logics of LVSNever Queue Shceduling The never queue scheduling algorithm adpots a two-speed model When there is an idel server avaiable, the job will be sent to the idel server, instead of waiting for a fast one. When there is no idel server avaiable, the job will be sent to the server that minimize it’s expected delay. ExamplesSetup LVS123ipvsadm -A -t 192.168.0.1:80 -s rripvsadm -a -t 192.168.0.1:80 -r 172.16.0.1:80 -mipvsadm -a -t 192.168.0.1:80 -r 172.16.0.2:80 -m The first command assign TCP port 80 on IP address 192.168.0.1 to the virtual server, the shceduling algorithm for load balancing is -s rr means using round-robin The 2nd and 3rd commands are adding IP addresss of real servers to the LVS setup the forwarded network packets shall be masked -m To query LVS status1234567ipvsadm -L -nIP Virtual Server version 1.0.8 (size=65536)Prot LocalAddress:Port Scheduler Flags -&gt; RemoteAddress:Port Forward Weight ActiveConn InActConnTCP 192.168.0.1:80 rr -&gt; 172.16.0.2:80 Masq 1 3 1 -&gt; 172.16.0.1:80 Masq 1 4 0 Strucutre of LVS in wikipedia]]></content>
      <tags>
        <tag>MobileInternet</tag>
        <tag>DevOps</tag>
        <tag>CTO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker Errors and Fixes]]></title>
    <url>%2F2016-06-09-Docker-Errors-Fixes%2F</url>
    <content type="text"><![CDATA[Docker Errors Cannot connect to the Docker daemon. Is the docker daemon running on this host?The solution is to run under root user, e.g. 1sudo docker run hello-world Docker service 123sudo service docker statussudo service docker startsudo docker run hello-world]]></content>
      <tags>
        <tag>DevOps</tag>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JavaScript tips]]></title>
    <url>%2F2016-06-12-JavaScript-tips%2F</url>
    <content type="text"><![CDATA[includes() vs some() The includes() method is used to check if a specific string exists in a collection, and returns true or false. Keep in mind that it is case sensitive: if the item inside the collection is SCHOOL, and you search for school, it will return false. The some() method checks if some elements exists in an array, and returns true or false. This is somewhat similar to the concept of the includes() method, the key diffence is the argument is a function but not a string. splice vs slicesplice: Join or connect (a rope or ropes) by interweaving the strands at the ends. ‘we learned how to weave and splice ropes’ slice: Cut (something, especially food) into slices. ‘slice the onion into rings’ The splice() method returns the removed item(s) in an array and slice() method returns the selected element(s) in an array, as a new array object. The splice() method changes the original array and slice() method doesn’t change the original array. The splice() method can take n number of arguments: Argument 1: Index, Required. An integer that specifies at what position to add /remove items, Use negative values to specify the position from the end of the array. Argument 2: Optional. The number of items to be removed. If set to 0(zero), no items will be removed. And if not passed, all item(s) from provided index will be removed. Argument 3…n: Optional. The new item(s) to be added to the array. -5 -4 -3 -2 -1 | | | | |var array4=[16,17,18,19,20]; | | | | | 0 1 2 3 4 12345 console.log(array4.splice(-2,1,"me"));// shows [19] console.log(array4);// shows [16, 17, 18, "me", 20] The slice() method can take 2 arguments: Argument 1: Required. An integer that specifies where to start the selection (The first element has an index of 0). Use negative numbers to select from the end of an array. Argument 2: Optional. An integer that specifies where to end the selection. If omitted, all elements from the start position and to the end of the array will be selected. Use negative numbers to select from the end of an array. 123var array=[1,2,3,4,5]console.log(array.slice(2));// shows [3, 4, 5], returned selected element(s). Observable is lazyRemember that observables are lazy — if we want to pull a value out of an observable, we must subscribe(). mergeAll vs mergeMap in reduxmergeAllWhen the inner observable emits, let me know by merging the value to the outer observable. Under the hood, the mergeAll() operator basically does takes the inner observable, subscribes to it, and pushes the value to the observer. Here is one sample: 1234567891011121314151617181920const click$ = Observable.fromEvent(button, ‘click’);const interval$ = Observable.interval(1000);const observable$ = click$.map(event =&gt; &#123; return interval$;&#125;);observable$.mergeAll().subscribe(num =&gt; console.log(num));Because this is a common pattern in Rx, there is a shortcut that achieves the same behaviour — mergeMap().const click$ = Observable.fromEvent(button, ‘click’);const interval$ = Observable.interval(1000);const observable$ = click$.mergeMap(event =&gt; &#123; return interval$;&#125;);observable$.subscribe(num =&gt; console.log(num)); more elegant, concise and flexible approach to check host string belongs to multiple value choicescheckStringAgainstMultipleLiteralValues.js 123if (host.match(/["uat" , "beta", "lab"].(api.)?yourdomain.(com.)?["au","io"]/)) &#123; console.log('matched')&#125; closure闭包就是一个函数引用另外一个函数的变量，因为变量被引用着所以不会被回收，因此可以用来封装一个私有变量。这是优点也是缺点，不必要的闭包只会徒增内存消耗！另外使用闭包也要注意变量的值是否符合你的要求，因为他就像一个静态私有变量一样。 In JavaScript, if you use the function keyword inside another function, you are creating a closure. Two one sentence summaries about closure closure is the local variable for a function — kept alive after the function has returned, or closure is a stack-frame which is not deallocated when the function returns (as if a ‘stack-frame’ were malloc’ed instead of being on the stack!). In JavaScript, if you declare a function within another function, then the local variables can remain accessible after returning from the function you called. Tips to redirect pageIt’s better to use window.location.replace(&quot;httpxxx&quot;)&#39;, rather than window.location.href=&quot;xxx&quot;. Becausereplace` will not save the page in the session history, so users won’t get stufy in never-ending back-button fiasco.]]></content>
      <tags>
        <tag>Coding</tag>
        <tag>JavaScript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git commands notes]]></title>
    <url>%2F2016-06-13-GIT%2F</url>
    <content type="text"><![CDATA[How to to list of what branch for a given commit1234$ git log --author todd --grep e2e$ git name-rev 78a4c4c340de70c726844e97233c58fa4738fea978a4c4c340de70c726844e97233c58fa4738fea9 remotes/origin/feature/term-deposit-protractors~1 To search keywords in js files only1$ find . -name '*.js' | xargs grep -r 'SearchKeywords' One line command to add and commit file1git status --short | awk '&#123;split($0, a);print a[2]&#125;' | xargs git add &amp;&amp; git commit -m 'summit status' to show files commited but not pushed1git diff --stat --cached origin/feature/BRANCH_NAME to view file content changed1git show PATH/FILE.sql One line to fetch, checkout newly created branch1git fetch &amp;&amp; git for-each-ref | grep 'AO-106' | awk '&#123;split($0,a);print a[3]&#125;' | awk "&#123;split($0,a, '/');print a[-1]&#125;" To get changed files NOT contains keyword1$ git status | grep -v "node_" -v is for reverse Cherry pickingCherry picking in Git is designed to apply some commit from one branch into another branch. It can be done if you eg. made a mistake and committed a change into wrong branch, but do not want to merge the whole branch. You can just eg. revert the commit and cherry-pick it on another branch. To use it, you just need git cherry-pick hash, where hash is a commit hash from other branch. Differences among Git and subversion The differences between git and other source control (like subversion) is Git have to merge conflict before upload to server, while subversion will merge conflicts at server side following lages are searchable in google alice gihub ErrorsWARN No layout: index.html Following errors when run hexo g12345678910111213141516171819Unhandled rejection Error: ENOENT: no such file or directory, open '/Users/todzhang/dev/git/blogSrc/themes/next/layout/_scripts/schemes/.swig' at Error (native) at Object.fs.openSync (fs.js:549:18) at Object.fs.readFileSync (fs.js:397:15) at Object.ret.load (/Users/todzhang/dev/git/blogSrc/node_modules/hexo/node_modules/swig/lib/loaders/filesystem.js:55:15) at compileFile (/Users/todzhang/dev/git/blogSrc/node_modules/hexo/node_modules/swig/lib/swig.js:695:31) at Object.eval [as tpl] (eval at &lt;anonymous&gt; (/Users/todzhang/dev/git/blogSrc/node_modules/hexo/node_modules/swig/lib/swig.js:498:13), &lt;anonymous&gt;:338:18) at compiled [as _compiledSync] (/Users/todzhang/dev/git/blogSrc/node_modules/hexo/node_modules/swig/lib/swig.js:619:18) at tryCatcher (/Users/todzhang/dev/git/blogSrc/node_modules/hexo/node_modules/bluebird/js/release/util.js:16:23) at null._compiled (/Users/todzhang/dev/git/blogSrc/node_modules/hexo/node_modules/bluebird/js/release/method.js:15:34) at View.render (/Users/todzhang/dev/git/blogSrc/node_modules/hexo/lib/theme/view.js:29:15) at /Users/todzhang/dev/git/blogSrc/node_modules/hexo/lib/hexo/index.js:387:25 at tryCatcher (/Users/todzhang/dev/git/blogSrc/node_modules/hexo/node_modules/bluebird/js/release/util.js:16:23) at /Users/todzhang/dev/git/blogSrc/node_modules/hexo/node_modules/bluebird/js/release/method.js:15:34 at RouteStream._read (/Users/todzhang/dev/git/blogSrc/node_modules/hexo/lib/hexo/router.js:134:3) at RouteStream.Readable.read (_stream_readable.js:336:10) at resume_ (_stream_readable.js:733:12) at nextTickCallbackWith2Args (node.js:442:9) at process._tickCallback (node.js:356:17) Solution:That’s because one extra space required after semi colon in _config.yml of Next, as following config 12# Duoshuo ShortNameduoshuo_shortname: cloudsdocker]]></content>
      <tags>
        <tag>Git</tag>
        <tag>GitPages</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RenMinBi International]]></title>
    <url>%2F2016-06-13-RQFII%2F</url>
    <content type="text"><![CDATA[RQFIIRQFII stands for Renminbi Qualified Foreign Institutional Investor. RQFII was introduced in 2011 to allow qualified foreign institutional investors to use RMB to buy securities such as bonds and shares and other investment instruments listed on the Chinese market. RQFII is an extension of the Qualified Foreign Institutional Investor (QFII) scheme established in 2002. What is the difference between QFII and RQFII?QFII (Qualified Foreign Institutional Investor) allows institutional investors to buy shares in mainland Chinese companies (A-shares) listed on stock exchanges in China. But they have to invest in their home currency, which is then converted. RQFII enables investors to buy into China using RMB. The government introduced it to facilitate the flow of offshore RMB into the country as the currency became more widely used globally. Who can use these investment programmes?Although China’s financial system is opening up, there are limits to who can invest. For a foreign institutional investor to qualify, it has to be licensed within its home country, demonstrate its capital strength, and have a track record in investments and asset management. How much can be invested?The Chinese government decides how much can be invested by allocating quotas to different countries. The limit is largely determined by demand. The RQFII quota for all countries is currently RMB970 billion (USD156 billion). Which countries have been granted an RQFII quota?Thirteen countries have been granted RMB quotas: Australia, Canada, Chile, France, Germany, Hong Kong, Hungary, Luxembourg, Qatar, South Korea, Singapore, Switzerland and the UK. The QFII programme is open to more than 60 countries.]]></content>
      <tags>
        <tag>RMB</tag>
        <tag>Finance</tag>
        <tag>FX</tag>
        <tag>Global Market</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Storage Management]]></title>
    <url>%2F2016-06-03-Storage-Management%2F</url>
    <content type="text"><![CDATA[RAIDRAID is Reductant Array Independent Disk, JBODJBOD is abbreviated from “Just a bunch of disks”, is an architecutre using multiple hard drives, but not in a RAID configuration, thus providing neither redundancy nor performance improvement.Hard drives may be handled independently as separate logical volumnes, or they may be combined into a single logical volume using a volume manager like LVM, such volumes are usually called “spanned” LVM‘LVM’ means Logical Volume Manager, is part of Linux kenel, is a device mapper. LVM is used to manage large hard disk farms by allowing disks to be added and replaced without downtime or service distruption.]]></content>
      <tags>
        <tag>MobileInternet</tag>
        <tag>CTO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Github page commands notes]]></title>
    <url>%2F2016-06-13-github-pages%2F</url>
    <content type="text"><![CDATA[404 error for customized domain (such as godday)12404There is not a GitHub Pages site here. Go to github master branch for gitpages site, manually add CNAME file following lages are searchable in google alice gihub Travis errors: Got following errors in Travis page: branch not included or excludedsolution: that’s because your source branch, such as ‘blogSrc’ should be added in whitelist of .travis.yml, for instance 123branches: only: - blogSrc fatal: empty ident nameBecause –global is requried when setting up travis , below is the sample 1git config --global user.email abc@def.com]]></content>
      <tags>
        <tag>Git</tag>
        <tag>GitPages</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SQL]]></title>
    <url>%2F2016-06-14-SQL%2F</url>
    <content type="text"><![CDATA[Differences between not in, not exists , and left join with null It seems to me that you can do the same thing in a SQL query using either NOT EXISTS, NOT IN, or LEFT JOIN WHERE IS NULL. For example: 123SELECT a FROM table1 WHERE a NOT IN (SELECT a FROM table2)SELECT a FROM table1 WHERE NOT EXISTS (SELECT * FROM table2 WHERE table1.a = table2.a)SELECT a FROM table1 LEFT JOIN table2 ON table1.a = table2.a WHERE table1.a IS NULL I’m not sure if I got all the syntax correct, but these are the general techniques I’ve seen. Why would I choose to use one over the other? Does performance differ…? Which one of these is the fastest / most efficient? (If it depends on implementation, when would I use each one?) answerIn a nutshell: NOT IN is a little bit different: it never matches if there is but a single NULL in the list. In MySQL, NOT EXISTS is a little bit less efficient In SQL Server, LEFT JOIN / IS NULL is less efficient In PostgreSQL, NOT IN is less efficient In Oracle, all three methods are the same.]]></content>
      <tags>
        <tag>DB</tag>
        <tag>Coding</tag>
        <tag>SQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[equity trading]]></title>
    <url>%2F2016-06-23-Equity%2F</url>
    <content type="text"><![CDATA[Difference between mutal funds and hedge fundsSimilarites:Both are managed portfolios. There are manager (or managers) pick securities by certain strategy or algorithm. Difference: Headge fund tend to aggressive, they take speculative postions in derviaties such as options are able to short sell. This iwll typically increase leverage (and also the risks). On the other hand, mutal fund are not permitted to take these highly leveraged postions and more safer as a result. Hedge funds are avaiable to a specific group of sophisticated investotrs with high net worth. While mutul fund are more accessibale with minimal amoutns of money. Differences among money markets and capital marketsThe money markets are used for the raising of short term finance, sometimes for loans that are expected to be paid back as early as overnight. Whereas the capital markets are used for the raising of long term finance, such as the purchase of shares, or for loans that are not expected to be fully paid back for at least a year.]]></content>
      <tags>
        <tag>fund</tag>
        <tag>finance</tag>
        <tag>Equity</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTTPS/2]]></title>
    <url>%2F2016-06-14-HTTP-HTTPS2%2F</url>
    <content type="text"><![CDATA[concepts HTTP : 传统 HTTP 采用明文，完全开放的编码，缺少加解密功能，很容易遭受窃取、篡改等安全威胁。尤其涉及在线交易的网站，遭遇攻击如同家常便饭。 HTTPS : HTTPS 协议传输过程全程加解密，相当于增加了一层 SSL/TSL 分层协议的 HTTP，让访问更加安全。 HTTPS2 : HTTP/2 本身是 HTTP 协议自 1999 年发表后的第一次更新，于 2015 年 5 月正式发布。掐指一算，这个协议积累了 16 年才更新，憋了这么久，放的一定是大招，它的更新主要体现在多路复用，二进制传输，使用 HPACK 压缩头部信息，服务端 Push 。]]></content>
      <tags>
        <tag>MobileInternet</tag>
        <tag>CTO</tag>
        <tag>HTTP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Something about authentication]]></title>
    <url>%2F2016-06-29-SOMETHING-ABOUT-AUTHENTICATION%2F</url>
    <content type="text"><![CDATA[It’s annoying to keep on repeating typing same login and password when you access multiple systems within office or for systems in external Internet. There are bunch of tools / technicles to cater for such. To my best knowledge, I’ll illustrate some as below. SSO (Single Sign On)After you successfully log into one system, when you hop onto other systems, so you’ll no need to furhter re-enter your user name and password, and you’ll in ‘logged in status’. Under the scene, its sync up your login information among systems. The transferred details is typically called ‘tickets’. One of the implementation logic is ‘kerberos’, which is one protocol developed by MIT and is widely used in such domain. In general, kerberos is supported by various systems and software, for instance, you log onto your windows desktop, your user name and password will be validated in LDAP via either client or API, you]]></content>
  </entry>
  <entry>
    <title><![CDATA[tips in as400 IBM Emulator]]></title>
    <url>%2F2016-06-29-TIPS-AS400-IBM-EMULATOR%2F</url>
    <content type="text"><![CDATA[Toggle crosshair Open IBM personal communication client Go to menu Edit-&gt;Preference-&gt;Apperance-&gt;Display settings-&gt;Ruler change Rule style to crosshair or turn it off]]></content>
      <tags>
        <tag>AS400</tag>
        <tag>IBM Personal Communications</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[heavy load web application]]></title>
    <url>%2F2016-06-29-heavy-load-web-app%2F</url>
    <content type="text"><![CDATA[Common solutionsWith cache + hash, seems nothing is impossible. hashWhen involving big data, heavey load and concurrent, the ultimate solution is hash, such as random insert, time complexity O(1) etc, the only downside is waste of space, however, storage is cheap recently. hash is one must-have for concurrent systems. cacheThere are many mature cache mechanism, such as KV DB like memcache, redis, and they are supporting clusters, operatibility is good as well. read-write segarationIn most cases, there are more read than write operation, so separate database to acccept write request to master DB, and redirect read request to slave DBs. At the same time, multiple salve DBs can be smoothly scale out, data replicate can leverage log bin. Database separation and shardingFor DB sharding, leverage hash to direct to corresponding DB shard. To ease the load on single host.]]></content>
      <tags>
        <tag>DevOps</tag>
        <tag>heavy load</tag>
        <tag>mobile internet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git raise error filename too long when checkout]]></title>
    <url>%2F2016-07-02-file-name-too-long-when-git-checkout%2F</url>
    <content type="text"><![CDATA[Symptoms:node_modules/grunt-contrib-imagemin/node_modules/pngquant-bin/node_modules/bin-wrapper/node_modules/download/node_modules/request/node_modules/form-data/node_modules/combined-stream/node_modules/delayed-stream/test/integration/test-handle-source-errors.js: Filename too long solution Execute following command 1git config core.longpaths true retry checkoutpap]]></content>
      <tags>
        <tag>DevOps</tag>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Useful bookmarks]]></title>
    <url>%2F2016-07-24-Bookmarks%2F</url>
    <content type="text"><![CDATA[eBookslist of various books Node.jsinstall nodejs without admin access]]></content>
  </entry>
  <entry>
    <title><![CDATA[Data Structure]]></title>
    <url>%2F2016-07-25-Data-Structure%2F</url>
    <content type="text"><![CDATA[Binary Tree A binary tree is a tree in which no node can have more than two children. A property of a binary tree that is sometimes important is that the depth of an averagebinary tree is considerably smaller than N. An analysis shows that the average depth is √O( N), and that for a special type of binary tree, namely the binary search tree, the average value of the depth is O(log N). Unfortunately, the depth can be as large as N − 1, Cyclic ListRound Robin scheduling using some form of an algorithm known as round-robin scheduling. A process is given a short turn to execute, known as a time slice, but it is interrupted when the slice ends, even if its job is not yet complete. doubly linked list a doubly linked list. These lists allow a greater variety of O(1)-time update operations, including insertions and deletions at arbitrary posi- tions within the list. Header and Trailer Sentinels a header node at the beginning of the list, and a trailer node at the end of the list. These “dummy” nodes are known as sentinels (or guards), and they do not store elements of the primary sequence. Advantages using sentinel Although we could implement a doubly linked list without sentinel nodes (as we did with our singly linked list in Section 3.2), the slight extra memory devoted to the sentinels greatly simplifies the logic of our operations. Most notably, the header and trailer nodes never change—only the nodes between them change. Furthermore, we can treat all insertions in a unified manner, because a new node will always be placed between a pair of existing nodes. In similar fashion, every element that is to be deleted is guaranteed to be stored in a node that has neighbors on each side. Equivalence relationequivalence relation in mathematics, satisfying the following properties: Treatment of null: For any nonnull reference variable x, the call x.equals(null) should return false (that is, nothing equals null except null). Reflexivity: Foranynonnullreferencevariablex,thecallx.equals(x)should return true (that is, an object should equal itself). Symmetry: For any non null reference variables x and y,the calls x.equals(y) and y.equals(x) should return the same value. Transitivity: For any nonnull reference variables x, y, and z, if both calls x.equals(y) and y.equals(z) return true, then call x.equals(z) must return true as well. Ring BufferIn other words, the circular buffer is well-suited as a FIFO buffer while a standard, non-circular buffer is well suited as a LIFO buffer.Circular buffering makes a good implementation strategy for a queue that has fixed maximum size. Ref to this wiki page]]></content>
      <tags>
        <tag>programming</tag>
        <tag>data strucutre</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Something about RESTful architect]]></title>
    <url>%2F2016-08-02-RESTful%2F</url>
    <content type="text"><![CDATA[REST API must be hypertext driverRoy’s interview Key points for a REST structureAPI designers, please note the following rules before calling your creation a REST API: A REST API should not be dependent on any single communication protocol, though its successful mapping to a given protocol may be dependent on the availability of metadata, choice of methods, etc. In general, any protocol element that uses a URI for identification must allow any URI scheme to be used for the sake of that identification. [Failure here implies that identification is not separated from interaction.] A REST API should not contain any changes to the communication protocols aside from filling-out or fixing the details of underspecified bits of standard protocols, such as HTTP’s PATCH method or Link header field. Workarounds for broken implementations (such as those browsers stupid enough to believe that HTML defines HTTP’s method set) should be defined separately, or at least in appendices, with an expectation that the workaround will eventually be obsolete. [Failure here implies that the resource interfaces are object-specific, not generic.] A REST API should spend almost all of its descriptive effort in defining the media type(s) used for representing resources and driving application state, or in defining extended relation names and/or hypertext-enabled mark-up for existing standard media types. Any effort spent describing what methods to use on what URIs of interest should be entirely defined within the scope of the processing rules for a media type (and, in most cases, already defined by existing media types). [Failure here implies that out-of-band information is driving interaction instead of hypertext.] A REST API must not define fixed resource names or hierarchies (an obvious coupling of client and server). Servers must have the freedom to control their own namespace. Instead, allow servers to instruct clients on how to construct appropriate URIs, such as is done in HTML forms and URI templates, by defining those instructions within media types and link relations. [Failure here implies that clients are assuming a resource structure due to out-of band information, such as a domain-specific standard, which is the data-oriented equivalent to RPC’s functional coupling]. A REST API should never have “typed” resources that are significant to the client. Specification authors may use resource types for describing server implementation behind the interface, but those types must be irrelevant and invisible to the client. The only types that are significant to a client are the current representation’s media type and standardized relation names. [ditto] A REST API should be entered with no prior knowledge beyond the initial URI (bookmark) and set of standardized media types that are appropriate for the intended audience (i.e., expected to be understood by any client that might use the API). From that point on, all application state transitions must be driven by client selection of server-provided choices that are present in the received representations or implied by the user’s manipulation of those representations. The transitions may be determined (or limited by) the client’s knowledge of media types and resource communication mechanisms, both of which may be improved on-the-fly (e.g., code-on-demand). [Failure here implies that out-of-band information is driving interaction instead of hypertext.] What’s hyptertextI have a slide in my REST talk that explains what hypertext (and hypermedia) means. Hypertext has many definitions:` Ted Nelson’s original definition was focused on non-linear documents. By ‘hypertext,’ I mean non-sequential writing — text that branches and allows choices to the reader, best read at an interactive screen. As popularly conceived, this is a series of text chunks connected by links which offer the reader different pathways. [Theodor H. Nelson] Later, hypertext became associated with a particular user interface mechanism. Hypertext is a computer-supported medium for information in which many interlinkeddocuments are displayed with their links on a high-resolution computer screen. [Jeffrey Conklin] When I say hypertext, I mean the simultaneous presentation of information and controls such that the information becomes the affordance through which the user (or automaton) obtains choices and selects actions. Hypermedia is just an expansion on what text means to include temporal anchors within a media stream; most researchers have dropped the distinction. Hypertext does not need to be HTML on a browser. Machines can follow links when they understand the data format and relationship types. Sample about web Think of it in terms of the Web. How many Web browsers are aware of the distinction between an online-banking resource and a Wiki resource? None of them. They don’t need to be aware of the resource types. What they need to be aware of is the potential state transitions — the links and forms — and what semantics/actions are implied by traversing those links. A browser represents them as distinct UI controls so that a user can see potential transitions and anticipate the effect of chosen actions. A spider can follow them to the extent that the relationships are known to be safe. Typed relations, specific media types, and action-specific elements provide the guidance needed for automated agents. Notworthy points REST is intended for long-lived network-based applications that span multiple organizations. If you don’t see a need for the constraints, then don’t use them. Don’t confuse application state (the state of the user’s application of computing to a given task) with resource state (the state of the world as exposed by a given service). They are not the same thing. A Sample of implementing REST via a distributed queuethe usecase of a distributed queue? The queue should not care about the media types it is exchanging between senders and receivers, yet it can quite easily satisfy the addressability, uniform interface, and statelessness constraints of a RESTful architecture. Can’t a service that is media-type agnostic be a RESTful API? Such a service would require assigning semantic meaning to a specific HTTP method and URI pattern and would not receive state transition information from its exchanged media types (as the service doesn’t care or need to know about the media types). I hope I am explaining myself well. For example:POST /{queue-name} sends a message to the queueGET /{queue-name}/top is the current message waiting to be processed. It returnsLocation: /messages/111DELETE /messages/111 tells server you are done processing that message. HATEOASDefinitions]]></content>
      <tags>
        <tag>RESTul</tag>
        <tag>Architect</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Business Analysis]]></title>
    <url>%2F2016-08-04-BA%2F</url>
    <content type="text"><![CDATA[Purpose of BA 带来一些商业价值（收益） 解决业务痛点 Best Parctices for BA/Architect 架构设计可以大而全，但实施过程最好是简化，化整为零，以最快速度先上线，通过迭代方式不断的完善。（_船小好调头_） 理想的架构应该像是App Store那样，设计成一个窗口，只要符合平台规范和协议就可以发布上去，关注是应用本身是否work，其他交给平台完成。 收益可以通过多种形式表现：提高产量、节约成本、挖掘用户价值、加强服务质量、优化用户体验、快速响应市场变化等 架构：将产品、技术、运营有机的结合起来 一个成功的互联网应用，背后一定是一群既懂得经营，又明白如何利用技术的业务和一群懂得驱动业务的技术大神们的故事。 任何一个产生生产力革命的行业一定是由业务和技术结合而产生的，比如网上订餐，叫车行业 Samples tips for online eCommerence From API to CRS (中央预定系统 Centeral Reservation System)]]></content>
      <tags>
        <tag>Architect</tag>
        <tag>BA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[iConnect]]></title>
    <url>%2F2016-08-04-iConnect%2F</url>
    <content type="text"><![CDATA[UIHTML5, AngularJS, BootStrap, REST API, JSON BackendHadoop core (HDFS), Hive, HBase, MapReduce, Oozie, Pig, Solr]]></content>
  </entry>
  <entry>
    <title><![CDATA[Anatomy of ThreadLocal]]></title>
    <url>%2F2016-08-05-Anatomy-of-ThreadLocal%2F</url>
    <content type="text"><![CDATA[Design philosophiesfast-path123456789101112131415161718/** * Get the entry associated with key. This method * itself handles only the fast path: a direct hit of existing * key. It otherwise relays to getEntryAfterMiss. This is * designed to maximize performance for direct hits, in part * by making this method readily inlinable. * * @param key the thread local object * @return the entry associated with key, or null if no such */ private Entry getEntry(ThreadLocal key) &#123; int i = key.threadLocalHashCode &amp; (table.length - 1); Entry e = table[i]; if (e != null &amp;&amp; e.get() == key) return e; else return getEntryAfterMiss(key, i, e); &#125; Magic number 0x61c88647This is a golden ratio,]]></content>
      <tags>
        <tag>java</tag>
        <tag>source code</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kibana, view layer of elasticsearch]]></title>
    <url>%2F2016-08-11-Kibana%2F</url>
    <content type="text"><![CDATA[What’s Kibanakibana is an open source data visualization plugin for Elasticsearch. It provides visualization capabilities on top of the content indexed on an Elasticsearch cluster. Users can create bar, line and scatter plots, or pie charts and maps on top of large volumes of data. Why Kibana Flexible analytics and visualization platform Real-time summary and charting of streaming data Intuitive interface for a variety of users Instant sharing and embedding of dashboards Referencedigital oceansetup ELK]]></content>
      <tags>
        <tag>DevOps</tag>
        <tag>elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kibana, view layer of elasticsearch]]></title>
    <url>%2F2016-08-11-Sentry%2F</url>
    <content type="text"><![CDATA[What’s Kibanakibana is an open source data visualization plugin for Elasticsearch. It provides visualization capabilities on top of the content indexed on an Elasticsearch cluster. Users can create bar, line and scatter plots, or pie charts and maps on top of large volumes of data. Why Kibana Flexible analytics and visualization platform Real-time summary and charting of streaming data Intuitive interface for a variety of users Instant sharing and embedding of dashboards Referencedigital oceansetup ELK]]></content>
      <tags>
        <tag>elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[unmodifiableList, unmodifiableSet,unmodifiableMap]]></title>
    <url>%2F2016-08-12-unmodifiableCollection%2F</url>
    <content type="text"><![CDATA[What’s itReturns an unmodifiable view of the specified set. This method allowsmodules to provide users with “read-only” access to internal sets.Query operations on the returned set “read through” to the specifiedset, and attempts to modify the returned set, whether direct or via itsiterator, result in an UnsupportedOperationException. Impelement logicIn general, it re-created a new Collection, and inherit read-only methods from superclass, while raise Exception for those modify functions. Here is the code snippet 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253static class UnmodifiableCollection&lt;E&gt; implements Collection&lt;E&gt;, Serializable &#123; private static final long serialVersionUID = 1820017752578914078L; final Collection&lt;? extends E&gt; c; UnmodifiableCollection(Collection&lt;? extends E&gt; c) &#123; if (c==null) throw new NullPointerException(); this.c = c; &#125; public int size() &#123;return c.size();&#125; public boolean isEmpty() &#123;return c.isEmpty();&#125; public boolean contains(Object o) &#123;return c.contains(o);&#125; public Object[] toArray() &#123;return c.toArray();&#125; public &lt;T&gt; T[] toArray(T[] a) &#123;return c.toArray(a);&#125; public String toString() &#123;return c.toString();&#125; public Iterator&lt;E&gt; iterator() &#123; return new Iterator&lt;E&gt;() &#123; private final Iterator&lt;? extends E&gt; i = c.iterator(); public boolean hasNext() &#123;return i.hasNext();&#125; public E next() &#123;return i.next();&#125; public void remove() &#123; throw new UnsupportedOperationException(); &#125; &#125;; &#125; public boolean add(E e) &#123; throw new UnsupportedOperationException(); &#125; public boolean remove(Object o) &#123; throw new UnsupportedOperationException(); &#125; public boolean containsAll(Collection&lt;?&gt; coll) &#123; return c.containsAll(coll); &#125; public boolean addAll(Collection&lt;? extends E&gt; coll) &#123; throw new UnsupportedOperationException(); &#125; public boolean removeAll(Collection&lt;?&gt; coll) &#123; throw new UnsupportedOperationException(); &#125; public boolean retainAll(Collection&lt;?&gt; coll) &#123; throw new UnsupportedOperationException(); &#125; public void clear() &#123; throw new UnsupportedOperationException(); &#125; &#125;]]></content>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[微服务]]></title>
    <url>%2F2016-08-24-microservices%2F</url>
    <content type="text"><![CDATA[可以想像一下，之前的传统应用系统，像是一个大办公室里面，有各个部门，销售部，采购部，财务部。办一件事情效率比较高。但是也有一些弊端，首先，各部门都在一个房间里。]]></content>
  </entry>
  <entry>
    <title><![CDATA[Spark-vs-Storm]]></title>
    <url>%2F2016-08-30-Spark-vs-Storm%2F</url>
    <content type="text"><![CDATA[The stark difference among Spark and Storm. Although both are claimed to process the streaming data in real time. But Spark processes it as micro-batches; whereas Storm processes per message - meaning if you intend to process things like social data, log data, etc.. you can actually apply CEP (Complex Event Processing) per tuples (i.e each social message in your example). Spark, on other hand is good at processing small blocks of data, for instance if you are streaming a whole blobs of data (say some huge files of medical record, for example). So, obviously, I would say, give your usecase, Storm may be a better fit for your needs. And last, between distributions, I have found Hortonworks to be more easier in standing up and managing the cluster than other distribution (please take this comment with a grain of salt, as I may be slightly biased towards HWX given my comfort-zone of working around this distribution more than others)]]></content>
      <tags>
        <tag>Spark</tag>
        <tag>Storm</tag>
        <tag>Apache</tag>
        <tag>Haddop</tag>
        <tag>BigData</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[http methods]]></title>
    <url>%2F2016-09-01-HTTP-Methods-RFC%2F</url>
    <content type="text"><![CDATA[RFC origion http://www.w3.org/Protocols/rfc2616/rfc2616-sec9.html#sec9.1.2) part of Hypertext Transfer Protocol – HTTP/1.1RFC 2616 Fielding, et al.9 Method Definitions The set of common methods for HTTP/1.1 is defined below. Although this set can be expanded, additional methods cannot be assumed to share the same semantics for separately extended clients and servers. The Host request-header field (section 14.23) MUST accompany all HTTP/1.1 requests. 9.1 Safe and Idempotent Methods 9.1.1 Safe Methods Implementors should be aware that the software represents the user in their interactions over the Internet, and should be careful to allow the user to be aware of any actions they might take which may have an unexpected significance to themselves or others. In particular, the convention has been established that the GET and HEAD methods SHOULD NOT have the significance of taking an action other than retrieval. These methods ought to be considered “safe”. This allows user agents to represent other methods, such as POST, PUT and DELETE, in a special way, so that the user is made aware of the fact that a possibly unsafe action is being requested. Naturally, it is not possible to ensure that the server does not generate side-effects as a result of performing a GET request; in fact, some dynamic resources consider that a feature. The important distinction here is that the user did not request the side-effects, so therefore cannot be held accountable for them. 9.1.2 Idempotent Methods Methods can also have the property of “idempotence” in that (aside from error or expiration issues) the side-effects of N &gt; 0 identical requests is the same as for a single request. The methods GET, HEAD, PUT and DELETE share this property. Also, the methods OPTIONS and TRACE SHOULD NOT have side effects, and so are inherently idempotent. However, it is possible that a sequence of several requests is non- idempotent, even if all of the methods executed in that sequence are idempotent. (A sequence is idempotent if a single execution of the entire sequence always yields a result that is not changed by a reexecution of all, or part, of that sequence.) For example, a sequence is non-idempotent if its result depends on a value that is later modified in the same sequence. A sequence that never has side effects is idempotent, by definition (provided that no concurrent operations are being executed on the same set of resources). 9.2 OPTIONS The OPTIONS method represents a request for information about the communication options available on the request/response chain identified by the Request-URI. This method allows the client to determine the options and/or requirements associated with a resource, or the capabilities of a server, without implying a resource action or initiating a resource retrieval. Responses to this method are not cacheable. If the OPTIONS request includes an entity-body (as indicated by the presence of Content-Length or Transfer-Encoding), then the media type MUST be indicated by a Content-Type field. Although this specification does not define any use for such a body, future extensions to HTTP might use the OPTIONS body to make more detailed queries on the server. A server that does not support such an extension MAY discard the request body. If the Request-URI is an asterisk (“*”), the OPTIONS request is intended to apply to the server in general rather than to a specific resource. Since a server’s communication options typically depend on the resource, the “*” request is only useful as a “ping” or “no-op” type of method; it does nothing beyond allowing the client to test the capabilities of the server. For example, this can be used to test a proxy for HTTP/1.1 compliance (or lack thereof). If the Request-URI is not an asterisk, the OPTIONS request applies only to the options that are available when communicating with that resource. A 200 response SHOULD include any header fields that indicate optional features implemented by the server and applicable to that resource (e.g., Allow), possibly including extensions not defined by this specification. The response body, if any, SHOULD also include information about the communication options. The format for such a body is not defined by this specification, but might be defined by future extensions to HTTP. Content negotiation MAY be used to select the appropriate response format. If no response body is included, the response MUST include a Content-Length field with a field-value of “0”. The Max-Forwards request-header field MAY be used to target a specific proxy in the request chain. When a proxy receives an OPTIONS request on an absoluteURI for which request forwarding is permitted, the proxy MUST check for a Max-Forwards field. If the Max-Forwards field-value is zero (“0”), the proxy MUST NOT forward the message; instead, the proxy SHOULD respond with its own communication options. If the Max-Forwards field-value is an integer greater than zero, the proxy MUST decrement the field-value when it forwards the request. If no Max-Forwards field is present in the request, then the forwarded request MUST NOT include a Max-Forwards field. 9.3 GET The GET method means retrieve whatever information (in the form of an entity) is identified by the Request-URI. If the Request-URI refers to a data-producing process, it is the produced data which shall be returned as the entity in the response and not the source text of the process, unless that text happens to be the output of the process. The semantics of the GET method change to a “conditional GET” if the request message includes an If-Modified-Since, If-Unmodified-Since, If-Match, If-None-Match, or If-Range header field. A conditional GET method requests that the entity be transferred only under the circumstances described by the conditional header field(s). The conditional GET method is intended to reduce unnecessary network usage by allowing cached entities to be refreshed without requiring multiple requests or transferring data already held by the client. The semantics of the GET method change to a “partial GET” if the request message includes a Range header field. A partial GET requests that only part of the entity be transferred, as described in section 14.35. The partial GET method is intended to reduce unnecessary network usage by allowing partially-retrieved entities to be completed without transferring data already held by the client. The response to a GET request is cacheable if and only if it meets the requirements for HTTP caching described in section 13. See section 15.1.3 for security considerations when used for forms. 9.4 HEAD The HEAD method is identical to GET except that the server MUST NOT return a message-body in the response. The metainformation contained in the HTTP headers in response to a HEAD request SHOULD be identical to the information sent in response to a GET request. This method can be used for obtaining metainformation about the entity implied by the request without transferring the entity-body itself. This method is often used for testing hypertext links for validity, accessibility, and recent modification. The response to a HEAD request MAY be cacheable in the sense that the information contained in the response MAY be used to update a previously cached entity from that resource. If the new field values indicate that the cached entity differs from the current entity (as would be indicated by a change in Content-Length, Content-MD5, ETag or Last-Modified), then the cache MUST treat the cache entry as stale. 9.5 POST The POST method is used to request that the origin server accept the entity enclosed in the request as a new subordinate of the resource identified by the Request-URI in the Request-Line. POST is designed to allow a uniform method to cover the following functions: - Annotation of existing resources; - Posting a message to a bulletin board, newsgroup, mailing list, or similar group of articles; - Providing a block of data, such as the result of submitting a form, to a data-handling process; - Extending a database through an append operation.The actual function performed by the POST method is determined by the server and is usually dependent on the Request-URI. The posted entity is subordinate to that URI in the same way that a file is subordinate to a directory containing it, a news article is subordinate to a newsgroup to which it is posted, or a record is subordinate to a database. The action performed by the POST method might not result in a resource that can be identified by a URI. In this case, either 200 (OK) or 204 (No Content) is the appropriate response status, depending on whether or not the response includes an entity that describes the result. If a resource has been created on the origin server, the response SHOULD be 201 (Created) and contain an entity which describes the status of the request and refers to the new resource, and a Location header (see section 14.30). Responses to this method are not cacheable, unless the response includes appropriate Cache-Control or Expires header fields. However, the 303 (See Other) response can be used to direct the user agent to retrieve a cacheable resource. POST requests MUST obey the message transmission requirements set out in section 8.2. See section 15.1.3 for security considerations. 9.6 PUT The PUT method requests that the enclosed entity be stored under the supplied Request-URI. If the Request-URI refers to an already existing resource, the enclosed entity SHOULD be considered as a modified version of the one residing on the origin server. If the Request-URI does not point to an existing resource, and that URI is capable of being defined as a new resource by the requesting user agent, the origin server can create the resource with that URI. If a new resource is created, the origin server MUST inform the user agent via the 201 (Created) response. If an existing resource is modified, either the 200 (OK) or 204 (No Content) response codes SHOULD be sent to indicate successful completion of the request. If the resource could not be created or modified with the Request-URI, an appropriate error response SHOULD be given that reflects the nature of the problem. The recipient of the entity MUST NOT ignore any Content-* (e.g. Content-Range) headers that it does not understand or implement and MUST return a 501 (Not Implemented) response in such cases. If the request passes through a cache and the Request-URI identifies one or more currently cached entities, those entries SHOULD be treated as stale. Responses to this method are not cacheable. The fundamental difference between the POST and PUT requests is reflected in the different meaning of the Request-URI. The URI in a POST request identifies the resource that will handle the enclosed entity. That resource might be a data-accepting process, a gateway to some other protocol, or a separate entity that accepts annotations. In contrast, the URI in a PUT request identifies the entity enclosed with the request – the user agent knows what URI is intended and the server MUST NOT attempt to apply the request to some other resource. If the server desires that the request be applied to a different URI, it MUST send a 301 (Moved Permanently) response; the user agent MAY then make its own decision regarding whether or not to redirect the request. A single resource MAY be identified by many different URIs. For example, an article might have a URI for identifying “the current version” which is separate from the URI identifying each particular version. In this case, a PUT request on a general URI might result in several other URIs being defined by the origin server. HTTP/1.1 does not define how a PUT method affects the state of an origin server. PUT requests MUST obey the message transmission requirements set out in section 8.2. Unless otherwise specified for a particular entity-header, the entity-headers in the PUT request SHOULD be applied to the resource created or modified by the PUT. 9.7 DELETE The DELETE method requests that the origin server delete the resource identified by the Request-URI. This method MAY be overridden by human intervention (or other means) on the origin server. The client cannot be guaranteed that the operation has been carried out, even if the status code returned from the origin server indicates that the action has been completed successfully. However, the server SHOULD NOT indicate success unless, at the time the response is given, it intends to delete the resource or move it to an inaccessible location. A successful response SHOULD be 200 (OK) if the response includes an entity describing the status, 202 (Accepted) if the action has not yet been enacted, or 204 (No Content) if the action has been enacted but the response does not include an entity. If the request passes through a cache and the Request-URI identifies one or more currently cached entities, those entries SHOULD be treated as stale. Responses to this method are not cacheable. 9.8 TRACE The TRACE method is used to invoke a remote, application-layer loop- back of the request message. The final recipient of the request SHOULD reflect the message received back to the client as the entity-body of a 200 (OK) response. The final recipient is either the origin server or the first proxy or gateway to receive a Max-Forwards value of zero (0) in the request (see section 14.31). A TRACE request MUST NOT include an entity. TRACE allows the client to see what is being received at the other end of the request chain and use that data for testing or diagnostic information. The value of the Via header field (section 14.45) is of particular interest, since it acts as a trace of the request chain. Use of the Max-Forwards header field allows the client to limit the length of the request chain, which is useful for testing a chain of proxies forwarding messages in an infinite loop. If the request is valid, the response SHOULD contain the entire request message in the entity-body, with a Content-Type of “message/http”. Responses to this method MUST NOT be cached. 9.9 CONNECT This specification reserves the method name CONNECT for use with a proxy that can dynamically switch to being a tunnel (e.g. SSL tunneling [44]).]]></content>
      <tags>
        <tag>http</tag>
        <tag>rfc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[漫谈开发设计中的一些‘原则’及'设计哲学']]></title>
    <url>%2F2016-09-02-Design-Principals%2F</url>
    <content type="text"><![CDATA[在开发设计中有一些常用原则或者潜规则，根据笔者的经验，这里稍微总结一下最最常用的，以飨读者。 POLAThe principle of least astonishment (POLA) is: “If a necessary feature has a high astonishment factor, it may be necessary to redesign the feature.” In general engineering design contexts, the principle means that a component of a system should behave in a way that users expect it to behave; that is, users should not be astonished by its behavior. A textbook formulation is: “People are part of the system. The design should match the user’s experience, expectations, and mental models.” In more practical terms, the principle aims to leverage the pre-existing knowledge of users to minimize the learning curve, for instance by designing interfaces that borrow heavily from “functionally similar or analogous programs with which your users are likely to be familiar”. Example:A web site could declare an input that should autofocus when the page is loaded,[8] such as a search field (e.g., Google.com), or the username field of a login form. DRY这里的DRY是Do Not Repeat Yourself的缩写。具体解释参见 ,严谨的定义是 Every piece of knowledge must have a single, unambiguous, authoritative representation within a system，意思是：任何一部分知识在系统中必须只有单一的，清晰并且权威的展示。？？？这是啥意思，没懂。简单说就是不要重复自己任何一部分的工作。比如说，有一段代码是用于清除字条串中的HTML符号，在多个程序中会用到此功能，如果每个地方都使用如下代码 123html = html.replaceAll("\\&lt;.*?&gt;","") html = html.replaceAll("&amp;nbsp;","");html = html.replaceAll("&amp;amp;".""); 如果是只有２，３个地方用到(Martin曾经提到过Rule of three,意思是一段代码如果被copy３次以上就应该重构到一个单独的子方法中)，你可能就直接复制过来用，但是想想是２，３百个地方用到呢？如果上面需要再做个修改(如下)，你是不是要去这个２，３百个地方去改代码。 12html = html.replaceAll("&amp;lt;"."&lt;");html = html.replaceAll("&amp;gt;"."&gt;"); 所以这个DRY的规则就是推荐使用 子方法 的方式，这样只需要修改一次即可. 与之类似的编程思想有 DIE（Duplication is Evil）,SPoT(Single Point of Truth), SSOT (Singel Source of Truth) 。 题外话，和DRY对应的是WET,意思是 “write everything twice”（任何东西都写两遍）或者”we enjoy typing” （我们就是喜欢打字编码）。 :-)。 KISSKISS 是 Keep it simple, stupid （或者Keep it short and simple ）的简称。意思是在设计时保持简约，通俗。这个很像是现在推畅的“极简风”。使用KISS有什么好处呢？如下是其中的一些： 你可以更快的解决更多的问题 你可以使用更少的代码来解决复杂的问题 你可以写出更高质量的代码 你可以创建更大的系统，更好的去运维 你的代码将更加灵活，当有新需求时可以更好的支持扩展，修改或者重构 等等 在软件设计领域， 有一些技术具体实现这个精髓，比如 DDD (Domain Driven Design),TDD (Test Driven Develop),这个使代码集中在真正需要的功能上，而不需要其他额外的。另外一个建议是 不要试图通过注释来提高代码的可读性，而应该从代码本身提高。比如如下是不太好的变量定义 12// i is for 'counter' and j means total sumint i, j; 而如下是好的设计 12// more intuitive oneint counter,sum; 与此相呼应的是称作 奥卡姆剃刀 或者 简约之法则： Occam’s razorThe simplest (explanation|solution) is usually the best one.往往最简单的解决方案是最好的解决方案 具体到以Java为例的程序设计，如下有一些实践KISS的建议 首先，认清自己，不要认为自己是个天才，这往往是你犯的第一个错。 把你的工作打散成几个子工作，每个部分不会耗费超过4-12个小时去完成 把一个问题分成几个小的子问题，每个问题可以通过1个或者只要几个类就能解决。 保持每个方法只做一件事，并且不要超过30-40行的代码量 保持每个类的体积不要太大。 不要害怕扔掉不用的代码。就像家里用不到的东西就及时扔掉一样。 New Jersey style （Worse is better）新泽西风格，也叫做“Worse is better”。此原则指出，系统的质量不会因为新功能的增多而提高。 比如一个软件，只提供一些功能，但是用户很方便使用，有可能比一些提供了非常多令人眼花缭乱功能的“大杂烩”似的软件。比如像 Linux下面的 vi/vim, 浏览器中的Chrome. SOLIDSOLID是几个编程哲学的总称，即 SOLID (Single responsibility, Open-closed, Liskov substitution, Interface segregation and Dependency inversion) ，下面我们分别解释一下： Single responsibility （SRP）单一功能原则。Robert描述这个为“A class should have only one reason to change.”，即修改一个类（或者模块）有（且只能有）一个理由。简单说就是 一个类或者模块只能负责一个功能。举个例子，有一个模块负责生成一个报表，可以想像可能有两个理由去修改此模块，第一，报表的内容要变，第二，报表的格式要改。这两个改动是出于不同的理由，一个是内容的一个美化版面的。 而 “单一职责” 规则认为这是两个不同的职责，因此应该分成两个不同的子模块。如果把两件事情放在一起，并且不同的改动是出于不同的原因，这种设计是不好的。此规则方便系统各模块间解耦合。 Open/closed principle （OCP）开闭原则。Bertrand描述为“”software entities (classes, modules, functions, etc.) should be open for extension, but closed for modification”;”，也就是说对于一个实体（类，模块，方法等）允许在不修改源代码的前提下扩展它的功能行为。即，你可以把 新代码放到新的类或者方法中，新类通过继承来重用已有代码及功能。而 已有的代码只有在修bug时才去修改。 此原则主要用于降低在添加新加功能时引入新的bug的风险。 The Liskov Substitution Principle （LSP）里氏替换原则. 原文是 “Derived classes must be substitutable for their base classes.”，意思是，派生类（子类）对象能够替换其基类（超类）对象被使用。 比如说，如果 S 是T 的子类， 那么任何T类的具体实现对象都可以替换S的实现对象出现的地方，具体的调用者也不知道具体是父类还是子类，还不会出现任何错误。比如下图，调用者可以2来替换1的地方 。 Interface segregation principle （ISP）接口隔离。原文是 many client-specific interfaces are better than one general-purpose interface. 意思是多个特定客户端接口要好于一个宽泛用途的接口。Make fine grained interfaces that are client specific. 应该定义粒度合适的一系列接口(像下图)，以便于每个客户去实现具体的功能请求。换句话说是，客户（client）不应该必须去依赖于它用不到的功能方法。此原则的目的是系统解开耦合，从而容易重构，更改和重新部署。 Dependency inversion principle (DIP)依赖反转原则. 原文是 One should “Depend upon Abstractions. Do not depend upon concretions.” 意思是 一个方法应该遵从“依赖于抽象而不是一个实例”。该原则规定： 高层次的模块不应该依赖于低层次的模块，两者都应该依赖于抽象接口。 抽象接口不应该依赖于具体实现。而具体实现则应该依赖于抽象接口。这个就像是设计模式中的Adaptor适配器模式。下图解释了这个原理。图1中，高层对象A依赖于底层对象B的实现；图2中，把高层对象A对底层对象的需求抽象为一个接口A，底层对象B实现了接口A，这就是依赖反转。 SOCSeparation of concerns,?即关注点分离。 是处理复杂性的一个原则。由于关注点混杂在一起会导致复杂性大大增加，所以能够把不同的关注点分离开来，分别处理就是处理复杂性的一个原则，一种方法。这个与SOLID中的 SRP很类似。 YANGI是”You aren’t gonna need it”的缩写，直译是“你将来用不到它的”。这个是极限编程的一个编程思想。意思是说,永远不要因为 预计你会用到某个功能就去写一段代码去实现，总是只有问题出现了，真的需要这个功能时才去写。 参考 https://en.wikipedia.org/wiki/Principle_of_least_astonishment DRY 六大设计原则–里氏替换原则【Liskov Substitution Principle】 SOLID how to keep code simple 奥卡姆剃刀 Apache KISS Worse is better]]></content>
      <tags>
        <tag>development</tag>
        <tag>desgin</tag>
        <tag>principals</tag>
        <tag>MyBlog</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes 与 Docker Swarm的对比]]></title>
    <url>%2F2016-09-04-Kubernets-vs-Swarm%2F</url>
    <content type="text"><![CDATA[Kubernetes 和Docker Swarm 可能是使用最广泛的工具，用于在集群环境中部署容器。但是这两个工具还是有很大的差别。 KubernetesGoogle根据其在Linux上容器管理经验，改造到docker管理上，就是kubernetes。他的在许多方面表现良好。最重要的是构造于Google多年的宝贵经验只上。 如果你从docker1.0以上开始使用kubernetes，你得到的用户体验会非常良好。比如你会发现kubernetes解决一些docker自身的问题。例如你可以mount（绑定）持久化存储卷（volume），以便于在迁移docker时不至于丢失数据。 kubernetes使用flannel（一个使用go写的虚拟网络的开源系统）构造容器间的网络通信。它还内置有负载均衡。除此之外，它的“服务发现”使用了etcd（一个使用golang编写的开源虚拟网络系统）。然而，使用kubernetes是有代价的。首先，它用一个不同的命令行接口，不同的编程接口及不同的YAML文件定义等。换言之，你不能使用docker命令行接口也不能用docker compose来定义容器。为了使用kubernetes，所有所有的东西都需要从头开始。这就好像这个工具并不是为了docker写的一样（这个在某种程度上确实是）。kubernetes把集群带到了一个全新的高度，代价是学习曲线比较陡。 Docker Swarmdocker-swarm 使用了一个不同的方式。它是docker原生的集群工具。 最方便的部分是它暴露了docker标准的编程接口，意味着你之前一直在使用的任何与docker沟通的工具（docker命令行接口，docker compose，dokku，krane等等），都可以无缝的在docker swarm上使用。 这个其实是个双刃剑，毁誉参半。一直可以使用自己得心应手熟悉的工具，这再好不过了，然而，这样意味着我们又被docker紧紧的“耦合”了（而非业界一直追求的松耦合”）。如果你需要的都能满足，那皆大欢喜。可是如果不是呢，要是你有特殊需求这些API满足不了怎么办？这是就不得不去耍一些“小聪明”。 我们来仔细剖析下这两个工具，主要会从如何在集群中运行容器，这两个工具分别如何安装以及他们提供的功能。 安装设置安装设置swarm非常简单，简单明了并且很灵活。我们需要做的就是安装一个服务发现工具，然后在所有的节点上安装swarm容器。因为它自己就是以一个docker容器来部署的，因此它在不同的操作系统上运行方式都是没有差别的。我们运行swarm容器，开放一个端口，然后告知服务发现模块的地址。这不能再简单了。我们甚至可以在没有任何服务发现模块的情况下开始使用，来看看我们是否喜欢它，当开始真正认真的要使用时再去添加etcd，consul或者其他支持的工具。 相比较而言，kubernetes的安装就有点复杂晦涩了。不同的操作系统上安装都不同。每个操作系统都有自己的独立安装指令和运维团队。比如说，如果你选择使用vagrant来试用，然后在Fedora这里遇到问题卡住了，但这不是意味着其他的（比如Ubuntu或者coreos）也不能用。你可以，不过要开始在kubernetes官方以外到处搜索. 你所需要的很可能在某个社区论坛里提供的解决方案，但是你需要时间去查询，运气好的话可以在第一次尝试就能用。一个严重的问题是安装kubernetes依赖于bash脚本。如果我们是处于配置管理不是必须的情况下是，这个本身可能不是大问题。我们可能不希望运行一个脚本，而是希望kubernetes成为puppet，chef或者ansible的一部分。同样，这个也是可以解决的。你可以找到ansible 的安装手册来动行kubernetes或者你自己去写。跟swarm比这些都不是什么大问题,只是一点点的小痛苦而已。使用刀砍请不要期待任何的安装文档，除非都可使用docker命令行的时候运行的参数而已。我们打算去运行容器，swarm可以实现这个，但kubernetes 没有。有些人可能并不在意具体是使用哪个服务发现的工具。我喜欢swarm背后提倡的那种极简风格，以及他背后的逻辑，内置电池，拆留由已。任何东西都是拆箱可用但是我们还提供了选项让你去替换其中的任一个模块。 与swarm不同的是，kubernetes 是一个可配置的工具。你需要跟kubernetes提供的各个选项来共生存。例如，如果你打算使用kubernets,你必须要使用etcd.我不是说etcd不好(实际上正好相反)，但是如果你习惯于，比如你在非常复杂的业务场景下使用consul，如果要使用服务发现，必须特别针对kubernets使用一个，而剩下的其他部分使用其他一个产品。另外一个对使用Kubernets觉得不方便的地方就是你需要在事先了解各种事情。比如，你需要告诉他要用到的所有节点的地址，每个节点的职责是什么,这个集群有多少个“小黄人” (minion,是kubernet对于一个集群中机器之前叫的名字)，等等。而使用Swarm，我们只需要启动一个节点，告诉他加入网络，就可以了。我们不需要提前告诉关于集群其他的信息，因为这些信息会通过各个节点间的 “八卦”（gossip protocol）来传输。 配置本身也许并不是这些工具之间最大的差别。不管你使用哪个工具，或早或晚，所有都会配置好并运行起来，到时候你们就会忘掉在配置时的痛苦经历。你可能会说我们之所以选择某个工具就是因为它安装配置简单吧。很公平的。我们继续往下讨论如何定义容器及之上的这些工具。 运行容器如果使用Swarm来运行Docker容器，你如何去定义所有的参数呢？实际上你根本不需要！你所做的跟使用Swarm之前没有什么不同。比如，你习惯于使用Docker CLI（命令行接口），你可以继续使用几乎相同的命令。如果你习惯于使用Docker Componse来运行容器，你可以继续在Swarm集群中使用。不管你之前习惯于怎么使用容器，你仍旧可以使用，只是在更大级别的集群中使用。 Kubernetes要求你去学习它自己的CLI（命令行接口）和配置。你不能使用你之前创建的docker-compose.yml配置，你必须要去新建与Kubernetes对应的配置。你也不能使用之前学习的Docker CLI（命令行接口）。你必须要去学习 Kubernetes CLI（命令行接口），并且很有可能，确保你整个公司机构也要去学习。 不管你选择哪个工具来部署集群，你都要先熟悉Docker。你可能已经习惯于使用 使用Docker Compose来定义你运行容器的参数。如果你曾经使用它超过几个小时，你可能就会直接使用它而扔掉Docker CLI。你可以使用它运行容器，跟踪日志变化，等等。另外一方面，你可能是Docker的 “死忠”，看不上 Docker Compose，而是任何事都使用Docker CLI，或者你甚至自己写bash脚本来运行容器。不管哪种方式，这些都可以在Docker Swarm上使用。 如果你选择Kubernetes，请先准备好同一件事需要有多个定义配置。你需要使用 Docker Compose来运行Kubernetes范围外的容器。开发人员需要继续在他们的笔记本电脑上运行容器，你的测试环境可能也不是一个大集群，等等。换言之，如果你选择了Docker，那么 Docker Compose 和 Docker CLI将是不可避免的。你肯定会在某些地方或者以某种方式用到它们。一旦你开始使用 Kubernetes，你就会发现你所有的 Docker Compose的配置都要翻译成 Kubernetes的方式，从这个时候，你也要开始维护这两个版本了。使用 Kubernetes，这些重复的事情意味着维护成本的提高。重复只是一个问题，另外的是你在集群中运行的命令将于集群外使用的命令不一样了。你之前学习并喜爱的Docker的所有命令在 Kubernetes集群内将是完全行不通了。 Kubernetes的开发团队强制你去按照他们的办事方式行事，其实不是为了让你过的那么苦。如此巨大差别的主要原因是 Swarm 和 Kubernetes针对同一问题采取了不同的处理手段。 Swarm团队决定使用跟Docker相同的API接口，因此我们看到这些之前有如此好的兼容性。结果就是，你可以使用几乎所有之前的东西，只是在更大级别的集群中使用而已。没有新的东西要去做，不需要去重复配置参数，也不需要去新学习什么。不管你是直接使用Docker CLIgipj使用Swarm，这些API接口是基本上一致的。不好的方面是如果你想Swarm去做某件事，但是这个在Docker API中没有，这样你就悲催了。简单来说，如果你在找一个工具，可以部署使用Docker API的容器到集群中，那么 Swarm就是解决方案。另一方面，如果你想要一个工具，可以解决Docker API办不到的事情，你就应该去试试 Kubernetes了。这就是功能性（ Kubernetes）VS. 简易性（Swarm）。 这里唯一还没有回答的问题就是这些限制是什么。主要的限制中有两个，网络配置和持续化存储卷。走到Swarm1.0，我们不能连接运行于不同服务器上的容器。事实上，现在我们仍然不能连接他们，但是我们可能通过跨主机网络来协助连接运行于不同服务器上的容器。这是个非常强大的功能。而 Kubernetes使用 flannel（一个使用go写的虚拟网络的开源系统）来实现网络路由。目前自从Docker1.0开始， 这个功能也成为Docker CLI的一部分了。 另外一个问题是持续化存储卷。Docker在1.9版本中引入此功能。直到最近，如果你持久化一个数据卷，这个容器就绑定到这个持续化卷所在的服务器上了。它不能再次移动，除非你使用一些恶心的小花招，比如在不同的服务器间复制这些数据卷文件。这些本身是一些比较慢的操作，这与Swarm等工具的初衷也是相违背的。即便你有时间去复制，你也不知道从哪里去复制，因为集群工具会认为你整个数据中心是一个实体。你的容器会部署到它认为最合适的地方（比如，运行最少容器，CPU或者内容使用率最低，等等）。现在已经有Docker内置的持久化卷了。网络和持久化卷缺失曾经是许多人放弃Swarm而去选择 Kubernetes。自从Docker1.9，这此已经成为过去式。 选择当需要在Docker Swarm 和 Kubernetes做出选择时，可以考虑如下几点。你是否想依赖于Docker自己来解决集群的问题。如果是，选择Swarm。如果某些功能在Docker中不支持，那它也非常可能在Swarm中找不到，因为Swarm是依赖于Docker API的。另外一方面，如果你想要一个工具可以解决Docker的限制，Kubernetes将是不错的选择。Kubernetes不是基于Docker，而是基于Google多年对于管理容器的经验。它是按照自己的方式来行事。 真正的问题是Kubernetes这种自我的方式（与Docker非常的不同）相比于它提供的优点是否值得。或者，我们是不是应该押宝在Docker本身上，期待Docker将来会解决这些难题。在回答这些问题之前，请先看一下Docker1.9之后的版本。它已经有个网络配置及持久化卷。也有了所谓的“除非死掉 才去重启”的策略，这次方便去管理那些讨厌的错误。现在Kubernetes 和 Swarm之间的差异又少了3个。实际上，现如今，Kubernetes 相对于 Swarm的优势很少了。另一方面，Swarm使用了Docker API意味着你可以共用命令的配置。个人认为，我倾向于押宝于Docker引擎变得越来越好，这样Docker Swarm也会受益。这两者之间的差异已经非常小。两个都是可用于生产环境，但是Swarm更易于去配置，易于使用，并且可以重用在上集群之前的配置，不需要在集群和非集群环境下重复工作。 我个人的建议是使用Docker Swarm。而 Kubernetes太“任性”了，不易于配置，与Docker CLI，API差别太大，并且在Docker1.0之后，相对于Swarm来说没有太多的优势。他们之间其他的差距影响真的是不太大。但是Docker Swarm更易于配置。 其他 使用kubernetes的好处是在其前后基于Google对container管理几十年的经验，比如Borg。 使用某个特定容器其实并不是特别重要的事，最主要的还是集群技术。kubernetes类似于数据库操作领域的hibernate/ibates，即解耦合。在需要的时候我们可以使用Rocket替换Docker，对使用者透明。kubernetes这个功能在swarm中是找不到的。在生产环境中，如果需要，对于上层是透明的，没有任何影响。 不得不承认，Swarm更多的是一个部署工具，而Kubernetes是用于HA（高可用）架构的大规模的编配平台。 Kubernetes是介于中间的产品，没有Swarm那么简单友好，也没有Mesos那么功能强大。因此很有可能是在Mesos上层使用Docker，而非与Kubernetes集成。 人们使用Mesos是因为它本身不是为了Docker或者容器设计的，它只是个集群抽象层。人们用就是因为它是唯一一个既支持部署应用程序又可以同时管理hadoop。（将来有可能不一定） 如果我是个开发人员，我会选择 Compose+Swarm,它简单易用。但是如果我是CTO，我会选择k8s，因为这样我就不用关于Docker API的兼容性问题了。 原文地址 ： https://technologyconversations.com/2015/11/04/docker-clustering-tools-compared-kubernetes-vs-docker-swarm/]]></content>
      <tags>
        <tag>DevOps</tag>
        <tag>Mac</tag>
        <tag>shortcut</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Head First Blockchina 1]]></title>
    <url>%2F2016-09-11-head-first-blockchian-1%2F</url>
    <content type="text"><![CDATA[深入浅出区块链系统：第一章.what you should know about blockchain 考虑的大家现在很多都是碎片化阅读，不知道大家如何，反正如果我是在在只有很短一段时间里，不太容易切换状态静下心来读一篇洋洋洒洒的长文章。因此此系统会保持短小精悍，把整套分成一系列小文章，每个文章有分成若干个段（sections）。既KISS原则，查看这里什么是KISS. 目前区块链（blockchain）应该是在最近比较火的新技术之一了，这个不光在银行金融行业，也在其他诸多行业逐渐成为热点。区块链基于被认为是继互联网之后最重要的技术发明之一。看到过很多关于区块链的文章，要么是太过于学术，要么又局限于某个行业领域。对于一般人来说不太容易去理解其所以然。因此，笔者提笔自己写一个关于区块链的系列文章，以方便大家。 比特币说到区块链就不得不提比特币。在进入讨论blockchain之前，先上张图片。 这个是在说雷曼兄弟公司的破产，背景是发生于2008年金融危机。当时被媒体及世人厌恶的贪婪，低效的传统金融体系垮塌，与此同时，不知道是不是巧合，比特币（bitcoin）诞生了（其实电子货币已经有几十年的历史了）。 比特币，就像是美元，人民币一样是个货币，只是这个是数字化的货币，没有一个具体的国家或者机构管理。既然是货币就要拿来用，要流通。当有任何变动，就会产生所谓的 money in money out， balance changed。即这些比特币的变动，最新的余额是多少，等都要记录下来。在现实世界中，这些记录在具体你的每个银行账户中，由一些监管机构监视并确保准确性。 但是比特币是个电子货币，没有一个具体的营业监管机构。怎么破？ 这时区块链就被发明出来（大约是在2009年），区块链起源于比特币，就是当比特币从一个人转移到另一个人时，用于记录这些变动。换句话说，区块链（Blockchain） 就是比特币（bitcoin）的不可变动（immutable）的记账系统。 记账有人可能会觉得“记账系统”太过于笼统，不太明白，因此首先这里说一下什么是记账，举个例子，你在淘宝上买东西，整个过程需要有多个记账操作，包括可能需要在购物车里添加一条记录，你买好了需要在商家那里记一笔账，然后支付时需要记下一笔，快递再需要记录一笔。如果你买的不是一般的小东西，比如是一个房子，那样还要在相关管理部门还要记录一笔。有没有发现这个是非常低效的，需要花费很多的重复资源的过程？换句话说，这些低效都是最终都要转嫁到我们消费者头上。区块链却在设计之初很好的解决上面这些弊端。 什么是区块链区块链是专门针对比特币设计开发的记账系统，用于所有比特币的记账。因为区块链本身良好的设计，区块链被服务于比特币仅仅只是一个用例和开端，其还可以用于很多地方。 记住这几个词可以帮助理解什么是区块链 chain/链。 像下面这个图，数据的组织是由一个一个大小相同的块(block)组成一个链条(chain)，就像是DNA里每个基因有机的组织在一起。 . 下面使用一个例子来解释一下。当有如下三个操作时就会在区块链中添加三条记录。 去中心化，或者说是“分布式” ， 也就是具体的这些记账数据是分布式的散落在各个节点，而且每个节点都存有一份所有的交易。这样有个好处，就是每个交易有多份副本，互相之间可以对比查对，那些欺诈，篡改数据就没有可能了。其实，传统金融业有一个问题就是各个金融机构间互相的不信任，想像一下在2008年金融危机时，各个金融机构竞相抢购那些credit default derivatives等产品，就是因为大家对对方的不信任所引起的。 挖矿。 “矿工”，其实就是链中一个个能够保存对账信息的节点的别名. 当有新的对账或者交易时，应该就是把数据写到某个节点，然后再需要加入到区块链中时。但问题是“链”只有一个，节点有很多，到底由哪个节点来完成这个任务呢？解决办法就是“打”，看谁厉害。其实就是许多的矿工节点会互相竞争，使用一些非常耗费计算资源，后台使用复杂的算法，最后使用一种叫做 PoW （Proof Of Work,是一种快速断定工作量的技术，比如你安排工人来给把一车箱子从仓库搬到车间，你并不需要从一开始紧盯着他去搬每一个箱子，只要看到最终的所有的箱子都已经在车间，即可证明他做完了工作，可以给相应的报酬了。这个我们在后续章节详细解释）的机制来决定最终哪个节点获胜，由它有资格来写这个区块，并加入到区块链中，同时这个获胜的节点可以得到相应财务上的奖励，即若干的比特币，这也正是不断激励人们投入更加强大计算能力的机器来挖矿的原因。这个过程被比特币平台很好的控制节奏，也就是大概每10分钟左右产生一个新的“区块”以添加到区块链路中。 可以参考https://blockchain.info/?currency=CNY， 这里是以人民币滚动显示当前所有挖矿的更新，下图是此屏幕截图。 区块链的应用领域金融业对于金融业来说，在进行远程转账时一直在使用的所谓“关系银行”，比如你想要通过中国工商银行给朋友在澳洲的汇丰银行的账户转账，这时在中间可能要经过在香港的汇丰以及悉尼的银行等多个第三方机构来中转，不光要多花手续费，真正拿到手时可能已经1周时间过去了。如果使用区块链，转账其实就是添加一个“对账”信息块并加入到区块链中即可，对方银行可以立即在链中发现此交易。这样此过程就流水化（streamline）了，就跟发个电子邮件似的。 相对于之前，区块链会有3个优势，（1）不需要经过第三方 （2）快 （3）省钱 一般商业公司设想一下普通办公场景，一般业务处理都会涉及到许多纸制的表格，文件，请多文件的复印件等等。一是方便文件信息的传递，记录，另外一个原因是为了应付内部外部的审计。如果使用区块链，这些问题都得到很好的解决。比如，由于区块链的系统架构，其本身数据就是自动审计的。简言之，在这个领域，区块链有3大优势，（1）数据透明（2）数据安全性验证（3）审计。 对于零售行业比如说你想知道这个食品是不是有机食品，而每个环节的数据都是散落在各个地方，不便于统一追踪。另外，数据的来源又是多种多样，又容易被篡改，比如作为一个钻石加工商，我是无法确定这批钻石是不是血钻。而“区块链”本身的特性保证了可以跟踪产品生命周期的每个阶段详细信息，而且区块链的“只能添加”的属性也确保了数据数据的准确性，不容易被后期篡改。因此区块链也可以用以政府类的投票，这样就更加具有合法性，不可能被人为篡改结果。 小结以上的案例都涉及到一个关键词 “信任”。 你不信任供应链路，你不信任 “相关银行”， 等。但是你可以信任 “区块链”，它是允许多个不同的机构一起协同工作，但是不需要他们之间相互信任。 FAQ （常问的问题）这些 “区块” 具体是什么样子的？首先每个区块包含有一个时间戳，包含一个哈希码，指向其前面链接的区块，然后就是对账交易数据本身。每个区块都有一个唯一编号，生成这个编号是需要大量的计算工作及验证。我们在后续章节详细介绍此功能。 什么样新的块才能够加入当需要添加新的块时，需要有所有节点中50%认为正确同意才可以。这样可以保证恶意的数据被加入到链中。 每个节点都存一份不也是有额外的成本吗其实在2010年，1P (Peta byte)数据存储一个月是 $80,000/month,预计在2020年，也就是10年后，同样的1P的数据存储一个月只要 $4/month. 可见，存储本身的成本几乎可以越来越忽略不计了。 总之，BlockChain的出现，是由于人们的预期， 技术的进步。所有上面提到的东西，包括此文章的markdown源代码，mindmap思维导图等等都可以在我的github上找到。此文章是我在GitBook上此系统的第一篇，链接。如果有任何建议或者想法，请联系我。 联系我： phray.zhang@gmail.com (email/邮件，whatsapp, linkedin) helloworld_2000 (wechat/微信) github [简书 jianshu]（http://www.jianshu.com/users/a9e7b971aafc） 微信公众号：vibex]]></content>
      <tags>
        <tag>MyBlog</tag>
        <tag>blockchain</tag>
        <tag>hyperledger</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JetBrains/IntelliJ tips]]></title>
    <url>%2F2016-09-14-JetBrains-Tips%2F</url>
    <content type="text"><![CDATA[ShortcutsOpen files by nameTo navigate to a class, file or symbol with the specified name: On the main menu, point to Navigate, and then choose Class, File, or Symbol respectively, or use the following shortcuts:Class: ⌘OFile (directory): ⇧⌘OSymbol: ⌥⌘O Search keywords in filesShift+Command+F: Fine in path Re-open filesCommand +E]]></content>
      <tags>
        <tag>Java</tag>
        <tag>Coding</tag>
        <tag>JetBrains</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅谈软件单元测试中的“断言” (assert)，从石器时代进步到黄金时代。]]></title>
    <url>%2F2016-09-17-Assert-In-Unit-Test-CN%2F</url>
    <content type="text"><![CDATA[大家都知道，在软件测试特别是在单元测试时,必用的一个功能就是“断言”（Assert)，可能有些人觉得不就一个Assert语句，没啥花头，也有很多人用起来也是懵懵懂懂，认为只要是Assert开头的方法，拿过来就用。一个偶然的机会跟人聊到此功能，觉得还是有必要在此整理一下如何使用以及对“断言”的理解。希望可以帮助大家对此有一个系统的理解，也趁机聊聊“断言”发展一路过来的心路历程。 基础知识首先稍微介绍一下断言相关知识，对于有经验的程序员请移步到下面的“断言”进化史部分。 什么是断言在单元测试时，程序员预计在程序运行到某个节点位置，需要判断某些逻辑条件必须满足，这样下面的一些业务逻辑才可以进行下去，如果不满足，程序就会”报错”甚至是”崩溃”。比如说，一段程序是负责“转账”，在真正开始转账操作前首先需要“断言”这个账户是一个“合法”的账户，比如账户不是null。当出现些状况时，程序开发人员就可以在第一时间知道这个问题，可以去debug除错，而非等到交付给用户后才发现问题。其实这个功能是TDD (Test Driven Develop)的基石之一。 “断言” vs “异常”或者错误， 即 Assert vs. Exception/Error “断言”通常是给程序开发人员自己使用，并且在开发测试期间使用。而异常等在程序运行期间触发 通常“断言”触发后程序“崩溃”退出，不需要从错误中恢复。而“异常”通常会使用try/catch等结构从错误中恢复并继续运行程序。 “断言”进化史“石器时代”一开始的一些单元测试框架（比如JUnit）提供的断言语句，这样在程序某个地方确保某个逻辑关系肯定返回是true,如果不是true,这个单元测试就是没有测试通过。如下就是一个例子,如果程序运行到此行时返回false程序就会抛出一个错误（如下图一）并停止运行，开发人员可以去检查下为什么出现此问题。非常的简单粗爆。 1assert(x=y); “青铜时代”上面这种断言除了简单之外，是有一个问题，就是当断言被触发时显示出来的错误消息不是很友好。如上图一，只是知道出错了，但是并没有太多有用的信息，比如最好是能显示出x与y的值来，这样好更快的理解为啥出错。后来，支持断言的单元测试框架升级版本出现了，它们提供了一系列的高级”断言“语句，添加了一些更加友好的程序接口，同时还提供比较亲民的错误消息，比如下面的例子使用了两个单独的断言语句。 1234int x=111;int y=222; assertEquals(x, y);assertNotEquals(x, y); 执行的结果如下图二，你可以看到这个错误结果相对于上面“石器时代”已经包括了不少有用的信息，比如除了知道断言失败外还显示了期望的值以及实际值。 “黄金时代”但是上面这种方式有一个弊端，就是需要大量的预置断言方法（比如判断相等一个方法，判断不相等一个方法等），去支持各种场景。接下来又出现了新的解决方案，其中的明星就是Hamcrest (其实这个词是使用一种叫做angram的文字游戏，即把一个原来单词中的字母顺序改变，这个Hamcrest就是从Matchers的变形)框架。是使用一种assertThat组合上Matcher来使用。 这个有多个好处， 首先是支持了在Java8中才迟迟引入的流式编程(Stream)，即每个Matcher执行完后会再返回一个Matcher，这样可以一个套一个组成一个Matcher链 另外Hamcrest还使用了非常接近于人类自然语言以及使用and/or/not等逻辑判断的方式来写测试方法，比如当你看到下面的测试语句肯定会一目了然： 1assertThat(actual, is(not(equalTo(expected))); 还有一个好处是输出的断言消息更加易读。 另外还有一个好处即Hamcrest框架支持泛型TypeSafe，即在编译时就会找到类型不匹配的错误。比如下面第一个是传统的断言，在编译期不会报错，但是运行时会失败，而第二个会在编译时报错，就不用等到运行期。 12assertEquals("abc", 123); // 1assertThat(123, is("abc")); // 2 使用Hamcrest的最后一个好处是对测试框架的“解耦合”，即，使用此框架你可以现在使用Junit后面可以转到TestNG。甚至你自己去扩展自己实现。 总结上面说了这么多，是不是感觉平时经常使用的一个看似简简单单的Assert还有不少的东西可以深挖一下滴。这个只是抛砖引玉，如果大家还有什么点子或建议请使用下面的方式。 联系我： phray.zhang@gmail.com (email/邮件，whatsapp, linkedin) helloworld_2000 (wechat/微信) blog on github pages 简书 jianshu github 微信公众号：vibex Reference: benefit of assertThat]]></content>
      <tags>
        <tag>java</tag>
        <tag>MyBlog</tag>
        <tag>testing</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[How to customize Sublime syntax highlights]]></title>
    <url>%2F2016-10-06-Sublime-Syntax-Highlights%2F</url>
    <content type="text"><![CDATA[Reference Sublime Scope Naming Syntax Guide]]></content>
      <tags>
        <tag>tag</tag>
        <tag>efficiency</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【原创】深入浅出区块链系统：第二章]]></title>
    <url>%2F2016-10-13-head-first-blockchian-2%2F</url>
    <content type="text"><![CDATA[使用Solidity创建以太坊(Ethereum)智能合约(Smart Contract) 引言前面第一章 (位于微博上的链接)主要介绍了区块链的概念，我们知道区块链分为两大类，一是以公有链为代表的无权限控制区块链，第二是有权限控制的区块链，这个又包括了私有链(Private Blockchain,以Overstock为代表)和联盟链(Consortium Blockchain,以R3为代表)，相对于公有链来说，这些链一般都是没有电子货币，因为他们不需要像公有链那样要靠电子货币作为挖矿的奖励来激励参与，所以速度也是比较快的。 上一章都是讲的抽象的概括，下面我们就深入讲一些具体的东西，这样以便于大家有一个形象的概念，方便理解。我们这一章主要讲讲公有链，以方便讲解以及大家去继续研究，尝试，这里选择在公有链领域社区最为活跃以太坊（Ethereum），对于中国用户来说，其于2016年9月19号刚刚在中国上海举办了DevCon 2区块链峰会，很多人可能有所印象。第一步，这个东西怎么读啊？其实这是新构造的一个单词，而非一个已有的英语单词，其读作[i’θi:’riəm]。接下来我们会一起过一下涉及的一些概念，后面我会介绍几个如何进行太坊开发的技术工具，以及两个比较好用的应用框架。 大家都知道，学习一个新技术最好的方式就是亲自动手试一把，几乎学习所有新编程语言上来都会写个HelloWorld并运行一把，在这一章最后一部分我会手把手的带领大家创建并运行一个智能合约。 概念Ethereum:Ethereum (官方链接) ，是个区块链公有链解决方案，如果比特币的区块链称作区块链1.0的话，那Ethereum可以称为区块链2.0 。 其主要特色就是支持可编程的智能合约。这个开源的系统相当于计算机中的操作系统一样，其是一个平台，提供了API及接口，以供其上运行不同的程序共享使用。同时因为它本质上是去中心化的区块链，因此号称是零宕机，零审查，以及不会有欺诈与人为篡改。就像所有的公有链需要激励机制的“代币”一样，它除了底层的区块链外，还有自己的加密电子货币，Ether，即以太币，在国内有些人戏称为“姨太”。目前一个“姨太”大约11美元，实时的价格趋势可以参见 这个交易所链接 智能合约智能合约 (解释链接 )，其实这个概念本身是远远早于区块链产生的（早在1994年就出现了）。智能合约，说白了就是自己写的一段代码放到区块链上，在这里可以添加自己需要的业务逻辑等，只是这段代码在创建后不像传统应用是部署到服务器上运行，而是放到区块链上，并且自动执行(其运行部署都会消耗Gas(气，也就是若干的以太币))。各个参与方不需要像以前需要一个或若干个中心节点/服务器，大家都各自在自己那里完全按照“合约”执行，中间没有人可以去篡改或者停止，此设计会大大提高flexibility(灵活性）以及互相不信任的问题。比如有一个智能合约定义的逻辑是：当A收到钱后，B就会收到货物，这些操作都是按照合约自动执行，中间不再会有违约或者被人为修改的风险。 这些智能合约是以DAPP (Decentralized Application)的形式存在。智能合约是部署在区块链上，由于区块链的透明性，这些合约对任何人都是可见的，当然这个有利有弊。如果其有bug或者漏洞，就有可能被人抓住并利用，比如2016年6月的The DAO攻击，就造成相当于5千万美元的以太币丢失，这也直接导致了以太坊后面的一次更分叉，这块笔者后面会撰文详解。 Web 3.0大家可能听说过web 1.0， 其是指之前传统的网页技术，比如HTML，传统的JavaScript,VBScript,CSS。而web 2.0 是使用所谓的DHMTL,HTML5, Ajax,等众多的JavaScript技术，来创作类似于桌面程序效果般的网页应用。Web 2.0这些技术有个问题，就是过于依赖中心化的服务器/第三方机构，比如除了其应该做的提供网页访问服务外，还有验证，用户行为记录分析等。 而这里提出的web 3.0是有如下几个特性，首先是去中心化，比如通用的后台端，使用Swarm与bzz来作为内容寻址的存储系统，基于区块链的共识形成机制，基于Whisper的异步消息机制等，这样具体的业务逻辑都会分发到每个客户端去执行，而非位于昂贵且易于出问题的少数中心节点。 这是刚刚提到的架构图 Solidity上面说到的这些智能合约一般来说是使用一种特殊的编程语言来创建的，即Solidity，这个语言是以太坊提出并创造的，面向对象的DSL特定领域编程语言(Domain Specific Language),它是以太坊支持的4种语言（另外三个是Serpent, LLL 和 Mutan），只不过其是最流行的一个语言。从技术上来讲，solidify源代码会编译成字节码，然后运行于EVM（Ethereum Virtual Machine）上面。如果你看到源代码后就会觉得其实Solidity是与JavaScript十分类似的语言，如下是一段代码： Gavin Wood (Solidity之父)说Solidity就是根据ECMAScript（是JavaScript,ActionScript等的标准祖先）所创建的，这样对于大多数开发人员来说学习曲线会很平滑。 开发工具由于发展时间不是很长，目前市面上可用的开发环境IDE不太多。下面介绍下稍微比较成熟可靠的开发工具。 Microsoft Visual Studio Ethereum 插件没错，就是那个市面上已经非常常见的visual studio，也就是dot net的开发工具，不是一个全新的开发工具。此开发集成环境只需要安装solidify插件即可。 这个也从侧面可以看到微软对于以太坊以及区块链的野心。 安装此插件后在微软的Visual Studio后就可以在新建项目时的模板里看到这个Solidity 选项： 当选择此模板后，visual studio他会自动构造出一个应用的基本文件结构。这样你可以省去一些每次开发一个智能合约都要重复的工作。你就可以集中时间精力到真正业务代码上。 如下就是这个IDE自动生成的代码 Ethereum Studio除了背靠微软这个大旗的visual studio集成开发环境外，还有一个方便大家使用的免费的IDE。这个是基于Cloud9平台的一个在线IDE，其完全运行于浏览器中，不用安装，可以用于任何的操作系统。如下就是这个在线集成开发环境的样子。这个还是比较推荐的开发环境： 智能合约应用开发框架目前比较常用的智能合约构架有如下几个，都是开源并且免费的。这里我们来手把手的创建并运行一个智能合约，来体会一下。 Embark首先推荐的是这个叫做Embark的框架，他是一个让你可以轻松开发部署Dapps的平台，它支持的功能包括，在JS代码中部署智能合约，智能合约的热部署，可以集成grunt等构造工具。支持TDD（即测试驱动的开发）比如支持mocha等测试框架，可以方便的使用IPFS等去中心化的系统，支持增量，智能的部署修改过的智能合约等。这个工具是使用nodejs写的，因此你需要先安装nodejs的环境。这个平台会在你本地启动一个区块链服务模拟器，这样你就可以完全在本地开发测试，大大提高了工作效率。如下是启动后的截图。Embark的安装及源代码位于Gitub这里 首先你需要来安装Embark以及区块链模拟器。 12345678# 安装Embarknpm -g install embark-framework# 安装区块链模拟器npm -g install ethersim# 启动RPC模拟器embark simulator 启动的模拟器是下面这个样子 然后我们去创建一个新的智能合约： 123456# 创建一个叫做demo的智能合约基础结构embark demo# 进入这个目录，下面含有配置文件 embark.yamlcd embark_demo# 启动应用embark run 启动后,首先在后台看，Emark帮忙使用coffee script等构造并部署了合约。 你可以使用浏览器试验一下，比如打开http://localhost:8000，然后你可以试着输入个数值，去试试看看它是不是已经能够响应处理你的输入了： 是不是很神奇，短短两三分钟，已经从零开始构造出一个可以运行的以太坊DAPP，并运行于区块链之上。 接下来我们介绍另外一个框架选择方案。 TruffleTruffle,是跟前面提到的 Ethereum Studio 同一个公司（ConsenSys）开发的一个框架， 这个跟前面的embark类似，也是可以提供一个智能合约的开发测试平台,他的一个特色就是它可以集成nodejs里面强大的测试功能，比如Mocha, Chai等等. 像Embark一样，你需要另外安装运行其他软件，来启动以太坊客户端模拟器，最常用的是EthereumJS TestRPC Github link, 它会在内存中启动一个Ethereum的客户端， 这样可以快速测试你开发的应用。 因为这个也是使用nodejs创建的应用，因此使用如下命令来安装此程序，安装好了启动此应用 1234#安装以太坊模拟器npm install -g ethereumjs-testrpc# 启动模拟器testrpc 启动后是这样子的 模拟器启动好了，接下来执行下面的命令来初始化truffle应用。 123mkdir firstAppcd firstApptruffle init 上面最后一个命令就会自动帮你构造好的程序框架，包括一些最基本的JavaScript文件，几个智能合约源代码，主应用程序的HTML代码及配套的CSS等文件 。如下是这个基本框架： 接下来你可以添加自己的代码到contracts目录下的智能合约文件，也可以什么都不动，因为truffle已经自动生成了最基本的框架。 12#这个命令会把智能合约源代码编译成字节码truffle compile 编译好的代码需要部署到区块链上才可以执行，在truffle中这个工作是由migrates目录下定义的migrate作业执行的，我们去修改文件2_deploy_contracts.js为如下： 123456module.exports = function(deployer) &#123; // deployer.deploy(ConvertLib); // deployer.autolink(); // deployer.deploy(MetaCoin); deployer.deploy(HelloEthereum);&#125;; 然后执行如下命令去执行代码部署，它除了把你的智能合约发布到区块链之外，还会做一些相关工作，比如link用到的library等。deployer可以使用promise的方式 (e.g. .then(function(xx)))来执行其他额外的工作等，比如创建一个其他的合约并调用，等。这个便于你来灵活的扩展应用。 12truffle migratetruffle build 结语好了，至此我们已经了解了什么是以太坊已经其上运行的智能合约，DAPP等概念。后面又介绍了开发智能合约的工具已经可复用的框架，最后又手把手亲自做了一个智能合约。这样大家应该对区块链以及以太坊等公有链有了一个形象具体的感觉了吧。如果这里有什么问题或者建议，欢迎通过下面的联系方式与我沟通。 Referece Introduction to Smart Contracts Installing Solidity 以太坊官方教程 Swarm Truffle Official Doc 联系我： phray.zhang@gmail.com (email/邮件，whatsapp, linkedin) helloworld_2000 (wechat/微信) github [简书 jianshu]（http://www.jianshu.com/users/a9e7b971aafc） 微信公众号：vibex webo/微博: cloudsdocker]]></content>
      <tags>
        <tag>MyBlog</tag>
        <tag>blockchain</tag>
        <tag>ethereum</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Head First Blockchina 3]]></title>
    <url>%2F2016-10-19-head-first-blockchian-3%2F</url>
    <content type="text"><![CDATA[Hyperledger Fabric for Mortals HyperLedger, from Arvind Krishna, Director of IBM research lab, meant to cross borders. e.g. for credit ledger, the importer and exporter in China need to make preparations few months beforehand, this is similiar to Marco Polo went to China 300 years ago. Another usage in stock market is stock loan, by using blockchain, we are going to know whose the stock came from. And the borrowers need to know who can borrow, and I need to prepare collaterials, need to check whether it comply with the processes, when I pay it back, etc. there are lots of things. right now they are handled by people manually, so going forward, if I have some stocks and I want to borrow it to earn some money, those can be programmed in blockchain. they’ll be processed by machines automatically. this will help to show there are no inside tradings, etc. but initially as blockchian are annoymous, we are not sure whthere there are any money laundering, regulators may have concerns. So we need permisisoned blockchains. we need to one identity put aside of the transactions, so if reuiqred, can be recorded and checked, e.g. whether this is dealing with corrupted governments, etc. in addtion, it’s not requried that only one player or government to provide identity, there are many entities can provide identity service, but off couse need to follow some standards agreed. JPMorgan Chase &amp; Co has announced its second entrant into the blockchain fray, this time utilizing the smart contract enabled ethereum blockchain in creating a system called Quorum. 前面第一章讲过了基本概念，可能有此抽象。接下来我们来看一些目前已经存在的真实项目和案例，好有一些形象的理解，以便为后面深入一些概念准备。 JPMorgan 的 Juno Ripple 全球化交割网络 R3 DEV 以态坊 万象Lab All of the advantages derived from basic blockchain technology can be boiled down to only two benefits; corruption resistance and redundancy. All of the advantages derived from basic blockchain technology can be boiled down to only two benefits; corruption resistance and redundancy. The crux of the issue comes down to whether or not private blockchains can ever be made secure enough to use for large amounts of value. No hacker is going to bother to attack your blockchain if it’s only being used for bingo night at a retirement home. However, the moment the world finds out that your blockchain has millions of dollars worth of payments flowing across it, you’ve basically just launched the latest hackathon, complete with a multi-million dollar, winner-take-all grand prize. For instance, a public blockchain is a transparency engine. In Vitalik Buterin’s blog post ‘On Public and Private Blockchains’ ‘On Public and Private Blockchains’ written last August, he pointed out that public blockchains “protect the users of an application from the developers, establishing that there are certain things that even the developers of an application have no authority to do.” A good example of this is a user of a social network or some other membership site where the owner changing http://www.youtube.com/watch?v=810aKcfM__Q The main takeaway is that permissioned blockchains create an environment where malware has an advantage,so security problems are constant and sometimes completely overcome your network. First of all, the transaction speed of a privately-run blockchain can be faster than any other blockchain solution, approaching even the speeds of a normal database that isn’t a blockchain. This is because there are few nodes all with high trust levels. No need for every node to verify a transaction, in fact, they’re all mostly trusted so there is no need to do all of the meticulous work. Complete agreement between nodes isn’t required, so fewer nodes need to do the work for any one transaction. Lastly, and perhaps most importantly in the current environment of banks embracing private blockchains so readily, choosing a private blockchain can help protect their underlying product from disruption. Lastly, there is the desktop route, deploying a private blockchain on your desktop computer, even in a windows environment, with Multichain. It allows rapid design, deployment and operation of private blockchains to your custom specification. http://r3members.com/ like unique keys and check constraints cannot protect a database against malicious modifications. The bottom line is this:We need a whole bunch of new stuff for shared write databases to work, and it just so happens that blockchains provide them. Some key elements include regular peer-to-peer techniques, grouping transactions into blocks, one-way cryptographic hash functions, a multi-party consensus algorithm, Some key elements include regular peer-to-peer techniques, grouping transactions into blocks, one-way cryptographic hash functions, a multi-party consensus algorithm, These types of rules can be expressed as bitcoin-style transaction constraints or Ethereum-style enforced stored procedures (“smart contracts”), each of which has advantages and disadvantages. One key difference is that private blockchains don’t need proof of work mining,since blocks are created by a closed set of identified participants. First, fear of the loss of raw power they would sustain if they went with public blockchains. Second, the fear of being supplanted by a new rising, bitcoin-centric financial industry. In fact, they would prefer that blockchain technology had never been invented. 区块链都足以产生影响和改变金融业的力量。这是由于区块链具有以下特点：（1）去中心化：区块链技术不依赖额外的第三方管理机构或硬件设施，没有中心管制，除了自成一体的区块链本身，通过分布式核算和存储，各个节点实现了信息自我验证、传递和管理。（2）开放性：区块链技术基础是开源的，除了交易各方的私有信息被加密外，区块链的数据对所有人开放，任何人都可以通过公开的接口查询区块链数据和开发相关应用，因此整个系统信息高度透明。（3）独立性：基于协商一致的规范和协议（类似比特币采用的哈希算法等各种数学算法），整个区块链系统不依赖其他第三方，所有节点能够在系统内自动安全地验证、交换数据，不需要任何人为的干预。（4）安全性：只要不能掌控全部数据节点的51%，就无法肆意操控修改网络数据，这是区块链本身变得相对安全，避免了主观人为的数据变更。（5）匿名 区块链都足以产生影响和改变金融业的力量。这是由于区块链具有以下特点：（1）去中心化：区块链技术不依赖额外的第三方管理机构或硬件设施，没有中心管制，除了自成一体的区块链本身，通过分布式核算和存储，各个节点实现了信息自我验证、传递和管理。（2）开放性：区块链技术基础是开源的，除了交易各方的私有信息被加密外，区块链的数据对所有人开放，任何人都可以通过公开的接口查询区块链数据和开发相关应用，因此整个系统信息高度透明。（3）独立性：基于协商一致的规范和协议（类似比特币采用的哈希算法等各种数学算法），整个区块链系统不依赖其他第三方，所有节点能够在系统内自动安全地验证、交换数据，不需要任何人为的干预。（4）安全性：只要不能掌控全部数据节点的51%，就无法肆意操控修改网络数据，这是区块链本身变得相对安全，避免了主观人为的数据变更。（5）匿名. 正是这些优势特点，决定区块链技术能够提高系统的追责性，降低系统的信任风险，对优化金融机构业务流程、提高金融机构的竞争力具有相当重要的意义；通过使用区块链技术，金融信息和金融价值能够得到更加严格的保护，能够实现更加高效、更低成本的流动，从而实现价值和信息的共享。 区块链技术可通过程序化记录、存储、传递、核实、分析信息数据，从而形成信用，可以大量省去人力成本、中介成本，信用记录完整、难以造假，同时摧毁某些节点对系统没有影响。目前最大区块链比特币链存在费用增加、容量限制、确认时间变长、能耗走高的缺点。但例如Ripple、以太坊等另类区块链，以及公共、私有、联盟链等多种形式的涌现将区块链在金融领域造成颠覆式创新变成可能。 https://ripple.com/ 只要诚实的节点所控制的计算能力的总和，大于有合作关系的(cooperating)攻击者的计算能力的总和，该系统就是安全的 Retail Giant Overstock to Issue its Own Stock on Blockchain Platform Referece Hyperledger Whitepaper About blockchain IBM Blockchain for developers Getting started with IBM Blockchain IBM Blockchain 101: Quick-start guide for developers]]></content>
      <tags>
        <tag>blockchain</tag>
        <tag>hyperledger</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Simpler chronicle of CI(Continuous Integration) “乱弹系列”之持续集成工具]]></title>
    <url>%2F2016-10-26-Continuous-Integretaion-Simpler-Chronicle%2F</url>
    <content type="text"><![CDATA[引言有句话说有人的地方就有江湖，同样，有江湖的地方就有恩怨。在软件行业历史长河（虽然相对于其他行业来说，软件行业的历史实在太短了，但是确是充满了智慧的碰撞也是十分的精彩）中有一些恩怨情愁，分分合合的小故事，比如类似的有，从一套代码发展出来后面由于合同到期就分道扬镳，然后各自发展成独门产品的Sybase DB和微软的SQL Server；另外一个例子是，当时JBPM的两个主要开发的小伙伴离开当时的RedHat，在JBPM基础上自立门户新创建的Java工作流管理软件Activiti，等等。在持续集成工具龙头老大这个宝座，也曾经发生过合作合并，吵架分家，再对着干的事情，今天分享一下这前前后后有趣的故事。 DevOps首先，防止先入为主,以为大家都知道这个那个的。先普及下相关背景知识，如果已经了解的同学可以跳过。目前在软件工程领域已经火了好几年的DevOps领域，核心的模块就是CI与’CD’，即Continuous Integration与Continuous Deployment,也就是持续集成与持续部署，这个对于处于敏捷开发环境下尤其是互联网等需要高速迭代是个核心的功能，可以说没有CI，就不可能达到像Google或者Facebook这些一天有多个release的情况。 CICI(Continuous Integration) 持续集成起源于 XP(极限编程)与 TDD (Test Driven Develop)也就是以测试驱动的开发模式，是防止出现所谓的’集成地狱’,即防止程序员在正常编码工作中，需要写新的业务逻辑，添加新的代码，但是同时也新引入了bug。CI会持续的（重复的）进行一些小的工作，比如不断的跑测试用例，去扫描代码等工作。以减轻或者很大程度上避免这个个新引入的bug对软件交付质量引起的负面影响。目前，市场上有很多的CI解决方案及工具，常用的如下几个， CI 的进化史世界上本来没有CI,用的人多了也就成就了CI。本来软件工程里是没有这个概念的。最开始，就像下图中描述的帝国时代里，整个社会节奏平稳而缓慢，每个程序员自己做自己的开发，然后各自把自己的工作上次（提交），整个团队把代码放在一起，然后整个人过来，启动make/build，后面有个人去把编译好的代码放到测试机器上，每个程序员自己或者单独的测试团队去测试程序，如果没有问题，另外的人去发布到生产环境上。这些都是或多或少由人手工去做的。 但是就像很多人类的发明就是为了人类”偷懒”一样，CI慢慢在一些想偷懒的牛人脑子里形成。这其中就有Kent Beck （多说一句，这个现在工作于Facebook的牛人，还发明创造了很多到现在还在流行的东西，比如Agile敏捷开发，以JUnit为代码的xUnit测试理念，TDD测试驱动开发等等），在上个世纪最后几年，Kent Beck创造了XP（注意这个不是Bill的那个XP操作系统），是eXtreme Programming，即极限编程。虽然现在看起来极限编程有很多很诡异不太现实的方式，比如两个程序员坐在一起，使用一台电脑一起写一段程序等天马行空的想法。但是其中一个理念就是“持续集成”（CI)。以此理念，后面出现了使用各种语言写的CI的工具，其中的老大是CruiseControl。这个就像是上图中那个跑车一样，在当时整个缓慢的大环境下其提升工作效率的效果十分的吸眼。 到了2005年，当时就职于Sun(没错，就是创造了Java的那家公司)的一个叫川口浩介（Kohsuke Kawaguchi）的日本人，就是上图这位“霓虹金”，敢于冒险，重新“发明轮子”，不顾如日中天的CruiseControl，设计并开发了一个新的持续集成的软件，起名叫做Hudson。它提供了很多强大的功能，比如提供插件机制，这样就使其几乎集成了市面上所有的源代码管理工具，比如CVS, Subversion, Git, Perforce等。除此之外，它还提供了界面的扩展能力，另外还支持基于Apache Ant 和 Apache Maven的项目，除了xNix,还支持Windows环境等一众强大功能。听起来这么牛逼的工具，很快，在大约2007年的时候Hudson已经超越CruiseControl。然后在2008年5月的JavaOne大会上，Hudson获得了开发解决方案类的Duke’s Choice奖项。从此，小弟翻身做大哥，Hudson成为CI的代名词。其主要开发者 Kohsuke Kawaguchi 还获得了Google-O’Reilly Open Source Award。他后来也不用自己苦逼的写代码了，只要到处受邀去演讲做是如何受什么启发创造并发明了这么好的工具，造福大批程序员。再后来他还离职创立了公司CloudBees，出任CEO，迎娶白富美，走上人生新巅峰。（也难怪上图中他笑的如此开心） 一切看起来都是那么美好。但是，天有不测风云，在2009年6月，Oracle收购Sun，所有人都蒙逼了，是不是写反了？一个传统数据库的公司收购了在Java及开源老大的Sun？！！这个消息公布之后，两个公司内部各个产品及项目就被整合，调整，Hudson也不例外。这也就算了，反正谁给钱不是干活哪，但是在2010年9月，Oracle竟然暗戳啜的把Hudson变成了注册商标。2010年11月，Hudson社区的核心开发人员发现了这个事情，他们觉得这对于一个一直标榜自己是开源CI领域“诚实可靠小郎君”的Hudson来说是个玷污。双方进行了会谈，过程不太友好，然后就不出意料的谈崩了。2011年圣诞节过后，几个秃顶的大叔觉得不要再跟Oracle的律师在这里瞎扯淡了，他们决定自立门户，自己起个新的名字叫Jenkins。然后凑钱注册网址，买服务器，列出下面的清单，统统改名， hudson-labs.org -&gt; jenkins-ci.org @hudsonci -&gt; @jenkinsci http://github.com/hudson -&gt; http://github.com/jenkinsci hudson-dev -&gt; jenkins-dev hudson-users -&gt; jenkins-users hudson-commits -&gt; jenkins-commits hudson-issues -&gt; jenkins-issues 然后把代码fork出一份来（这里好笑的是Hudson与Jenkins都声称对方是自己这里的子分叉，都跟孩子斗气似的），即便分出来了，但是绝大部分还是基于之前的核心代码，所以你可以通过下图看到Hudson与Jenkins的界面都十分类似。 Jenkins的界面 Hudson的界面 但是有一个值得注意的地方就是两个系统的logo，其中Hudson是一个高傲的老头子，而Jenkins是一个谦卑为你服务的老头子。 分家之后，Hudson有Oracle和Sonatype’s corporate的支持和Hudson的注册商标，而Jenkins拥有的是大多数的核心开发者，社区，和后续更多的commit。比如下图是分家之后两个软件的对比。两个软件的活跃程度十分明显，Jenkins遥遥领先。 CI持续集成的工作原理上面讲完了主流CI工具的江湖故事后，我们来看下这类工具本身的技术情况。其实这类工具的工作原理大同小异，比如下图，一个典型的用例是 程序员在本地开发完成后把代码提交到VCS (Version Control System)比如SVN, Git, Perforce, RTS等 CI工具发现有新的check in 自动启动去抓取最新的代码。当然这里有很多不同的配置，比如除了主动监视VSC外，还可以使用CRON等配置按时启动，比如每隔一个小时启动一次，或者每两次check in 启动一次，等等很多的策略。 CI可以配置使用集群的编译机器，去选择最合适的机器（有不同的策略，比如找到最清闲或者离代码文件距离最近的机器等）来编译源代码 根据不同的配置，CI有可能会调用配置好的测试用例，如果测试失败，根据策略（比如少于几个错误就先忽略）要么通知用户，要么继续跑测试用例 根据配置，CI可能会去执行其他操作，比如静态源代码分析，如代码有没有不符合公司安全要求，把连接密码写在代码里面等等，还有比如生成文档，测试报告，等。 如果所有定义好的jobs跑完，去生成最终报告并送给用户 生成一些分析报表，比如最近成功率，最近哪些程序员造成的错误最多等等。 一些高级的CI,比如Jenkinsg还支持自定义扩展，也会去按配置去执行。 这其中如果任何一步出现了错误，比如某个程序员在提交代码时忘记同时提交一个新写的类，造成失败，首先在CI（比如Jekins，或者Travis）上会显示错误 （比如下图），同时还可以配置CI工具会发出邮件提醒，甚至可以根据提交信息智能的显示出来是哪个程序员搞砸的。 总而言之，这个自动化的过程就像是一个可以配置的流水线，在其上可以添加任意个不同类型的节点，在每个节点可以通过灵活的配置来设置需要完成的工作，还提供了统计及报表，邮件通知等功能，方便团队高效的管理软件的持续集成。 发展及未来目前的CI也在处于高速发展期，比如最新的Jenkins 2 可以支持使用Groovy编写插件，pipeline等。同时也出现了像是开源的Travis之类的持续集成service，即你不用自己去安装调试Jenkins，直接写个YAML文件 （.travis.yaml）放到云上，自动就可以使用其提供的服务了。 另外，持续集成也在跟其他新兴技术相结合使用，比如结合云计算及分布式处理，可以提高CI的运行速度和容错能力，比如下图中的各个服务器可以分别使用cluster(集群)而非一台机器，这样就可以避免所谓的SPOF (Single Point of Failure)单点故障。 如果有什么问题或者想要跟我讨论，请通过如下方式找到我。 联系我： phray.zhang@gmail.com (email/邮件，whatsapp, linkedin) helloworld_2000 (wechat/微信) github [简书 jianshu]（http://www.jianshu.com/users/a9e7b971aafc） 微信公众号：vibex webo/微博: cloudsdocker Reference CI in wikipedia Slide share]]></content>
      <tags>
        <tag>DevOps</tag>
        <tag>MyBlog</tag>
        <tag>CI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python Scraphy]]></title>
    <url>%2F2016-11-04-Python-Scrapy%2F</url>
    <content type="text"><![CDATA[Python Scraphy‘https://www.seek.com.au/jobs-in-information-communication-technology?highpay=True&amp;salaryrange=150000-999999&amp;salarytype=annual&#39; scrapy shell httxxxxscrapy extrac response.css(‘title::text’).re(r’Quotes.*’)[‘Quotes to Scrape’] response.css(‘title::text’)[0].extract()‘Quotes to Scrape’ response.xpath(‘//title’) view(response) Reference https://doc.scrapy.org/en/latest/intro/tutorial.html]]></content>
      <tags>
        <tag>python</tag>
        <tag>scraphy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JSON lines]]></title>
    <url>%2F2016-11-05-JSON-Lines%2F</url>
    <content type="text"><![CDATA[JSON linesThe JSON Lines format is useful because it’s stream-like, you can easily append new records to it. It doesn’t have the same problem of JSON when you run twice. Also, as each record is a separate line, you can process big files without having to fit everything in memory, there are tools like JQ to help doing that at the command-line. Reference http://jsonlines.org https://en.wikipedia.org/wiki/JSON_Streaming]]></content>
  </entry>
  <entry>
    <title><![CDATA[Eslastic Search]]></title>
    <url>%2F2016-11-12-elastic%2F</url>
    <content type="text"><![CDATA[Eslastic Search`Elastic Search notes List indices1http://localhost:9200/_cat/indices?v pretty-print JSON message (if any)1curl -XPUT 'localhost:9200/customer?pretty' 如果有任何建议或者想法，请联系我。 联系我： phray.zhang@gmail.com (email/邮件，whatsapp, linkedin) helloworld_2000 (wechat/微信) github [简书 jianshu]（http://www.jianshu.com/users/a9e7b971aafc） 微信公众号：vibex 微博: cloudsdocker]]></content>
  </entry>
  <entry>
    <title><![CDATA[用10几行代码自己写个人脸识别程序]]></title>
    <url>%2F2016-11-22-Facial-Recognition%2F</url>
    <content type="text"><![CDATA[用10几行代码自己写个人脸识别程序CV (Computer Vision)最近在研究CV的一些开源库(OpenCV)，有一个体会就是在此领域，除了一些非常学术的机器学习, _深度学习_等概念外，其实还有一些很有趣的_现实的_应用场景。比如之前很流行的微软的 https://how-old.net, 你使用自己指定或者上传的照片进行面部识别_猜年龄_。 如下图所示： 细想一下这个很吸引眼球的程序，其实技术本身打散了就包括两大块，一是从图片中扫描并进行面部识别，二是对找到的人脸根据算法去猜个年龄。大家可以猜猜实现第一个功能需要多少核心代码量？其实不用上万行，在这里我就使用短短几行代码（去除空格换行什么的，有效代码只要10行）就实现一个_高大上_面部识别的功能。在此文容我细述一下具体实现代码以及我对机器识别图像领域技术的理解。 面部识别,刷脸 _人脸识别_技术大家应该都不陌生，之前大家使用的数码相机，或者现在很多手机自带的相机都有人脸识别的功能。其效果就像是下图这样。近的看，_剁手节_刚刚过了没有多久 , 背后的马老板一直在力推的刷脸支付也是一个此领域的所谓“黑科技”。比如在德国汉诺威电子展上，马云用支付宝“刷脸”买了一套纪念邮票。人脸识别应用市场也从爆发。随后，各大互联网巨头也纷纷推出了刷脸相关的应用。 如果要加个定义，人脸识别又叫做人像识别、面部识别，是一种通过用摄像机或摄像头采集含有人脸的图像或视频流，并自动在图像中检测和跟踪人脸，进而对检测到的人脸进行脸部的一系列相关技术。 我的十行代码程序OK，长话短说，先上 干货 ，下面就是此程序的带注释 版本，完整的程序以及相关配套文件可以在 这个github库 https://github.com/CloudsDocker/pyFacialRecognition 中找到，有兴趣可以fork 下来玩玩。下面是整个程序的代码样子，后面我会逐行去解释分析。 就这短短的十行代码代码？seriously？“有图有真相”，我们先来看下运行的效果： 首先是原始的图片 运行程序后识别出面部并高亮显示的结果请注意 K歌二人组 的脸上的红色框框，这就是上面十行代码的成果。 代码解析准备工作因为此程序使用是的Python,因此你需要去安装Python。这里就不赘述了。除此之外，还需要安装 OpenCV (http://opencv.org/downloads.html)。多说一句,这个 OpenCV正如其名，是一个开源的机器识别的深度学习框架。这是Intel（英特尔）实验室里的一个俄罗斯团队创造的，目前在开源社区非常的活跃。 特别提一下，对于Mac的用户，推荐使用brew去安装 （下面第一条语句可能会执行报错，我当时也是搞了好久。如果遇到第一条命令不过可以通过文尾的方式联系作者） 12brew tap homebrew/sciencebrew install opencv 安装完成之后,在python的命令行中输入如下代码验证，如果没有报错就说明安装好了。 1&gt;&gt;&gt; import cv2 程序代码“庖丁解牛”12# -*- coding: utf-8 -*-import cv2,sys 由于这里注释及窗口标题中使用了中文，因此加上utf-8字符集的支持 引入Opencv库以及Python的sys内建库，用于解析输入的图片参数 1inputImageFile=sys.argv[1] 在运行程序时将需要测试的照片文件名作为一个参数传进来 1faceClassifier=cv2.CascadeClassifier('haarcascade_frontalface_default.xml') 加载OpenCV中自带预先培训好的人脸识别层级分类器 HAAR Casscade Classifier，这个会用来对我们输入的图片进行人脸判断。 这里有几个在深度学习及机器图像识别领域中的几个概念，稍微分析一下，至于深入的知识，大家可以自行搜索或者联系作者。 Classifer在机器深度学习领域，针对识别不同物体都有不同的classifier,比如有的classifier来识别洗车，还有识别飞机的classifier，有classifier来识别照片中的笑容，眼睛等等。而我们这个例子是需要去做人脸识别，因此需要一个面部识别的classifier。 物体识别的原理一般来说，比如想要机器学习着去识别“人脸”，就会使用大量的样本图片来事先培训，这些图片分为两大类，positive和negative的，也就是分为包“含有人脸”的图片和“不包含人脸”的图片，这样当使用程序去一张一张的分析这些图片，然后分析判断并对这些图片“分类” (classify),即合格的图片与不合格的图片，这也就其为什么叫做 classifier ， 这样学习过程中积累的”知识”，比如一些判断时的到底临界值多少才能判断是positive还是negative什么的，都会存储在一个个XML文件中，这样使用这些前人经验（这里我们使用了 哈尔 分类器）来对新的图片进行‘专家判断’分析，是否是人脸或者不是人脸。 Cascade这里的 Cascade是 层级分类器 的意思。为什么要 分层 呢？刚才提到在进行机器分析照片时，其实是对整个图片从上到下，从左到右，一个像素一个像素的分析，这些分析又会涉及很多的 特征分析 ，比如对于人脸分析就包含识别眼睛，嘴巴等等，一般为了提高分析的准确度都需要有成千上万个特征，这样对于每个像素要进行成千上万的分析，对于整个图片都是百万甚至千万像素，这样总体的计算量会是个天文数字。但是，科学家很聪明，就想到分级的理念，即把这些特征分层，这样分层次去验证图片，如果前面层次的特征没有通过，对于这个图片就不用判断后面的特征了。这有点像是系统架构中的 FF (Fail Fast),这样就提高了处理的速度与效率。 1objImage=cv2.imread(inputImageFile) 使用OpenCV库来加载我们传入的测试图片 1cvtImage=cv2.cvtColor(objImage,cv2.COLOR_BGR2GRAY) 首先将图片进行灰度化处理，以便于进行图片分析。这种方法在图像识别领域非常常见，比如在进行验证码的机器识别时就会先灰度化，去除不相关的背景噪音图像，然后再分析每个像素，以便抽取出真实的数据。不对针对此，你就看到非常多的验证码后面特意添加了很多的噪音点，线，就是为了防止这种程序来灰度化图片进行分析破解。 1foundFaces=faceClassifier.detectMultiScale(cvtImage,scaleFactor=1.3,minNeighbors=9,minSize=(50,50),flags = cv2.cv.CV_HAAR_SCALE_IMAGE) 执行detectMultiScale方法来识别物体，因为我们这里使用的是人脸的cascade classifier分类器，因此调用这个方法会来进行面部识别。后面这几个参数来设置进行识别时的配置，比如 scaleFactor: 因为在拍照，尤其现在很多都是自拍，这样照片中有的人脸大一些因为离镜头近，而有些离镜头远就会小一些，因为这个参数用于设置这个因素，如果你在使用不同的照片时如果人脸远近不同，就可以修改此参数，请注意此参数必须要大于1.0 minNeighbors: 因为在识别物体时是使用一个移动的小窗口来逐步判断的，这个参数就是决定是不是确定找到物体之前需要判断多少个周边的物体 minSize：刚才提到识别物体时是合作小窗口来逐步判断的，这个参数就是设置这个小窗口的大小 1print(" 在图片中找到了 &#123;&#125; 个人脸".format(len(foundFaces))) 显示出查找到多少张人脸，需要提到的识别物体的方法返回的一个找到的物体的位置信息的列表，因此使用 len 来打印出找到了多少物体。 12for (x,y,w,h) in foundFaces: cv2.rectangle(objImage,(x,y),(x+w,y+h),(0,0,255),2) 遍历发现的“人脸”，需要说明的返回的是由4部分组成的位置数据，即这个“人脸”的横轴，纵轴坐标，宽度与高度。 然后使用 OpenCV 提供的方法在原始图片上画出个矩形。其中 (0,0,255) 是使用的颜色，这里使用的是R/G/B的颜色表示方法，比如 (0,0,0)表示黑色，(255,255,255)表示白色，有些网页编程经验的程序员应该不陌生。 12cv2.imshow(u'面部识别的结果已经高度框出来了。按任意键退出'.encode('gb2312'), objImage)cv2.waitKey(0) 接下来是使用 opencv 提供的imshow方法来显示这个图片，其中包括我们刚刚画的红色的识别的结果 最后一个语句是让用户按下键盘任意一个键来退出此图片显示窗口 总结好了，上面是这个程序的详细解释以及相关的知识的讲解。其实这个只是个_抛砖引玉_的作用，还用非常多的应用场景，比如程序解析网页上的图片验证码，雅虎前几个月开源的 NSFW, Not Suitable for Work (NSFW)，即判断那些不适合工作场所的图片，内容你懂的。 :-) 最后，再提一下，所有这些源代码及相关文件都开源在 https://github.com/CloudsDocker/pyFacialRecognition ，在fork并下载到本地后执行下面代码来测试运行 123git clone https://github.com/CloudsDocker/pyFacialRecognition.gitcd pyFacialRecognition./run.sh 如果有任何建议或者想法，请联系我。 联系我： phray.zhang@gmail.com (email/邮件，whatsapp, linkedin) helloworld_2000 (wechat/微信) 微博: cloudsdocker github [简书 jianshu]（http://www.jianshu.com/users/a9e7b971aafc） 微信公众号：vibex Reference OpenCV HAAR 哈尔特征 Face Detection using Haar Cascades NSFW]]></content>
      <tags>
        <tag>MyBlog</tag>
        <tag>DeepLearning</tag>
        <tag>FacialRecognition</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java new features]]></title>
    <url>%2F2016-12-01-Java-New-Features-Chronicle%2F</url>
    <content type="text"><![CDATA[JDK Versions JDK 1.5 in 2005 JDK 1.6 in 2006 JDK 1.7 in 2011 JDK 1.8 in 2014Sun之前风光无限，但是在2010年1月27号被Oracle收购。在被Oracle收购后对外承诺要回到每2年一个realse的节奏。但是2014年就公布由于Java Security的问题delay一年 Java新功能一览 Generics Concurrent Framework Numberic Literal Lambda JDK New release vs. Stock Price最近在做一个Java项目的相关工作，发现很多程序员还在使用一些最基本的Java语言特性，其实可以理解为很多人入门Java要么在学校里要么通过某些书籍，而这些在国内的学习资料更新比较慢，很多都还是基于JDK1.4或者更老。其实Java这个语言一直在进步。下面我就以Java最近几个主版本引入的新功能来梳理一下这些“黑科技” Java 1.5这个版本个人认为在Java最近10年的历史中是一个最重要的升级。JDK1.5，内部版本号是Tiger，引入了一些颠覆性的功能，列在如下：GenericsConncurrecnt Framework，应该是继Collection Framework后又一个块头的“框架”了。 Java 1.6所有上面提到的东西，包括此文章的markdown源代码，mindmap思维导图等等都可以在我的github上找到。联系我： phray.zhang@gmail.com (email/邮件，whatsapp, linkedin) helloworld_2000 (wechat/微信) github Reference JDK History]]></content>
  </entry>
  <entry>
    <title><![CDATA[Random number in java]]></title>
    <url>%2F2016-12-09-HeadsFirst-RandomNumber-In-Java%2F</url>
    <content type="text"><![CDATA[ThreadLocalRandom, SecureRandm, java.util.Random, java.math.Random Instances of java.util.Random are threadsafe. However, the concurrent use of the same java.util.Random instance across threads may encounter contention and consequent poor performance. Consider instead using ThreadLocalRandom in multithreaded designs. The Java Math library function Math.random() generates a double value in the range [0,1). Notice this range does not include the 1. 1int rand = ThreadLocalRandom.current().nextInt(x,y); Reference How to generate a range of random integers in Java Random JavaDoc SecureRandom JavaDoc ThreadLocalRandom JavaDoc Apache Common Math LCG wikipedia Blog about this topic ImportNew]]></content>
      <tags>
        <tag>Java</tag>
        <tag>MyBlog</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Angulary Misc]]></title>
    <url>%2F2016-12-12-Angular%2F</url>
    <content type="text"><![CDATA[Dependency InjectionAngular doesn’t automatically know how you want to create instances of your services or the injector to create your service. You must configure it by specifying providers for every service. Providers tell the injector how to create the service. Without a provider, the injector would not know that it is responsible for injecting the service nor be able to create the service. What is difference between declarations, providers and import in NgModule imports: is used to import supporting modules likes FormsModule, RouterModule, CommonModule, or any other custom-made feature module. makes the exported declarations of other modules available in the current module declarations are to make directives (including components and pipes) from the current module available to other directives in the current module. Selectors of directives, components or pipes are only matched against the HTML if they are declared or imported. declaration is used to declare components, directives, pipes that belongs to the current module. Everything inside declarations knows each other. For example, if we have a component, say UsernameComponent, which display list of the usernames, and we also have a pipe, say toupperPipe, which transform string to uppercase letter string. Now If we want to show usernames in uppercase letters in our UsernameComponent, we can use the toupperPipe which we had created before but how UsernameComponent know that the toupperPipe exist and how we can access and use it, here comes the declarations, we can declare UsernameComponent and toupperPipe. providers are to make services and values known to DI. They are added to the root scope and they are injected to other services or directives that have them as dependency.provider is used to inject the services required by components, directives, pipes in our module. CLI to create new component123456789101112ng generate component components/xxx-table -m deposit.module.ts --specng generate component components/comp1 -m core.module.ts --specError: Specified module does not existSpecified module does not exist$ ng generate component core/components/comp1 -m core/core.module --specError: dryRunSink.commit(...).ignoreElements is not a functiondryRunSink.commit(...).ignoreElements is not a functionng generate component app/core/components/comp1 -m app/core/core.module --spec Angular JS notesAngularJS applications are built around a design pattern called Model-View-Controller (MVC), which places an emphasis on creating applications that are Extendable: It is easy to figure out how even a complex AngularJS app works once you understand the basics—and that means you can easily enhance applications to create new and useful features for your users. Maintainable: AngularJS apps are easy to debug and fix, which means that long-term maintenance is simplified. Testable: AngularJS has good support for unit and end-to-end testing, meaning that you can find and fix defects before your users do. Standardized: AngularJS builds on the innate capabilities of the web browser without getting in your way, allowing you to create standards-compliant web apps that take advantage of the latest features (such as HTML5 APIs) and popular tools and frameworks.]]></content>
      <tags>
        <tag>JavaScript</tag>
        <tag>Angular</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache Tips]]></title>
    <url>%2F2016-12-24-Apache-Tips%2F</url>
    <content type="text"><![CDATA[Apache123sudo su -apachectl startapachectl stop Open a browser to access http://localhost/]]></content>
      <tags>
        <tag>DevOps</tag>
        <tag>Apache</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java Enum Misc]]></title>
    <url>%2F2017-01-02-Enum-Misc%2F</url>
    <content type="text"><![CDATA[Enum Misc All enums implicitely extends java.lang.Enum Enum in Java are type-safe You can specify values of enum constants at the creation time. Enum.values() returns an array of MyEnum’s values. Enum constants are implicitly static and final and can not be changed once created. Enum can be safely compare using: Switch-Case Statement == Operator .equals() method You can not create instance of enums by using new operator in Java because constructor of Enum in Java can only be private and Enums constants can only be created inside Enums itself. Instance of Enum in Java is created when any Enum constants are first called or referenced in code. An enum can be declared outside or inside a class, but NOT in a method. An enum declared outside a class must NOT be marked static, final , abstract, protected , or private Enums can contain constructors, methods, variables, and constant class bodies. enum constructors can have arguments, and can be overloaded. enum constructors can NEVER be invoked directly in code. They are always called automatically when an enum is initialized. The semicolon at the end of an enum declaration is optional. These are legal: 12enum Foo &#123; ONE, TWO, THREE&#125;enum Foo &#123; ONE, TWO, THREE&#125;; type safeThe advantage of this style of enumeration over the C/C++-style enum or constants is that they are type-safe, meaning that, for example, if you define a method 1public void setSuit(Suit suit) &#123; ... &#125; the caller cannot pass in a value that does not correspond to an enumeration value Language level features Since enumeration instances are all effectively singletons, they can be compared for equality using identity (“==”). In Java 5.0, the enum keyword is introduced as a special type of class that always extends java.lang.Enum. Note that the enumeration values are still static class members, though not declared as such. Enum is implemented using Arrays and common operations result in constant time. So if you are thinking of an high-performance Map, EnumMap could be a decent choice for enumeration data. We have already seen many examples of Java enum in our article 10 Examples of enum in Java and using Enum as thread-safe Singleton. In this Java tutorial, we will see simple examples of using EnumMap in Java. All keys used in EnumMap must be from same Enum type which is specified while creating EnumMap in Java. For example if you can not use different enum instances from two different enum. EnumMap is ordered collection and they are maintained in the natural order of their keys( natural order of keys means the order on which enum constant are declared inside enum type ). you can verify this while Iterating over an EnumMap in Java. Iterators of EnumMap are fail-fast Iterator , much like of ConcurrentHashMap and doesn’t throw ConcurrentModificationException and may not show effect of any modification on EnumMap during Iteration process. You can not insert null keys inside EnumMap in Java. EnumMap doesn’t allow null key and throw NullPointerException, at same time null values are permitted. EnumMap is not synchronized and it has to be synchronized manually before using it in a concurrent or multi-threaded environment. like synchronized Map in Java you can also make EnumMap synchronized by using Collections.synchronizedMap() method and as per javadoc this should be done while creating EnumMap in java to avoid accidental non synchronized access. The constructor of enum in java must be private any other access modifier will result in compilation error. Now to get the value associated with each coin you can define a public getValue() method inside Java enum like any normal Java class. Also, the semicolon in the first line is optional. 12345678public enum Currency &#123; PENNY(1), NICKLE(5), DIME(10), QUARTER(25); private int value; private Currency(int value) &#123; this.value = value; &#125;&#125;; Since constants defined inside Enum in Java are final you can safely compare them using “==”, the equality operator as shown in following example of Java Enum: Java compiler automatically generates static values() method for every enum in java. Values() method returns array of Enum constants in the same order they have listed in Enum and you can use values() to iterate over values of Enum in Java as shown in below example. Notice the order is exactly the same as defined order in the Enum. 123for(Currency coin: Currency.values())&#123; System.out.println("coin: " + coin);&#125; In Java, Enum can override methods also. Let’s see an example of overriding toString() method inside Enum in Java to provide a meaningful description for enums constants. 123456789101112131415161718192021public enum Currency &#123; ........ @Override public String toString() &#123; switch (this) &#123; case PENNY: System.out.println("Penny: " + value); break; case NICKLE: System.out.println("Nickle: " + value); break; case DIME: System.out.println("Dime: " + value); break; case QUARTER: System.out.println("Quarter: " + value); &#125; return super.toString(); &#125;&#125;; EnumSet doesn’t have any public constructor instead it provides factory methods to create instance e.g. EnumSet.of() methods. This design allows EnumSet to internally choose between two different implementations depending upon the size of Enum constants. If Enum has less than 64 constants than EnumSet uses RegularEnumSet class which internally uses a long variable to store those 64 Enum constants and if Enum has more keys than 64 then it uses JumboEnumSet. See my article the difference between RegularEnumSet and JumboEnumSet for more details. An instance of Enum in Java is created when any Enum constants are first called or referenced in code. Enum in Java can *implement the interface and override any method *like normal class It’s also worth noting that Enum in java implicitly implements both Serializable and Comparable interface. Let’s see and example of how to implement interface using Java Enum: 1234567891011public enum Currency implements Runnable&#123; PENNY(1), NICKLE(5), DIME(10), QUARTER(25); private int value; ............ @Override public void run() &#123; System.out.println("Enum in Java implement interfaces"); &#125;&#125; You can define abstract methods inside Enum in Java and can also provide a different implementation for different instances of enum in java. Let’s see an example of using abstract method inside enum in java 1234567891011121314151617181920212223242526272829303132333435 public enum Currency &#123; PENNY(1) &#123; @Override public String color() &#123; return "copper"; &#125; &#125;, NICKLE(5) &#123; @Override public String color() &#123; return "bronze"; &#125; &#125;, DIME(10) &#123; @Override public String color() &#123; return "silver"; &#125; &#125;, QUARTER(25) &#123; @Override public String color() &#123; return "silver"; &#125; &#125;; private int value; public abstract String color(); private Currency(int value) &#123; this.value = value; &#125; &#125; In this example since every coin will have the different color we made the color() method abstract and let each instance of Enum to define their own color. You can get color of any coin by just calling the color() method as shown in below example of Java Enum: System.out.println(“Color: “ + Currency.DIME.color()); So that was the comprehensive list of properties, behavior and capabilities of Enumeration type in Java. I know, it’s not easy to remember all those powerful features and that’s why I have prepared this small Microsoft powerpoint slide containing all important properties of Enum in Java. You can always come back and check this slide to revise important features of Java Enum. RegularEnumSet vs. JumboEnumSet Since Enum always has fixed number of instances, data-structure which is used to store Enum can be optimized depending upon number of instances and that’s why we have two different implementation of EnumSet in Java. We will take a closer look on this concept in next paragraph. How EnumSet is implemented in JavaEnumSet is an abstract class and it provides two concrete implementations, java.util.RegularEnumSet and java.util.JumboEnumSet. Main difference between RegularEnumSet and JumboEnumSet is that former uses a long variable to store elements while later uses a long[] to store its element. Since RegularEnumSet uses long variable, which is a 64 bit data type, it can only hold that much of element. That’s why when an empty EnumSet is created using EnumSet.noneOf() method, it choose RegularEnumSet if key universe (number of enum instances in Key Enum) is less than or equal to 64 and JumboEnumSet if key universe is more than 64. Here is the code which does that : 1234567public static &lt;E extends Enum&lt;E&gt;&gt; EnumSet&lt;E&gt; noneOf(Class&lt;E&gt; elementType) &#123; .. ............ if (universe.length &lt;= 64) return new RegularEnumSet&lt;E&gt;(elementType, universe); else return new JumboEnumSet&lt;E&gt;(elementType, universe); &#125; EnumSet is not thread-safe, which means if it needs to be externally synchronized, when multiple thread access it and one of them modifies the Collection. EnumSet can not be used to store any other object except Enum, at the same time you can not store instances of two different Enum. EnumSet doesn’t allow Null elements. EnumSet Iterators are fail-safe in nature. Difference between EnumMap vs HashMap Internally EnumMap is represented using Array and provides constant time performance for common methods e.g. get() or put(). Now let’s see few differences between EnumMap vs HashMap : As said earlier, first and foremost difference between EnumMap and HashMap is that EnumMap is optimized for enum keys while HashMap is a general purpose Map implementation similar to Hashtable. you can not use any type other than Enum as key in EnumMap but you can use both Enum and any other Object as key in HashMap. Another difference between EnumMap and HashMap is performance. as discussed in the previous point, due to specialized optimization done for Enum keys, EnumMap is likely to perform better than HashMap when using enum as key object. One more thing which can be considered as the difference between HashMap and EnumMap is the probability of Collision. Since Enum is internally maintained as array ** and they are **stored in their natural order using ordinal(), as shown in following code which is taken from put() method of EnumMap.Since EnumMap doesn’t call hashCode method on keys, there is no chance of collision. 123int index = ((Enum)key).ordinal(); Object oldValue = vals[index]; vals[index] = maskNull(value); you can use both == and equals() method to compare Enum, they will produce same result because equals() method of Java.lang.Enum internally uses == to compare enum in Java. Since every Enum in Java implicitly extends java.lang.Enum ,and since equals() method is declared final, there is no chance of overriding equals method in user defined enum But there are still some slight difference, because == (equality operator) being operator and equals() being method. Using == for comparing Enum can prevent NullPointerException == method provides type safety during compile time A fail-fast system is nothing but immediately report any failure that is likely to lead to failure. When a problem occurs, a fail-fast system fails immediately. In Java, we can find this behavior with iterators. Incase, you have called iterator on a collection object, and another thread tries to modify the collection object, then concurrent modification exception will be thrown. This is called fail-fast. - See more at: Real world Examples of Enum in Java Enum as Thread Safe Singleton Strategy Pattern using Enum. to implement the Strategy interface and define individual strategy Enum as replacement of Enum String or int pattern. There is now no need to use String or integer constant to represent fixed set of things e.g. status of object like ON and OFF for a button or START, IN PROGRESS and DONE for a Task. Enum is much better suited for those needs as it provide compile time type safety and better debugging assistent than String or Integer. Enum as State Machine Enum Java valueOf example. “You could also include valueOf() method of enum in java which is added by compiler in any enum along with values() method. Enum valueOf() is a static method which takes a string argument and can be used to convert a String into an enum. One think though you would like to keep in mind is that valueOf(String) method of enum will throw “Exception in thread “main” java.lang.IllegalArgumentException: No enum const class” if you supply any string other than enum values. Reverse lookupOften in your object model it is common to have data that is naturally “associated” with an enumeration. Since an enum is a class, it is easy to represent this associated information as class fields. Often it is desirable to “lookup” the associated enumeration using the field value. This is easy to do using a static java.util.Map. Take, for example, a Status enum that has an associated status code. 123456789101112131415161718192021222324252627public enum Status&#123; WAITING(0), READY(1), SKIPPED(-1), COMPLETED(5); private static final Map&lt;Integer,Status&gt; lookup = new HashMap&lt;Integer,Status&gt;(); static &#123; for(Status s : EnumSet.allOf(Status.class)) lookup.put(s.getCode(), s); &#125; private int code; private Status(int code) &#123; this.code = code; &#125; public int getCode() &#123; return code; &#125; public static Status get(int code) &#123; return lookup.get(code); &#125;&#125; Sleek EnumMapWhy would I use an EnumMap rather than a HashMap? The primary reasons boil down to some inherent advantages of Java’s enum as stated in the Javadoc documentation for EnumMap: “Enum maps are represented internally as arrays. This representation is extremely compact and efficient.” Later in the same Javadoc documentation, there is an “Implementation Note” that states: “All basic operations execute in constant time. They are likely (though not guaranteed) to be faster than their HashMap counterparts.” The Javadoc documentation states similar advantages for the EnumSet over the HashSet: Enum sets are represented internally as bit vectors. This representation is extremely compact and efficient. The space and time performance of this class should be good enough to allow its use as a high-quality, typesafe alternative to traditional int-based ‘bit flags.’ … Implementation note: All basic operations execute in constant time. They are likely (though not guaranteed) to be much faster than their HashSet counterparts. Even bulk operations execute in constant time if their argument is also an enum set. Reference http://crunchify.com/why-and-for-what-should-i-use-enum-java-enum-examples/ http://www.ajaxonomy.com/2007/java/making-the-most-of-java-50-enum-tricks http://www.javaworld.com/article/2073430/the-sleek-enummap-and-enumset.html http://javarevisited.blogspot.com/2011/08/enum-in-java-example-tutorial.html#ixzz4W6bC2mUV http://javarevisited.blogspot.in/2011/08/enum-in-java-example-tutorial.html http://www.java67.com/2013/11/difference-between-regularenumset-and-jumboenumset-java.html#ixzz4WANO22q5 http://javarevisited.blogspot.com/2011/08/enum-in-java-example-tutorial.html#ixzz4WAP43bUB http://javarevisited.blogspot.com/2012/09/difference-between-enummap-and-hashmap-in-java-vs.html#ixzz4WAQcrjkC http://javarevisited.blogspot.com/2013/04/how-to-compare-two-enum-in-java-equals.html#ixzz4WARs9slQ]]></content>
      <tags>
        <tag>java</tag>
        <tag>enum</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IntelliJ Tips]]></title>
    <url>%2F2017-01-03-IntelliJ-Tips%2F</url>
    <content type="text"><![CDATA[Shortcuts Ctrl+J: To show JavaDoc Cmd+Alt+B: To show interface implementations Alt+Enter: when cursor on class declare line, press Alt+Enter can quickly create a unit test against this test class Ctrl+Shift+A: Action window, just like sublime, search IDE actions F11: add bookmark, Shift+F11: show bookmarks Shift + Escape: close bottom source pannel or left side barGo to settings, Editor-&gt;General-&gt;Mouse-&gt; change font size (Zoom) Alt + F: Git refresh : Assign it in Intellij key map Ctrl + T: swich different editorsAlt + F1: Show in , then preses ‘1’ to show file in project viewer, like “scroll from source”use Ctrl+Q to show quick documentation for the element at caret. to show javadocAlt + F12: to open terminal window, then ‘mvn package’ Ctrl+I: implement methodsAlt+A: code completion, this is customized in keymap, for ‘basic’Press Tab / Shift+Tab. To change indentation of a text fragmentCtrl+Alt+I To fix indentationCtrl+D Duplicate Line or SelectionAlt + Insert: generate code (getter or setter, toString)Ctrl+F12: List all opened files, press Enter to selectAlt+1: to show the project tree window, press Alt+1 again will close the project structureAlt+7: show structureGo to interface implementations, the shortcut In PC, it is CTRL + ALT + B: JetBrains navigation documentation. maximize edito pane: Ctrl + Shift + F12 (Default keymap).- Ctrl+R: repalce- F6: move file, Shift+F6: rename file- Ctrl+Alt+ &lt;-: go back to previous position- Ctrl+Alt+S: show settings- bookmakr: F11 toggle for annonymous bookmark, Shift+F11: show all bookmarks. Ctrl+F11: then press 0,1,2,3, etc. then Ctrl +1, Ctrl +3, to go to that bookmarkmaster password: hello123- copy file name: focus file in “project view”, press Ctrl+C- Incremental search: Ctrl+F, then use up/down arrow keys to navigate- Ctrl+Shift+N: open files by file name pattern- Press Ctrl+Shift+B . Press the Ctrl+Shift keys and hover your mouse pointer over the symbol. When the symbol turns to a hyperlink, click it without releasing Ctrl+Shift keys. The type declaration of the symbol opens in the editor. Ctrl+Shift+F7: highlight all references (of selected method)put carpet on a name, Ctrl+B will show the definitionCtrl+Shift+Alt+F: copy current file name go to matching braceCtrl+} will move to the close bracket.Ctrl+{ will move to the open bracket split editorgo to menu “window” -&gt; “editor tabs” -&gt; splitFirslty open wealth-access-ui project, which only contains files at root, then right click in left pane, and chose import wealth-acecss-ui, it will load module.Use F2/Shift+F2 keys to jump between highlighted syntax errors.Use Ctrl+Alt+Up/Ctrl+Alt+Down shortcuts to jump between compiler error messages or search operation results.Use Ctrl+J to complete any valid Live Template abbreviation if you don’t remember it. For example, type it and press Ctrl+J to see what happens. Navigating to the declaration of a symbolPlace the caret at the desired symbol in the editor.Do one of the following:On the main menu, choose Navigate | Declaration.Press Ctrl+B.Click the middle mouse button.Keeping Ctrl pressed, point to the symbol, and click, when it turns to a hyperlink. You can also see declaration at the tooltip while keeping Ctrl pressed. Ctrl+Shift+A: Action window, just like sublime, search IDE actionsF11: add bookmark,Shift+F11: show bookmarksShift + Escape: close bottom source pannel or left side barGo to settings, Editor-&gt;General-&gt;Mouse-&gt; change font size (Zoom)Alt + F: Git refresh : Assign it in Intellij key mapCtrl + T: swich different editorsAlt + F1: Show in , then preses ‘1’ to show file in project viewer, like “scroll from source”use Ctrl+Q to show quick documentation for the element at caret. to show javadocAlt + F12: to open terminal window, then ‘mvn package’Ctrl+I: implement methodsAlt+A: code completion, this is customized in keymap, for ‘basic’Press Tab / Shift+Tab. To change indentation of a text fragmentCtrl+Alt+I To fix indentationCtrl+D Duplicate Line or SelectionAlt + Insert: generate code (getter or setter, toString)Ctrl+F12: List all opened files, press Enter to selectAlt+1: to show the project tree window, press Alt+1 again will close the project structureAlt+7: show structureGo to interface implementations, the shortcut In PC, it is CTRL + ALT + B: JetBrains navigation documentation. maximize edito pane: Ctrl + Shift + F12 (Default keymap). Ctrl+R: repalce F6: move file, Shift+F6: rename file Ctrl+Alt+ &lt;-: go back to previous position Ctrl+Alt+S: show settings bookmakr: F11 toggle for annonymous bookmark, Shift+F11: show all bookmarks. Ctrl+F11: then press 0,1,2,3, etc. then Ctrl +1, Ctrl +3, to go to that bookmarkmaster password: hello123 copy file name: focus file in “project view”, press Ctrl+C Incremental search: Ctrl+F, then use up/down arrow keys to navigate Ctrl+Shift+N: open files by file name pattern Press Ctrl+Shift+B . Press the Ctrl+Shift keys and hover your mouse pointer over the symbol. When the symbol turns to a hyperlink, click it without releasing Ctrl+Shift keys. The type declaration of the symbol opens in the editor. Ctrl+Shift+F7: highlight all references (of selected method) put carpet on a name, Ctrl+B will show the definition Ctrl+Shift+Alt+F: copy current file name – go to matching braceCtrl+} will move to the close bracket.Ctrl+{ will move to the open bracket – split editorgo to menu “window” -&gt; “editor tabs” -&gt; split Firslty open wealth-access-ui project, which only contains files at root, then right click in left pane, and chose import wealth-acecss-ui, it will load module.Use F2/Shift+F2 keys to jump between highlighted syntax errors.Use Ctrl+Alt+Up/Ctrl+Alt+Down shortcuts to jump between compiler error messages or search operation results.Use Ctrl+J to complete any valid Live Template abbreviation if you don’t remember it. For example, type it and press Ctrl+J to see what happens. Navigating to the declaration of a symbol Place the caret at the desired symbol in the editor.Do one of the following:On the main menu, choose Navigate | Declaration.Press Ctrl+B.Click the middle mouse button.Keeping Ctrl pressed, point to the symbol, and click, when it turns to a hyperlink. You can also see declaration at the tooltip while keeping Ctrl pressed. By defining a Scope when searching, you can include/exclude arbitrary files/folders from that scope.Detailed AnswerOne way to achieve your requirement (excluding files and folders from a search) is to define a custom scope. This is specifically useful because sometimes you just want to exclude a folder from your search and not from the whole project.Follow these steps:Edit -&gt; Find -&gt; Find in path or press Ctrl+Shift+F.Choose Custom in the Scope section and then choose how to add xx-properties project to xx project as dependenciesselect project and press F4 to open properites, chose ‘module’ in left pane and then click “+”, chose ‘import module’, then chose the properties project Intelij classesAnnotation Nullable123456789101112131415161718192021222324252627282930313233/* * Copyright 2000-2009 JetBrains s.r.o. * * Licensed under the Apache License, Version 2.0 (the "License"); * you may not use this file except in compliance with the License. * You may obtain a copy of the License at * * http://www.apache.org/licenses/LICENSE-2.0 * * Unless required by applicable law or agreed to in writing, software * distributed under the License is distributed on an "AS IS" BASIS, * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. * See the License for the specific language governing permissions and * limitations under the License. */package org.jetbrains.annotations;import java.lang.annotation.*;/** * An element annotated with Nullable claims &lt;code&gt;null&lt;/code&gt; value is perfectly &lt;em&gt;valid&lt;/em&gt; * to return (for methods), pass to (parameters) and hold (local variables and fields). * Apart from documentation purposes this annotation is intended to be used by static analysis tools * to validate against probable runtime errors and element contract violations. * @author max */@Documented@Retention(RetentionPolicy.CLASS)@Target(&#123;ElementType.METHOD, ElementType.FIELD, ElementType.PARAMETER, ElementType.LOCAL_VARIABLE&#125;)public @interface Nullable &#123; String value() default "";&#125;]]></content>
      <tags>
        <tag>java</tag>
        <tag>intelliJ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java GC notes]]></title>
    <url>%2F2016-12-30-Java-GC%2F</url>
    <content type="text"><![CDATA[verbose:gcverbose:gc prints right after each gc collection and prints details about each generation memory details. Here is blog on how to read verbose gc If you are trying to look for memory leak, verbose:gc may not be enough. Use some visualization tools like jhat (or) visualvm etc., 4416K-&gt;512K(4928K), 0.0081170 secs Before GC used memory is 4416KAfter GC used memory is 512KTotal allocated memory is 4928K -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -Xloggc:C:/Users/tzhang17/temp/gc/gc.log a typical ratio of YoungGen vs. OldGen is 1:3 or 33%. Minimizing the frequency of major GC collections is a key aspect for optimal performance so it is very important that you understand and estimate how much memory you need during your peak volume. Again, your type of application and data will dictate how much memory you need. Shopping cart type of applications (long lived objects) involving large and non-serialized session data typically need large Java Heap and lot of OldGen space. Stateless and XML processing heavy applications (lot of short lived objects) require proper YoungGen space in order to minimize frequency of major collections. Generational collectionAccording to the generational hypothesis [21], most objects die young and consequently older objects tend to live longer. Generational collection capitalises on the generational hypothesis by dividing the available memory space into multiple regions called generations. Garbage collector passes are less frequent as the generations grow older and objects are always allocated into the newest generation. If the object survives a garbage collection, it is promoted to an older generation. Each generation can have aseparate garbage collection strategy. Reference counting Reference counting uses a counter per object to record the number ofreferences to the object. The pointer is incremented each time a reference towards theobject is created. The object is reclaimed when its reference count drops to zero. Referencecounting is being extensively used by scripting languages such as Perl. GC strategyMark-Sweep garbage collection is most times followed by a compaction phase in order to avoid memory fragmentation. The compaction phase requires moving the objects to adjacent memory locations, thus making Mark-Sweep quite an expensive algorithm for large memory multiprocessor environments, unless a multithreaded heap compactor is employed.Simple reference counting is also unsuitable for high throughput environments because it requires objects to be reclaimed on pointer updates; if a pointer is removed and the reference count of the pointed object drops to zero, the runtime system is required to collect both that object and the objects it references. Furthermore, a major drawback of reference counting is its inability to collect circular data structures, such as doubly linked lists. Despite its drawbacks, the simplicity in the implementation of reference counting made it the preferred garbage collection strategy in runtime environments with a limited lifetime, such as scripting languages. mutable memoryMemory in a typical JVM is organised in a series of mutable (garbage collected) and immutable zones. Class code is usually loaded in immutable space1 and remains there until the JVM is stopped. Also, the code emitted from the JIT compiler is temporarily stored in immutable space. The actual allocations take place in the heap, which is a contiguous memory area. Apart from the class member values, each object also containsadditional data such as a pointer to the respective class methods and flags related to locking andgarbage collection. In most virtual machines, object headers take up to 8–12 bytes of additionalstorage space for each object, and can therefore sadle a program with significant performanceand space overhead. A lot of work has been put into compacting the object header [6], which,in some cases, resulted in space savings of up to 20%. A failure to allocate space for an object triggers a garbage collection cycle. The root setis determined by conservatively scanning the stacks of running or suspended threads and thecurrent values of the processor registers for potential pointers to the heap. Root set acquisitioncan also be a performance bottleneck in the case when a large number of threads is executedconcurrently, though these costs can be amortised using clever co-operation of the garbagecollector with the JIT. JDK 1.5Sun’s JVM is an implementation of the 1.5 version of the Java language specification. It features an adaptive optimising JIT compiler, the well-known Hotspot engine, and a choice of three garbage collectors [2, 12]. Sun’s JVM is based upon a generational copying garbage collector that utilises two generations Figure 1 presents the heap organisation, which is shared among all collectors. Allocations initially occur in the eden space and survivors are promoted to one of the survivor spaces in a copying fashion. Optionally, portions of the heap space can be allocated to individual threads (Thread-Local Heaps (TLHs)), in order to speed up allocations on large-heap multithreaded environments. Objects that reach a certain age threshold, usually measured in minor garbage collection cycles, are copied to the tenured generation where they are left untouched until a major collection occurs. A mark-compact garbage collector is used for the tenured generation. Tuning advise Unless you have specific hardware constraints, devote as much memory as you can to the virtual machine. A big heap size offers the opportunity for less frequent, albeit more time consuming, full heap collections. In a throughput-oriented environment sacrificing pause times to allow more CPU time for the executed application is often a good compromise. Do not allow the virtual machine to be swapped out to disk, as this is catastrophic for performance. In an application server that only runs a single virtual machine, you could devote about 90% of its available RAM to it and turn off paging, without risking the failure of either the virtual machine or the operating system. Calculate the memory allocation rate for your application. It is a significant measurement that you should perform by exposing the application to full workload. Its impact varies depending on the underlying hardware. As a rule of thumb, on a multiprocessor machine, each processor could easily generate more than 150MB of garbage per second. High allocation rates can be efficiently dealt with by using parallel collectors or large eden heap sizes. Committed heapA MemoryUsage object represents a snapshot of memory usage. Instances of the MemoryUsage class are usually constructed by methods that are used to obtain memory usage information about individual memory pool of the Java virtual machine or the heap or non-heap memory of the Java virtual machine as a whole.A MemoryUsage object contains four values: init represents the initial amount of memory (in bytes) that the Java virtual machine requests from the operating system for memory management during startup. The Java virtual machine may request additional memory from the operating system and may also release memory to the system over time. The value of init may be undefined.used represents the amount of memory currently used (in bytes).committed represents the amount of memory (in bytes) that is guaranteed to be available for use by the Java virtual machine. The amount of committed memory may change over time (increase or decrease). The Java virtual machine may release memory to the system and committed could be less than init. committed will always be greater than or equal to used.max represents the maximum amount of memory (in bytes) that can be used for memory management. Its value may be undefined. The maximum amount of memory may change over time if defined. The amount of used and committed memory will always be less than or equal to max if max is defined. A memory allocation may fail if it attempts to increase the used memory such that used &gt; committed even if used &lt;= max would still be true (for example, when the system is low on virtual memory).Below is a picture showing an example of a memory pool: +———————————————-+ +//////////////// | + +//////////////// | + +———————————————-+ |--------| init |---------------| used |---------------------------| committed |----------------------------------------------|Garbage Collection for JVMInterpreting vs compile The HotSpot JVM (and other modern JVMs) uses a combination of bytecode interpretation and dynamic compilation. When a class is first loaded, the JVM executes it by interpreting the bytecode. At some point, if a method is run often enough, the dynamic compiler kicks in and converts it to machine code; when compilation completes, it switches from interpretation to direct execution. Code may also be decompiled (reverting to interpreted execution) and recompiled for various reasons, such as loading a class that invalidates assumptions made by prior compilations, or gathering sufficient profiling data to decide that a code path should be recompiled with different optimizations. One of the challenges of writing good benchmarks (in any language) is that optimizing compilers are adept at spotting and eliminating dead code—code that has no effect on the outcome. Since benchmarks often don’t compute anything, they are an easy target for the optimizer. Most of the time, it is a good thing when the optimizer prunes dead code from a program, but for a benchmark this is a big problem because then you are measuring less execution than you think. Many microbenchmarks perform much “better” when run with HotSpot’s -server compiler than with -client, not just because the server compiler can produce more efficient code, but also because it is more adept at optimizing dead code. Writing effective performance tests requires tricking the optimizer into not optimizing away your benchmark as dead code. This requires every computed result to be used somehow by your program—in a way that does not require synchronization or substantial computation. We happen to need it to verify the correctness of the algorithm, but you can ensure that a value is used by printing it out. However, you should avoid doing I/O while the test is actually running, so as not to distort the run time measurement. A cheap trick for preventing a calculation from being optimized away without introducing too much overhead is to compute the hashCode of the field of some derived object, compare it to an arbitrary value such as the current value of System. nanoTime, and print a useless and ignorable message if they happen to match:12if(foo.x.hashCode()==System.nanoTime()) System.out.println(" "); The comparison will rarely succeed, and if it does, its only effect will be to insert a harmless space character into the output. (The print method buffers output until println is called, so in the rare case that hashCode and System.nanoTime are equal no I/O is actually performed.) Not only should every computed result be used, but results should also be unguessable. Otherwise, a smart dynamic optimizing compiler is allowed to replace actions with precomputed results. Java GCJava’s GC considers objects “garbage” if they aren’t reachable through a chain starting at a garbage collection root, so these objects will be collected. Even though objects may point to each other to form a cycle, they’re still garbage if they’re cut off from the root. See the section on unreachable objects in Appendix A: The Truth About Garbage Collection in Java Platform Performance: Strategies and Tactics (free ebook, also available on Safari) for the gory details. Java Garbage collector handles circular-reference!How? There are special objects called called garbage-collection roots (GC roots). These are always reachable and so is any object that has them at its own root. A simple Java application has the following GC roots: Local variables in the main method The main thread Static variables of the main classTo determine which objects are no longer in use, the JVM intermittently runs what is very aptly called a mark-and-sweep algorithm. It works as follows The algorithm traverses all object references, starting with the GC roots, and marks every object found as alive. All of the heap memory that is not occupied by marked objects is reclaimed. It is simply marked as free, essentially swept free of unused objects.So if any object is not reachable from the GC roots(even if it is self-referenced or cyclic-referenced) it will be subjected to garbage collection.Ofcourse sometimes this may led to memory leak if programmer forgets to dereference an object. The actual answer to this is implementation dependent. The Sun JVM keeps track of some set of root objects (threads and the like), and when it needs to do a garbage collection, traces out which objects are reachable from those and saves them, discarding the rest. It’s actually more complicated than that to allow for some optimizations, but that is the basic principle. This version does not care about circular references: as long as no live object holds a reference to a dead one, it can be GCed. Other JVMs can use a method known as reference counting. When a reference is created to the object, some counter is incremented, and when the reference goes out of scope, the counter is decremented. If the counter reaches zero, the object is finalized and garbage collected. This version, however, does allow for the possibility of circular references that would never be garbage collected. As a safeguard, many such JVMs include a backup method to determine which objects actually are dead which it runs periodically to resolve self-references and defrag the heap. A garbage collector starts from some “root” set of places that are always considered “reachable”, such as the CPU registers, stack, and global variables. It works by finding any pointers in those areas, and recursively finding everything they point at. Once it’s found all that, everything else is garbage. There are, of course, quite a few variations, mostly for the sake of speed. For example, most modern garbage collectors are “generational”, meaning that they divide objects into generations, and as an object gets older, the garbage collector goes longer and longer between times that it tries to figure out whether that object is still valid or not – it just starts to assume that if it has lived a long time, chances are pretty good that it’ll continue to live even longer. Nonetheless, the basic idea remains the same: it’s all based on starting from some root set of things that it takes for granted could still be used, and then chasing all the pointers to find what else could be in use. Interesting aside: may people are often surprised by the degree of similarity between this part of a garbage collector and code for marshaling objects for things like remote procedure calls. In each case, you’re starting from some root set of objects, and chasing pointers to find all the other objects those refer to… How Garbage Collection Really WorksMany people think garbage collection collects and discards dead objects. In reality, Java garbage collection is doing the opposite! Live objects are tracked and everything else designated garbage. As you’ll see, this fundamental misunderstanding can lead to many performance problems. Garbage-Collection Roots—The Source of All Object TreesEvery object tree must have one or more root objects. As long as the application can reach those roots, the whole tree is reachable. But when are those root objects considered reachable? Special objects called garbage-collection roots (GC roots; see Figure 2.2) are always reachable and so is any object that has a garbage-collection root at its own root. There are four kinds of GC roots in Java: Local variables are kept alive by the stack of a thread. This is not a real object virtual reference and thus is not visible. For all intents and purposes, local variables are GC roots. Active Java threads are always considered live objects and are therefore GC roots. This is especially important for thread local variables. Static variables are referenced by their classes. This fact makes them de facto GC roots. Classes themselves can be garbage-collected, which would remove all referenced static variables. This is of special importance when we use application servers, OSGi containers or class loaders in general. We will discuss the related problems in the Problem Patterns section. JNI References are Java objects that the native code has created as part of a JNI call. Objects thus created are treated specially because the JVM does not know if it is being referenced by the native code or not. Such objects represent a very special form of GC root, which we will examine in more detail in the Problem Patterns section below. Therefore, a simple Java application has the following GC roots: Local variables in the main method The main thread Static variables of the main class Marking and Sweeping Away GarbageTo determine which objects are no longer in use, the JVM intermittently runs what is very aptly called a mark-and-sweep algorithm. As you might intuit, it’s a straightforward, two-step process: The algorithm traverses all object references, starting with the GC roots, and marks every object found as alive. All of the heap memory that is not occupied by marked objects is reclaimed. It is simply marked as free, essentially swept free of unused objects. Garbage collectors which rely solely on reference counting are generally vulnerable to failing to collection self-referential structures such as this. These GCs rely on a count of the number of references to the object in order to calculate whether a given object is reachable. Non-reference counting approaches apply a more comprehensive reachability test to determine whether an object is eligible to be collected. These systems define an object (or set of objects) which are always assumed to be reachable. Any object for which references are available from this object graph is considered ineligible for collection. Any object not directly accessible from this object is not. Thus, cycles do not end up affecting reachability, and can be collected. Tracing collector vs. countering collectorThere are two primary types of garbage collectors, although often a hybrid approach is found between these to suit particular needs. The first type, the one which might be the most intuitive, is a reference counting collector. The second one, which is most similar to what we described above, is a tracing collector. Reference Counting CollectorWhen a new memory object is allocated by the GC, it is given an integer count field. Every time a pointer is made to that object, a reference, the count is increased. So long as the count is a positive non-zero integer, the object is actively being referenced and is still alive.When a reference to the object is removed, the count is decremented. When the count reaches zero, the object is dead and can be immediately reclaimed.There are a number of points to remember about Reference Counting collectors: Circular references will never be reclaimed, even if the entire set of objects is dead. Reference counting is pervasive: The entire program must be made aware of the system, and every pointer reference or dereference must be accompanied by an appropriate increment or decrement. Failing to maintain the count, even once in a large program, will create memory problems for your program. Reference counting can be costly, because counts must be manipulated for every pointer operation, and the count must be tested against zero on ever decrement. These operations can, if used often enough, create a performance penalty for your program. These types of collectors are often called cooperative collectors because they require cooperation from the rest of the system to maintain the counts. Tracing CollectorTracing collectors are entirely dissimilar from reference counting collectors, and have opposite strengths and weaknesses.When the Tracing GC allocates a new memory chunk, the GC does not create a counter, but it does create a flag to determine when the item has been marked, and a pointer to the object that the GC keeps. The flags are not manipulated by the program itself, but are only manipulated by the GC when it performs a run. During a GC run, the program execution typically halts. This can cause intermittent pauses in the program, pauses which can be quite long if there are many memory objects to trace. The GC selects a set of root objects which are available to the current program scope and parent scopes. Starting from these objects, the GC identifies all pointers within the objects, called children. The object itself is marked as being alive, and then the collector moves to each child and marks it in the same way. The memory objects form a sort of tree structure, and the GC traverses this tree using recursive or stack-based methods. At the end of the GC run, when there are no more children to be marked, all unmarked objects are considered unreachable and therefore dead. All dead objects are collected. A few points to remember about Tracing GCs: Tracing GCs can be used to find cycles, memory objects whose pointers form circular structures. Reference Counting schemes cannot do this. Tracing GCs cause pauses in the program, and these pauses can become unbearably long in some complex programs that use many small memory objects. Dead objects are not reclaimed immediately. Reclamation only occurs after a GC run. This causes a certain inefficiency in memory usage. Tracing collectors do not require the program to account explicitly for memory counts or memory status updates. All memory tracking logic is stored inside the GC itself. This makes it easier to write extensions for these systems, and also makes it easier to install a Tracing GC in an existing system then to install a Reference Counting one. Tracing GCs are often called uncooperative collectors because they do not require cooperation from the rest of the system to function properly.Hybrid Collectors Sometimes, reference counting schemes will utilize Tracing systems to find cyclical garbage. Tracing systems may employ reference counts on very large objects to ensure they are reclaimed quickly. These are just two examples of hybridized garbage collectors that are more common then either of the two “pure” types described above. In later chapters, we will discuss garbage collectors and their algorithms in more detail. Java runtime data areaThere are 5 areas Heap Java Stack Method Area Native method area PC/Register Java GCJava’s GC considers objects “garbage” if they aren’t reachable through a chain starting at a garbage collection root, so these objects will be collected. Even though objects may point to each other to form a cycle, they’re still garbage if they’re cut off from the root. See the section on unreachable objects in Appendix A: The Truth About Garbage Collection in Java Platform Performance: Strategies and Tactics (free ebook, also available on Safari) for the gory details. Java Garbage collector handles circular-reference!How? There are special objects called called garbage-collection roots (GC roots). These are always reachable and so is any object that has them at its own root. A simple Java application has the following GC roots: Local variables in the main method The main thread Static variables of the main classTo determine which objects are no longer in use, the JVM intermittently runs what is very aptly called a mark-and-sweep algorithm. It works as follows The algorithm traverses all object references, starting with the GC roots, and marks every object found as alive. All of the heap memory that is not occupied by marked objects is reclaimed. It is simply marked as free, essentially swept free of unused objects.So if any object is not reachable from the GC roots(even if it is self-referenced or cyclic-referenced) it will be subjected to garbage collection.Ofcourse sometimes this may led to memory leak if programmer forgets to dereference an object. The actual answer to this is implementation dependent. The Sun JVM keeps track of some set of root objects (threads and the like), and when it needs to do a garbage collection, traces out which objects are reachable from those and saves them, discarding the rest. It’s actually more complicated than that to allow for some optimizations, but that is the basic principle. This version does not care about circular references: as long as no live object holds a reference to a dead one, it can be GCed. Other JVMs can use a method known as reference counting. When a reference is created to the object, some counter is incremented, and when the reference goes out of scope, the counter is decremented. If the counter reaches zero, the object is finalized and garbage collected. This version, however, does allow for the possibility of circular references that would never be garbage collected. As a safeguard, many such JVMs include a backup method to determine which objects actually are dead which it runs periodically to resolve self-references and defrag the heap. A garbage collector starts from some “root” set of places that are always considered “reachable”, such as the CPU registers, stack, and global variables. It works by finding any pointers in those areas, and recursively finding everything they point at. Once it’s found all that, everything else is garbage. There are, of course, quite a few variations, mostly for the sake of speed. For example, most modern garbage collectors are “generational”, meaning that they divide objects into generations, and as an object gets older, the garbage collector goes longer and longer between times that it tries to figure out whether that object is still valid or not – it just starts to assume that if it has lived a long time, chances are pretty good that it’ll continue to live even longer. Nonetheless, the basic idea remains the same: it’s all based on starting from some root set of things that it takes for granted could still be used, and then chasing all the pointers to find what else could be in use. Interesting aside: may people are often surprised by the degree of similarity between this part of a garbage collector and code for marshaling objects for things like remote procedure calls. In each case, you’re starting from some root set of objects, and chasing pointers to find all the other objects those refer to… How Garbage Collection Really WorksMany people think garbage collection collects and discards dead objects. In reality, Java garbage collection is doing the opposite! Live objects are tracked and everything else designated garbage. As you’ll see, this fundamental misunderstanding can lead to many performance problems. Garbage-Collection Roots—The Source of All Object TreesEvery object tree must have one or more root objects. As long as the application can reach those roots, the whole tree is reachable. But when are those root objects considered reachable? Special objects called garbage-collection roots (GC roots; see Figure 2.2) are always reachable and so is any object that has a garbage-collection root at its own root. There are four kinds of GC roots in Java: Local variables are kept alive by the stack of a thread. This is not a real object virtual reference and thus is not visible. For all intents and purposes, local variables are GC roots. Active Java threads are always considered live objects and are therefore GC roots. This is especially important for thread local variables. Static variables are referenced by their classes. This fact makes them de facto GC roots. Classes themselves can be garbage-collected, which would remove all referenced static variables. This is of special importance when we use application servers, OSGi containers or class loaders in general. We will discuss the related problems in the Problem Patterns section. JNI References are Java objects that the native code has created as part of a JNI call. Objects thus created are treated specially because the JVM does not know if it is being referenced by the native code or not. Such objects represent a very special form of GC root, which we will examine in more detail in the Problem Patterns section below. Therefore, a simple Java application has the following GC roots: Local variables in the main method The main thread Static variables of the main class Marking and Sweeping Away GarbageTo determine which objects are no longer in use, the JVM intermittently runs what is very aptly called a mark-and-sweep algorithm. As you might intuit, it’s a straightforward, two-step process: The algorithm traverses all object references, starting with the GC roots, and marks every object found as alive. All of the heap memory that is not occupied by marked objects is reclaimed. It is simply marked as free, essentially swept free of unused objects. Garbage collectors which rely solely on reference counting are generally vulnerable to failing to collection self-referential structures such as this. These GCs rely on a count of the number of references to the object in order to calculate whether a given object is reachable. Non-reference counting approaches apply a more comprehensive reachability test to determine whether an object is eligible to be collected. These systems define an object (or set of objects) which are always assumed to be reachable. Any object for which references are available from this object graph is considered ineligible for collection. Any object not directly accessible from this object is not. Thus, cycles do not end up affecting reachability, and can be collected. Tracing collector vs. countering collectorThere are two primary types of garbage collectors, although often a hybrid approach is found between these to suit particular needs. The first type, the one which might be the most intuitive, is a reference counting collector. The second one, which is most similar to what we described above, is a tracing collector. Reference Counting CollectorWhen a new memory object is allocated by the GC, it is given an integer count field. Every time a pointer is made to that object, a reference, the count is increased. So long as the count is a positive non-zero integer, the object is actively being referenced and is still alive.When a reference to the object is removed, the count is decremented. When the count reaches zero, the object is dead and can be immediately reclaimed.There are a number of points to remember about Reference Counting collectors: Circular references will never be reclaimed, even if the entire set of objects is dead. Reference counting is pervasive: The entire program must be made aware of the system, and every pointer reference or dereference must be accompanied by an appropriate increment or decrement. Failing to maintain the count, even once in a large program, will create memory problems for your program. Reference counting can be costly, because counts must be manipulated for every pointer operation, and the count must be tested against zero on ever decrement. These operations can, if used often enough, create a performance penalty for your program. These types of collectors are often called cooperative collectors because they require cooperation from the rest of the system to maintain the counts. Tracing CollectorTracing collectors are entirely dissimilar from reference counting collectors, and have opposite strengths and weaknesses.When the Tracing GC allocates a new memory chunk, the GC does not create a counter, but it does create a flag to determine when the item has been marked, and a pointer to the object that the GC keeps. The flags are not manipulated by the program itself, but are only manipulated by the GC when it performs a run. During a GC run, the program execution typically halts. This can cause intermittent pauses in the program, pauses which can be quite long if there are many memory objects to trace. The GC selects a set of root objects which are available to the current program scope and parent scopes. Starting from these objects, the GC identifies all pointers within the objects, called children. The object itself is marked as being alive, and then the collector moves to each child and marks it in the same way. The memory objects form a sort of tree structure, and the GC traverses this tree using recursive or stack-based methods. At the end of the GC run, when there are no more children to be marked, all unmarked objects are considered unreachable and therefore dead. All dead objects are collected. A few points to remember about Tracing GCs: Tracing GCs can be used to find cycles, memory objects whose pointers form circular structures. Reference Counting schemes cannot do this. Tracing GCs cause pauses in the program, and these pauses can become unbearably long in some complex programs that use many small memory objects. Dead objects are not reclaimed immediately. Reclamation only occurs after a GC run. This causes a certain inefficiency in memory usage. Tracing collectors do not require the program to account explicitly for memory counts or memory status updates. All memory tracking logic is stored inside the GC itself. This makes it easier to write extensions for these systems, and also makes it easier to install a Tracing GC in an existing system then to install a Reference Counting one. Tracing GCs are often called uncooperative collectors because they do not require cooperation from the rest of the system to function properly.Hybrid Collectors Sometimes, reference counting schemes will utilize Tracing systems to find cyclical garbage. Tracing systems may employ reference counts on very large objects to ensure they are reclaimed quickly. These are just two examples of hybridized garbage collectors that are more common then either of the two “pure” types described above. In later chapters, we will discuss garbage collectors and their algorithms in more detail. to be callibratedG1 is a concurrent collector that operates on discrete regions within the heap. Each region (there are by default around 2,048 of them) can belong to either the old or new generation, and the generational regions need not be contiguous. The idea behind having regions in the old generation is that when the concurrent background threads look for unreferenced objects, some regions will contain more garbage than other regions. The actual collection of a region still requires that application threads be stopped, but G1 can focus on the regions that are mostly garbage and only spend a little bit of time emptying those regions. This approach—clearing out only the mostly garbage regions—is what gives G1 its name: Garbage First.That doesn’t apply to the regions in the young generation: during a young GC, the entire young generation is either freed or promoted (to a survivor space or to the old generation). Still, the young generation is defined in terms of regions, in part because it makes resizing the generations much easier if the regions are predefined.G1 has four main operations:A young collectionA background, concurrent cycleA mixed collectionIf necessary, a full GCWe’ll look at each of those in turn, starting with the G1 young collection shown in Figure 6-6. Reference http://www.ibm.com/developerworks/java/library/j-jtp10283/ https://blogs.oracle.com/jonthecollector/entry/our_collectors https://en.wikipedia.org/wiki/Garbage_collection_%28computer_science%29#Tracing_garbage_collectors http://users.cecs.anu.edu.au/~steveb/pubs/papers/urc-oopsla-2003.pdf https://www.dynatrace.com/resources/ebooks/javabook/ https://en.wikipedia.org/wiki/Tracing_garbage_collection https://en.wikibooks.org/wiki/Memory_Management/Garbage_Collection http://flyingfrogblog.blogspot.com/2013/09/how-do-reference-counting-and-tracing.html https://www.dynatrace.com/resources/ebooks/javabook/how-garbage-collection-works/ http://stackoverflow.com/questions/1910194/how-does-java-garbage-collection-work-with-circular-references http://www.java-books.us/j2ee_0003.php http://www.ibm.com/developerworks/java/library/j-jtp10283/ https://blogs.oracle.com/jonthecollector/entry/our_collectors https://en.wikipedia.org/wiki/Garbage_collection_%28computer_science%29#Tracing_garbage_collectors http://users.cecs.anu.edu.au/~steveb/pubs/papers/urc-oopsla-2003.pdf https://www.dynatrace.com/resources/ebooks/javabook/ https://en.wikipedia.org/wiki/Tracing_garbage_collection https://en.wikibooks.org/wiki/Memory_Management/Garbage_Collection http://flyingfrogblog.blogspot.com/2013/09/how-do-reference-counting-and-tracing.html https://www.dynatrace.com/resources/ebooks/javabook/how-garbage-collection-works/ http://stackoverflow.com/questions/1910194/how-does-java-garbage-collection-work-with-circular-references http://www.java-books.us/j2ee_0003.php]]></content>
      <tags>
        <tag>java</tag>
        <tag>GC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hash Code Misc]]></title>
    <url>%2F2016-12-30-HashCode-Contract%2F</url>
    <content type="text"><![CDATA[contract of hashCode : Whenever it is invoked on the same object more than once during an execution of a Java application, the hashCode method must consistently return the same integer, provided no information used in equals comparisons on the object is modified. This integer need not remain consistent from one execution of an application to another execution of the same application. If two objects are equal according to the equals(Object) method, then calling the hashCode method on each of the two objects must produce the same integer result. It is not required that if two objects are unequal according to the equals(java.lang.Object) method, then calling the hashCode method on each of the two objects must produce distinct integer results. However, the programmer should be aware that producing distinct integer results for unequal objects may improve the performance of hash tables. As much as is reasonably practical, the hashCode method defined by class Object does return distinct integers for distinct objects. (This is typically implemented by converting the internal address of the object into an integer, but this implementation technique is not required by the JavaTM programming language.) Hashcode in Java CollectionsHashcode vs. equalsEqualsFor usage in java collections framework, equals() used in following scenarios: contains or not to remove one item Below is one implementation of a Equals 123456789public class Employee&#123; public boolean equals(Object o)&#123; if(o==null) return false; if(!(o instanceof Employee)) return false; Employee other=(Employee)o; return this.employeeID==other.employeeID; &#125;&#125; HashcodeWhen insert data into HashTable, HashMap, HashSet, the hashcode used to determine where to store/search the value in list/bucket. The hashcode only point to an area in list/bucket of data. The hashtable then iterates this area (all keys with the same hash code) and uses the key’s equals() method to find the right key. Once the right key is found, the object stored for that key is returned. After you override the hashcode, you are still be able to get origional hashcode via calling int originalHashCode = System.identityHashCode(emp1); RulesIn this regard there is a rule of thumb that if you are going to overide the one of the methods (equals, hashcode), you have to override both, othersise it’s a violation of contract . HashMap tips Since searching inlined list is O(n) operation, in worst case hash collision reduce a map to linked list. This issue is recently addressed in Java 8 by replacing linked list to the tree to search in O(logN) HashMap works on the principle of hashing. HashMap stores both Key and Value in LinkedList node or as Map using immutable, final object with proper equals() and hashcode() implementation would act as perfect Java HashMap keys and improve the performance of Java HashMap by reducing collision. Immutability also allows caching their hashcode of different keys which makes overall retrieval process very fast and suggest that String and various wrapper classes e.g. Integer very good keys in Java HashMap. if the load factor is .75 it will act to re-size the map once it filled 75%. Java HashMap re-size itself by creating a new bucket array of size twice of the previous size of HashMap and then start putting every old element into that new bucket array. This process is called rehashing because it also applies the hash function to find new bucket location. there is potential race condition exists while resizing HashMap in Java, if two thread at the same time found that now HashMap needs resizing and they both try to resizing. on the process of resizing of HashMap in Java, the element in the bucket which is stored in linked list get reversed in order during their migration to new bucket because Java HashMap doesn’t append the new element at tail instead it append new element at the head to avoid tail traversing. If race condition happens then you will end up with an infinite loop. Why String, Integer and other wrapper classes are considered good keys? String, Integer and other wrapper classes are natural candidates of HashMap key, and String is most frequently used key as well because String is immutable and final, and overrides equals and hashcode() method. Immutability is best as it offers other advantages as well like thread-safety, If you can keep your hashCode same by only making certain fields final, then you go for that as well. ConcurrentHashMap ConcurrentHashMap provides better concurrency by only locking portion of map determined by concurrency level. but Hashtable provides stronger thread-safety than ConcurrentHashMap ConcurrentHashMap performs better than earlier two because it only locks a portion of Map, instead of whole Map, which is the case with Hashtable and synchronized Map. provided all functions supported by Hashtable with an additional feature called “concurrency level”, which allows ConcurrentHashMap to partition Map. ConcurrentHashMap allows multiple readers to read concurrently without any blocking. This is achieved by partitioning Map into different parts based on concurrency level and locking only a portion of Map during updates. Default concurrency level is 16, and accordingly Map is divided into 16 part and each part is governed with a different lock. This means, 16 thread can operate on Map simultaneously until they are operating on different part of Map. Since update operations like put(), remove(), putAll() or clear() is not synchronized, concurrent retrieval may not reflect most recent change on Map. ConcurrentHashMap also uses ReentrantLock to internally lock its segments. In hashmap and hashtable, you can check one item first and add it if not present. Though this code will work fine in HashMap and Hashtable, This won’t work in ConcurrentHashMap; because, during put operation whole map is not locked, and while one thread is putting value, other thread’s get() call can still return null which result in one thread overriding value inserted by other thread. Ofcourse, you can wrap whole code in synchronized block and make it thread-safe but that will only make your code single threaded. ConcurrentHashMap provides putIfAbsent(key, value) which does same thing but atomically and thus eliminates above race condition ConcurrentHashMap is best suited when you have multiple readers and few writers. If writers outnumber reader, or writer is equal to reader, than performance of ConcurrentHashMap effectively reduces to synchronized map or Hashtable. Performance of CHM drops, because you got to lock all portion of Map, and effectively each reader will wait for another writer, operating on that portion of Map. ConcurrentHashMap is a good choice for caches, which can be initialized during application start up and later accessed my many request processing threads. ConcurrentHashMap allows concurrent read and thread-safe update operation. Iterator returned by ConcurrentHashMap is weekly consistent, fail-safe and never throw ConcurrentModificationException. In Java.ConcurrentHashMap doesn’t allow null as key or value. How null work in HashMap How null key is handled in HashMap? Since equals() and hashCode() are used to store and retrieve values, how does it work in case of the null key? The null key is handled specially in HashMap, there are two separate methods for that putForNullKey(V value)and getForNullKey(). Later is offloaded version of get() to look up null keys. Null keys always map to index 0. This null case is split out into separate methods for the sake of performance in the two most commonly used operations (get and put), but incorporated with conditionals in others. In short, equals()and hashcode() method are not used in case of null keys in HashMap. Performance changes in JDK 1.7 and 1.8 There is some performance improvement done on HashMap and ArrayList from JDK 1.7, which reduce memory consumption. Due to this empty Map are lazily initialized and will cost you less memory. Earlier, when you create HashMap e.g. new HashMap() it automatically creates an array of default length e.g. 16. After some research, Java team found that most of this Map are temporary and never use that many elements, and only end up wasting memory. Also, From JDK 1.8 onwards HashMap has introduced an improved strategy to deal with high collision rate. Since a poor hash function e.g. which always return location of same bucket, can turn a HashMap into linked list, i.e. converting get() method to perform in O(n) instead of O(1) and someone can take advantage of this fact, Java now internally replace linked list to a binary true once certain threshold is breached. This ensures performance or order O(log(n)) even in the worst case where a hash function is not distributing keys properly. such change is creating empty ArrayList and HashMap with size zero in JDK 1.7.0_40 update. If you are running on Java 1.6 or earlier version of Java 1.7, you can open code of java.util.ArrayList and check that, currently empty ArrayList is initialized with Object array of size 10. If you create several temporary list in your program, which remains uninitialized, due to any reason then you are not only losing memory but also losing performance by giving your garbage collector more work. Same is true for empty HashMap, which was initialized by default initial capacity of 16. This changes are result of observation made by Nathan Reynolds, and Architect at Oracle, which apparently analysed 670 Java heap dumps from different Java programs to find out memory hogs. By the way, it’s not just memory, it’s also extra work-load for Garbage collector Hashtable One of the major differences between HashMap and Hashtable is that HashMap is non-synchronized whereas Hashtable is synchronized Another difference is HashMap allows one null key and null values but *Hashtable doesn’t allow null * key or values. Another significant difference between HashMap vs Hashtable is that Iterator in the HashMap is a fail-fast ** iterator while the enumerator for the **Hashtable is not and throw ConcurrentModificationException if any other Thread modifies the map structurally by adding or removing any element except Iterator’s own remove() method ConcurrentHashMap vs Hashtable vs Synchronized Map Though all three collection classes are thread-safe and can be used in multi-threaded, concurrent Java application, there is a significant difference between them, which arise from the fact that how they achieve their thread-safety. Hashtable is a legacy class from JDK 1.1 itself, which uses synchronized methods to achieve thread-safety. All methods of Hashtable are synchronized which makes them quite slow due to contention if a number of thread increases. Synchronized Map is also not very different than Hashtable and provides similar performance in concurrent Java programs. The only difference between Hashtable and Synchronized Map is that later is not a legacy and you can wrap any Map to create it’s synchronized version by using Collections.synchronizedMap() Unlike Hashtable and Synchronized Map, it never locks whole Map, instead, it divides the map into segments and locking is done on those. Though it performs better if a number of reader threads are greater than the number of writer threads. ConcurrentHashMap and CopyOnWriteArrayList implementations provide much higher concurrency while preserving thread safety, with some minor compromises in their promises to callers. ConcurrentHashMap and CopyOnWriteArrayList are not necessarily useful everywhere you might use HashMap or ArrayList, but are designed to optimize specific common situations. ConcurrentHashMap does not allow null keys or null values while synchronized HashMap allows one null key. Synchronized List CopyOnWriteArrayList and CopyOnWriteArraySetCopyOnWriteArrayList is a concurrent alternative of synchronized List. CopyOnWriteArrayList provides better concurrency than synchronized List by allowing multiple concurrent reader and replacing the whole list on write operation. Yes, write operation is costly on CopyOnWriteArrayList but it performs better when there are multiple reader and requirement of iteration is more than writing. Since CopyOnWriteArrayList Iterator also don’t throw ConcurrencModificationException it eliminates need to lock the collection during iteration. Remember both ConcurrentHashMap and CopyOnWriteArrayList doesn’t provides same level of locking as Synchronized Collection and achieves thread-safety by there locking and mutability strategy. So they perform better if requirements suits there nature. Similarly, CopyOnWriteArraySet is a concurrent replacement to Synchronized Set. See What is CopyOnWriteArrayList in Java for more details BlockingQueueBlockingQueue is also one of better known collection class in Java 5. BlockingQueue makes it easy to implement producer-consumer design pattern by providing inbuilt blocking support for put() and take() method. put() method will block if Queue is full while take() method will block if Queue is empty. Java 5 API provides two concrete implementation of BlockingQueue in form of ArrayBlockingQueue and LinkedBlockingQueue, both of them implement FIFO ordering of element. ArrayBlockingQueue is backed by Array and its bounded in nature while LinkedBlockingQueue is optionally bounded. Consider using BlockingQueue to solve producer Consumer problem in Java instead of writing your won wait-notify code. Java 5 also provides PriorityBlockingQueue, another implementation of BlockingQueue which is ordered on priority and useful if you want to process elements on order other than FIFO. Deque interfaceis added in Java 6 and it extends Queue interface to support insertion and removal from both end of Queue referred as head and tail. Java6 also provides concurrent implementation of Deque like ArrayDeque and LinkedBlockingDeque. Deque Can be used efficiently to increase parallelism in program by allowing set of worker thread *to help each other by taking some of work load from other thread by utilizing Deque double end consumption property. So if all Thread has there *own set of task Queue and they are consuming from head; helper thread can also share some work load via consumption from tail. ConcurrentSkipListMap and ConcurrentSkipListSetJust like ConcurrentHashMap provides a concurrent alternative of synchronized HashMap. ConcurrentSkipListMap and ConcurrentSkipListSet provide concurrent alternative for synchronized version of SortedMap and SortedSet. For example instead of using TreeMap or TreeSet wrapped inside synchronized Collection, You can consider using ConcurrentSkipListMap or ConcurrentSkipListSet from java.util.concurrent package. They also implement NavigableMap and NavigableSet to add additional navigation method we have seen in our post How to use NavigableMap in Java. To sort hashmap by key and value Why can’t we use TreeMap in place of HashMap is the question appears in most Java programmer’s mind when they asked to sort HashMap in Java. Well, TreeMap is way slower than HashMap because it runs sorting operation with each insertion, update and removal and sometimes you don’t really need an all time sorted Map, What you need is an ability to sort any Map implementation based upon its key and value.Sort by Key As I said Map or HashMap in Java can be sorted either on keys or values. Sorting Map on keys is rather easy than sorting on values because Map allows duplicate values but doesn’t allow duplicates keys. You can sort Map, be it HashMap or Hashtable by copying keys into List than sorting List by using Collections.sort() method, here you can use either Comparator or Comparable based upon whether you want to sort on a custom order or natural order. Once List of keys is sorted, we can create another Map, particularly LinkedHashMap to insert keys in sorted order. LinkedHashMap will maintain the order on which keys are inserted, the result is a sorted Map based on keys. This is shown in the following example by writing a generic parameterized method to sort Map based on keys. You can also sort Map in Java by using TreeMap and Google Collection API (Guava). The advantage of using Guava is that you get some flexibility on specifying ordering.Sorting Map in Java - By Value To implement Collection.sort(map, new Comparator&lt;Map.Entry&lt;K,V&gt;&gt;(){ o1.getValue().compareTo(o2.getValue()) But need to take care of null and duplication Reference http://javarevisited.blogspot.in/2011/02/how-hashmap-works-in-java.html http://javarevisited.blogspot.in/2011/04/difference-between-concurrenthashmap.html http://javarevisited.blogspot.com/2013/02/concurrent-collections-from-jdk-56-java-example-tutorial.html#ixzz4WBYzKelD http://javarevisited.blogspot.com/2012/12/how-to-sort-hashmap-java-by-key-and-value.html#ixzz4WBgfeGVM]]></content>
      <tags>
        <tag>java</tag>
        <tag>hashcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Arbitrage vs Heading]]></title>
    <url>%2F2017-01-04-Arbitrage-vs-Hedging%2F</url>
    <content type="text"><![CDATA[What is the difference between arbitrage and hedging? Hedging involves the concurrent use of more than one bet in opposite directions to limit risk of serious investment loss. Arbitrage is the practice of trading a price difference between more than one market for the same good in an attempt **to profit from the imbalance. These two concepts play important roles in finance, economics and investments. Each transaction relies involves two competing types of trades: betting short versus betting long (hedging) and *buying versus selling *(arbitrage). Both are used by traders who operate in volatile, dynamic market environments. Other than these two similarities, however, they are very different techniques that are used for very different purposes. ArbitrageArbitrage involves both purchase and sale within a very short period of time. If a good is being sold for $100 in one market and $108 in another market, a savvy trader could purchase the $100 item and then sell it in the other market for $108. The trader enjoys a risk-free return of 8% ($8 / $100), minus any transaction or transportation expenses. With the proliferation of high-speed computing technology and constant price information, arbitrage is much more difficult in financial markets than it used to be. Still, arbitrage opportunities can be found in the forex market, in bonds, in futures markets and sometimes in equities. HedgingHedging is not the pursuit of risk-free *trades; instead, it is an attempt to *reduce known risks while trading. Options contacts, forward contracts, swaps and derivatives are all used by traders to purchase opposite positions in the market. By betting against both upward and downward movement, the hedger can ensure a certain amount of *reduced gain or loss * on a trade. Hedging can take place almost anywhere, but it has become a particularly important aspect of financial markets, business management and gambling. Much like any other risk/reward trade, hedging results in lower returns for the party involved, but it can offer significant protection against downside risk.]]></content>
  </entry>
  <entry>
    <title><![CDATA[Java Collections Misc]]></title>
    <url>%2F2017-01-17-Java-Collections-Misc%2F</url>
    <content type="text"><![CDATA[Difference between equals and deepEquals of Arrays in Java Arrays.equals() method does not compare recursively if an array contains another array on other hand Arrays.deepEquals() method compare recursively if an array contains another array. Arrays.equals() check is if element is null or not and then calls equals() method, it does not check for Array type. It’s better to use Arrays.equals() to compare non-nested Array and Arrays.deepEquals() to compare nested Array, as former is faster than later in the case of non-nested Array. Checking Array for duplicate elements Java brute force method which compares each element of Array to all other elements and returns true if it founds duplicates. Though this is not an efficient choice it is the one which first comes to mind. complexity on order of O(n^2) not advised in production Another quick way of checking if a Java array contains duplicates or not is to convert that array into Set. Since Set doesn’t allow duplicates size of the corresponding Set will be smaller than original Array if Array contains duplicates otherwise the size of both Array and Set will be same. 12List inputList = Arrays.asList(input);Set inputSet = new HashSet(inputList); One more way to detect duplication in java array is adding every element of the array into HashSet which is a Set implementation. Since the add(Object obj) method of Set returns false if Set already contains an element to be added, it can be used to find out if the array contains duplicates in Java or not. If you don’t prefer converting List to Set than you can still go with copying data from one ArrayList to other ArrayList ** and removing duplicates **by checking with ArrayList.contains() method.List vs SetIn short main difference between List and Set in Java is that List is an ordered collection which allows duplicates while Set is an unordered collection which doesn’t allow duplicates. how to sort list in reverse order Collections.sort(unsortedList, Collections.reverseOrder()); customized comparator Collections.sort(unsortedList, String.CASE_INSENSITIVE_ORDER); // search insensitive order Find length of a linked listIterative Solutions12345678910public int length()&#123; int count=0; Node current = this.head; while(current != null)&#123; count++; current=current.next() &#125; return count;&#125; Recursive Solution:123456public int length(Node current)&#123; if(current == null)&#123; //base case return 0; &#125; return 1+length(current.next());&#125; You can see that we have used the fact that last node will point to null to terminate the recursion. This is called the base case. It’s very important to identify a base case while coding a recursive solution, without a base case, your program will never terminate and result in StackOverFlowError. Iterator is nothing but a traversing object, made specifically for Collection objects like List and Set. we have already aware about different kind of traversing methods like for-loop ,while loop,do-while,for each lop etc,they all are index based traversing but as we know Java is purely object oriented language there is always possible ways of doing things using objects so Iterator is a way to traverse as well as access the data from the collection. ListIterator in Java is an Iterator which allows user to traverse Collection like ArrayList and HashSet in both direction by using method previous() and next (). You can obtain ListIterator from all List implementation including ArrayList and LinkedList. ListIterator doesn’t keep current index and its current position is determined by call to next() or previous() based on direction of traversing. List collection type also supports ListIterator which has add() method to add elements** in collection while Iterating. Blocking queue BlockingQueue in Java doesn’t allow null elements, various implementation of BlockingQueue like ArrayBlockingQueue, LinkedBlockingQueue throws NullPointerException when you try to add null on queue.BlockingQueue can be bounded or unbounded. A bounded BlockingQueue is one which is initialized with initial capacity and call to put() will be blocked if BlockingQueue is full and size is equal to capacity. This bounding nature makes it ideal to use a shared queue between multiple threads like in most common Producer consumer solutions in Java. An unbounded Queue is one which is initialized without capacity, actually by default it initialized with Integer.MAX_VALUE. most common example of BlockingQueue uses bounded BlockingQueue as shown in below example.1BlockingQueue&lt;String&gt; bQueue = new ArrayBlockingQueue&lt;String&gt;(2); BlockingQueue implementations like ArrayBlockingQueue, LinkedBlockingQueue and PriorityBlockingQueue are thread-safe. All queuing method uses concurrency control and internal locks to perform operation atomically. BlockingQueue interface extends Collection, Queue and Iterable interface which provides it all Collection and Queue related methods like poll(), and peak(), unlike take(), peek() method returns head of the queue without removing it, poll() also retrieves and removes elements from head but can wait till specified time if Queue is empty.Reference http://javarevisited.blogspot.com/2012/12/how-to-compare-arrays-in-java-equals-deepequals-primitive-object.html#ixzz4WBifkyjg http://javarevisited.blogspot.in/2012/02/how-to-check-or-detect-duplicate.html http://javarevisited.blogspot.com/2012/12/how-to-remove-duplicates-elements-from-ArrayList-Java.html#ixzz4WBkudSwR http://javarevisited.blogspot.in/2012/04/difference-between-list-and-set-in-java.html http://javarevisited.blogspot.in/2012/01/how-to-sort-arraylist-in-java-example.html http://javarevisited.blogspot.in/2016/05/how-do-you-find-length-of-singly-linked.html http://javarevisited.blogspot.in/2011/10/java-iterator-tutorial-example-list.html http://javarevisited.blogspot.in/2012/12/blocking-queue-in-java-example-ArrayBlockingQueue-LinkedBlockingQueue.html]]></content>
      <tags>
        <tag>java</tag>
        <tag>Collections</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HashMap in JDK]]></title>
    <url>%2F2017-01-16-Java-HashMap%2F</url>
    <content type="text"><![CDATA[Hashmap in JDKSome note worth points about hashmap Lookup process Step# 1: Quickly determine the bucket number in which this element may reside (using key.hashCode()). Step# 2: Go over the mini-list and return the element that matches the key (using key.equals()). Immutability of keys In Node&lt;K,V&gt; node of hashMap, besides key, value, hash, there is Node next link inside. So undelrying table is a linked list. For get(), firstly using hashcode calculation, divide by bucket number and get reminder, to locate the bucket, then compare key via “.euqals()”, if matched, the value will be returned. Load factor and resizeWhen new hashHap is being populated, the linkedList associated with each bucket of source hashMap is iterated and nodes are copied to the destination bucket. However, note that these new nodes are prepended to the head of the destination linkedList. So resizing has an side effect of reversing the order of the items in the list. Default load factor for hashMap is 0.75. Worst-case performance:In the worst case, a hashMap reduces to a linkedList.However with Java 8, there is a change,Java 8 intelligently determines if we are running in the worst-case performance scenario and converts the list into a binary search tree. CollisionsCollisions happen when 2 distinct keys generate the same hashCode() value. Multiple collisions are the result of bad hashCode() algorithm.There are many collision-resolution strategies - chaining, double-hashing, clustering.However, java has chosen chaining strategy for hashMap, so in case of collisions, items are chained together just like in a linkedList. Some specialized hashMaps for specific purposes: ConcurrentHashMap: HashMap to be used in multithreaded applications. EnumMap: HashMap with Enum values as keys. LinkedHashMap: HashMap with predictable iteration order (great for FIFO/LIFO caches) WeakReferenceMap, SoftReference Map etc.Weak references (a WeakHashMap) aren’t particularly good for this either, because as the elements in the cache become dereferenced by the application code they will quickly be removed from the cache by the garbage collector. This basically means there will be cache faults often (in other words, the cache lookups will fail). Soft references can be very handy in situations such as this. Because soft references only exist if the memory is available, they can make very effective use of the space that is available. Unfortunately, although there is a WeakHashMap , there is no java.util.SoftHashMap . Why? I’m not really sure. Thankfully, a little trip back over to Jakarta-Commons-Collections digs up the org.apache.commons.collections.map.ReferenceMap . SoftReferences are typically used for implementing memory caching. The JVM should try to keep softly referenced objects in memory as long as possible, and when memory is low clear the oldest soft references first. According to the JavaDoc, there are no guarantees though. WeakReferences is the reference type I use most frequently. It’s typically used when you want weak listeners or if you want to connect additional information to an object (using WeakHashMap for example). Very useful stuff when you want to reduce class coupling. Phantom references can be used to perform pre-garbage collection actions such as freeing resources. Instead, people usually use the finalize() method for this which is not a good idea. Finalizers have a horrible impact on the performance of the garbage collector and can break data integrity of your application if you’re not very careful since the “finalizer” is invoked in a random thread, at a random time. In the constructor of a phantom reference, you specify a ReferenceQueue where the phantom references are enqueued once the referenced objects becomes “phantom reachable”. Phantom reachable means unreachable other than through the phantom reference. The initially confusing thing is that although the phantom reference continues to hold the referenced object in a private field (unlike soft or weak references), its getReference() method always returns null. This is so that you cannot make the object strongly reachable again. From time to time, you can poll the ReferenceQueue and check if there are any new PhantomReferences whose referenced objects have become phantom reachable. In order to be able to to anything useful, one can for example derive a class from java.lang.ref.PhantomReference that references resources that should be freed before garbage collection. The referenced object is only garbage collected once the phantom reference becomes unreachable itself. Source code analysisInternal data structure transient Entry&lt;K, V&gt;[] elementData; transient int modCount = 0; private transient V[] cache; PutIt will call return putImpl(key, value); directly V putImpl(K key, V value) if(key == null) entry = findNullKeyEntry();12345678entry = findNullKeyEntry(); if (entry == null) &#123; modCount++; entry = createHashedEntry(null, 0, 0); if (++elementCount &gt; threshold) &#123; rehash(); &#125; &#125; — findNullKeyEntryIterate the internal array to locate null entry 1234567final Entry&lt;K,V&gt; findNullKeyEntry() &#123; Entry&lt;K,V&gt; m = elementData[0]; while (m != null &amp;&amp; m.key != null) &#123; m = m.next; &#125; return m; &#125; else (if key is not null)]]></content>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NavigableMap Misc]]></title>
    <url>%2F2017-01-19-Java-NavigableMap%2F</url>
    <content type="text"><![CDATA[What is NavigableMap NavigableMap in Java 6 is an extension of SortedMap like TreeMap which provides convenient navigation method like lowerKey, floorKey, ceilingKey and higherKey. NavigableMap is added on Java 1.6 and along with these popular navigation method it also provide ways to create a Sub Map from existing Map in Java e.g. headMap whose keys are less than specified key, tailMap whose keys are greater than specified key and a subMap which is strictly contains keys which falls between toKey and fromKey. All of these methods also provides a boolean to include specified key or not. TreeMap and ConcurrentSkipListMap are two concrete implementation of NavigableMap in Java 1.6 API. use NavigableMapHow to create subMap from Map using Navigable Map in Java with ExampleIn this Java tutorial we will explore some API methods of NavigableMap to show its functionality. This Java program shows example of lowerKey which returns keys less than specified, floorKey returns key less than or equal to, ceilingKey return greater than or equal to and higherKey which returns keys which are greater than specified key. This Java example also demonstrate use of headMap(), tailMap() and subMap() method which is used to create Map from an existing Map in Java. headMap returns a Map whose keys are lower than specified keys while tailMap returns Map which contains keys, those are higher than specified. Here is complete code example of How to use NavigableMap in Java. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667import java.util.NavigableMap;import java.util.TreeMap;/** * * Java program to demonstrate What is NavigableMap in Java and How to use NavigableMap * in Java. NavigableMap provides two important features navigation methods * like lowerKey(), floorKey, ceilingKey() and higherKey(). * There Entry counterpart and methods to create subMap e.g. headMap(), tailMap() * and subMap(). * * @author Javin Paul */public class NavigableMapExample &#123; public static void main(String args[]) &#123; //NavigableMap extends SortedMap to provide useful navigation methods NavigableMap&lt;String, String&gt; navigableMap = new TreeMap&lt;String, String&gt;(); navigableMap.put("C++", "Good programming language"); navigableMap.put("Java", "Another good programming language"); navigableMap.put("Scala", "Another JVM language"); navigableMap.put("Python", "Language which Google use"); System.out.println("SorteMap : " + navigableMap); //lowerKey returns key which is less than specified key System.out.println("lowerKey from Java : " + navigableMap.lowerKey("Java")); //floorKey returns key which is less than or equal to specified key System.out.println("floorKey from Java: " + navigableMap.floorKey("Java")); //ceilingKey returns key which is greater than or equal to specified key System.out.println("ceilingKey from Java: " + navigableMap.ceilingKey("Java")); //higherKey returns key which is greater specified key System.out.println("higherKey from Java: " + navigableMap.higherKey("Java")); //Apart from navigagtion methodk, it also provides useful method //to create subMap from existing Map e.g. tailMap, headMap and subMap //an example of headMap - returns NavigableMap whose key is less than specified NavigableMap&lt;String, String&gt; headMap = navigableMap.headMap("Python", false); System.out.println("headMap created form navigableMap : " + headMap); //an example of tailMap - returns NavigableMap whose key is greater than specified NavigableMap&lt;String, String&gt; tailMap = navigableMap.tailMap("Scala", false); System.out.println("tailMap created form navigableMap : " + tailMap); //an example of subMap - return NavigableMap from toKey to fromKey NavigableMap&lt;String, String&gt; subMap = navigableMap.subMap("C++", false , "Python", false); System.out.println("subMap created form navigableMap : " + subMap); &#125;&#125;Output:SorteMap : &#123;C++=Good programming language, Java=Another good programming language, Python=Language which Google use, Scala=Another JVM language&#125;lowerKey from Java : C++floorKey from Java: JavaceilingKey from Java: JavahigherKey from Java: PythonheadMap created form navigableMap : &#123;C++=Good programming language, Java=Another good programming language&#125;tailMap created form navigableMap : &#123;&#125;subMap created form navigableMap : &#123;Java=Another good programming language&#125; That’s all on What is NavigableMap in Java and How to use NavigableMap with example. We have seen examples of popular navigation method on TreeMap e.g. floorKey. You can also use similar method like lowerEntry, floorEntry, ceilingEntry and higherEntry to retrieve Entry instead of key. NavigableMap is also a good utility to create subset of a Map in Java. Referencehttp://javarevisited.blogspot.com/2013/01/what-is-navigablemap-in-java-6-example-submap-head-tail.html#ixzz4WBdqjJCC]]></content>
      <tags>
        <tag>java</tag>
        <tag>NavigableMap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Compare-In-Java]]></title>
    <url>%2F2017-01-19-Compare-In-Java%2F</url>
    <content type="text"><![CDATA[Concepts If you implement Comparable interface and override compareTo() method it must be consistent with equals() method i.e. for equal object by equals() method compareTo() must return zero. failing to so will affect contract of SortedSet e.g. TreeSet and SortedMap like TreeMap, which uses compareTo() method for checking equality Remember to use Collections.reverseOrder() comparator for sorting Object in reverse order or descending order, as shown in this example. Use Generics while implementing Comparator and Comparable interface, that prevents error of accidentally overloading compareTo() and compare() method instead of overriding it because both of these methods accept Object as a parameter. By using Generics and @Override annotation we effectively remove that subtle error. Comparable vs Comparator Comparator in Java is defined in java.util package while Comparable interface in Java is defined in java.lang package, which very much says that Comparator should be used as an utility to sort objects which Comparable should be provided by default Comparator interface in Java has method public int compare (Object o1, Object o2) which returns a negative integer, zero, or a positive integer as the first argument is less than, equal to, or greater than the second. While Comparable interface has method public int compareTo(Object o) which returns a negative integer, zero, or a positive integer as this object is less than, equal to, or greater than the specified object. If any class implement Comparable interface in Java then collection of that object either List or Array can be sorted automatically by using Collections.sort() or Arrays.sort() method and object will be sorted based on there natural order defined by CompareTo method. Objects which implement Comparable in Java can be used as keys in a SortedMap like TreeMap or elements in a SortedSet for example TreeSet, without specifying any Comparator. Generally you should not use difference of integers to decide output of compareTo method as result of *integer subtraction can overflow * but if you are sure that both operands are positive then its one of the quickest way to compare two objects. Some time you write code to sort object of a class for which you are not the original author, or you don’t have access to code. In these cases you can not implement Comparable and Comparator is only way to sort those objects. Beware with the fact that How those object will behave if stored in SorteSet or SortedMap like TreeSet and TreeMap. If an object doesn’t implement Comparable than while putting them into SortedMap, always provided corresponding Comparator which can provide sorting logic. Comparator has a distinct advantage of being self descriptive for example if you are writing Comparator to compare two Employees based upon there salary than name that comparator as SalaryComparator, on the other hand compareTo() So in Summary if you want to sort objects based on natural order then use Comparable in Java and if you want to *sort on some other attribute of object then use Comparator * in Java. Samples of Java class How to Compare String in JavaString is immutable in Java and one of the most used value class. For comparing String in Java we should not be worrying because String implements Comparable interface and provides a lexicographic implementation for CompareTo method which compare two strings based on contents of characters or you can say in lexical order. You just need to call String.compareTo(AnotherString) and Java will determine whether specified String is greater than , equal to or less than current object. Dates are represented by java.util.Date class in Java and like String, Date also implements Comparable in Java so they will be automatically sorted based on there natural ordering if they got stored in any sorted collection like TreeSet or TreeMap. If you explicitly wants to compare two dates in Java you can call Date.compareTo(AnotherDate) method in Java and it will tell whether specified date is greater than , equal to or less than current String. ExceptionsOne example where compareTo is not consistent with equals in JDK is BigDecimal class. two BigDecimal number for which compareTo returns zero, equals returns false as clear from following BigDecimal comparison example: 12345678910BigDecimal bd1 = new BigDecimal("2.0");BigDecimal bd2 = new BigDecimal("2.00");System.out.println("comparing BigDecimal using equals: " + bd1.equals(bd2));System.out.println("comparing BigDecimal using compareTo: " + bd1.compareTo(bd2));Output:comparing BigDecimal using equals: falsecomparing BigDecimal using compareTo: 0 How does it affect BigDecimal ? well if you store these two BigDecimal in HashSet you will end up with duplicates (violation of Set Contract) i.e. two elements while if you store them in TreeSet you will end up with just 1 element because HashSet uses equals to check duplicates while TreeSet uses compareTo to check duplicates. That’s why its suggested to keep compareTo consistent with equals method in java. Another important point to note is don’t use subtraction for comparing integral values because result of subtraction can overflow as every int operation in Java is modulo 2^32. use either Integer.compareTo() or logical operators for comparison. In summary compareTo should provide natural ordering and compareTo must be consistent with equals() method in Java. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128package test;import java.util.ArrayList;import java.util.Collections;import java.util.Comparator;import java.util.List;/** * * Java program to test Object sorting in Java. This Java program * test Comparable and Comparator implementation provided by Order * class by sorting list of Order object in ascending and descending order. * Both in natural order using Comparable and custom Order using Comparator in Java * * @author http://java67.blogspot.com */public class ObjectSortingExample &#123; public static void main(String args[]) &#123; //Creating Order object to demonstrate Sorting of Object in Java Order ord1 = new Order(101,2000, "Sony"); Order ord2 = new Order(102,4000, "Hitachi"); Order ord3 = new Order(103,6000, "Philips"); //putting Objects into Collection to sort List&lt;Order&gt; orders = new ArrayList&lt;Order&gt;(); orders.add(ord3); orders.add(ord1); orders.add(ord2); //printing unsorted collection System.out.println("Unsorted Collection : " + orders); //Sorting Order Object on natural order - ascending Collections.sort(orders); //printing sorted collection System.out.println("List of Order object sorted in natural order : " + orders); // Sorting object in descending order in Java Collections.sort(orders, Collections.reverseOrder()); System.out.println("List of object sorted in descending order : " + orders); //Sorting object using Comparator in Java Collections.sort(orders, new Order.OrderByAmount()); System.out.println("List of Order object sorted using Comparator - amount : " + orders); // Comparator sorting Example - Sorting based on customer Collections.sort(orders, new Order.OrderByCustomer()); System.out.println("Collection of Orders sorted using Comparator - by customer : " + orders); &#125;&#125;/* * Order class is a domain object which implements * Comparable interface to provide sorting on the natural order. * Order also provides couple of custom Comparators to * sort object based upon amount and customer */class Order implements Comparable&lt;Order&gt; &#123; private int orderId; private int amount; private String customer; /* * Comparator implementation to Sort Order object based on Amount */ public static class OrderByAmount implements Comparator&lt;Order&gt; &#123; @Override public int compare(Order o1, Order o2) &#123; return o1.amount &gt; o2.amount ? 1 : (o1.amount &lt; o2.amount ? -1 : 0); &#125; &#125; /* * Anohter implementation or Comparator interface to sort list of Order object * based upon customer name. */ public static class OrderByCustomer implements Comparator&lt;Order&gt; &#123; @Override public int compare(Order o1, Order o2) &#123; return o1.customer.compareTo(o2.customer); &#125; &#125; public Order(int orderId, int amount, String customer) &#123; this.orderId = orderId; this.amount = amount; this.customer = customer; &#125; public int getAmount() &#123;return amount; &#125; public void setAmount(int amount) &#123;this.amount = amount;&#125; public String getCustomer() &#123;return customer;&#125; public void setCustomer(String customer) &#123;this.customer = customer;&#125; public int getOrderId() &#123;return orderId;&#125; public void setOrderId(int orderId) &#123;this.orderId = orderId;&#125; /* * Sorting on orderId is natural sorting for Order. */ @Override public int compareTo(Order o) &#123; return this.orderId &gt; o.orderId ? 1 : (this.orderId &lt; o.orderId ? -1 : 0); &#125; /* * implementing toString method to print orderId of Order */ @Override public String toString()&#123; return String.valueOf(orderId); &#125;&#125;OutputUnsorted Collection : [103, 101, 102]List of Order object sorted in natural order : [101, 102, 103]List of object sorted in descending order : [103, 102, 101]List of Order object sorted using Comparator - amount : [101, 102, 103]Collection of Orders sorted using Comparator - by customer : [102, 103, 101] Reference http://www.java67.com/2012/10/how-to-sort-object-in-java-comparator-comparable-example.html]]></content>
      <tags>
        <tag>java</tag>
        <tag>compare</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java-Tricky-Tech-Questions.md]]></title>
    <url>%2F2017-01-19-Java-Tricky-Tech-Questions%2F</url>
    <content type="text"><![CDATA[What is the difference between Serializable and Externalizable in Java? In earlier version of Java, reflection was very slow, and so serializaing large object graphs (e.g. in client-server RMI applications) was a bit of a performance problem. To handle this situation, the java.io.Externalizable interface was provided, which is like java.io.Serializable but with custom-written mechanisms to perform the marshalling and unmarshalling functions (you need to implement readExternal and writeExternal methods on your class). This gives you the means to get around the reflection performance bottleneck. In recent versions of Java (1.3 onwards, certainly) the performance of reflection is vastly better than it used to be, and so this is much less of a problem. I suspect you’d be hard-pressed to get a meaningful benefit from Externalizable with a modern JVM. Also, the built-in Java serialization mechanism isn’t the only one, you can get third-party replacements, such as JBoss Serialization, which is considerably quicker, and is a drop-in replacement for the default. A big downside of Externalizable is that you have to maintain this logic yourself - if you add, remove or change a field in your class, you have to change your writeExternal/readExternal methods to account for it. In summary, Externalizable is a relic of the Java 1.1 days. There’s really no need for it any more. write transcient fields Storing and reconstituting the transient data can also be achieved by implementing the Externalizable interface and implementing the writeExternal( ) and the readExternal( ) methods of that interface. Java Class Class objects for known types can also be written as “class literals”: 1234// Express a class literal as a type name followed by ".class"c = int.class; // Same as Integer.TYPEc = String.class; // Same as "a string".getClass()c = byte[].class; // Type of byte arrays For primitive types and void, we also have class objects that are represented as literals: 123456// Obtain a Class object for primitive types with various // predefined constantsc = Void.TYPE; // The special "no-return-value" typec = Byte.TYPE; // Class object that represents a bytec = Integer.TYPE; // Class object that represents an intc = Double.TYPE; // etc; see also Short, Character, Long, Float Convert arrays to ArrayList, two different approaches// View array as an ungrowable listList l = Arrays.asList(a);// Make a growable copy of the viewList m = new ArrayList(l); What’s seed in JavaSince the next number in a pseudorandom generator is determined by the pre- vious number(s), such a generator always needs a place to start, which is called its seed. The sequence of numbers generated for a given seed will always be the same. The seed for an instance of the java.util.Random class can be set in its constructor or with its setSeed() method. Cipher Given a variable c that is known to be an uppercase letter, the Java computation, j = c − ‘A’ produces the desired index j. As a sanity check, if character c is ‘A’, then j = 0. When c is ‘B’, the difference is 1. Uncaught exception handler The Thread API also provides the UncaughtExceptionHandler facility, which lets you detect when a thread dies due to an uncaught exception. The two approaches are complementary: taken together, they provide defense-in- depth against thread leakage. When a thread exits due to an uncaught exception, the JVM reports this event to an application-provided UncaughtExceptionHandler (see Listing 7.24); if no handler exists, the default behavior is to print the stack trace to System.err In long-running applications, always use uncaught exception handlers for all threads that at least log the exception. To set an UncaughtExceptionHandler for pool threads, provide a ThreadFac- tory to the ThreadPoolExecutor constructor. Somewhat confusingly, exceptions thrown from tasks make it to the uncaught exception handler only for tasks submitted with execute ; for tasks submitted with submit, any thrown exception, checked or not, is considered to be part of the task’s return status. If a task submitted with submit terminates with an exception, it is rethrown by Future.get, wrapped in an ExecutionException.123456public class UEHLogger implements Thread.UncaughtExceptionHandler&#123; public void uncaughtException(Thread t, Throwable e)&#123; Logger logger=Logger.getAnonymousLogger(); logger.log(Level.SEVERE, "Thread terminated with exception:"+e.getName(),e); &#125;&#125; What is the difference between Iterator and Enumeration Iterator duplicate functionality of Enumeration with one addition of remove() method Another difference is that Iterator is more safe than Enumeration and doesn’t allow another thread to modify collection object during iteration except remove() method and throws ConcurrentModificaitonException. How does HashSet is implemented in Java, How does it use Hashing HashSet is built on top of HashMap. If you look at source code of java.util.HashSet class, you will find that that it uses a HashMap with same values for all keys, as shown below:1234567private transient HashMap map;// Dummy value to associate with an Object in the backing Mapprivate static final Object PRESENT = new Object();// When you call add() method of HashSet, it put entry in HashMap :public boolean add(E e) &#123; return map.put(e, PRESENT)==null;&#125; What do you need to do to use a custom object as a key in Collection classes like Map or Set? (answer) If you are using any custom object in Map as key, you need to override equals() and hashCode() method, and make sure they follow their contract. On the other hand if you are storing a custom object in Sorted Collection e.g. SortedSet or SortedMap, you also need to make sure that your equals() method is consistent to compareTo() method, otherwise that collection will not follow there contacts e.g. Set may allow duplicates. Java Generics ? , E and T what is the difference?Well there’s no difference between the first two - they’re just using different names for the type parameter (E or T). The third isn’t a valid declaration - ? is used as a wildcard which is used when providing a type argument, e.g. List&lt;?&gt; foo = … means that foo refers to a list of some type, but we don’t know what. Question: What does the following Java program print?public class Test { public static void main(String[] args) { System.out.println(Math.min(Double.MIN_VALUE, 0.0d)); }} Anwser 0.0dThe Double.MIN_VALUE is 2^(-1074), a double constant whose magnitude is the least among all double values. What will happen if you put return statement or System.exit () on try or catch block? Will finally block execute?Answer of this tricky question in Java is that finally block will execute even if you put a return statement in the try block or catch block but finally block won’t run if you call System.exit() from try or catch block. Question: Can you override a private or static method in Java?Another popular Java tricky question, As I said method overriding is a good topic to ask trick questions in Java. Anyway, you can not override a private or static method in Java, if you create a similar method with same return type and same method arguments in child class then it will hide the superclass method, this is known as method hiding. Similarly, you cannot override a private method in sub class because it’s not accessible there, what you do is create another private method with the same name in the child class. See Can you override a private method in Java or more details Question: What do the expression 1.0 / 0.0 will return? will it throw Exception? any compile time error?Answer: This is another tricky question from Double class. Though Java developer knows about the double primitive type and Double class, while doing floating point arithmetic they don’t pay enough attention to Double.INFINITY , NaN, and -0.0and other rules that govern the arithmetic calculations involving them. The simple answer to this question is that it will not throw ArithmeticExcpetion and return Double.INFINITY. Also, note that the comparison x == Double.NaN always evaluates to false, even if x itself is a NaN. To test if x is a NaN, one should use the method call Double.isNaN(x) to check if given number is NaN or not. If you know SQL, this is very close to NULL there. Overloading vs Overriding Main difference comes from the fact that method overloading is resolved during compile time, while method overriding is resolved at runtime Also rules of overriding or overloading a method are different in Java. a private, static and final method cannot be overriding in Java but you can still overload them overriding both name and signature of the method must remain same, but in for overloading method, the signature must be different call to overloaded methods are resolved using static binding while the call to overridden method is resolved using dynamic binding in Java. Java programmer to declare method with same name but different behavior. Method overloading and method overriding is based on Polymorphism in Java. In case of method overloading, method with same name co-exists in same class but they must have different method signature, while in case of method overriding, method with same name is declared in derived class or sub class If you have two methods with same name in one Java class with different method signature than its called overloaded method in Java. You can also overload constructor in Java Since return type is not part of method signature simply changing return type will result in duplicate method and you will get compile time error in Java. you can also overload private and final method in Java but you can not override them When you override a method in Java its signature remains exactly same including return type. Rules of override: Method signature must be same including return type, number of method parameters, type of parameters and order of parameters . Be advised: as of Java 5, you’re allowed to change the return type in the overriding method as long as the new return type is a subtype of the declared return type of the overridden (super class) method Overriding method can not throw higher Exception than original or overridden method. means if original method throws IOException than overriding method can not throw super class of IOException e.g. Exception but it can throw any sub class of IOException or simply does not throw any Exception. This rule only applies to checked Exception in Java, overridden method is free to throw any unchecked Exception. Overriding method can not reduce accessibility of overridden method , means if original or overridden method is public than overriding method can not make it protected. Another important point is that you can not override static method in Java because they are associated with Class rather than object and resolved and bonded during compile time and that’s the reason you cannot override main method in Java From Java 5 onwards you can use annotation in Java to declare overridden method just like we did with @override. ** @override annotation allows compiler** , IDE like NetBeans and Eclipse to cross verify or check if this method is really overrides super class method or not. Static vs. Dynamic binding private, final and static methods and variables uses static binding and bonded by compiler while virtual methods are bonded during runtime based upon runtime object. Here is sample of static binding 1234567891011121314151617181920public class StaticBindingTest &#123; public static void main(String args[]) &#123; Collection c = new HashSet(); StaticBindingTest et = new StaticBindingTest(); et.sort(c); &#125; //overloaded method takes Collection argument public Collection sort(Collection c)&#123; System.out.println("Inside Collection sort method"); return c; &#125; //another overloaded method which takes HashSet argument which is sub class public Collection sort(HashSet hs)&#123; System.out.println("Inside HashSet sort method"); return hs; &#125; &#125;Output: Inside Collection sort method because it was bonded on compile time based on type of variable (Static binding) which was collection. Here is sample of dynamic binding 123456789101112131415161718192021public class DynamicBindingTest &#123; public static void main(String args[]) &#123; Vehicle vehicle = new Car(); //here Type is vehicle but object will be Car vehicle.start(); //Car's start called because start() is overridden method &#125;&#125;class Vehicle &#123; public void start() &#123; System.out.println("Inside start method of Vehicle"); &#125;&#125;class Car extends Vehicle &#123; @Override public void start() &#123; System.out.println("Inside start method of Car"); &#125;&#125;Output: Inside start method of Car In summary, bottom line is static binding is a compile time operation while dynamic binding is a runtime. one uses Type and other uses Object to bind. static, private and final methods and variables are resolved using static binding which makes there execution fast because no time is wasted to find correct method during runtime. Static in Java static keyword can not be applied on top level class. Making a top level class static in Java will result in compile time error. Beware that if your static initialize block throws Exception than you may get java.lang.NoClassDefFoundError when you try to access the class which failed to load. Static method can not be overridden in Java as they belong to class and not to object 1234567891011121314151617181920212223242526272829 public class TradingSystem &#123; public static void main(String[] args) &#123; TradingSystem system = new DirectMarketAccess(); DirectMarketAccess dma = new DirectMarketAccess(); // static method of Instrument class will be called, // even though object is of sub-class DirectMarketAccess system.printCategory(); //static method of EquityInstrument class will be called dma.printCategory(); &#125; public static void printCategory()&#123; System.out.println("inside super class static method"); &#125;&#125;class DirectMarketAccess extends TradingSystem&#123; public static void printCategory()&#123; System.out.println("inside sub class static method"); &#125;&#125;Output:inside super class static methodinside sub class static method This shows that static method can not be overridden in Java and concept of method overloading doesn’t apply to static methods. Instead declaring same static method on Child class is known as method hiding in Java Consider making a static variable final in Java to make it constant and avoid changing it from anywhere in the code when to use Singleton vs Static Class in Java for those purpose,answer is that if its completely stateless and it work on provided data then you can go for static class otherwise Singleton pattern is a better choice. When to make a method static in Java Method doesn’t depends on object’s state, in other words doesn’t depend on any member variable and everything they need is passes as parameter to them Utility methods are good candidate of making static in Java because then they can directly be accessed using class name without even creating any instance. Classic example is java.lang.Math Static method in Java is very popular to implement Factory design pattern. Since Generics also provides type inference during method invocation, use of static factory method to create object is popular Java idiom. Phone Key to success in telephonic is to the point and concise answer. Difference between String, StringBuffer and StringBuilder in Java String is immutable while both StringBuffer and StringBuilder is mutable, which means any change e.g. converting String to upper case or trimming white space will produce another instance rather than changing the same instance. On later two, StringBuffer is synchronized while StringBuilder is not, in fact its a ditto replacement of StringBuffer added in Java 1.5.Difference between extends Thread vs implements Runnable in Java? Difference comes from the fact that you can only extend one class in Java, which means if you extend Thread class you lose your opportunity to extend another class, on the other hand if you implement Runnable, you can still extend another class. Difference between Runnable and Callable interface in Java? Runnable was the only way to implement a task in Java which can be executed in parallel before JDK 1.5 adds Callable. Just like Runnable, Callable also defines a single call() method but unlike run() it can return values and throw exceptions. Difference between ArrayList and LinkedList in Java? In short, ArrayList is backed by array in Java, while LinkedList is just collection of nodes, similar to linked list data structure. ArrayList also provides random search if you know the index, while LinkedList only allows sequential search. On other hand, adding and removing element from middle is efficient in LinkedList as compared to ArrayList because it only require to modify links and no other element is rearranged. What is difference between wait and notify in Java? Both wait and notify methods are used for inter thread communication, where wait is used to pause the thread on a condition and notify is used to send notification to waiting threads. Both must be called from synchronized context e.g. synchronized method or block. Difference between HashMap and Hashtable in Java? Though both HashMap and Hashtable are based upon hash table data structure, there are subtle difference between them. HashMap is non synchronized while Hashtable is synchronized and because of that HashMap is faster than Hashtable, as there is no cost of synchronization associated with it. One more minor difference is that HashMap allows a null key but Hashtable doesn’t. Check a number is prime or not We learned numbers are prime if the only divisors they have are 1 and itself. Trivially, we can check every integer from 1 to itself (exclusive) and test whether it divides evenly. Check the source code of ** PrimeTester.java** naive approach: We learned numbers are prime if the only divisors they have are 1 and itself. Trivially, we can check every integer from 1 to itself (exclusive) and test whether it divides evenly. 12for(int i=2;2*i&lt;=n;i++)&#123;if(n%i==0) power of 2 approach: further enhance, as if 2 divides some interger n, then (n/2) divides n as well so we’ll times of 2. Please be advised in for loop, should use 2*i&lt;=n, rather than “&lt;n”, otherwise, 4 will be return as ture mistakely 12for(int i=2;2*i&lt;=n;i++)&#123;if(n%i==0) isPrimeSquare approach: we notice that you really only have to go up to the square root of n, because if you list out all of the factors of a number, the square root will always be in the middle Finally, we know 2 is the “oddest” prime - it happens to be the only even prime number. Because of this, we need only check 2 separately, then traverse odd numbers up to the square root of n. In the end, our code will resemble this: 123456 // check if n is a multiple of 2 if(n&gt;2&amp;&amp;n%2==0) return false; // if not, then just check the oddsfor(int i=3;i*i&lt;=n;i+=2)&#123; if(n%i==0) Difference between abstract class and interface? From Java 8 onwards difference between abstract class and interface in Java has minimized, now even interface can have implementation in terms of default and static method. BTW, in Java you can still extend just one class but can extend multiple inheritance. Abstract class is used to provide default implementation with just something left to customize, while interface is used heavily in API to define contract of a class. How to Swap Two Numbers without Temp or Third variable in JavaSwapping two numbers without using temp variable in Java12345678910111213int a = 10;int b = 20;System.out.println("value of a and b before swapping, a: " + a +" b: " + b);//swapping value of two numbers without using temp variablea = a+ b; //now a is 30 and b is 20b = a -b; //now a is 30 but b is 10 (original value of a)a = a -b; //now a is 20 and b is 10, numbers are swappedSystem.out.println("value of a and b after swapping, a: " + a +" b: " + b);Output:value of a and b before swapping, a: 10 b: 20value of a and b after swapping, a: 20 b: 10 Swapping two numbers without using temp variable in Java with bitwise operator123456789101112int a = 2; //0010 in binaryint b = 4; //0100 in binarySystem.out.println("value of a and b before swapping, a: " + a +" b: " + b);// 6 is 0110//swapping value of two numbers without using temp variable and XOR bitwise operator a = a^b; //now a is 6 (0110) and b is 4(0100)b = a^b; //now a is 6 but b is 2 (0010) (original value of a)a = a^b; //now a is 4 and b is 2, numbers are swappedSystem.out.println("value of a and b after swapping using XOR bitwise operation, a: " + a +" b: " + b);value of a and b before swapping, a: 2 b: 4value of a and b after swapping using XOR bitwise operation, a: 4 b: 2 Swapping two numbers without using temp variable in Java with division and multiplication There is another, third way of swapping two numbers without using third variable, which involves multiplication and division operator. 12345678910111213int a = 6;int b = 3;System.out.println("value of a and b before swapping, a: " + a +" b: " + b);//swapping value of two numbers without using temp variable using multiplication and divisiona = a*b; //now a is 18 and b is 3b = a/b; //now a is 18 but b is 6 (original value of a)a = a/b; //now a is 3 and b is 6, numbers are swappedSystem.out.println("value of a and b after swapping using multiplication and division, a: " + a +" b: " + b);Output:value of a and b before swapping, a: 6 b: 3value of a and b after swapping using multiplication and division, a: 3 b: 6 That’s all on 3 ways to swap two variables without using third variable in Java. Its good to know multiple ways of swapping two variables without using temp or third variable to handle any follow-up question. Swapping numbers using bitwise operator is the fastest among three, because it involves bitwise operation. It’s also great way to show your knowledge of bitwise operator in Java and impress interviewer, which then may ask some question on bitwise operation. A nice trick to drive interview on your expert area. Bitwise operator “~” inverts a bit pattern; it can be applied to any of the integral types, making every “0” a “1” and every “1” a “0”. The bitwise &amp; operator performs a bitwise AND operation. The bitwise ^ operator performs a bitwise exclusive OR operation. The bitwise | operator performs a bitwise inclusive OR operation. How to check if linked list contains loop in Java? Algorithm to find if linked list contains loops or cycles. Two pointers, fast and slow is used while iterating over linked list. Fast pointer moves two nodes in each iteration, while slow pointer moves to one node. If linked list contains loop or cycle than both fast and slow pointer will meet at some point during iteration. If they don’t meet and fast or slow will point to null, then linked list is not cyclic and it doesn’t contain any loop. Use two pointers fast and slow Move fast two nodes and slow one node in each iteration If fast and slow meet then linked list contains cycle if fast points to null or fast.next points to null then linked list is not cyclic This algorithm is also known as Floyd’s cycle finding algorithm and popularly known as tortoise and hare algorithm to find cycles in linked list. Sample can be found via “LoopInList.java” Reference http://www.java67.com/2012/09/top-10-tricky-java-interview-questions-answers.html http://javarevisited.blogspot.in/2012/03/what-is-static-and-dynamic-binding-in.html http://www.java67.com/2015/03/top-40-core-java-interview-questions-answers-telephonic-round.html http://www.mkyong.com/java/how-to-determine-a-prime-number-in-java/ http://javarevisited.blogspot.in/2013/02/swap-two-numbers-without-third-temp-variable-java-program-example-tutorial.html http://docs.oracle.com/javase/tutorial/java/nutsandbolts/op3.html http://javarevisited.blogspot.in/2013/05/find-if-linked-list-contains-loops-cycle-cyclic-circular-check.html]]></content>
      <tags>
        <tag>java</tag>
        <tag>Questions</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dead Lock]]></title>
    <url>%2F2017-01-20-DeadLock%2F</url>
    <content type="text"><![CDATA[Concept Two or more threads are waiting for each other to release lock and get stuck for infinite time. It will only happen in case of multitasking. One screnario of dead lockIf method1() and method2() both will be called by two or many threads , there is a good chance of deadlock because if thread 1 acquires lock on Sting object while executing method1() and thread 2 acquires lock on Integer object while executing method2() both will be waiting for each other to release lock on Integer and String to proceed further which will never happen. The root cause is NOT multithreading, but the way they are requiring lockNow there would not be any deadlock because both methods are accessing lock on Integer and String class literal in same order. So, if thread A acquires lock on Integer object , thread B will not proceed until thread A releases Integer lock, same way thread A will not be blocked even if thread B holds String lock because now thread B will not expect thread A to release Integer lock to proceed further. Reference http://javarevisited.blogspot.in/2010/10/what-is-deadlock-in-java-how-to-fix-it.html]]></content>
      <tags>
        <tag>java</tag>
        <tag>dead lock</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Algorithm]]></title>
    <url>%2F2017-01-21-Algorithm%2F</url>
    <content type="text"><![CDATA[This page is about key points about AlgorithmMethodology The easiest way to improve search efficiency on a set of data is to put it in a data structure that allows more efficient searching.What data structures can be searched more efficiency than O(n)? Binary tree can be searched in O(log(n)). Arrays and hash tables both have constant time element look up (has tables have worse-case lookup of O(n) but the average case is O(1)). Then need to determine which data structure to be used. If the underlying characters are just ASCII, then a array[128] would be enough. But characters are UNICODe, then it need 100,000 (100K) array, which is a concern of memory, so hash table would be a better option, which only keep exist characters.In general, arrays are a better choice for long strings with a limited set of possible characters values, hash tables are more efficient for shorter strings or when there are many possible character values. For some problems, obvious iterative alternatives like the one just shown don’t exist, but it’s always possible to implement a recursive algorithm without using recursive calls. For a simple recursive function like factorial, many computer architectures spend more time on call overhead than on the actual calculation. Iterative functions, which use looping constructs instead of recursive function calls, do not suffer from this overhead and are frequently more efficient. NOTE Iterative solutions are usually more efficient than recursive solutions. NOTE Every recursive case must eventually lead to a base case. NOTE Recursive algorithms have two cases: recursive cases and base cases SortI collections.sort()1234567Object[] a = list.toArray(); Arrays.sort(a); ListIterator&lt;T&gt; i = list.listIterator(); for (int j=0; j&lt;a.length; j++) &#123; i.next(); i.set((T)a[j]); &#125; Arrays.sort1234567891011121314151617181920212223public static void sort(Object[] a) &#123; if (LegacyMergeSort.userRequested) legacyMergeSort(a); else ComparableTimSort.sort(a); &#125;private static void mergeSort(Object[] src, Object[] dest, int low, int high, int off) &#123; int length = high - low; // Insertion sort on smallest arrays if (length &lt; INSERTIONSORT_THRESHOLD) &#123; // threshold is 7 for (int i=low; i&lt;high; i++) for (int j=i; j&gt;low &amp;&amp; ((Comparable) dest[j-1]).compareTo(dest[j])&gt;0; j--) swap(dest, j, j-1); return; &#125;// else use mergeSort Self reviewCeasarCipher:Generally: it’s a rotation of English alphabic. E.g. if rotation is 2, the encode is start from A+2, i.e. A is at -2 of the encode array.And decode is start with 26-2, and “A” start at positon 2, then the increase by 1 character to constitute the arrayThat’s why need to “%26”, to make it loop across 26 characters 123456If rotation is 2:--- encrytpion code is:[C, D, E, F, G, H, I, J, K, L, M, N, O, P, Q, R, S, T, U, V, W, X, Y, Z, A, B]--- decrytpion code is:[Y, Z, A, B, C, D, E, F, G, H, I, J, K, L, M, N, O, P, Q, R, S, T, U, V, W, X]If rotation is 4:--- encrytpion code is:[E, F, G, H, I, J, K, L, M, N, O, P, Q, R, S, T, U, V, W, X, Y, Z, A, B, C, D]--- decrytpion code is:[W, X, Y, Z, A, B, C, D, E, F, G, H, I, J, K, L, M, N, O, P, Q, R, S, T, U, V] 123456789101112131415161718192021ABC:Msg =&#123;‘A’,’B’,’C’&#125;;Char[] encode=’A’+(k+rotation)%26);Char[] decode=’A’+(k-rotation+26)%26); // +26 to avoid negativeEncode=&#123;‘C’,’D’,’E’&#125;; // rotation=3, so A+3, A+4,A+5,xxx, A+26=&gt;3,4,5,6,xxx,0Decode=&#123;‘M’,’N’,’O’&#125;;//as k-rotation+26 % 26, so it’s 26-3+0,26-3+1 ,xx: =&gt; 23,24,25,0,1,2,3,4,is: A+23,A+24,A+25=&gt;‘M’,’N’,’O’. that’s rotation, rotain-1, rotaion -2 xxxxFor(int i=0;i&lt;msg.length;i++)&#123; Int j=msg[i]-‘A’; // to remove the base ‘A”, so sync with the “k” in encode, 3,4,5,xxx 3+26 Msg[i]=codes[j];&#125;// encodeInt j=’A’-‘A’; //0Msg[0]=’C’;Msg[1]=’D’;Msg[2]=’E’;//decodeInt j=msg[i]-‘A’; //’C’-‘A’=3Msg[i]=decode[j]; // correspoindg to the postion 0,-xxx, 26 in decode, Msg[0]= If you says “tree,” it’s a good idea to clarify whether she is referring to a generic tree or a binary tree. To print content of Array123Import java.util.Arrays;Arrays.toString(ary);Arrays.deepToString(ary); search without recursiveNode findNode( Node root, int value ){ while( root != null ){ int currval = root.getValue(); if( currval == value ) break; if( currval &lt; value ){ root = root.getRight(); } else { // currval &gt; value root = root.getLeft(); } } return root;} preceding lookup operation can be reimplemented recursively as follows:Node findNode( Node root, int value ){ if( root == null ) return null; int currval = root.getValue(); if( currval == value ) return root; if( currval &lt; value ){ return findNode( root.getRight(), value ); } else { // currval &gt; value return findNode( root.getLeft(), value ); }} This subtree property is conducive to recursion because recursion generally involves solving a problem in terms of similar subproblems and a base case. Big O sequencey1, logn, n, n log n, n2, n3, 2n(2 power n). Big O It is also considered poor taste to include constant factors and lower-order terms in the big-Oh notation. For example, it is not fashionable to say that the function 2n2 is O(4n2 + 6n log n), although this is completely correct. We should strive instead to describe the function in the big-Oh in simplest terms. So, for example, we would say that an algorithm that runs in worst-case time 4n2 + n log n is a quadratic-time algorithm, since it runs in O(n2) time. Likewise, an algorithm running in time at most 5n + 20logn + 4 would be called a linear-time algorithm. Big Omega Just as the big-Oh notation provides an asymptotic way of saying that a function is “less than or equal to” another function, the following notations provide an asymptotic way of saying that a function grows at a rate that is “greater than or equal to” that of another. Example 4.14: 3n log n ? 2n is Ω(n log n). Big-Theta In addition, there is a notation that allows us to say that two functions grow at the same rate, up to constant factors. We say that f(n) is Θ(g(n)), pronounced “f(n) is big-Theta of g(n),” Comparative Analysis asymptotically[,?simp’t?tik,-k?l] better Suppose two algorithms solving the same problem are available: an algorithm A, which has a running time of O(n), and an algorithm B, which has a running time of O(n2). Which algorithm is better? We know that n is O(n2), which implies that algorithm A is asymptotically better than algorithm B, although for a small value of n, B may have a lower running time than A. Some Words of Caution First, note that the use of the big-Oh and related notations can be somewhat misleading should the constant factors they “hide” be very large. For example, while it is true that the function 10100n is O(n), if this is the running time of an algorithm being compared to one whose running time is 10n log n, we should prefer the O(nlog n)-time algorithm, even though the linear-time algorithm is asymptotically faster. This preference is because the constant factor, 10100, which is called “one googol,” is believed by many astronomers to be an upper bound on the number of atoms in the observable universe. So we are unlikely to ever have a real-world problem that has this number as its input size. Exponential [,eksp?’nen?(?)l] Running Times To see how fast the function 2n grows, consider the famous story about the inventor of the game of chess. He asked only that his king pay him 1 grain of rice for the first square on the board, 2 grains for the second, 4 grains for the third, 8 for the fourth, and so on. The number of grains in the 64th square would be263 = 9, 223, 372, 036, 854, 775, 808,which is about nine billion billions! If we must draw a line between efficient and inefficient algorithms, therefore, it is natural to make this distinction be that between those algorithms running in polynomial [,p?l?’n??m??l] time and those running in exponential time. That is, make the distinction between algorithms with a running time that is O(nc) (power c based on n), for some constant c &gt; 1, and those with a running time that is O(bn) (power n based on b), for some constant b &gt; 1. Like so many notions we have discussed in this section, this too should be taken with a “grain of salt,” for an algorithm running in O(n100) time should probably not be considered “efficient.” Even so, the distinction between polynomial-time and exponential-time algorithms is considered a robust measure of tractability. Examples of Algorithm Analysisconstant time operation All of the primitive operations, originally described on page 154, are assumed to run in constant time; Assume that variable A is an array of n elements. The expression A.length in Java is evaluated in constant time, because arrays are represented internally with an explicit variable that records the length of the array. Another central behavior of arrays is that for any valid index j, the individual element, A[j], can be accessed in constant time. This is because an array uses a consecutive block of memory. The jth element can be found, not by iterating through the array one element at a time, but by validating the index, and using it as an offset from the beginning of the array in determining the appropriate memory address. Therefore, we say that the expression A[j] is evaluated in O(1) time for an array. Finding the Maximum of an ArrayProposition 4.16: The algorithm, arrayMax, for computing the maximum element of an array of n numbers, runs in O(n) time. Justification: The initialization at lines 3 and 4 and the return statement at line 8 require only a constant number of primitive operations. Each iteration of the loop also requires only a constant number of primitive operations, and the loop executes n ? 1 times. Composing Long Strings Therefore, the overall time taken by this algorithm is proportional to1 + 2 + ··· + n,which we recognize as the familiar O(n2) summation from Proposition 4.3. Therefore, the total time complexity of the repeat1 algorithm is O(n2). x = logbn if and only if bx = n.The value b is known as the base of the logarithm. Note that by the above definition, for any base b &gt; 0, we have that logb 1 = 0. Three-Way Set DisjointnessOrigional solutionSuppose we are given three sets, A, B, and C, stored in three different integer arrays. We will assume that no individual set contains duplicate values, but that there may be some numbers that are in two or three of the sets. The three-way set disjointness problem is to determine if the intersection of the three sets is empty, namely, that there is no element x such that x ∈ A, x ∈ B, and x ∈ C. 123456789101112private static boolean disjoint1(int[] groupA, int[] groupB, int[] groupC)&#123; for ( int i : groupA) &#123; for (int j : groupB) &#123; for (int k : groupC) &#123; if(i==j &amp;&amp; j==k)&#123; return false; &#125; &#125; &#125; &#125; return true;&#125; This simple algorithm loops through each possible triple of values from the three sets to see if those values are equivalent. If each of the original sets has size n, then the worst-case running time of this method is O(n3) .12345678910111213141516private static boolean disjoint2(int[] groupA, int[] groupB, int[] groupC)&#123; for ( int i : groupA) &#123; for (int j : groupB) &#123; if(i==j)&#123; // add this checking to reduce complexitiy for (int k : groupC) &#123; if(j==k)&#123; return false; &#125; &#125; &#125; &#125; &#125; return true; &#125; In the improved version, it is not simply that we save time if we get lucky. We claim that the worst-case running time for disjoint2 is O(n2). by sortingSorting algorithms will be the focus of Chapter 12. The best sorting algorithms (including those used by Array.sort in Java) guarantee a worst-case running time of O(nlog n). Once the data is sorted, the subsequent loop runs in O(n) time, and so the entire unique2 algorithm runs in O(n log n) time. Exercise C-4.35 explores the use of sorting to solve the three-way set disjointness problem in O(n log n) time. prefixAverageCheck the source code at PrefixAverage.java, the inital implementation is two for loop, which is O(n2), while the better approach is reuse existing total sum. 1234567891011121314// naiive approach,for (int i = 0; i &lt; n; i++) &#123; double total=0; for (int j = 0; j &lt;=i; j++) &#123; // be awre it's &lt;=, instead of "&lt;" total+=x[j]; &#125; a[i]=total/(i+1); &#125;// better approachdouble total=0; for (int i = 0; i &lt; n; i++) &#123; total += x[i]; a[i]=total/(i+1); &#125; RecursiveDefinitions We have one or more base cases, which refer to fixed values of the function. e.g. for n!=1 as n=1 is base. Then we have one or more recursive cases, which define the function in terms of itself. for n!, it’s =n*(n-1)! for n&gt;=1 Repetition is achieved through repeated recursive invocations of the method. The process i finite because each time the method is invoked, its argument is smaller by one, and when a base case is reached, no further recursive calls are made. In the case of computing the factorial function, there is no compelling reason for prefereing recursion over a direct iteration with a loop. TreeADT (Abstract Data Type) we define a tree ADT using the concept of a position as an abstraction for a node of a tree. An element is stored at each position, and positions satisfy parent-child relationships that define the tree structure. Depth and HeightDepth The depth of p is the number of ancestors of p, other than p itself. The running time of depth(p) for position p is O(dp + 1), where dp denotes the depth of p in the tree, because the algorithm performs a constant-time recursive step for each ancestor of p. Thus, algorithm depth(p) runs in O(n) worst-case time, where n is the total number of positions of T, because a position of T may have depth n - 1 if all nodes form a single branch. Method depth, as implemented within the AbstractTree class.123456public int depth(Position&lt;E&gt; p)&#123; if(isRoot(p)) return 0; else return 1+depth(parent(p));&#125; Height We next define the height of a tree to be equal to the maximum of the depths of its positions (or zero, if the tree is empty). Folloing worst time cost is O(n), it progresses in a top-down fashion. If the method is initially called on the root of T, it will eventually be called once for each position of T. This is because the root eventually invokes the recursion on each of its children, which in turn invokes the recursion on each of their children, and so on.123456public int height(Position&lt;E&gt; p)&#123; int h=0; for(Position&lt;E&gt; c: children(p)) h=Math.max(h,1+height(c)); return h;&#125; Binary Tree A binary tree is an ordered tree with the following properties: Every node has at most two children. Each child node is labeled as being either a left child or a right child. A left child precedes a right child in the order of children of a node. A binary tree is proper if each node has either zero or two children. Some people also refer to such trees as being full binary trees. Thus, in a proper binary tree, every internal node has exactly two children. A binary tree that is not proper is improper. Some binary treesdecision tree An important class of binary trees arises in contexts where we wish to represent a number of different outcomes that can result from answering a series of yes-or-no questions. Each internal node is associated with a question. Starting at the root, we go to the left or right child of the current node, depending on whether the answer to the question is “Yes” or “No.” With each decision, we follow an edge from a parent to a child, eventually tracing a path in the tree from the root to a leaf. Such binary trees are known as decision trees, because a leaf position p in such a tree represents a decision of what to do if the questions associated with p’s ancestors are answered in a way that leads to p. A decision tree is a proper binary tree. Arithmetic expression An arithmetic expression can be represented by a binary tree whose leaves are associated with variables or constants, and whose internal nodes are associated with one of the operators +, ?, *, and /, as demonstrated in Figure 8.6. Each node in such a tree has a value associated with it. If a node is leaf, then its value is that of its variable or constant. If a node is internal, then its value is defined by applying its operation to the values of its children.Properties of Binary trees level d has at most 2d nodes Let T be a nonempty binary tree, and let n, nE, nI, and h denote the number of nodes, number of external nodes, number of internal nodes, and height of T, respectively. Then T has the following properties: h + 1 ≤ n ≤ 2h+1 - 1 1 ≤ nE ≤ 2h h ≤ nI ≤ 2h - 1 log(n + 1) - 1 ≤ h ≤ n - 1 Also, if T is proper, then T has the following properties: 2h + 1 ≤ n ≤ 2h+1 - 1 h + 1 ≤ nE ≤ 2h h ≤ nI ≤ 2h - 1 log(n + 1) - 1 ≤ h ≤ (n - 1)/2 In a nonempty proper binary tree T, with nE external nodes and nI internal nodes, we have nE = nI + 1. Why use tree You can search, insert/delete items quickly in a tree Ordered Arrays are bad at Insertions/Deletions Finding items in a Linkedlist is slow Time needed to perform an operation on a tree is O(log N) On average a tree is more efficient if you need to perform many different types of operations. Code practicehttp://www.practice.geeksforgeeks.org/problem-page.php?pid=700159 Geek IDEhttp://code.geeksforgeeks.org/index.php Reference http://www.geeksforgeeks.org/maximum-width-of-a-binary-tree/]]></content>
      <tags>
        <tag>Dev</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IT-Architect]]></title>
    <url>%2F2017-01-26-IT-Architect%2F</url>
    <content type="text"><![CDATA[SOA SOA is a set of design principles for building a suite of interoperable, flexible and reusable services based architecture. top-down and bottom-up approach SOA patterns Design PatternsSingleton Being single sometimes has its advantages you know. I’m often used to manage pools of resources, like connection or thread pools. The Singleton Pattern ensures a class has only one instance, and provides a global point of access to it. WATCH IT! Double-checked locking doesn’t work in Java 1.4 or earlier! Unfortunately, in Java version 1.4 and earlier, many JVMs contain implementations of the volatile keyword that allow improper synchronization for double-checked locking. If you must use a JVM earlier than Java 5, consider other methods of implementing your Singleton. Reference]]></content>
      <tags>
        <tag>Architect</tag>
        <tag>Questions</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java New IO]]></title>
    <url>%2F2017-02-02-Java-New-IO%2F</url>
    <content type="text"><![CDATA[Notes JDK 1.0 introduced rudimentary I/O facilities for accessing the file system (to create a directory, remove a file, or perform another task), accessing file content randomly (as opposed to sequentially), and streaming byte-oriented data between sources and destinations in a sequential manner.]]></content>
      <tags>
        <tag>java</tag>
        <tag>IO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Network Protocols]]></title>
    <url>%2F2017-01-28-Network-Protocols%2F</url>
    <content type="text"><![CDATA[Net Protocols Like most models, this OSI physical layer contains the electrical, mechanical, and functional means to establish physical connections between Layer-2 devices. In addition to interfacing with the Network Layer, the data link connection can be built upon one or more physical layer interfaces.]]></content>
      <tags>
        <tag>TCP</tag>
        <tag>Protocols</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java Concurrent Column 2]]></title>
    <url>%2F2017-02-01-Concurrent-In-Java-col-2%2F</url>
    <content type="text"><![CDATA[This is the second half about Java Concurrent of my blognon-blocking synchronization Much of the recent research on concurrent algorithms has focused on nonblock- ing algorithms, which use low-level atomic machine instructions such as compare- and-swap instead of locks to ensure data integrity under concurrent access. Non- blocking algorithms are used extensively in operating systems and JVMs for thread and process scheduling, garbage collection, and to implement locks and other concurrent data structures. Nonblocking algorithms are considerably more complicated to design and im- plement than lock-based alternatives, but they can offer significant scalability and liveness advantages. They coordinate at a finer level of granularity and can greatly reduce scheduling overhead because they don’t block when multiple threads contend for the same data. Further, they are immune to deadlock and other liveness problems. In lock-based algorithms, other threads cannot make progress if a thread goes to sleep or spins while holding a lock, whereas nonblocking algorithms are impervious to individual thread failures. As of Java 5.0, it is possible to build efficient nonblocking algorithms in Java using the atomic variable classes such as AtomicInteger and AtomicReference. Atomic variables can also be used as “better volatile variables” even if you are not developing nonblocking algorithms. Atomic variables offer the same memory semantics as volatile variables, but with additional support for atomic updates— making them ideal for counters, sequence generators, and statistics gathering while offering better scalability than lock-based alternatives. Coordinating access to shared state using a consistent locking protocol ensures that whichever thread holds the lock guarding a set of variables has exclusive access to those variables, and that any changes made to those variables are visible to other threads that subsequently acquire the lock. Volatile variables are a lighter-weight synchronization mechanism than locking because they do not involve context switches or thread scheduling. However, volatile variables have some limitations compared to locking: while they provide similar visibility guarantees, they cannot be used to construct atomic compound actions. This means that volatile variables cannot be used when one variable de- pends on another, or when the new value of a variable depends on its old value. This limits when volatile variables are appropriate, since they cannot be used to reliably implement common tools such as counters or mutexes. This can be a serious problem if the blocked thread is a high-priority thread but the thread holding the lock is a lower-priority thread—a performance hazard known as priority inversion. Even though the higher-priority thread should have precedence, it must wait until the lock is released, and this effectively down- grades its priority to that of the lower-priority thread. If a thread holding a lock is permanently blocked (due to an infinite loop, deadlock, livelock, or other liveness failure), any threads waiting for that lock can never make progress. hardware Exclusive locking is a pessimistic technique—it assumes the worst (if you don’t lock your door, gremlins will come in and rearrange your stuff) and doesn’t proceed until you can guarantee, by acquiring the appropriate locks, that other threads will not interfere. For fine-grained operations, there is an alternate approach that is often more efficient—the optimistic approach, whereby you proceed with an update, hopeful that you can complete it without interference. This approach relies on collision detection to determine if there has been interference from other parties during the update, in which case the operation fails and can be retried (or not). The optimistic approach is like the old saying, “It is easier to obtain forgiveness than permission”, where “easier” here means “more efficient”. Processors designed for multiprocessor operation provide special instructions for managing concurrent access to shared variables. Early processors had atomic test-and-set, fetch-and-increment, or swap instructions sufficient for implementing mutexes that could in turn be used to implement more sophisticated concurrent objects. Today, nearly every modern processor has some form of atomic read- modify-write instruction, such as compare-and-swap or load-linked/store-conditional. Operating systems and JVMs use these instructions to implement locks and con- current data structures, but until Java 5.0 they had not been available directly to Java classes. Compare and swap The approach taken by most processor architectures, including IA32 and Sparc, is to implement a compare-and-swap (CAS) instruction. (Other processors, such as PowerPC, implement the same functionality with a pair of instructions: load- linked and store-conditional.) CAS has three operands—a memory location V on which to operate, the expected old value A, and the new value B. CAS atomically updates V to the new value B, but only if the value in V matches the expected old value A; otherwise it does nothing. In either case, it returns the value currently in V. (The variant called compare-and-set instead returns whether the operation succeeded.) CAS means “I think V should have the value A; if it does, put B there, otherwise don’t change it but tell me I was wrong.” CAS is an optimistic technique—it proceeds with the update in the hope of success, and can detect failure if another thread has updated the variable since it was last examined. SimulatedCAS in Listing 15.1 illustrates the semantics (but not the implementation or performance) of CAS. When multiple threads attempt to update the same variable simultaneously using CAS, one wins and updates the variable’s value, and the rest lose. But the losers are not punished by suspension, as they could be if they failed to acquire a lock; instead, they are told that they didn’t win the race this time but can try again. Because a thread that loses a CAS is not blocked, it can decide whether it wants to try again, take some other recovery action, or do nothing.3 This flexibility eliminates many of the liveness hazards associated with locking (though in unusual cases can introduce the risk of livelock—see Section 10.3.3). CAS addresses the problem of implementing atomic read-modify-write sequences without locking, because it can detect interference from other threads. counter implemented by CAS At first glance, the CAS-based counter looks as if it should perform worse than a lock-based counter; it has more operations and a more complicated control flow, and depends on the seemingly complicated CAS operation. But in reality, CAS-based counters significantly outperform lock-based counters if there is even a small amount of contention, and often even if there is no contention. The fast path for uncontended lock acquisition typically requires at least one CAS plus other lock-related housekeeping, so more work is going on in the best case for a lock-based counter than in the normal case for the CAS-based counter. Since the CAS succeeds most of the time (assuming low to moderate contention), the hardware will correctly predict the branch implicit in the while loop, minimizing the overhead of the more complicated control logic. The language syntax for locking may be compact, but the work done by the JVM and OS to manage locks is not. Locking entails traversing a relatively com- plicated code path in the JVM and may entail OS-level locking, thread suspension, and context switches. In the best case, locking requires at least one CAS, so using locks moves the CAS out of sight but doesn’t save any actual execution cost. On the other hand, executing a CAS from within the program involves no JVM code, system calls, or scheduling activity. What looks like a longer code path at the ap- plication level is in fact a much shorter code path when JVM and OS activity are taken into account. The primary disadvantage of CAS is that it forces the caller to deal with contention (by retrying, backing off, or giving up), whereas locks deal with contention automatically by blocking until the lock is available. Competitive forces will likely result in continued CAS performance improvement over the next sev- eral years. A good rule of thumb is that the cost of the “fast path” for uncontended lock acquisition and release on most processors is approximately twice the cost of a CAS. CAS support in JVM So, how does Java code convince the processor to execute a CAS on its behalf? Prior to Java 5.0, there was no way to do this short of writing native code. In Java 5.0, low-level support was added to expose CAS operations on int, long, and object references, and the JVM compiles these into the most efficient means provided by the underlying hardware. On platforms supporting CAS, the run- time inlines them into the appropriate machine instruction(s); in the worst case, if a CAS-like instruction is not available the JVM uses a spin lock. This low-level JVM support is used by the atomic variable classes (AtomicXxx in java.util.con- current.atomic) to provide an efficient CAS operation on numeric and reference types; these atomic variable classes are used, directly or indirectly, to implement most of the classes in java.util.concurrent. Other liveness hazards While deadlock is the most widely encountered liveness hazard, there are sev- eral other liveness hazards you may encounter in concurrent programs including starvation, missed signals, and livelock. Starvation Starvation occurs when a thread is perpetually denied access to resources it needs in order to make progress; the most commonly starved resource is CPU cycles. Starvation in Java applications can be caused by inappropriate use of thread prior- ities. It can also be caused by executing nonterminating constructs (infinite loops or resource waits that do not terminate) with a lock held, since other threads that need that lock will never be able to acquire it. The thread priorities defined in the Thread API are merely scheduling hints. The Thread API defines ten priority levels that the JVM can map to operating system scheduling priorities as it sees fit. This mapping is platform-specific, so two Java priorities can map to the same OS priority on one system and different OS priorities on another. Avoid the temptation to use thread priorities, since they increase platform dependence and can cause liveness problems. Most concurrent applica- tions can use the default priority for all threads. Poor responsiveness One step removed from starvation is poor responsiveness, which is not uncom- mon in GUI applications using background threads. If the work done by other threads are truly background tasks, lowering their priority can make the foreground tasks more responsive. Livelock Livelock is a form of liveness failure in which a thread, while not blocked, still cannot make progress because it keeps retrying an operation that will always fail. Livelock often occurs in transactional messaging applications, where the messaging infrastructure rolls back a transaction if a message cannot be processed successfully, and puts it back at the head of the queue. If a bug in the message handler for a particular type of message causes it to fail, every time the message is dequeued and passed to the buggy handler, the transaction is rolled back. Since the message is now back at the head of the queue, the handler is called over and over with the same result. (This is sometimes called the poison message problem.) The message handling thread is not blocked, but it will never make progress either. This form of livelock often comes from overeager error-recovery code that mistakenly treats an unrecoverable error as a recoverable one. This is similar to what happens when two overly polite people are walking in opposite directions in a hallway: each steps out of the other’s way, and now they are again in each other’s way. So they both step aside again, and again, and again. . . Solutions The solution for this variety of livelock is to introduce some randomness into the retry mechanism. For example, when two stations in an ethernet network try to send a packet on the shared carrier at the same time, the packets collide. The stations detect the collision, and each tries to send their packet again later. If they each retry exactly one second later, they collide over and over, and neither packet ever goes out, even if there is plenty of available bandwidth. To avoid this, we make each wait an amount of time that includes a random component. (The ethernet protocol also includes exponential backoff after repeated collisions, reducing both congestion and the risk of repeated failure with multiple colliding stations.) Retrying with random waits and backoffs can be equally effective for avoiding livelock in concurrent applications. Summary Liveness failures are a serious problem because there is no way to recover from them short of aborting the application. The most common form of liveness failure is lock-ordering deadlock. Avoiding lock ordering deadlock starts at design time: ensure that when threads acquire multiple locks, they do so in a consistent order. The best way to do this is by using open calls throughout your program. This greatly reduces the number of places where multiple locks are held at once, and makes it more obvious where those places are.Reference Performance One of the primary reasons to use threads is to improve performance. First make your program right, then make it fast—and then only if your performance requirements and measurements tell you it needs to be faster. In designing a con- current application, squeezing out the last bit of performance is often the least of your concerns. When the performance of an activity is limited by availability of a par- ticular resource, we say it is bound by that resource: CPU-bound, database-bound, etc. using multiple threads always introduces some performance costs compared to the single-threaded approach. These include the overhead associated with coordinating between threads (locking, signaling, and memory synchronization), increased context switching,thread creation and teardown, and scheduling overhead. When threading is employed effectively, these costs are more than made up for by greater throughput, responsiveness, or capacity. On the other hand, a poorly designed concurrent application can perform even worse than a comparable sequential one. we want to keep the CPUs busy with useful work Scalability Scalability describes the ability to improve throughput or capacity when additional computing resources (such as additional CPUs, memory, stor- age, or I/O bandwidth) are added. Nearly all engineering decisions involve some form of tradeoff. This is one of the reasons why most optimizations are premature: they are often undertaken before a clear set of requirements is available. Avoid premature optimization. First make it right, then make it fast—if it is not already fast enough. Measure, don’t guess. Amdahl’s law the theoretical speedup is always limited by the part of the task that cannot benefit from the improvement. If F is the fraction of the calculation that must be executed serially, then Amdahl’s law says that on a machine with N processors, we can achieve a speedup of at most:Speedup ≤ 1 / (F + (1 − F)/N) As N approaches infinity, the maximum speedup converges to 1/F, meaning that a program in which fifty percent of the processing must be executed serially can be sped up only by a factor of two, regardless of how many processors are available, and a program in which ten percent must be executed serially can be sped up by at most a factor of ten. Amdahl’s law also quantifies the efficiency cost of serialization. With ten processors, a program with 10% serialization can achieve at most a speedup of 5.3 (at 53% utilization), and with 100 processors it can achieve at most a speedup of 9.2 (at 9% utilization). It takes a lot of inefficiently utilized CPUs to never get to that factor of ten. It is clear that as processor counts increase, even a small percentage of serialized execution limits how much throughput can be increased with additional computing resources. All concurrent applications have some sources of serialization; if you think yours does not, look again. Amdahl’s law tells us that the scalability of an application is driven by the proportion of code that must be executed serially. Since the primary source of serialization in Java programs is the exclusive resource lock, scalability can often be improved by spending less time holding locks, either by reducing lock granu- larity, reducing the duration for which locks are held, or replacing exclusive locks with nonexclusive or nonblocking alternatives. Costs introduced by threadsContext switching Context switches are not free; thread scheduling requires manipulating shared data structures in the OS and JVM. The OS and JVM use the same CPUs your pro- gram does; more CPU time spent in JVM and OS code means less is available for your program. When a new thread is switched in, the data it needs is unlikely to be in the local processor cache, so a context switch causes a flurry of cache misses, and thus threads run a little more slowly when they are first scheduled. The actual cost of context switching varies across platforms, but a good rule of thumb is that a context switch costs the equivalent of 5,000 to 10,000 clock cycles, or several microseconds on most current processors. memory synchronization The performance cost of synchronization comes from several sources. The visibility guarantees provided by synchronized and volatile may entail using special instructions called memory barriers that can flush or invalidate caches, flush hard- ware write buffers, and stall execution pipelines. Memory barriers may also have indirect performance consequences because they inhibit other compiler optimizations; most operations cannot be reordered with memory barriers. When assessing the performance impact of synchronization, it is important to distinguish between contended and uncontended synchronization. The synchronized mechanism is optimized for the uncontended case (volatile is always uncontended), and at this writing, the performance cost of a “fast-path” uncontended synchronization ranges from 20 to 250 clock cycles for most systems. While this is certainly not zero, the effect of needed, uncontended synchronization is rarely significant in overall application performance, and the alternative involves compromising safety and potentially signing yourself (or your succes- sor) up for some very painful bug hunting later. Modern JVMs can reduce the cost of incidental synchronization by optimizing away locking that can be proven never to contend. If a lock object is accessible only to the current thread, the JVM is permitted to optimize away a lock acquisi- tion because there is no way another thread could synchronize on the same lock. For example, the lock acquisition in following Listing can always be eliminated by the JVM. Following synchronization has no effect 123synchronized (new Object()) &#123; // do something&#125; More sophisticated JVMs can use escape analysis to identify when a local object reference is never published to the heap and is therefore thread-local. As below sample: 123456public String getStoogeNames() &#123;List&lt;String&gt; stooges = new Vector&lt;String&gt;(); stooges.add("Moe");stooges.add("Larry");stooges.add("Curly");return stooges.toString();&#125; the only reference to the List is the local variable stooges, and stack-confined variables are automatically thread-local. A naive execution of getStoogeNames would acquire and release the lock on the Vector four times, once for each call to add or toString. However, a smart runtime compiler can inline these calls and then see that stooges and its internal state never escape, and therefore that all four lock acquisitions can be eliminated. Even without escape analysis, compilers can also perform lock coarsening, the merging of adjacent synchronized blocks using the same lock. For getStooge- Names, a JVM that performs lock coarsening might combine the three calls to add and the call to toString into a single lock acquisition and release, using heuristics on the relative cost of synchronization versus the instructions inside the synch- ronized block.5 Not only does this reduce the synchronization overhead, but it also gives the optimizer a much larger block to work with, likely enabling other optimizations. Don’t worry excessively about the cost of uncontended synchronization. The basic mechanism is already quite fast, and JVMs can perform addi- tional optimizations that further reduce or eliminate the cost. Instead, focus optimization efforts on areas where lock contention actually occurs. Synchronization by one thread can also affect the performance of other threads. Synchronization creates traffic on the shared memory bus; this bus has a limited bandwidth and is shared across all processors. If threads must compete for synchronization bandwidth, all threads using synchronization will suffer. Blocking Uncontended synchronization can be handled entirely within the JVM (Bacon et al., 1998); contended synchronization may require OS activity, which adds to the cost. When locking is contended, the losing thread(s) must block. The JVM can implement blocking either via spin-waiting (repeatedly trying to acquire the lock until it succeeds) or by suspending the blocked thread through the operating system. Which is more efficient depends on the relationship between context switch overhead and the time until the lock becomes available; spin-waiting is preferable for short waits and suspension is preferable for long waits. Some JVMs choose between the two adaptively based on profiling data of past wait times, but most just suspend threads waiting for a lock. Reducing lock contention We’ve seen that serialization hurts scalability and that context switches hurt performance. Contended locking causes both, so reducing lock contention can improve both performance and scalability. Access to resources guarded by an exclusive lock is serialized—only one thread at a time may access it. Of course, we use locks for good reasons, such as preventing data corruption, but this safety comes at a price. Persistent contention for a lock limits scalability. The principal threat to scalability in concurrent applications is the exclu- sive resource lock. Two factors influence the likelihood of contention for a lock: how often that lock is requested and how long it is held once acquired.7 If the product of these factors is sufficiently small, then most attempts to acquire the lock will be uncon- tended, and lock contention will not pose a significant scalability impediment. There are three ways to reduce lock contention: Reduce the duration for which locks are held; Reduce the frequency with which locks are requested; or Replace exclusive locks with coordination mechanisms that permitgreater concurrency. Narrowing lock scope An effective way to reduce the likelihood of contention is to hold locks as briefly as possible. This can be done by moving code that doesn’t require the lock out of synchronized blocks, especially for expensive operations and potentially block- ing operations such as I/O. It is easy to see how holding a “hot” lock for too long can limit scalability Reducing the scope of the lock in userLocationMatches substantially reduces the number of instructions that are executed with the lock held. By Amdahl’s law, this removes an impediment to scalability because the amount of serialized code is reduced. Because AttributeStore has only one state variable, attributes, we can im- prove it further by the technique of delegating thread safety (Section 4.3). By replacing attributes with a thread-safe Map (a Hashtable, synchronizedMap, or Con- currentHashMap), AttributeStore can delegate all its thread safety obligations to the underlying thread-safe collection. Reducing lock granularity The other way to reduce the fraction of time that a lock is held (and therefore the likelihood that it will be contended) is to have threads ask for it less often. This can be accomplished by lock splitting and lock striping, which involve using separate locks to guard multiple independent state variables previously guarded by a single lock. These techniques reduce the granularity at which locking occurs, potentially allowing greater scalability—but using more locks also increases the risk of deadlock. If a lock guards more than one independent state variable, you may be able to improve scalability by splitting it into multiple locks that each guard different variables. This results in each lock being requested less often. After splitting the lock, each new finer-grained lock will see less locking traffic than the original coarser lock would have. Lock stripping Splitting a heavily contended lock into two is likely to result in two heavily contended locks. Lock splitting can sometimes be extended to partition locking on a variable- sized set of independent objects, in which case it is called lock striping. For exam- ple, the implementation of ConcurrentHashMap uses an array of 16 locks, each of which guards 1/16 of the hash buckets; bucket N is guarded by lock N mod 16. One of the downsides of lock striping is that locking the collection for ex- clusive access is more difficult and costly than with a single lock. Usually an operation can be performed by acquiring at most one lock, but occasionally you need to lock the entire collection, as when ConcurrentHashMap needs to expand the map and rehash the values into a larger set of buckets. This is typically done by acquiring all of the locks in the stripe set common optimizations such as caching frequently computed values can introduce “hot fields” that limit scalability. A common optimization is to update a separate counter as entries are added or removed; this slightly increases the cost of a put or remove operation to keep the counter up-to-date, but reduces the cost of the size operation from O(n) to O(1). In this case, the counter is called a hot field because every mutative operation needs to access it. ConcurrentHashMap avoids this problem by having size enumerate the stripes and add up the number of elements in each stripe, instead of maintaining a global count. To avoid enumerating every element, ConcurrentHashMap maintains a separate count field for each stripe, also guarded by the stripe lock. Alternative to exclusive lock A third technique for mitigating the effect of lock contention is to forego the use of exclusive locks in favor of a more concurrency-friendly means of managing shared state. These include using the concurrent collections, read-write locks, immutable objects and atomic variables. ReadWriteLock enforces a multiple-reader, single-writer locking discipline: more than one reader can access the shared resource concurrently so long as none of them wants to modify it, but writers must acquire the lock excusively. For read-mostly data structures, ReadWriteLock can offer greater concurrency than exclusive locking; for read-only data structures, immutability can eliminate the need for locking entirely. Atomic variables (see Chapter 15) offer a means of reducing the cost of updat- ing “hot fields” such as statistics counters, sequence generators, or the reference If size is called frequently compared to mutative operations, striped data structures can optimize for this by caching the collection size in a volatile whenever size is called and invalidating the cache (setting it to -1) whenever the collection is modified. If the cached value is nonnegative on entry to size, it is accurate and can be returned; otherwise it is recomputed. The atomic variable classes pro- vide very fine-grained (and therefore more scalable) atomic operations on integers or object references, and are implemented using low-level concurrency primitives (such as compare-and-swap) provided by most modern processors. If your class has a small number of hot fields that do not participate in invariants with other variables, replacing them with atomic variables may improve scalability. Comparing Map The single-threaded performance of ConcurrentHashMap is slightly better than that of a synchronized HashMap, but it is in concurrent use that it really shines. The implementation of ConcurrentHashMap assumes the most common operation is retrieving a value that already exists, and is therefore optimized to provide highest performance and concurrency for successful get operations. The major scalability impediment for the synchronized Map implementations is that there is a single lock for the entire map, so only one thread can access the map at a time. On the other hand, ConcurrentHashMap does no locking for most successful read operations, and uses lock striping for write operations and those few read operations that do require locking. As a result, multiple threads can access the Map concurrently without blocking. The numbers for the synchronized collections are not as encouraging. Perfor- mance for the one-thread case is comparable to ConcurrentHashMap, but once the load transitions from mostly uncontended to mostly contended—which happens here at two threads—the synchronized collections suffer badly. This is common behavior for code whose scalability is limited by lock contention. So long as contention is low, time per operation is dominated by the time to actually do the work and throughput may improve as threads are added. Once contention becomes significant, time per operation is dominated by context switch and scheduling delays, and adding more threads has little effect on throughput. Building a asynchronous log Building a logger that moves the I/O to another thread may improve performance, but it also introduces a number of design complications, such as interruption (what happens if a thread blocked in a logging operation is interrupted?), service guarantees (does the logger guarantee that a success- fully queued log message will be logged prior to service shutdown?), saturation policy (what happens when the producers log messages faster than the logger thread can handle them?), and service lifecycle (how do we shut down the logger, and how do we communicate the service state to producers?). Reducing context switching The “get in, get out” principle of Section 11.4.1 tells us that we should hold locks as briefly as possible, because the longer a lock is held, the more likely that lock will be contended. If a thread blocks waiting for I/O while holding a lock, another thread is more likely to want the lock while the first thread is holding it. Concurrent systems perform much better when most lock acquisitions are uncontended, because contended lock acquisition means more context switches. A coding style that encourages more context switches thus yields lower overall throughput. Testing concurrency we defined safety as “nothing bad ever happens” and liveness as “something good eventually happens”. when interrupted, it throws InterruptedException. This is one of the few cases in which it is appropriate to subclass Thread explicitly instead of using a Runnable in a pool: in order to test proper termination with join. The result of Thread.getState should not be used for concurrency control, and is of limited usefulness for testing—its primary utility is as a source of debugging information. a common error in implementing semaphore-controlled buffers is to forget that the code actually doing the insertion and extraction requires mutual exclu- sion (using synchronized or ReentrantLock). A sample run of PutTakeTest with a version of BoundedBuffer that omits making doInsert and doExtract synch- ronized fails fairly quickly. Tests should be run on multiprocessor systems to increase the diversity of potential interleavings. However, having more than a few CPUs does not necessarily make tests more effective. To maximize the chance of detecting timing-sensitive data races, there should be more active threads than CPUs, so that at any given time some threads are running and some are switched out, thus reducing the predicatability of interactions between threads. Tests like PutTakeTest tend to be good at finding safety violations. For exam- ple, a common error in implementing semaphore-controlled buffers is to forget that the code actually doing the insertion and extraction requires mutual exclu- sion (using synchronized or ReentrantLock). A sample run of PutTakeTest with a version of BoundedBuffer that omits making doInsert and doExtract synch- ronized fails fairly quickly. Running PutTakeTest with a few dozen threads iterating a few million times on buffers of various capacity on various systems increases our confidence about the lack of data corruption in put and take. The source code PutTakeTest.java demonstreated aforesaid logic. Test resource management The tests so far have been concerned with a class’s adherence to its specifica- tion—that it does what it is supposed to do. A secondary aspect to test is that it does not do things it is not supposed to do, such as leak resources. Any object that holds or manages other objects should not continue to maintain references to those objects longer than necessary. Such storage leaks prevent garbage collectors from reclaiming memory (or threads, file handles, sockets, database connections, or other limited resources) and can lead to resource exhaustion and application failure.]]></content>
      <tags>
        <tag>java</tag>
        <tag>concurrent</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java JVM]]></title>
    <url>%2F2017-02-05-JVM%2F</url>
    <content type="text"><![CDATA[Class loading subsystemConsist of three sections loadThere are three class loaders bootstrap class loader, e.g. rt.jar extension class loader, e.g. jre/lib/ext application class loader, e.g. -cpLink Verify Prepare ResolveInitialize While deadlock is the most widely encountered liveness hazard, there are sev- eral other liveness hazards you may encounter in concurrent programs including starvation, missed signals, and livelock. MetaspaceSince Java 8, the introduce of metaspace is kind of using memory or even virtual memory in OS, so theriotically there is no limit of metaspace. But PermGen is part of Method area, so that is upper limit for PermGen.]]></content>
      <tags>
        <tag>java</tag>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Maven-Notes]]></title>
    <url>%2F2017-02-03-Maven-Notes%2F</url>
    <content type="text"><![CDATA[Maven philosophy “It is important to note that in the pom.xml file you specify the what and not the how. The pom.xml file can also serve as a documentation tool, conveying your project dependencies and their versions.” ConceptsLife cyclesMaven defines three Lifecycles ֠default, clean and site and each Lifecycle consists of predefined Phases. The clean argument which we pass to mvn command is phase which is in lifecycle named clean. Maven Lifecycle Phases clean lifecycleThe clean lifecycle contains three phases pre-clean, clean and post-clean .When we invoke command mvn clean, Maven loads Clean Lifecycle and executes the phases pre-clean and clean.We can invoke any of these three phases and all preceding phases up to and including the invoked phase are executed sequentially. Lifecycle phases can’s do anything by themselves. For example, phase clean by itself doesnӴ have ability or functionality to delete the build directory. It delegate the task to a plugin named maven-clean-plugin. So, lifecycles phases are just some predefined steps which Maven invokes sequentially one after another. As we can see, phases are similar to the steps in a job which are executed one after another. Concepts summaryLifecycles, Lifecycle Phases, Plugins and Plugin Goals are the core of Maven. and we summarize the concepts learned so far:Maven comes with three lifecycles ֠default, clean and site.each lifecycle is made up of lifecycle phases and in all, there are 28 phases ֠default 21, clean 3 and site 4.when a lifecycle phase is invoked using mvn command, all preceding phases are executed sequentially one after another.lifecycle phases by themselves doesnӴ have any capabilities to accomplish some task and they rely on plugins to carryout the task.depending on project and packaging type, Maven binds various plugin goals to lifecycle phases and goals carryout the task entrusted to them. default lifecycleDefault LifecycleThe most important of the three lifecycles is the Default Lifecycle . Maven uses default lifecycle to build, test and distribute the project. Default lifecycle contains 21 phases. Project may contain resources such as properties, XML configuration files etc., and phases process-resources and process-test-resources copy and process such resources files.The phases compile and test-compile complies the source Java files and test files respectively.The phases package, install, and deploy are used to distribute the project. As we have already seen, the package phase creates JAR file of resources and compiled classes for distribution. The phase install, installs the artifacts of the project i.e jar and pom.xml to the local repository at $HOME/.m2 so that other projects can use them as dependencies. The phase deploy installs the artifacts of the project to a remote repository (probably on Internet) so that a wider group of projects can use it as dependency. We will cover these phases in a later chapter. goalIf we see the usage description of mvn command, apart from the options it accepts only two things ֠goal or phase.Maven Lifecycle Phases - mvn usage description mvn [options] [&lt;goal(s)&gt;] [&lt;phase(s)&gt;] For example, we can directly compile the source with the following command.$ cd simple-app$ mvn compiler:compileTo run a goal with mvn, use the format :. In the above example, compiler:compile, the compiler is plugin prefix of maven-compiler-plugin and compile is the goal name. We can get the prefix of all plugins from Maven Plugin Directory. –When we invoke a goal directly, Maven executes just that goal, whereas when we invoke a lifecycle phase all the phases up to that phase are executed. We can see this in action with following example.$ cd simple-app$ mvn clean$ mvn surefire:test In very few situations we invoke plugin goals directly and more often than not, lifecycle phases are preferred. Lifecycle Phases and Plugin GoalsWhen a lifecycle phase is run, depending on project type and packaging type, Maven binds plugin goals to lifecycle phases.When we run mvn package in a Java Project. To process-resources phase, Maven binds resources goal of maven-resources-plugin and to test phase, it binds test goal of maven-surefire-plugin and so on.Whatӳ happens at package phase is bit interesting. In a Java Project, Maven binds jar goal of maven-jar-plugin. However, when we run the same command in a webapp project, up to test phase Maven binds same goals, but to the package phase Maven binds war goal of maven-war-plugin the war:war instead of jar:jar. SamplesFor example, consider the command below. The clean and package arguments are build phases, while the dependency:copy-dependencies is a goal (of a plugin). 1mvn clean dependency:copy-dependencies package If this were to be executed, the clean phase will be executed first (meaning it will run all preceding phases of the clean lifecycle, plus the clean phase itself), and then the dependency:copy-dependencies goal, before finally executing the package phase (and all its preceding build phases of the default lifecycle). Moreover, if a goal is bound to one or more build phases, that goal will be called in all those phases. To see what goals bined to lifecycle phase1mvn help:describe -Dcmd=install [INFO] ‘install’ is a phase corresponding to this plugin:org.apache.maven.plugins:maven-install-plugin:2.4:install It is a part of the lifecycle for the POM packaging ‘pom’. This lifecycle includes the following phases: validate: Not defined initialize: Not defined generate-sources: Not defined process-sources: Not defined generate-resources: Not defined process-resources: Not defined compile: Not defined process-classes: Not defined generate-test-sources: Not defined process-test-sources: Not defined generate-test-resources: Not defined process-test-resources: Not defined test-compile: Not defined process-test-classes: Not defined test: Not defined prepare-package: Not defined package: Not defined pre-integration-test: Not defined integration-test: Not defined post-integration-test: Not defined verify: Not defined install: org.apache.maven.plugins:maven-install-plugin:2.4:install deploy: org.apache.maven.plugins:maven-deploy-plugin:2.7:deploy PluginMaven is ֠at its heart ֠a plugin execution framework; all work is done by plugins. Plugins are broadly grouped as build plugins and reporting plugins. Plugins are artifacts that provide goals to Maven. Furthermore, a plugin may have one or more goals wherein each goal represents a capability of that plugin. For example, the Compiler plugin has two goals: compile and testCompile. The former compiles the source code of your main code, while the latter compiles the source code of your test code. Plugin GoalsMaven plugin is a collection of one or more goals which do some task or job. It is also known as Mojo ֠Maven Plain Old Java Object. Similarly, Maven uses maven-compiler-plugin to compile the source and test files and it provides three goals ֠compiler:compile, compiler:testCompile and compiler:help. Suffice it to say for now that a plugin is a collection of goals with a general common purpose. For example the jboss-maven-plugin, whose purpose is “deal with various jboss items”. To configure plugins, we use project build element in pom.xml. The next listing shows the top level elements used to configure a plugin.pom.xml 1234567891011121314151617&lt;project&gt;... &lt;build&gt; ... &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;...&lt;/groupId&gt; &lt;artifactId&gt;...&lt;/artifactId&gt; &lt;version&gt;...&lt;/version&gt; &lt;configuration&gt;...&lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt;...&lt;/executions&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; The elements are build : defines the project build. plugins : parent element for one or more elements. plugin : the plugin to configure. groupId, artifactId, version : coordinates of the plugin to configure. configuration : holds the parameters and properties to be passed to the plugin. executions : parent element for one or more element. execution : configures the execution of a goal of the plugin. In Maven, there are the build and the reporting plugins: Build plugins will be executed during the build and then, they should be configured in the element. Reporting plugins will be executed during the site generation and they should be configured in the element. positionNormally, the block is placed after the project coordinates block and before the dependencies block. To show plugin details in configurationEasiest way to know the available parameters for a goal is to run plugin help goal.$ mvn compiler:help -Dgoal=compile -DdetailIt will list the available parameters for compile goal of compiler.But it will not show the default value of the parameters and to know the available parameters and also, the default value for each parameter, run help:describe goal of Maven Help plugin (maven-help-plugin).$ mvn help:describe -Dplugin=compiler -Dmojo=compile -DdetailNote that maven-help-plugins uses -Dmojo for goal, instead of -Dgoal, Help GoalRecent Maven plugins have generally an help goal to have in the command line the description of the plugin, with their parameters and types. For instance, to understand the javadoc goal, you need to call: 1mvn javadoc:help -Ddetail -Dgoal=javadoc Samples To include or exclude certain files or directories in the project Jar, configure Maven Jar Plugin with and parameters.simple-app/pom.xml… org.apache.maven.plugins maven-jar-plugin **/service/* ... Maven Clean Plugin deletes the target directory by default and we may configure it to delete additional directories and files.simple-app/pom.xml… maven-clean-plugin src/main/generated false true *.java Template* ... The above configuration forces the clean plugin to delete *.java files in src/main/generated directory, but excludes the Template*. plugin executionFirstly demonstrate plugin execution with an example. In Apache Ant, itӳ quite easy to output echo any property or message during the build and many of us frequently use it to understand the build flow or to debug. But, Maven comes with no such feature, and only way to echo any message is to use Ant within Maven. The Apache AntRun Plugin provides the ability to run Ant tasks within Maven.Letӳ configure maven-antrun-plugin to output message to console.simple-app/pom.xml … maven-antrun-plugin run compile Build Dir: ${project.build.directory} …Build the project with mvn package and it echoes the build directory name in compile phase. sampleThe clean and package arguments are build phases, while the dependency:copy-dependencies is a goal (of a plugin). mvn clean dependency:copy-dependencies packageIf this were to be executed, the clean phase will be executed first (meaning it will run all preceding phases of the clean lifecycle, plus the clean phase itself), and then the dependency:copy-dependencies goal, before finally executing the package phase (and all its preceding build phases of the default lifecycle). conceptThe element / allows you to configure the execution of a plugin goal. With it, you can accomplish the following things.bind a plugin goal to a lifecycle phase.configure plugin parameters of a specific goal.configure plugin parameters of a specific goal such as compiler:compile, surefire:test etc., that are by default binds to a lifecycle phase core conceptsphaseyou may notice the second is simply a single word - package. Rather than a goal, this is a phase. A phase is a step in the build lifecycle, which is an ordered sequence of phases. When a phase is given, Maven will execute every phase in the sequence up to and including the one defined. For example, if we execute the compile phase, the phases that actually get executed are: validate generate-sources process-sources generate-resources process-resources compile Maven PhasesAlthough hardly a comprehensive list, these are the most common default lifecycle phases executed. validate: validate the project is correct and all necessary information is available compile: compile the source code of the project test: test the compiled source code using a suitable unit testing framework. These tests should not require the code be packaged or deployed package: take the compiled code and package it in its distributable format, such as a JAR. integration-test: process and deploy the package if necessary into an environment where integration tests can be run verify: run any checks to verify the package is valid and meets quality criteria install: install the package into the local repository, for use as a dependency in other projects locally deploy: done in an integration or release environment, copies the final package to the remote repository for sharing with other developers and projects.There are two other Maven lifecycles of note beyond the default list above. They are clean: cleans up artifacts created by prior builds site: generates site documentation for this projectPhases are actually mapped to underlying goals. The specific goals executed per phase is dependant upon the packaging type of the project. For example, package executes jar:jar if the project type is a JAR, and war:war if the project type is - you guessed it - a WAR. An interesting thing to note is that phases and goals may be executed in sequence. mvn clean dependency:copy-dependencies packageThis command will clean the project, copy dependencies, and package the project (executing all phases up to package, of course). – Lifecycle and phases are just formal names for jobs and steps. Maven calls the jobs as lifecycles and tasks (or steps) as phases.when a phase is invoked using mvn command, all preceding phases up to and including the invoked phase are executed sequentially. For example,mvn compile ֠will run phases process-resources and then compile.mvn test ֠will run phases process-resources, compile, process-test-resources, test-compile and finally test.mvn install ֠will run phases process-resources, compile, process-test-resources, test-compile, test and finally install.Like in clean lifecycle, in default lifecycle too, lifecycle phases by themselves donӴ have capabilities to accomplish some task. For example, compile phase by itself canӴ do anything but, it delegates compilation job to a plugin named maven-compiler-plugin. Some Phases Are Not Usually Called From the Command LineThe phases named with hyphenated-words (pre-, post-, or process-*) are not usually directly called from the command line. These phases sequence the build, producing intermediate results that are not useful outside the build. In the case of invoking integration-test, the environment may be left in a hanging state. Maven default repositoryhttp://repo.maven.apache.org/maven2/ Maven SettingsThe settings element in the settings.xml file contains elements used to define values which configure Maven execution in various ways, like the pom.xml, but should not be bundled to any specific project, or distributed to an audience. These include values such as the local repository location, alternate remote repository servers, and authentication information. There are two locations where a settings.xml file may live: The Maven install: ${maven.home}/conf/settings.xml A user’s install: ${user.home}/.m2/settings.xmlThe former settings.xml are also called global settings, the latter settings.xml are referred to as user settings. If both files exists, their contents gets merged, with the user-specific settings.xml being dominant. Tip: If you need to create user-specific settings from scratch, it’s easiest to copy the global settings from your Maven installation to your ${user.home}/.m2 directory. Maven’s default settings.xml is a template with comments and examples so you can quickly tweak it to match your needs. The contents of the settings.xml can be interpolated using the following expressions:${user.home} and all other system properties (since Maven 3.0)${env.HOME} etc. for environment variablesNote that properties defined in profiles within the settings.xml cannot be used for interpolation. All Maven pom.xml inherits from super POMCoc: Convention over Configuration Convention over configuration is a simple concept. Systems, libraries, and frameworks should assume reasonable defaults without requiring that unnecessary configuration systems should “just work.” Popular frameworks such as Ruby on Rails and EJB3 have started to adhere to these principles in reaction to the configuration complexity of frameworks such as the initial Enterprise JavaBeans? (EJB) specifications. “Popularized by the Ruby on Rails community, CoC emphasizes sensible defaults, thereby reducing the number of decisions to be made.” “Gradle’s flexibility, like that of Ant, can be abused, which results in difficult and complex builds.” Notes This is the Project Object Model (POM), a declarative description of a project. Coordinate “group, artifact, and version (GAV) coordinates” We’ve highlighted the Maven coordinates for this project: groupId, artifactId, version and packaging. These combined identifiers make up a project’s coordinates.[3] Just as in any other coordinate system, a Maven coordinate is an address for a specific point in “space”: from general to specific. Maven pinpoints a project via its coordinates when one project relates to another, either as a dependency, a plugin, or a parent project reference. Maven coordinates are often written using a colon as a delimiter in the following format: groupId:artifactId:packaging:version. Projects undergoing active development can use a special identifier that marks a version as a SNAPSHOT. The packaging format of a project is also an important component in the Maven coordinates, but it isn’t a part of a project’s unique identifiers. A project’s groupId:artifactId:version make that project unique; you can’t have a project with the same three groupId, artifactId, and version identifiers. packaging: The type of project, defaulting to jar, describing the packaged output produced by a project. A project with packaging jar produces a JAR archive; a project with packaging war produces a web application. The core of Maven is pretty dumb; it doesn’t know how to do much beyond parsing a few XML documents and keeping track of a lifecycle and a few plugins. Maven has been designed to delegate most responsibility to a set of Maven plugins that can affect the Maven lifecycle and offer access to goals. Most of the action in Maven happens in plugin goals that take care of things like compiling source, packaging bytecode, publishing sites, and any other task that needs to happen in a build. You benefit from the fact that plugins are downloaded from a remote repository and maintained centrally. This is what is meant by universal reuse through Maven plugins. working developers are starting to realize that Maven not only simplifies the task of build management, it is helping to encourage a common interface between developers and software projects. Maven vs AntThe differences between Ant and Maven in this example are:Apache AntAnt doesn’t have formal conventions such as a common project directory structure; you have to tell Ant exactly where to find the source and where to put the output. Informal conventions have emerged over time, but they haven’t been codified into the product. Ant is procedural; you have to tell Ant exactly what to do and when to do it. You have to tell it to compile, then copy, then compress. Ant doesn’t have a lifecycle; you have to define goals and goal dependencies. You have to attach a sequence of tasks to each goal manually.Apache Maven Maven has conventions: in the example, it already knew where your source code was because you followed the convention. It put the bytecode in target/classes, and it produced a JAR file in target. Maven is declarative; all you had to do was create a pom.xml file and put your source in the default directory. Maven took care of the rest. Maven has a lifecycle, which you invoked when you executed mvn install. This command told Maven to execute a series of sequence steps until it reached the lifecycle. As a side effect of this journey through the lifecycle, Maven executed a number of default plugin goals that did things such as compile and create a JAR. A Maven plugin is a collection of one or more goals (see Figure 3-1). Examples of Maven plugins can be simple core plugins such as the Jar plugin that contains goals for creating JAR files, the Compiler plugin that contains goals for compiling source code and unit tests, or the Surefire plugin that contains goals for executing unit tests and generating reports. Maven life cycle The second command we ran in the previous section was mvn install. This command didn’t specify a plugin goal; instead, it specified a Maven lifecycle phase. A phase is a step in what Maven calls the “build lifecycle.” The build lifecycle is an ordered sequence of phases involved in building a project. Plugin goals can be attached to a lifecycle phase. As Maven moves through the phases in a lifecycle, it will execute the goals attached to each particular phase. Each phase may have zero or more goals bound to it. In the previous section, when you ran mvn install, you might have noticed that more than one goal was executed. Examine the output after running mvn install and take note of the various goals that are executed. Maven steps through the phases preceding package in the Maven lifecycle; executing a phase will first execute all proceeding phases in order, ending with the phase specified on the command line. Each phase corresponds to zero or more goals, and since we haven’t performed any plugin configuration or customization, this example binds a set of standard plugin goals to the default lifecycle. Dependency “Maven provides declarative dependency management. With this approach, you declare your project’s dependencies in an external file called pom.xml. Maven will automatically download those dependencies and hand them over to your project for the purpose of building, testing, or packaging.” “The default remote repository with which Maven interacts is called Maven Central, and it is located at repo.maven.apache.org and uk.maven.org.” “The internal repository manager acts as a proxy to remote repositories. Because you have full control over the internal repository, you can regulate the types of artifacts allowed in your company. Additionally, you can also push your organization’s artifacts onto the server, thereby enabling collaboration. ”, such as Nexus Transitive Dependcies “A key benefit of Maven is that it automatically “deals with transitive dependencies and includes them in your project. “Maven uses a technique known as dependency mediation to resolve version conflicts. Simply stated, dependency mediation allows Maven to pull the dependency that is closest to the project in the tree. In Figure 3-3, there are two versions of dependency B: 0.0.8 and 1.0.0. In this scenario, version 0.0.8 of dependency B is included in the project, because it is a direct dependency and closest to the tree. Now look at the three versions of dependency F: 0.1.3, 1.0.0, and 2.2.0. All three dependencies are at the same depth. In this scenario, Maven will use the first-found dependency, which would be 0.1.3, and not the latest 2.2.0 version.” Dependency Scope “Maven uses the concept of scope, which allows you to specify when and where you need a particular dependency.” “Maven provides the following six scopes:” “compile: Dependencies with the compile scope are available in the class path in all phases on a project build, test, and run. This is the default scope.” “provided: Dependencies with the provided scope are available in the class path during the build and test phases. They don’t get bundled within the generated artifact. Examples of dependencies that use this scope include Servlet api, JSP api, and so on.” “runtime: Dependencies with the runtime scope are not available in the class path during the build phase. Instead they get bundled in the generated artifact and are available during runtime.” “test: Dependencies with the test scope are available during the test phase. JUnit and TestNG are good examples of dependencies with the test scope.” “system: Dependencies with the system scope are similar to dependencies with the provided scope, except that these dependencies are not retrieved from the repository. Instead, a hard-coded path to the file system is specified from which the dependencies are used.” “import: The import scope is applicable for .pom file dependencies only. It allows you to include dependency management information from a remote .pom file. The import scope is available only in Maven 2.0.9 or later.” Common errorsNot authorized , ReasonPhrase:UnauthorizedThis is likely you are behind corporate proxy, so you have to config to make Maven can connect to nexus successfully: Go to make sure your settings.xml is correct, normally, it’s under your home directory, e.g. tzhang/.m2/settings.xml to make sure the username and password same as the settings in nexus 123456789101112 &lt;server&gt; &lt;id&gt;my-nexus&lt;/id&gt;&lt;username&gt;1jhAX6LZ&lt;/username&gt; &lt;password&gt;V7MxFm3k9fiodhqNQDT/kSh8V81JT8bUWRHEel339rwq&lt;/password&gt; &lt;/server&gt; &lt;mirrors&gt; &lt;mirror&gt; &lt;id&gt;my-nexus&lt;/id&gt; &lt;url&gt;https://nexus.internal.abc.com/content/groups/public&lt;/url&gt; &lt;mirrorOf&gt;*,!sonar,!eclipse-misc,!m2-proxy,!eclipse-releases,!eclipse-snapshots,!mfs-m2-repository,!fsg_internal_repository,!fsg_snapshot_repository&lt;/mirrorOf&gt; &lt;/mirror&gt; &lt;/mirrors&gt; double check your login token in nexus, log into nexus, click your login name in top right dropdown, chose “profile” and then trigger button “Access User Token” Newly added dependency in pom not working, class not foundIf you are in intellij, please go to the exact module (the child module, rather than parent pom), right click the pom.xml, chose maven -&gt; Reinstall Setup Proxy “Maven requires an Internet connection to download plug-ins and dependencies. Some companies employ HTTP proxies to restrict access to the Internet. In those scenarios, running Maven will result in Unable to download artifact errors. To address this, edit the settings.xml file and add the proxy information specific to your company.” 1234567891011121314151617&lt;settings xmlns="http://maven.apache.org/SETTINGS/1.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/SETTINGS/1.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd"&gt; &lt;proxies&gt; &lt;proxy&gt; &lt;id&gt;companyProxy&lt;/id&gt; &lt;active&gt;true&lt;/active&gt; &lt;protocol&gt;http&lt;/protocol&gt; &lt;host&gt;proxy.company.com&lt;/host&gt; &lt;port&gt;8080&lt;/port&gt; &lt;username&gt;proxyusername&lt;/username&gt; &lt;password&gt;proxypassword&lt;/password&gt; &lt;nonProxyHosts /&gt; &lt;/proxy&gt; &lt;/proxies&gt; &lt;/settings&gt; Multimodule projectsGenerate parent1mvn archetype:generate -DgroupId=com.apress.gswmbook -DartifactId=gswm-parent -Dversion=1.0.0-SNAPSHOT -DarchetypeGroupId=org.codehaus.mojo.archetypes -DarchetypeArtifactId=pom-root Listing 6-5. Parent pom.xml File with Modules 123456789101112131415&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.apress.gswmbook&lt;/groupId&gt; &lt;artifactId&gt;gswm-parent&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;pom&lt;/packaging&gt; &lt;name&gt;gswm-parent&lt;/name&gt; &lt;modules&gt; &lt;module&gt;gswm-web&lt;/module&gt; &lt;module&gt;gswm-service&lt;/module&gt; &lt;module&gt;gswm-repository&lt;/module&gt; &lt;/modules&gt;&lt;/project&gt; Generate web module1mvn archetype:generate -DgroupId=com.todzhang.mywebApp -DartifactId=main-web -Dversion=1.0.0-SNAPSHOT -Dpackage=war -DarchetypeArtifactId=maven-archetype-webapp 123456789101112131415161718192021222324252627&lt;?xml version="1.0"?&gt;&lt;project xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd" xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;com.apress.gswmbook&lt;/groupId&gt; &lt;artifactId&gt;gswm-parent&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;/parent&gt; &lt;groupId&gt;com.apress.gswmbook&lt;/groupId&gt; &lt;artifactId&gt;gswm-web&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;war&lt;/packaging&gt; &lt;name&gt;gswm-web Maven Webapp&lt;/name&gt; &lt;url&gt;http://maven.apache.org&lt;/url&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;3.8.1&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;finalName&gt;gswm-web&lt;/finalName&gt; &lt;/build&gt;&lt;/project&gt; Generate a service jar module1mvn archetype:generate -DgroupId=com.todzhang.mywebApp -DartifactId=back-service -Dversion=1.0.0-SNAPSHOT -DarchetypeArtifactId=maven-archetype-quickstart -DinteractiveMode=false Notice that you didn’t provide the package parameter, as the maven-archetype-quickstart produces a JAR project by default.To start the module1mvn packages Create archetype from project1mvn archetype:create-from-project Site life cycle1mvn site The site life cycle uses Maven’s site plug-in to generate the site for a single project. Once this command completes, a site folder gets created under the project’s target. Open the index.html file to browse the generated site. You will notice that Maven used the information provided in the pom.xml file to generate most of the documentation. It also automatically applied the default skin and generated the corresponding images and CSS files. To regenerate site1mvn clean site JavaDoc Maven provides a Javadoc plug-in, which uses the Javadoc tool for generating Javadocs. Integrating the Javadoc plug-in simply involves declaring it in the reporting element of pom.xml file, as shown in Listing 7-4. Plug-ins declared in the pom reporting element are executed during site generation.123456789101112&lt;project&gt; &lt;!—Content removed for brevity--&gt; &lt;reporting&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-javadoc-plugin&lt;/artifactId&gt; &lt;version&gt;2.10.1&lt;/version&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/reporting&gt;&lt;/project&gt; Then run mvn clean site to generate javadoc, the apidocs folder created under gswm /target/site Unit test report Maven offers the Surefire plug-in that provides a uniform interface for running tests created by frameworks such as JUnit or TestNG. It also generates execution results in various formats such as XML and HTML. These published results enable developers to find and fix broken tests quickly.The Surefire plug-in is configured in the same way as the Javadoc plug-in in the reporting section of the pom file. Listing 7-5 shows the Surefire plug-in configuration. 123456789101112&lt;project&gt; &lt;!—Content removed for brevity--&gt; &lt;reporting&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-surefire-report-plugin&lt;/artifactId&gt; &lt;version&gt;2.17&lt;/version&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/reporting&gt;&lt;/project&gt; you will see a Surefire Reports folder generated under gswm\target. It contains the test execution results in XML and TXT formats. The same information will be available in HTML format in the surefire-report.html file under site folder. Code coverate report Code coverage is a measurement of how much source code is being exercised by automated tests. Essentially, it provides an indication of the quality of your tests. Emma and Cobertura are two popular open source code coverage tools for Java.In this section, you will use Cobertura for measuring this project’s code coverage. Configuring Cobertura is similar to other plug-ins, as shown in123456789101112&lt;project&gt; &lt;!—Content removed for brevity--&gt; &lt;reporting&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;cobertura-maven-plugin&lt;/artifactId&gt; &lt;version&gt;2.6&lt;/version&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/reporting&gt;&lt;/project&gt; Find bug reports FindBugs is a tool for detecting defects in Java code. It uses static analysis to detect bug patterns, such as infinite recursive loops and null pointer dereferences. Listing 7-7 shows the FindBugs configuration. 123456789101112&lt;project&gt; &lt;!—Content removed for brevity--&gt; &lt;reporting&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;findbugs-maven-plugin&lt;/artifactId&gt; &lt;version&gt;3.0.0&lt;/version&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/reporting&gt;&lt;/project&gt; Integration with Nexus Repository managers act as a proxy of public repositories, facilitate artifact sharing and team collaboration, ensure build stability, and enable the governance of artifacts used in the enterprise. Nexus is a popular open source repository manager from Sonatype. It is a web application that allows you to maintain internal repositories and access external repositories. It allows repositories to be grouped and accessed via a single URL. This enables the repository administrator to add and remove new repositories behind the scenes without requiring developers to change the configuration on their computers. Additionally, it provides hosting capabilities for sites generated using Maven site and artifact search capabilities. To enable nexus in Maven You will start by adding a distributionManagement element in the pom.xml file, as shown in Listing 8-1. This element is used to declare the location where the project’s artifacts will be when deployed. The repository element indicates the location where released artifacts will be deployed. Similarly, the snapshotRepository element identifies the location where the SNAPSHOT versions of the project will be stored. 1234567891011121314151617&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi=http://www.w3.org/2001/XMLSchema-instance” xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;!-- Content removed for brevity --&gt; &lt;distributionManagement&gt; &lt;repository&gt; &lt;id&gt;nexusReleases&lt;/id&gt; &lt;name&gt;Releases&lt;/name&gt; &lt;url&gt;http://localhost:8081/nexus/content/repositories/releases&lt;/url&gt; &lt;/repository&gt; &lt;snapshotRepository&gt; &lt;id&gt;nexusSnapshots&lt;/id&gt; &lt;name&gt;Snapshots&lt;/name&gt; &lt;url&gt;http://localhost:8081/nexus/content/repositories/snapshots&lt;/url&gt; &lt;/snapshotRepository&gt; &lt;/distributionManagement&gt;&lt;!-- Content removed for brevity --&gt;&lt;/project&gt; Out of the box, Nexus comes with Releases and Snapshots repositories. By default, SNAPSHOT artifacts will be stored in the Snapshots Repository, and release artifacts will be stored in the Releases repository.Like most repository managers, deployment to Nexus is a protected operation. You provide the credentials needed to interact with Nexus in the settings.xml file. Listing 8-2. Settings.xml File with Server Information Listing 8-2 shows the settings.xml file with the server information. The Nexus deployment user with password deployment123 is provided out of the box. Notice that the IDs declared in the server tag — nexusReleases and nexusSnapshots must match the IDs of the repository and snapshotRepository declared in the pom.xml file. Replace the contents of the settings.xml file in the C:\Users&lt;&gt;.m2 folder with the code in Listing 8-2. 123456789101112131415&lt;?xml version="1.0" encoding="UTF-8" ?&gt;&lt;settings xmlns="http://maven.apache.org/SETTINGS/1.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/SETTINGS/1.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd"&gt;&lt;servers&gt; &lt;server&gt; &lt;id&gt;nexusReleases&lt;/id&gt; &lt;username&gt;deployment&lt;/username&gt; &lt;password&gt;deployment123&lt;/password&gt; &lt;/server&gt; &lt;server&gt; &lt;id&gt;nexusSnapshots&lt;/id&gt; &lt;username&gt;deployment&lt;/username&gt; &lt;password&gt;deployment123&lt;/password&gt; &lt;/server&gt;&lt;/servers&gt;&lt;/settings&gt; This concludes the configuration steps for interacting with Nexus. At the command line, run the command mvn deploy under the directory C:\apress\gswm-book\chapter8\gswm. Upon successful execution of the command, you will see the SNAPSHOT artifact under Nexus POMEffective POMAt the start of every build, Maven internally merges project pomx.ml with Super POM and constructs a new POM which is known as Effective POM . Earlier in Maven Lifecycle and Plugin Goals, we learned that Maven binds plugins goals to lifecycle phases. Actually, this magic happens in effective POM and it is highly instructive to go through the effective POM to know the what goes on under the hood. To dump effective POMuse Maven Help Plugin to dump the effective POM to a file for investigation. 1$ mvn help:effective-pom -Doutput=target/effective-pom.xml PackagingThe first, and most common way, is to set the packaging for your project via the equally named POM element . Some of the valid packaging values are jar, war, ear and pom. If no packaging value has been specified, it will default to jar. Each packaging contains a list of goals to bind to a particular phase. For example, the jar packaging will bind the following goals to build phases of the default lifecycle. This is an almost standard set of bindings; however, some packagings handle them differently. For example, a project that is purely metadata (packaging value is pom) only binds goals to the install and deploy phases (for a complete list of goal-to-build-phase bindings of some of the packaging types, refer to the Lifecycle Reference).build tools optionsMaven and Ant are just different approaches: imperative and declarative (see Imperative vs Declarative build systems) Maven is better for managing dependencies (but Ant is ok with them too, if you use Ant+Ivy) and build artefacts. The main benefit from maven - its lifecycle. You can just add specific actions on correct phase, which seems pretty logical: just launch you integration tests on integration-test phase for example. Also, there are many existing plugins, which can could almost everything. Maven archetype is powerful feature, which allows you to quickly create project. Ant is better for controlling of build process. Before your very first build you have to write you build.xml. If your build process is very specific, you have to create complicated scripts. For long-term projects support of ant-scripts could become really painful: scripts become too complicated, people, who has written them, could leave project, etc. Both of them use xml, which could become too big in big long-term projects. Anyway, you shoud read specific documentation (not hate-articles) on both. Also, there is ant-maven-plugin, which allow to launch ant-scripts with maven. P.S. You can take a look on Gradle, which for me could provide more freedom than Maven, but is easier to use than Ant.]]></content>
      <tags>
        <tag>java</tag>
        <tag>maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Eclipse notes]]></title>
    <url>%2F2017-02-06-Eclipse-tips%2F</url>
    <content type="text"><![CDATA[How do I remove a plug-in?Run Help &gt; About Eclipse &gt; Installation Details, select the software you no longer want and click Uninstall. (On Macintosh it is Eclipse &gt; About Eclipse &gt; Installation Details.) Where is the Eclipse Plugin update error log?The log is located at current workspace: {workspace_path}/.metadata/.log - also you can view this log in view “Error Log”: Window &gt; Show View &gt; Other &gt; Find here “Error Log”]]></content>
      <tags>
        <tag>Java</tag>
        <tag>Eclipse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[R Language]]></title>
    <url>%2F2017-02-09-R-Lang%2F</url>
    <content type="text"><![CDATA[123456s&lt;-read.csv("C:/Users/xxx/dev/R/IRS/SHH_SCHISHG.csv")# aggregate s2&lt;-table(s$Original.CP)s3&lt;-as.data.frame(s2)# extract by Frequency ordered s3[order(-s3$Freq),]]]></content>
      <tags>
        <tag>R</tag>
        <tag>Data Science</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SeriableVersionUID]]></title>
    <url>%2F2017-02-14-SerialVersionUID%2F</url>
    <content type="text"><![CDATA[Noteworthy points about SeriableVersionUID in JavaPreventing ClassCastExceptions with SerialVersionUID Problem Your classes were recompiled, and you’re getting ClassCastExceptions that you shouldn’t.Solution Run serialver to generate a “serial version UUID” and paste its output into your classes before you start. Or use your IDE’s tools for this purpose.Discussion When a class is undergoing a period of evolution—particularly a class being used in a networking context such as RMI or servlets—it may be useful to provide a serialVersionUID value in this class. This is a long that is basically a hash of the methods and fields in the class. Both the object serialization API (see Saving and Restoring Java Objects) and the JVM, when asked to cast one object to another (common when using collections, as in Chapter 7), either look up or, if not found, compute this value. If the value on the source and destination do not match, a ClassCastException is thrown. Most of the time, this is the correct thing for Java to do.However, sometimes you may want to allow a class to evolve in a compatible way, but you can’t immediately replace all instances in circulation. You must be willing to write code to account for the additional fields being discarded if restoring from the longer format to the shorter and having the default value (null for objects, 0 for numbers, and false for Boolean) if you’re restoring from the shorter format to the longer. If you are only adding fields and methods in a reasonably compatible way, you can control the compatibility by providing a long int named serialVersionUID. The initial value should be obtained from a JDK tool called serialver, which takes just the class name. Consider a simple class called SerializableUser:123456789public class SerializableUser implements java.io.Serializable &#123; public String name; public String address; public String country; public String phoneNum; // other fields, and methods, here... static final long serialVersionUID = -7978489268769667877L;&#125; I first compiled it with two different compilers to ensure that the value is a product of the class structure, not of some minor differences in class file format that different compilers might emit: 123456$ javac SerializableUser.java$ serialver SerializableUserSerializableUser: static final long serialVersionUID = -7978489268769667877L;$ jikes +E SerializableUser.java$ serialver SerializableUserSerializableUser: static final long serialVersionUID = -7978489268769667877L; Sure enough, the class file from both compilers has the same hash. Now let’s change the file. I go in with an editor and add a new field, phoneNum, right after country: 12 public String country;public String phoneNum; // Added this line. 123$ javac SerializableUser.java$ serialver SerializableUserSerializableUser: static final long serialVersionUID = -8339341455288589756L; Notice how the addition of the field changed the serialVersionUID! Now, if I had wanted this class to evolve in a compatible fashion, here’s what I should have done before I started expanding it. I copy and paste the original serialver output into the source file (again using an editor to insert a line before the last line): 12 // The following is the line I added to SerializableUser.javaprivate static final long serialVersionUID = -7978489268769667877L; 1234$ javac SerializableUser.java$ serialver SerializableUserSerializableUser: static final long serialVersionUID = -7978489268769667877L;$ Now all is well: I can interchange serialized versions of this file.Note that serialver is part of the “object serialization” mechanism, and, therefore, it is meaningful only on classes that implement the Serializable interface described in Saving and Restoring Java Objects.Note also that some developers use serialVersionUID values that start at 1 (a choice offered by some IDEs when they note that a class that appears to be serializable lacks a serialVersionUID), and then simply increment it by one each time the class changes in an incompatible way.]]></content>
      <tags>
        <tag>java</tag>
        <tag>serialVersionUID</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SSH and Cryptography]]></title>
    <url>%2F2017-02-06-SSH-Cryptography%2F</url>
    <content type="text"><![CDATA[SFTP versus FTPS SS: Secure Shell An increasing number of our customers are looking to move away from standard FTP for transferring data, so we are often asked which secure FTP protocol we recommend. In the next few paragraphs, we will explain what options are available and their main differences. The two mainstream protocols available for Secure FTP transfers are named SFTP (FTP over SSH) and FTPS (FTP over SSL). Both SFTP and FTPS offer a high level of protection since they implement strong algorithms such as AES and Triple DES to encrypt any data transferred. Both options also support a wide variety of functionality with a broad command set for transferring and working with files. So the most notable differences between SFTP and FTPS is how connections are authenticated and managed. Authentication: SFTP vs. FTPS With SFTP (FTP over SSH), a connection can be authenticated using a couple different techniques. For basic authentication, you (or your trading partner) may just require a user id and password to connect to the SFTP server. Its important to note that any user ids and passwords supplied over the SFTP connection will be encrypted, which is a big advantage over standard FTP. SSH keys can also be used to authenticate SFTP connections in addition to, or instead of, passwords. With key-based authentication, you will first need to generate a SSH private key and public key beforehand. If you need to connect to a trading partner’s SFTP server, you would send your SSH public key to them, which they will load onto their server and associate with your account. When you connect to their SFTP server, your client software will transmit your public key to the server for authentication. If the keys match, along with any user/password supplied, then the authentication will succeed. With FTPS (FTP over SSL), a connection is authenticated using a user id, password and certificate(s). Like SFTP, the users and passwords for FTPS connections will also be encrypted. When connecting to a trading partner’s FTPS server, your FTPS client will first check if the server’s certificate is trusted. The certificate is considered trusted if either the certificate was signed off by a known certificate authority (CA), like Verisign, or if the certificate was self-signed (by your partner) and you have a copy of their public certificate in your trusted key store. Your partner may also require that you supply a certificate when you connect to them. Your certificate may be signed off by a 3rd party CA or your partner may allow you to just self-sign your certificate, as long as you send them the public portion of your certificate beforehand (which they will load in their trusted key store). Implementation: SFTP vs. FTPS In regards to how easy each of the secure FTP protocols are to implement, SFTP is the clear winner since it is very firewall friendly. SFTP only needs a single port number (default of 22) to be opened through the firewall. This port will be used for all SFTP communications, including the initial authentication, any commands issued, as well as any data transferred. On the other hand, FTPS can be very difficult to patch through a tightly secured firewall since FTPS uses multiple port numbers. The initial port number (default of 21) is used for authentication and passing any commands. However, every time a file transfer request (get, put) or directory listing request is made, another port number needs to be opened. You and your trading partners will therefore have to open a range of ports in your firewalls to allow for FTPS connections, which can be a security risk for your network. In summary, SFTP and FTPS are both very secure with strong authentication options. However since SFTP is much easier to port through firewalls, and we are seeing an increasing percentage of trading partners adopting SFTP, we believe SFTP is the clear winner for your secure FTP needs. SSH There are several ways to use SSH; one is to use automatically generated public-private key pairs to simply encrypt a network connection, and then use password authentication to log on. Another is to use a manually generated public-private key pair to perform the authentication, allowing users or programs to log in without having to specify a password. In this scenario, anyone can produce a matching pair of different keys (public and private). The public key is placed on all computers that must allow access to the owner of the matching private key (the owner keeps the private key secret). While authentication is based on the private key, the key itself is never transferred through the network during authentication. SSH only verifies whether the same person offering the public key also owns the matching private key. In all versions of SSH it is important to verify unknown public keys, i.e. associate the public keys with identities, before accepting them as valid. Accepting an attacker’s public key without validation will authorize an unauthorized attacker as a valid user. SSH is important in cloud computing to solve connectivity problems, avoiding the security issues of exposing a cloud-based virtual machine directly on the Internet. An SSH tunnel can provide a secure path over the Internet, through a firewall to a virtual machine. The standard TCP port 22 has been assigned for contacting SSH servers. Key management On Unix-like systems, the list of authorized public keys is typically stored in the home directory of the user that is allowed to log in remotely, in the file ~/.ssh/authorized_keys. WinFTP cryptographic protocol is SSH-2 SSH implementation is OpenSSH_5.3 Server fingerprint: File transfer protocol = SFTP-3Cryptographic protocol = SSH-2SSH implementation = OpenSSH_5.3Encryption algorithm = aesCompression = No Server host key fingerprint ssh-rsa 2048 86:54:d9:09:25:c0:9b:f8:17:8c:c0:52:13:0c:9c:ccssh-keygen ssh-keygen is a standard component of the Secure Shell (SSH) protocol suite found on Unix and Unix-like computer systems used to establish secure shell sessions between remote computers over insecure networks, through the use of various cryptographic techniques. The ssh-keygen utility is used to generate, manage, and convert authentication keys. ssh-keygen is able to generate a key using one of three different digital signature algorithms. With the help of the ssh-keygen tool, a user can create passphrase keys for any of these key types (to provide for unattended operation, the passphrase can be left empty, at increased risk). algorithm SSH protocol version 1 (now deprecated) only the RSA algorithm was supported. The SSH protocol version 2 additionally introduced support for the DSA algorithm. Subsequently, OpenSSH added support for a third digital signature algorithm, ECDSA DSA The Digital Signature Algorithm (DSA) is a Federal Information Processing Standard for digital signatures. It was proposed by the National Institute of Standards and Technology (NIST) in August 1991 for use in their Digital Signature Standard (DSS) and adopted as FIPS 186 in 1993 HashIn cryptography applications, we often need a so-called secure hash function. Secure hash functions are generaelly a subset of hash functions that fulfil at least two extra criteria: it must be computationally impossible to reverse the mapping, that is, go from a hash code to a message or piece of data that would have generated that hash code; it must be infeasible for a collision to occur: that is, for two messages to be found that have the same hash code. In order to fulfil these criteria (or at least, as a by-product of needing to fulfil these criteria), secure hash functions generally have these characteristics: they are slower to compute than the hash codes typically used to key hash maps;they are wider (i.e. have more bits) than weak hash codes. Secure hash codes are typically 128 bits wide at the very least; compare that, for example, to the 32-bit codes returned by Java hashCode() method, or the 64-bit hash codes recommended for key-less hash maps in Numerical Recipes. Applications of secure hash functionsSecure hash functions actually have various applications. A very common case is verifying the integrity of data. When we send some data, we append a hash of that data; on the receiving end, we re-hash the received data and check that the computed hash equals that sent; if any of the data has changed then (with overwhelming probability), the computed hash value will no longer match the original. Another case is where we need to authenticate some data, i.e. produce a kind of integrity check that only a party with a given private key could produce. (In this case, the general solution is to combine a hash code with encryption.) In other cases, a secure hash function is useful to represent a particular item of data. For example, for the purpose of checking passwords, we need only store a hash of that password. When somebody enters their password, if the computed hash of what they entered matches the hash stored in the password file/database, we assume they knew the password. This scheme, sometimes called compare by hash (CBH) can be used to search for duplicates of data on a hard drive or for synching data between multiple machines. Similarly, another example are the databases that various law enforcement agencies keep of known “disapproved” files that they want to search peoples hard drives for. In these applications, keeping a database of the actual file contents, and/or transmitting and comparing those entire contents, would be impractical. Instead, only hashes are stored and compared. More broadly, secure hash functions are useful in a variety of cases where we need a trapdoor function (i.e. one that cannot feasibly be reversed), especially where we need one with a limited or fixed-size result. As mentioned, secure hashes are sometimes called message digests. And in fact, the main class for computing them in Java is java.security.MessageDigest. We get an instance of MessageDigest, feed it an array (or several arrays) of bytes, then call its digest() method to get the resulting hash: 12345byte[] data = ....MessageDigest md = MessageDigest.getInstance("SHA-1");md.update(data);byte[] hash = md.digest(); MD5MD5 is a later hash function developed by Ron Rivest. It is one of the most common hash algorithms in use today. Like MD2, it is a 128-bit hash function but, unlike its predecessor, it is one of the fastest “secure” hash functions in common use, and the fastest provided in Java 6. Unfortunately, it is now considered insecure. Aside from the relatively small hash size, there are well-published methods to find collisions analytically in a trivial amount of time. For example, Vlastimil Klima has published a C program to find MD5 collisions in around 30 seconds on an average PC. If you need security, dont use MD5! Although insecure, MD5 still makes a good general strong hash function due to its speed. In non-security applications such as finding duplicate files on a hard disk (where you are not trying to protect against the threat model of somebody deliberately fooling your system), MD5 makes a good choice. SHA algorithmsSHA (Secure Hash Algorithm) refers collectively to various hash functions developed by the US National Security Agency (NSA). The various algorithms are based on differing hash sizes and (in principle) offer corresponding levels of security: PBE password-based encryptionThe technique of generating a secret key from a user-generated passphrase is usually called password-based encryption (PBE). As you might imagine, it is fraught with difficulty. In particular: the user is requirement and the security requirement usually conflict: the user requires an easy-to-remember passphrase, or at least one that is made of recognisable characters and short enough to write down; yet for secure encryption by today is standards, we require at least 128 strongly random bits (and ideally more); password-based encryption is typically used in applications where an attacker can repeatedly try to guess the password undetected and beyond the control of the genuine sender/recipient (if the password is being used to log into our server, we can detect that so many invalid attempts were made and in the worst case shut down our server to prevent further attempts; but if an eavesdropper takes a copy of the encrypted ZIP file we e-mailed, we will never know that they are sitting there with a 100,000-processor botnet trying to brute-force the password, and they can essentially sit doing it for as long as they like).The typical result is fairly dire: most password-protected data is encrypted with weak encryption keys, and an attacker can spend all the processor time they like trying to guess that weak key with complete impunity. How to use PBEThere are two fundamental problems:(a) user-memorable passwords typically dont contain as much randomness as we need for a secure key;(b) in a typical application, an attacker gets as many tries as they like at the password. An additional problem is that if, say, the password abc123 always generated the same key in our application, then an attacker could calculate the key from this password once and then quickly decrypt any data protected with this password. Two common techniques are used in password-based encryption to try to alleviate these problems: a deliberately slow method is used to derive the encryption key from the password, reducing the number of guesses that an attacker can make in a given time frame; some random bytes, called a salt, are appended to the password before it is used to calculate the key. Cryptographic hash function A cryptographic hash function is a special class of hash function that has certain properties which make it suitable for use in cryptography. It is a mathematical algorithm that maps data of arbitrary size to a bit string of a fixed size (a hash function) which is designed to also be a one-way function, that is, a function which is infeasible to invert. The only way to recreate the input data from an ideal cryptographic hash function is output is to attempt a brute-force search of possible inputs to see if they produce a match. Bruce Schneier has called one-way hash functions the workhorses of modern cryptography.[1] The input data is often called the message, and the output (the hash value or hash) is often called the message digest or simply the digest. in information-security contexts, cryptographic hash values are sometimes called (digital) fingerprints, checksums, or just hash values, even though all these terms stand for more general functions with rather different properties and purposes. Another finalist from the NIST hash function competition, BLAKE, was optimized to produce BLAKE2 which is notable for being faster than SHA-3, SHA-2, SHA-1, or MD5, and is used in numerous applications and libraries. The ideal cryptographic hash function has five main properties: it is deterministic so the same message always results in the same hash it is quick to compute the hash value for any given message it is infeasible to generate a message from its hash value except by trying all possible messages a small change to a message should change the hash value so extensively that the new hash value appears uncorrelated with the old hash value it is infeasible to find two different messages with the same hash value Illustration An illustration of the potential use of a cryptographic hash is as follows: Alice poses a tough math problem to Bob and claims she has solved it. Bob would like to try it himself, but would yet like to be sure that Alice is not bluffing. Therefore, Alice writes down her solution, computes its hash and tells Bob the hash value (whilst keeping the solution secret). Then, when Bob comes up with the solution himself a few days later, Alice can prove that she had the solution earlier by revealing it and having Bob hash it and check that it matches the hash value given to him before. (This is an example of a simple commitment scheme; in actual practice, Alice and Bob will often be computer programs, and the secret would be something less easily spoofed than a claimed puzzle solution). ApplicationsVerifying the integrity of files or messages An important application of secure hashes is verification of message integrity. Determining whether any changes have been made to a message (or a file), for example, can be accomplished by comparing message digests calculated before, and after, transmission (or any other event). For this reason, most digital signature algorithms only confirm the authenticity of a hashed digest of the message to be “signed”. Verifying the authenticity of a hashed digest of the message is considered proof that the message itself is authentic. MD5, SHA1, or SHA2 hashes are sometimes posted along with files on websites or forums to allow verification of integrity.[6] This practice establishes a chain of trust so long as the hashes are posted on a site authenticated by HTTPS. Password verification Storing all user passwords as cleartext can result in a massive security breach if the password file is compromised. One way to reduce this danger is to only store the hash digest of each password. To authenticate a user, the password presented by the user is hashed and compared with the stored hash. The password is often concatenated with a random, non-secret salt value before the hash function is applied. The salt is stored with the password hash. Because users have different salts, it is not feasible to store tables of precomputed hash values for common passwords. MD5 The MD5 algorithm is a widely used hash function producing a 128-bit hash value. Although MD5 was initially designed to be used as a cryptographic hash function, it has been found to suffer from extensive vulnerabilities. It can still be used as a checksum to verify data integrity, but only against unintentional corruption. Like most hash functions, MD5 is neither encryption nor encoding. It can be reversed by brute-force attack and suffers from extensive vulnerabilities as detailed in the security section below. MD5 was designed by Ronald Rivest in 1991 to replace an earlier hash function MD4. The MD5 hash function receives its acronym MD from its structure using Merkle–Damg?rd construction. Collision resistance Collision resistance is a property of cryptographic hash functions: a hash function H is collision resistant if it is hard to find two inputs that hash to the same output; that is, two inputs a and b such that H(a) = H(b), and a ≠ b Collision resistance does not mean that no collisions exist; simply that they are hard to find. Cryptographic hash functions are usually designed to be collision resistant. But many hash functions that were once thought to be collision resistant were later broken. MD5 and SHA-1 in particular both have published techniques more efficient than brute force for finding collisions. RationaleCollision resistance is desirable for several reasons. In some digital signature systems, a party attests to a document by publishing a public key signature on a hash of the document. If it is possible to produce two documents with the same hash, an attacker could get a party to attest to one, and then claim that the party had attested to the other. In some proof-of-work systems (e.g. bitcoin mining), users provide hash collisions as proof that they have performed a certain amount of computation to find them. If there is an easier way to find collisions than brute force, users can cheat the system. In some distributed content systems, parties compare cryptographic hashes of files in order to make sure they have the same version. An attacker who could produce two files with the same hash could trick users into believing they had the same version of a file when they in fact did not. Algorithm The Merkle–Damg?rd hash function first applies an MD-compliant padding function to create an input whose size is a multiple of a fixed number (e.g. 512 or 1024) — this is because compression functions cannot handle inputs of arbitrary size. The hash function then breaks the result into blocks of fixed size, and processes them one at a time with the compression function, each time combining a block of the input with the output of the previous round.[1]:146 In order to make the construction secure, Merkle and Damg?rd proposed that messages be padded with a padding that encodes the length of the original message. This is called length padding or Merkle–Damg?rd strengthening. In the diagram, the one-way compression function is denoted by f, and transforms two fixed length inputs to an output of the same size as one of the inputs. The algorithm starts with an initial value, the initialization vector (IV). The IV is a fixed value (algorithm or implementation specific). For each message block, the compression (or compacting) function f takes the result so far, combines it with the message block, and produces an intermediate result. The last block is padded with zeros as needed and bits representing the length of the entire message are appended. (See below for a detailed length padding example.) To harden the hash further the last result is then sometimes fed through a finalisation function. The finalisation function can have several purposes such as compressing a bigger internal state (the last result) into a smaller output hash size or to guarantee a better mixing and avalanche effect on the bits in the hash sum. The finalisation function is often built by using the compression function[citation needed] (Note that in some documents instead the act of length padding is called “finalisation”). Merkle–Damg?rd construction In cryptography, the Merkle–Damg?rd construction or Merkle–Damg?rd hash function is a method of building collision-resistant cryptographic hash functions from collision-resistant one-way compression functions.[1]:145 This construction was used in the design of many popular hash algorithms such as MD5, SHA1 and SHA2. The Merkle–Damg?rd construction was described in Ralph Merkles Ph.D. thesis in 1979.[2] Ralph Merkle and Ivan Damg?rd independently proved that the structure is sound: that is, if an appropriate padding scheme is used and the compression function is collision-resistant, then the hash function will also be collision resistant. SHA In cryptography, SHA-1 (Secure Hash Algorithm 1) is a cryptographic hash function designed by the United States National Security Agency and is a U.S. Federal Information Processing Standard published by the United States NIST.[2] SHA-1 produces a 160-bit (20-byte) hash value known as a message digest. A SHA-1 hash value is typically rendered as a hexadecimal number, 40 digits long. Applications SHA-1 forms part of several widely used security applications and protocols, including TLS and SSL, PGP, SSH, S/MIME, and IPsec. Those applications can also use MD5; both MD5 and SHA-1 are descended from MD4. SHA-1 hashing is also used in distributed revision control systems like Git, Mercurial, and Monotone to identify revisions, and to detect data corruption or tampering. The algorithm has also been used on Nintendos Wii gaming console for signature verification when booting, but a significant flaw in the first implementations of the firmware allowed for an attacker to bypass the systems security scheme. SHA-1 and SHA-2 are the secure hash algorithms required by law for use in certain U.S. Government applications, including use within other cryptographic algorithms and protocols, for the protection of sensitive unclassified information. A prime motivation for the publication of the Secure Hash Algorithm was the Digital Signature Standard, in which it is incorporated. Revision control systems such as Git and Mercurial use SHA-1 not for security but for ensuring that the data has not changed due to accidental corruption. Salt (cryptography) In cryptography, a salt is random data that is used as an additional input to a one-way function that “hashes” a password or passphrase. Salts are closely related to the concept of nonce. The primary function of salts is to defend against dictionary attacks or against its hashed equivalent, a pre-computed rainbow table attack.[1] Salts are used to safeguard passwords in storage. The purpose of a hash and salt process in password security is not to prevent a password from being guessed, but to prevent a leaked password database from being used in further attacks. The idea of salt is that when the user enters the password, we dont actually use their raw password to generate the key. We first append some random bytes to the password. A new, random salt is used for every file/piece of data being encrypted. The salt bytes are not secret: they are stored unencrypted along side the encrypted data. This means that the salt bytes would add no extra security if there was only once piece of data in the world encrypted with a given password. But they prevent dictionary attacks, whereby an attacker pre-computes the keys from some common passwords and then tries those keys on the encrypted data. Without salt bytes, the dictionary attack would be worthwhile attack because we use a deliberately slow function to derive a key from a password. With the salt bytes, the attacker is forced to run the slow key derivation function for each password they want to try on each piece of data. To generate salt bytes in Java, we just need to make sure that we use a secure random number generator. Construct an instance of SecureRandom, create (say) a 20-byte array, and call nextBytes() on that array: 123Random r = new SecureRandom();byte[] salt = new byte[20];r.nextBytes(salt); Secure RandomThe SecureRandom class, housed in the java.security package, provides a drop-in replacement to java.lang.Random. But unlike the latter, java.security.SecureRandom is designed to be cryptographically secure. SecureRandom is typically used in cases where: random numbers are generated for security related purposes, such as generating an encryption key or session ID (see below);or, more generally, high-quality randomness is important and it is worth consuming CPU (or where CPU consumption is not an issue) to generate those high-quality random numbers. Properties of SecureRandomWe said that SecureRandom is designed to be cryptographically secure. In practice, this means that the generator has the following properties: given only a number produced by the generator, it is (to all intents and purposes) impossible to predict previous and future numbers; the numbers produced contain no known biases; the generator has a large period (in Suns standard implementation, based on the 160-bit SHA1 hash function, the period is 2160); the generator can seed itself at any position within that period with equal probability (or at least, it comes so close to this goal, that we have no practical way of telling otherwise). These properties are important in various security applications. The first is important, for eaxmple, if we use the generator to produce, say, a session ID on a web server: we donot want user n to predict user n+1 s session ID. Similarly, we donot want a user in an Internet cafe, based on the session ID or encryption key that they are given to access a web site, to be able to predict the value assigned to a previous user on that machine. The importance of producing all values with equal probabilityFor example, let s say that we want to pick a 128-bit AES encryption key. The idea of a strong encryption algorithm such as AES is that in order for an adversary to guess the key by “brute force” (which we assume is the only possible means), they would have to try every single possible key in turn until they hit on the right one. By law of averages, we would expect them to find it after half as many guesses as there are possible keys. A 128-bit key has 2128 possible values, so on average, they would have to try 2127 keys. In decimal 2127 is a 39-digit number. Or put another way, trying a million million keys per second, it would take 5x1015 millennia to try 2127 keys. Not even the British government wants to decrypt your party invitations that badly. So with current mainstream technology1, a 128-bit key is in principle sufficient for most applications. But these metrics hold true only if our key selection algorithm— i.e. our random number generator— genuinely can pick any of the possible keys. For example, we certainly should not choose the key as follows: 1234// This is WRONG!! Do not do this!Random ranGen = new Random();byte[] aesKey = new byte[16]; // 16 bytes = 128 bitsranGen.nextBytes(aesKey); The problem here is that the period of java.util.Random is only 248. Even though we are generating a 128-bit key, we will only ever pick from a subset of 248 of the possible keys. Or put another way: an attacker need only try on average 247 keys, and will find our key by trial and error in a couple of days if they try just a thousand million keys per second. And as if that wasnot bad enough, they probably donot even need to try anywhere near 247: for reasons discussed earlier, there is a good chance that an instance of java.util.Random created within a couple of minutes of bootup will actually be seeded from a about one thousandth of the 248 possible values. This time, HM Sniffing Service doesnot even need expensive hardware to find the secret location of your housewarming party: a trip to Staples will give them all the computing power they need. So as you’ve probably guessed, our solution to the problem is to use SecureRandom instead: import java.security.SecureRandom;..Random ranGen = new SecureRandom();byte[] aesKey = new byte[16]; // 16 bytes = 128 bitsranGen.nextBytes(aesKey);Now, there’s a good chance that any of the 2128 possible keys will be chosen. Seeding of SecureRandomIn order to provide this property of choosing any seed with “equal” likelihood, (or at least, with no bias that is practically detectable), SecureRandom seeds itself from sources of entropy available from the local machine, such as timings of I/O events. ## Rainbow table A rainbow table is a precomputed table for reversing cryptographic hash functions, usually for cracking password hashes. Tables are usually used in recovering a plaintext password up to a certain length consisting of a limited set of characters. It is a practical example of a space/time trade-off, using less computer processing time and more storage than a brute-force attack which calculates a hash on every attempt, but more processing time and less storage than a simple lookup table with one entry per hash. After gathering a password hash, using said hash as a password would fail since the authentication system would hash it a second time. In order to learn a users password, a password that produces the same hashed value must be found, usually through a brute-force or dictionary attack. Use of a key derivation function that employs a salt makes this attack infeasible. Passphrase A passphrase is a sequence of words or other text used to control access to a computer system, program or data. A passphrase is similar to a password in usage, but is generally longer for added security. Passphrases are often used to control both access to, and operation of, cryptographic programs and systems, especially those that derive an encryption key from a passphrase. The origin of the term is by analogy with password. The modern concept of passphrases is believed to have been invented by Sigmund N. Porter[1] in 1982. Compared to password Passphrases differ from passwords. A password is usually short—six to ten characters. Such passwords may be adequate for various applications (if frequently changed, if chosen using an appropriate policy, if not found in dictionaries, if sufficiently random, and/or if the system prevents online guessing, etc.) But passwords are typically not safe to use as keys for standalone security systems (e.g., encryption systems) that expose data to enable offline password guessing by an attacker.[citation needed] Passphrases are theoretically stronger, and so should make a better choice in these cases. First, they usually are (and always should be) much longer—20 to 30 characters or more is typical—making some kinds of brute force attacks entirely impractical. Second, if well chosen, they will not be found in any phrase or quote dictionary, so such dictionary attacks will be almost impossible. Third, they can be structured to be more easily memorable than passwords without being written down, reducing the risk of hardcopy theft.[citation needed] However, if a passphrase is not protected appropriately by the authenticator and the clear-text passphrase is revealed its use is no better than other passwords. For this reason it is recommended that passphrases not be reused across different or unique sites and services. Dictionary Attack In cryptanalysis and computer security, a dictionary attack is a technique for defeating a cipher or authentication mechanism by trying to determine its decryption key or passphrase by trying hundreds or sometimes millions of likely possibilities, such as words in a dictionary. A dictionary attack is based on trying all the strings in a pre-arranged listing, typically derived from a list of words such as in a dictionary (hence the phrase dictionary attack). In contrast to a brute force attack, where a large proportion of the key space is searched systematically, a dictionary attack tries only those possibilities which are deemed most likely to succeed. Dictionary attacks often succeed because many people have a tendency to choose short passwords that are ordinary words or common passwords, or simple variants obtained, for example, by appending a digit or punctuation character. Dictionary attacks are relatively easy to defeat, e.g. by using a passphrase or otherwise choosing a password that is not a simple variant of a word found in any dictionary or listing of commonly used passwords. It is possible to achieve a time-space tradeoff by pre-computing a list of hashes of dictionary words, and storing these in a database using the hash as the key. This requires a considerable amount of preparation time, but allows the actual attack to be executed faster. A more refined approach involves the use of rainbow tables, which reduce storage requirements at the cost of slightly longer lookup times. Pre-computed dictionary attacks, or “rainbow table attacks”, can be thwarted by the use of salt, a technique that forces the hash dictionary to be recomputed for each password sought, making precomputation infeasible provided the number of possible salt values is large enough. Avalanche effect In cryptography, the avalanche effect is the desirable property of cryptographic algorithms, typically block ciphers and cryptographic hash functions wherein if when an input is changed slightly (for example, flipping a single bit) the output changes significantly (e.g., half the output bits flip). In the case of high-quality block ciphers, such a small change in either the key or the plaintext should cause a drastic change in the ciphertext. The actual term was first used by Horst Feistel,[1] although the concept dates back to at least Shannon’s diffusion. The SHA-1 hash function exhibits good avalanche effect. When a single bit is changed the hash sum becomes completely different. If a block cipher or cryptographic hash function does not exhibit the avalanche effect to a significant degree, then it has poor randomization, and thus a cryptanalyst can make predictions about the input, being given only the output. This may be sufficient to partially or completely break the algorithm. Thus, the avalanche effect is a desirable condition from the point of view of the designer of the cryptographic algorithm or device. Public-key cryptography An unpredictable (typically large and random) number is used to begin generation of an acceptable pair of keys suitable for use by an asymmetric key algorithm. In an asymmetric key encryption scheme, anyone can encrypt messages using the public key, but only the holder of the paired private key can decrypt. Security depends on the secrecy of the private key. In the Diffie–Hellman key exchange scheme, each party generates a public/private key pair and distributes the public key. After obtaining an authentic copy of each other’s public keys, Alice and Bob can compute a shared secret offline. The shared secret can be used, for instance, as the key for a symmetric cipher. Public key cryptography, or asymmetric cryptography, is any cryptographic system that uses pairs of keys: public keys which may be disseminated widely, and private keys which are known only to the owner. This accomplishes two functions: authentication, which is when the public key is used to verify that a holder of the paired private key sent the message, and encryption, whereby only the holder of the paired private key can decrypt the message encrypted with the public key. In a public key encryption system, any person can encrypt a message using the public key of the receiver, but such a message can be decrypted only with the receiver’s private key. For this to work it must be computationally easy for a user to generate a public and private key-pair to be used for encryption and decryption. The strength of a public key cryptography system relies on the degree of difficulty (computational impracticality) for a properly generated private key to be determined from its corresponding public key. Security then depends only on keeping the private key private, and the public key may be published without compromising security. Because of the computational complexity of asymmetric encryption, it is usually used only for small blocks of data, typically the transfer of a symmetric encryption key (e.g. a session key). This symmetric key is then used to encrypt the rest of the potentially long message sequence. The symmetric encryption/decryption is based on simpler algorithms and is much faster. Usage of public keyTwo of the best-known uses of public key cryptography are: Public key encryption, in which a message is encrypted with a recipient’s public key. The message cannot be decrypted by anyone who does not possess the matching private key, who is thus presumed to be the owner of that key and the person associated with the public key. This is used in an attempt to ensure confidentiality. Digital signatures, in which a message is signed with the sender’s private key and can be verified by anyone who has access to the sender’s public key. This verification proves that the sender had access to the private key, and therefore is likely to be the person associated with the public key. This also ensures that the message has not been tampered with, as a signature is mathematically bound to the message it originally was made with, and verification will fail for practically any other message, no matter how similar to the original message. An analogy to public key encryption is that of a locked mail box with a mail slot. The mail slot is exposed and accessible to the public – its location (the street address) is, in essence, the public key. Anyone knowing the street address can go to the door and drop a written message through the slot. However, only the person who possesses the key can open the mailbox and read the message. An analogy for digital signatures is the sealing of an envelope with a personal wax seal. The message can be opened by anyone, but the presence of the unique seal authenticates the sender. Problems of public generated key A central problem with the use of public key cryptography is confidence/proof that a particular public key is authentic, in that it is correct and belongs to the person or entity claimed, and has not been tampered with or replaced by a malicious third party. The usual approach to this problem is to use a public key infrastructure (PKI), in which one or more third parties – known as certificate authorities – certify ownership of key pairs. PGP, in addition to being a certificate authority structure, has used a scheme generally called the “web of trust”, which decentralizes such authentication of public keys by a central mechanism, and substitutes individual endorsements of the link between user and public key. To date, no fully satisfactory solution to the “public key authentication problem” has been found RSA RSA is one of the first practical public-key cryptosystems and is widely used for secure data transmission. In such a cryptosystem, the encryption key is public and differs from the decryption key which is kept secret. In RSA, this asymmetry is based on the practical difficulty of factoring the product of two large prime numbers, the factoring problem. RSA is made of the initial letters of the surnames of Ron Rivest, Adi Shamir, and Leonard Adleman, who first publicly described the algorithm in 1977. A user of RSA creates and then publishes a public key based on two large prime numbers, along with an auxiliary value. The prime numbers must be kept secret. Anyone can use the public key to encrypt a message, but with currently published methods, if the public key is large enough, only someone with knowledge of the prime numbers can feasibly decode the message.[2] Breaking RSA encryption is known as the RSA problem; whether it is as hard as the factoring problem remains an open question. RSA is a relatively slow algorithm, and because of this it is less commonly used to directly encrypt user data. More often, RSA passes encrypted shared keys for symmetric key cryptography which in turn can perform bulk encryption-decryption operations at much higher speed. Operation The RSA algorithm involves four steps: key generation, key distribution, encryption and decryption. RSA involves a public key and a private key. The public key can be known by everyone and is used for encrypting messages. The intention is that messages encrypted with the public key can only be decrypted in a reasonable amount of time using the private key. Symmetric-key algorithm Symmetric-key algorithms[1] are algorithms for cryptography that use the same cryptographic keys for both encryption of plaintext and decryption of ciphertext. The keys may be identical or there may be a simple transformation to go between the two keys The keys, in practice, represent a shared secret between two or more parties that can be used to maintain a private information link. This requirement that both parties have access to the secret key is one of the main drawbacks of symmetric key encryption, in comparison to public-key encryption (also known as asymmetric key encryption) Implementations Examples of popular symmetric algorithms include Twofish, Serpent, AES (Rijndael), Blowfish, CAST5, Kuznyechik, RC4, 3DES, Skipjack, Safer+/++ (Bluetooth), and IDEA.[5][6] References https://www.javamex.com/tutorials/cryptography/hash_functions_algorithms.shtml https://www.goanywhere.com/blog/2011/10/20/sftp-ftps-secure-ftp-transfers https://en.wikipedia.org/wiki/Secure_Shell https://en.wikipedia.org/wiki/Ssh-keygen https://en.wikipedia.org/wiki/Cryptographic_hash_function https://en.wikipedia.org/wiki/Rainbow_table https://en.wikipedia.org/wiki/Merkle%E2%80%93Damg%C3%A5rd_construction https://en.wikipedia.org/wiki/Public-key_cryptography https://en.wikipedia.org/wiki/RSA_%28cryptosystem%29]]></content>
      <tags>
        <tag>SSH</tag>
        <tag>Cryptography</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java Security Notes]]></title>
    <url>%2F2017-02-15-Java-Security%2F</url>
    <content type="text"><![CDATA[Java Security well-behaved: programs should be prevent from consuming too much system resources Components JCE: Java Cryptography Extension JSSE: Java Secure Socketets Extension JAAS: Java Authentication and Authorization Service Anatomy of a Java Applicationthe bytecode verifierThe bytecode verifier ensures that Java class files follow the rules of the Java language. As the figure implies, not all classes are subject to bytecode verification. the class loaderOne or more class loaders load all Java classes. Programatically, the class loader can set permissions for each class it loads. the access controllerThe access controller allows (or prevents) most access from the core API to the operating system, based upon policies set by the end user or system administrator. the security managerThe security manager is the primary interface between the core API and the operating system; it has the ultimate responsibility for allowing or preventing access to all system resources. However, it exists mostly for historical reasons; it defers its actions to the access controller. The security package the security package is a complex API. This includes discussions of: The security provider interface −− the means by which different security implementations may be plugged into the security package Message digests Keys and certificates Digital signatures Encryption (through JCE and JSSE) Authentication (through JAAS) The key databaseThe key database is a set of keys used by the security infrastructure to create or verify digital signatures. In the Java architecture, it is part of the security package, though it may be manifested as an external file or database. Trusted and Untrusted Classes In Java 2, only classes in the core API are considered trusted. Other classes must be given explicit permission to perform the operations we’ve discussed. SummaryAlthough the security manager is the most commonly known feature of Java’s security story, it’s often misunderstood: there is no standard security manager among Java implementations, and Java applications, by default, have no security manager at all. Access Controller The implementation of most security managers, however, is based entirely upon the access controller. Permissions The basic entity that the access controller operates on is a permission object −− an instance of the Permission class (java.security.Permission). This class, of course, is the basis of the types that are listed in a policy file for the default security policy. The Permission class itself is an abstract class that represents a particular operation. The nomenclature here is a little misleading because a permission object can reflect two things. When it is associated with a class (through a code source and a protection domain), a permission object represents an actual permission that has been granted to that class. Otherwise, a permission object allows us to ask if we have a specific permission. For example, if we construct a permission object that represents access to a file, possession of that object does not mean we have permission to access the file. Rather, possession of the object allows us to ask if we have permission to access the file. The access controller is built upon the four concepts Code sourcesAn encapsulation of the location from which certain Java classes were obtained. PermissionsAn encapsulation of a request to perform a particular operation. PoliciesAn encapsulation of all the specific permissions that should be granted to specific code sources. Protection domainsAn encapsulation of a particular code source and the permissions granted to that code source. Byte code verifier The verifier is often referred to as a mini−theorem prover (a term first used in several documents from Sun). This sounds somewhat more impressive than it is; it’s not a generic, all−purpose theorem prover by any means. Instead, it’s a piece of code that can prove one (and only one) thing −− that a given series of ( Java) bytecodes represents a legal set of ( Java) instructions. ShiftingJava and JavaScript perform sign extension when shift¬ing right, filling the empty spaces with 1’s for negative numbers, so 10100110 &gt;&gt; 5 becomes 11111101.The &gt;&gt;&gt; operator is unique to Java and JavaScript. It does a logical shift right, filling the empty spaces with 0 no matter what the value, so 10100110 &gt;&gt;&gt; 5 becomes 00000101. The shift operators enable you to multiply and divide by powers of 2 very quickly. For non-negative numbers, shifting to the right one bit is equivalent to dividing by 2, and shifting to the left one bit is equivalent to multiplying by 2. For negative numbers, it obviously depends on the language being used. Specifically, the bytecode verifier can prove the following: The class file has the correct format. The full definition of the class file format may be found in the Java virtual machine specification; the bytecode verifier is responsible for making sure that the class file has the right length, the correct magic numbers in the correct places, and so on. Final classes are not subclassed, and final methods are not overridden. Every class (except for java.lang.Object) has a single superclass. There is no illegal data conversion of primitive data types (e.g., int to Object). No illegal data conversion of objects occurs. Because the casting of a superclass to its subclass maybe a valid operation (depending on the actual type of the object being cast), the verifier cannot ensure that such casting is not attempted −− it can only ensure that before each such attempt is made, the legality of the cast is tested. There are no operand stack overflows or underflows. Stacks In Java, there are two stacks for each thread. One stack holds a series of method frames, where each method frame holds the local variables and other storage for a particular method invocation. This stack is known as the data stack and is what we normally think of as the stack within a traditional program. The bytecode verifier cannot prevent overflow of this stack −− an infinitely recursive method call will cause this stack to overflow. However, each method invocation requires a second stack (which itself is allocated on the data stack) that is referred to as the operand stack; the operand stack holds the values that the Java bytecodes operate on. This secondary stack is the stack that the bytecode verifier can ensure will not overflow or underflow. Security Manager The implementation of the sandbox depends on three things: The security manager, which provides the mechanism that the Java API uses to see if security−related operations are allowed. The access controller, which provides the basis of the default implementation of the security manager. The class loader, which encapsulates information about security policies and classes. We’ll start by examining the security manager. From the perspective of the Java API, there is a security manager that actually is in control of the security policy of an application. The purpose of the security manager is to determine whether particular operations should be permitted or denied. In truth, the purpose of the access controller is really the same: it decides whether access to a critical system resource should be permitted or denied. Hence, the access controller can do everything the security manager can do. The reason there is both an access controller and a security manager is mainly historical: the access controller is only available in Java 2 and subsequent releases. Before the access controller existed, the security manager relied on its internal logic to determine the security policy that should be in effect, and changing the security policy required changing the security manager itself. Starting with Java 2, the security manager defers these decisions to the access controller. Since the security policy enforced by the access controller can be specified by using policy files, this allows a much more flexible mechanism for determining policies. The access controller also gives us a much simpler method of granting fine−grained, specific permissions to specific classes. That process was theoretically possibly with the security manager alone, but it was simply too hard to implement. The BasicPermission class If you need to implement your own permission class, the BasicPermission class (java.security.BasicPermission) provides some useful semantics. This class implements a basic permission −− that is, a permission that doesn’t have actions. Basic permissions can be thought of as binary permissions −− you either have them or you don’t. However, this restriction does not prevent you from implementing actions in your subclasses of the BasicPermission class (as the PropertyPermission class does). The prime benefit of this class is the manner in which it implements wildcards. Names in basic permissions are considered to be hierarchical, following a dot−separated convention. For example, if the XYZ corporation wanted to create a set of basic permissions, they might use the convention that the first word of the permission always be xyz: xyz.readDatabase, xyz.writeDatabase, xyz.runPayrollProgram, xyz.HRDepartment.accessCheck, and so on. These permissions can then be specified by their full name, or they can be specified with an asterisk wildcard: xyz.* would match each of these (no matter what depth), and * would match every possible basic permission. http://www.qidianlife.com/index.php?m=home&amp;c=discover&amp;a=article&amp;id=2351 保护密码的最好办法是使用加盐密码哈希（ salted password hashing）。 永远不要告诉用户输错的究竟是用户名还是密码。就像通用的提示那样，始终显示：“无效的用户名或密码。”就行了。这样可以防止攻击者在不知道密码的情况下枚举出有效的用户名。 应当注意的是，用来保护密码的哈希函数，和数据结构课学到的哈希函数是不同的。例如，实现哈希表的哈希函数设计目的是快速查找，而非安全性。只有加密哈希函数（ cryptographic hash function）才可以用来进行密码哈希加密。像 SHA256 、 SHA512 、 RIPEMD 和 WHIRLPOOL 都是加密哈希函数。 破解哈希加密最简单的方法是尝试猜测密码，哈希每个猜测的密码，并对比猜测密码的哈希值是否等于被破解的哈希值。如果相等，则猜中。猜测密码攻击的两种最常见的方法是字典攻击和暴力攻击 。 字典攻击使用包含单词、短语、常用密码和其他可能用做密码的字符串的字典文件。对文件中的每个词都进行哈希加密，将这些哈希值和要破解的密码哈希值比较。如果它们相同，这个词就是密码。字典文件是通过大段文本中提取的单词构成，甚至还包括一些数据库中真实的密码。还可以对字典文件进一步处理以使其更为有效：如单词 “hello” 按网络用语写法转成 “h3110” 。 暴力攻击是对于给定的密码长度，尝试每一种可能的字符组合。这种方式会消耗大量的计算，也是破解哈希加密效率最低的办法，但最终会找出正确的密码。因此密码应该足够长，以至于遍历所有可能的字符组合，耗费的时间太长令人无法承受，从而放弃破解。 目前没有办法来组织字典攻击或暴力攻击。只能想办法让它们变得低效。如果密码哈希系统设计是安全的，破解哈希的唯一方法就是进行字典攻击或暴力攻击遍历每一个哈希值了。 我们可以通过在密码中加入一段随机字符串再进行哈希加密，这个被加的字符串称之为盐值。如上例所示，这使得相同的密码每次都被加密为完全不同的字符串。我们需要盐值来校验密码是否正确。通常和密码哈希值一同存储在帐号数据库中，或者作为哈希字符串的一部分。 盐值无需加密。由于随机化了哈希值，查表法、反向查表法和彩虹表都会失效。因为攻击者无法事先知道盐值，所以他们就没有办法预先计算查询表或彩虹表。如果每个用户的密码用不同的盐再进行哈希加密，那么反向查表法攻击也将不能奏效。 一个常见的错误是每次都使用相同的盐值进行哈希加密，这个盐值要么被硬编码到程序里，要么只在第一次使用时随机获得。这样的做法是无效的，因为如果两个用户有相同的密码，他们仍然会有相同的哈希值。攻击者仍然可以使用反向查表法对每个哈希值进行字典攻击。他们只是在哈希密码之前，将固定的盐值应用到每个猜测的密码就可以了。如果盐值被硬编码到一个流行的软件里，那么查询表和彩虹表可以内置该盐值，以使其更容易破解它产生的哈希值。 用户创建帐号或者更改密码时，都应该用新的随机盐值进行加密。 出于同样的原因，不应该将用户名用作盐值。对每一个服务来说，用户名是唯一的，但它们是可预测的，并且经常重复应用于其他服务。攻击者可以用常见用户名作为盐值来建立查询表和彩虹表来破解密码哈希。 为使攻击者无法构造包含所有可能盐值的查询表，盐值必须足够长。一个好的经验是使用和哈希函数输出的字符串等长的盐值。例如， SHA256 的输出为256位（32字节），所以该盐也应该是32个随机字节。 每个用户的每一个密码都要使用独一无二的盐值。用户每次创建帐号或更改密码时，密码应采用一个新的随机盐值。永远不要重复使用某个盐值。这个盐值也应该足够长，以使有足够多的盐值能用于哈希加密。一个经验规则是，盐值至少要跟哈希函数的输出一样长。该盐应和密码哈希一起存储在用户帐号表中。 存储密码的步骤： 使用 CSPRNG 生成足够长的随机盐值。 将盐值混入密码，并使用标准的密码哈希函数进行加密，如Argon2、 bcrypt 、 scrypt 或 PBKDF2 。 将盐值和对应的哈希值一起存入用户数据库。 校验密码的步骤： 从数据库检索出用户的盐值和对应的哈希值。 将盐值混入用户输入的密码，并且使用通用的哈希函数进行加密。 比较上一步的结果，是否和数据库存储的哈希值相同。如果它们相同，则表明密码是正确的；否则，该密码错误。]]></content>
      <tags>
        <tag>java</tag>
        <tag>security</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java JIT compiler]]></title>
    <url>%2F2017-02-21-Java-JIT%2F</url>
    <content type="text"><![CDATA[This is talking about Java JIT (Just-In-Time) compiler That trade-off is one reason that the compiler executes the interpreted code first—the compiler can figure out which methods are called frequently enough to warrant their compilation.The second reason is one of optimization: the more times that the JVM executes a particular method or loop, the more information it has about that code. This allows the JVM to make a number of optimizations when it compiles the code. At some point in time, the suminstance variable must reside in main memory, but retrieving a value from main memory is an expensive operation that takes multiple cycles to complete. If the value of sumwere to be retrieved from (and stored back to) main memory on every iteration of this loop, performance would be dismal. Instead, the compiler will load a register with the initial value of sum, perform the loop using that value in the register, and then (at an indeterminate point in time) store the final result from the register back to main memory.This kind of optimization is very effective, but it means that the semantics of thread synchronization (see Chapter 9) are crucial to the behavior of the application One thread cannot see the value of a variable stored in the register used by another thread; synchronization makes it possible to know exactly when the register is stored to main memory and available to other threads.Register usage is a general optimization of the compiler, and when escape analysis is enabled (see the end of this chapter), register use is quite aggressive. If the size of your heap will be less than about 3 GB, the 32-bit version of Java will be faster and have a smaller footprint. This is because the memory references within the JVM will be only 32 bits, and manipulating those memory references is less expensive than manipulating 64-bit references (even if you have a 64-bit CPU). The 32-bit references also use less memory. JVM developers (and even some tools) often refer to the compilers by the names C1(compiler 1, client compiler) and C2 (compiler 2, server compiler). The primary difference between the two compilers is their aggressiveness in compilingcode. The client compiler begins compiling sooner than the server compiler does. Thismeans that during the beginning of code execution, the client compiler will be faster,because it will have compiled correspondingly more code than the server compiler. The engineering trade-off here is the knowledge the server compiler gains while it waits:that knowledge allows the server compiler to make better optimizations in the compiledcode. Ultimately, code produced by the server compiler will be faster than that producedby the client compiler. From a user’s perspective, the benefit to that trade-off is basedon how long the program will run, and how important the startup time of the programis. The obvious question here is why there needs to be a choice at all: couldn’t the JVMstart with the client compiler, and then use the server compiler as code gets hotter? Thattechnique is known as tiered compilation. With tiered compilation, code is first compiledby the client compiler; as it becomes hot, it is recompiled by the server compiler. To use tiered compilation, specifythe server compiler (either with -server or by ensuring it is the default for the particularJava installation being used), and ensure that the Java command line includes the flag XX:+TieredCompilation (the default value of which is false). In Java 8, tiered compilationis enabled by default. The client compiler is most often used when fast startup is the primary objective. The client compiler is most useful when the startup of an applicationis the overriding performance concern. Tiered compilation can achieve startup times very close to thoseobtained from the client compiler. It is also interesting that tiered compilation is always slightly better than the standardserver compiler. In theory, once the program has run enough to compile all the hotspots, the server compiler might be expected to achieve the best (or at least equal) performance.But in any application, there will almost always be some small section of codethat is infrequently executed. It is better to compile that code—even if the compilationis not the best that might be achieved—than to execute that code in interpreted mode. For jobs that run in a fixed amount of time, choose the compilerbased on which one is the fastest at executing the actual job. Tiered compilation provides a reasonable default choice for batchjobs. For long-running applications, always choose the server compiler, preferably in conjunction with tiered compilation.]]></content>
      <tags>
        <tag>java</tag>
        <tag>JIT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring notes]]></title>
    <url>%2F2017-03-05-Spring%2F</url>
    <content type="text"><![CDATA[Spring Bean Life Cycle Callback MethodsA bean life cycle includes the following steps. Within IoC container, a spring bean is created using class constructor. Now the dependency injection is performed using setter method. Once the dependency injection is completed, BeanNameAware.setBeanName() is called. It sets the name of bean in the bean factory that created this bean. Now &lt; code&gt;BeanClassLoaderAware.setBeanClassLoader() is called that supplies the bean class loader to a bean instance. Now &lt; code&gt;BeanFactoryAware.setBeanFactory() is called that provides the owning factory to a bean instance. Now the IoC container calls BeanPostProcessor.postProcessBeforeInitialization on the bean. Using this method a wrapper can be applied on original bean. Now the method annotated with @PostConstruct is called. After @PostConstruct, the method InitializingBean.afterPropertiesSet() is called. Now the method specified by init-method attribute of bean in XML configuration is called. And then BeanPostProcessor.postProcessAfterInitialization() is called. It can also be used to apply wrapper on original bean. Now the bean instance is ready to be used. Perform the task using the bean. Now when the ApplicationContext shuts down such as by using registerShutdownHook() then the method annotated with @PreDestroy is called. After that DisposableBean.destroy() method is called on the bean. Now the method specified by destroy-method attribute of bean in XML configuration is called. Before garbage collection, finalize() method of Object is called. Spring framework provides following 4 ways for controlling life cycle events of bean: InitializingBean and DisposableBean callback interfaces Other Aware interfaces for specific behavior Custom init() and destroy() methods in bean configuration file @PostConstruct and @PreDestroy annotations InitializingBeanThe org.springframework.beans.factory.InitializingBean interface specifies a single method − void afterPropertiesSet() throws Exception; Destruction callbacksThe org.springframework.beans.factory.DisposableBean interface specifies a single method − void destroy() throws Exception; Custom init() and destroy() methods in bean configuration fileThe default init and destroy methods in bean configuration file can be defined in two ways: Bean local definition applicable to a single beanGlobal definition applicable to all beans defined in beans context Local definition is given as below. 1234567&lt;beans&gt; &lt;bean id="demoBean" class="com.howtodoinjava.task.DemoBean" init-method="customInit" destroy-method="customDestroy"&gt;&lt;/bean&gt;&lt;/beans&gt; Where as global definition is given as below. These methods will be invoked for all bean definitions given under tag. They are useful when you have a pattern of defining common method names such as init() and destroy() for all your beans consistently. This feature helps you in not mentioning the init and destroy method names for all beans independently. 123456&lt;beans default-init-method="customInit" default-destroy-method="customDestroy"&gt; &lt;bean id="demoBean" class="com.howtodoinjava.task.DemoBean"&gt;&lt;/bean&gt;&lt;/beans&gt; @PostConstruct and @PreDestroy annotationsSpring 2.5 onwards, you can use annotations also for specifying life cycle methods using @PostConstruct and @PreDestroy annotations. @PostConstruct annotated method will be invoked after the bean has been constructed using default constructor and just before it’s instance is returned to requesting object.@PreDestroy annotated method is called just before the bean is about be destroyed inside bean container.A sample implementation will look like this: 12345678910111213141516171819package com.howtodoinjava.task;import javax.annotation.PostConstruct;import javax.annotation.PreDestroy;public class DemoBean &#123; @PostConstruct public void customInit() &#123; System.out.println("Method customInit() invoked..."); &#125; @PreDestroy public void customDestroy() &#123; System.out.println("Method customDestroy() invoked..."); &#125;&#125; Spring NotesAOP123456 execution(* concert.Performance.perform()) and !bean('woodstock') @Aspectpublic class Audience &#123; @Before("execution(** concert.Performance.perform(..))") public void silenceCellPhones() &#123;Before performance Fortunately, there’s a way: the @Pointcut annotation defines a reusable pointcut within an @AspectJ aspect. The next listing shows the Audience aspect, updated to use @Pointcut. Reuse pointuct 123456789101112@Aspectpublic class Audience &#123; @Pointcut("execution(** concert.Performance.perform(..))") public void performance() &#123;&#125; //Define named pointcut @Before("performance()") public void silenceCellPhones() &#123; System.out.println("Silencing cell phones"); @Before("performance()") public void takeSeats() &#123; System.out.println("Taking seats"); &#125; The body of the performance() method is irrelevant and, in fact, should be empty. The method itself is just a marker, giving the @Pointcut annotation something to attach itself to. 1234import org.aspectj.lang.ProceedingJoinPoint; public class Audience &#123;public void watchPerformance(ProceedingJoinPoint jp) &#123; try &#123;System.out.println("Silencing cell phones"); System.out.println("Taking seats"); 123456789&lt;aop:config&gt;&lt;aop:aspect ref="audience"&gt;&lt;aop:pointcutid="performance"expression="execution(** concert.Performance.perform(..))" /&gt;&lt;aop:around Declare around advice pointcut-ref="performance" method="watchPerformance"/&gt; &lt;/aop:aspect&gt;&lt;/aop:config&gt; to use AspectJ’s @DeclareParents annota¬tion to magically introduce a new method into an advised bean. But AOP introduc¬tions aren’t exclusive to AspectJ. Using the aop:declare-parents element from Spring’s aop namespace, you can do similar magic in XML. Listing 1.7 Spring offers Java-based configuration as an alternative to XML. 123456789101112131415package com.springinaction.knights.config;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import com.springinaction.knights.BraveKnight; import com.springinaction.knights.Knight; import com.springinaction.knights.Quest; import com.springinaction.knights.SlayDragonQuest;@Configurationpublic class KnightConfig &#123;@Beanpublic Knight knight() &#123;return new BraveKnight(quest());&#125;@Beanpublic Quest quest() &#123;return new SlayDragonQuest(System.out);&#125;&#125; In a Spring application, an application context loads bean definitions and wires them together. The Spring application context is fully responsible for the creation of and wiring of the objects that make up the application. Spring comes with several imple¬mentations of its application context, each primarily differing only in how it loads its configuration. When the beans in knights.xml are declared in an XML file, an appropriate choice for application context might be ClassPathXmlApplicationContext.1 These system services are commonly referred to as cross-cut¬ting concerns because they tend to cut across multiple components in a system. Your components are littered with code that isn’t aligned with their core func¬tionality. A method that adds an entry to an address book should only be con¬cerned with how to add the address and not with whether it’s secure or transactional. Spring seeks to eliminate boilerplate code by encapsulating it in templates. Spring’s JdbcTemplate makes it possible to perform database operations without all the ceremony required by traditional JDBC.The container is at the core of the Spring Framework. Spring’s container uses DI to manage the components that make up an application. This includes creating associa¬tions between collaborating components. As such, these objects are cleaner and easier to understand, they support reuse, and they’re easy to unit test. There’s no single Spring container. Spring comes with several container imple¬mentations that can be categorized into two distinct types. Bean factories (defined by the org.springframework.beans.factory.BeanFactory interface) are the simplest of containers, providing basic support for DI. Application contexts (defined by the org.springframework.context.ApplicationContext interface) build on the notion of a bean factory by providing application-framework services, such as the ability to resolve textual messages from a properties file and the ability to publish application events to interested event listeners. Although it’s possible to work with Spring using either bean factories or applica¬tion contexts, bean factories are often too low-level for most applications. Therefore, application contexts are preferred over bean factories. We’ll focus on working with application contexts and not spend any more time talking about bean factories. As you can see, a bean factory performs several setup steps before a bean is ready to use. Let’s break down figure 1.5 in more detail:1 Spring instantiates the bean.2 Spring injects values and bean references into the bean’s properties.3 If the bean implements BeanNameAware, Spring passes the bean’s ID to the set-BeanName() method.4 If the bean implements BeanFactoryAware, Spring calls the setBeanFactory() method, passing in the bean factory itself.5 If the bean implements ApplicationContextAware, Spring calls the set-ApplicationContext() method, passing in a reference to the enclosing appli¬cation context.6 If the bean implements the BeanPostProcessor interface, Spring calls its post-ProcessBeforeInitialization() method.7 If the bean implements the InitializingBean interface, Spring calls its after-PropertiesSet() method. Similarly, if the bean was declared with an init-method, then the specified initialization method is called.8 If the bean implements BeanPostProcessor, Spring calls its postProcess-AfterInitialization() method.9 At this point, the bean is ready to be used by the application and remains in the application context until the application context is destroyed.10 If the bean implements the DisposableBean interface, Spring calls its destroy() method. Likewise, if the bean was declared with a destroy-method, the specified method is called. Spring Boot heavily employs automatic configuration techniques that can elimi¬nate most (and in many cases, all) Spring configuration. It also provides several starter projects to help reduce the size of your Spring project build files, whether you’re using Maven or Gradle. · Spring began to support Servlet 3.0, including the ability to declare servlets and filters in Java-based configuration instead of web.xml.· You should now have a good idea of what Spring brings to the table. Spring aims to make enterprise Java development easier and to promote loosely coupled code. Vital to this are dependency injection and aspect-oriented programming.When it comes to expressing a bean wiring specification, Spring is incredibly flexible, offering three primary wiring mechanisms:· Explicit configuration in XML· Explicit configuration in Java· Implicit bean discovery and automatic wiring· in many cases, the choice is largely a matter of personal taste, and you’re welcome to choose the approach that feels best for you.Spring attacks automatic wiring from two angles:· Component scanning—Spring automatically discovers beans to be created in the application context.· Autowiring—Spring automatically satisfies bean dependencies.Working together, component scanning and autowiring are a powerful force and can help keep explicit configuration to a minimum. 123package soundsystem;public interface CompactDisc &#123; void play();&#125; The specifics of the CompactDisc interface aren’t important. What is important is that you’ve defined it as an interface. As an interface, it defines the contract through which a CD player can operate on the CD. And it keeps the coupling between any CD player implementation and the CD itself to a minimum. 123456789package soundsystem;import org.springframework.stereotype.Component;@Componentpublic class SgtPeppers implements CompactDisc &#123;private String title = "Sgt. Pepper's Lonely Hearts Club Band"; private String artist = "The Beatles";public void play() &#123;System.out.println("Playing " + title + " by " + artist);&#125;&#125; that SgtPeppers is annotated with @Component. This simple annotation identifies this class as a component class and serves as a clue to Spring that a bean should be created for the class.Component scanning isn’t turned on by default, however. You’ll still need to write an explicit configuration to tell Spring to seek out classes annotated with @Component and to create beans from them. The configuration class in the following listing shows the minimal configuration to make this possible. 12345package soundsystem;import org.springframework.context.annotation.ComponentScan; import org.springframework.context.annotation.Configuration;@Configuration @ComponentScanpublic class CDPlayerConfig &#123;&#125; you can explicitly identify any state as the starting state by setting the start-state attri¬bute in the element: 1234567891011121314151617&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;flow xmlns="http://www.springframework.org/schema/webflow" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.springframework.org/schema/webflow http://www.springframework.org/schema/webflow/spring-webflow-2.3.xsd" start-state="identifyCustomer"&gt;...&lt;/flow&gt;&lt;body&gt;&lt;h2&gt;Welcome to Spizza!!!&lt;/h2&gt;&lt;form:form&gt;&lt;input type="hidden" name="_flowExecutionKey"value="$&#123;flowExecutionKey&#125;"/&gt;&lt;input type="text" name="phoneNumber"/&gt;&lt;br/&gt;&lt;input type="submit" name="_eventId_phoneEntered"value="Lookup Customer" /&gt;&lt;/form:form&gt;&lt;/body&gt;&lt;/html&gt; The eventId portion of the button’s name is a clue to Spring Web Flow that what follows is an event that should be fired. When the form is submitted by clicking that button, a phoneEntered event is fired, triggering a transition to lookupCustomer. Flow execution key 12345678&lt;p&gt;The address is outside of our delivery area. You maystill place the order, but you will need to pick it upyourself.&lt;/p&gt;&lt;![CDATA[&lt;a href="$&#123;flowExecutionUrl&#125;&amp;_eventId=accept"&gt;Continue, I'll pick up the order&lt;/a&gt; |&lt;a href="$&#123;flowExecutionUrl&#125;&amp;_eventId=cancel"&gt;Never mind&lt;/a&gt;11&gt; Note that the customerReady end state includes an element. This ele-ment is a flow’s equivalent of Java’s return statement. It passes back some data from a subflow to the calling flow. In this case, returns the customer flow variable so that the identifyCustomer subflow state in the pizza flow can assign it to the order.you use the element to pass the Order in to the flow. Here you’re using it to accept that Order object. If you think of this subflow as being analo¬gous to a method in Java, the element used here is effectively defining the subflow’s signature. This flow requires a single parameter called order. States, transitions, and entire flows can be secured in Spring Web Flow by using the element as a child of those elements. For example, to secure access to a view state, you might use like this: As configured here, access to the view state will be restricted to only users who are granted ROLE_ADMIN access (per the attributes attribute). The attributes attribute takes a comma-separated list of authorities that the user must have to gain access to the state, transition, or flow. On the contrary, REST has little to do with RPC. Whereas RPC is service oriented and focused on actions and verbs, REST is resource oriented, emphasizing the things and nouns that comprise an application. Put more succinctly, REST is about transferring the state of resources—in a representational form that is most appropriate for the client or server—from a server to a client (or vice versa). It’s a small start, but you’ll build on this controller throughout this chapter as you learn the ins and outs of Spring’s REST programming model. Representation is an important facet of REST. It’s how a client and a server communicate about a resource. Any given resource can be represented in virtually any form. If the consumer of the resource prefers JSON, then the resource can be Meanwhile, a human user viewing the resource in a web browser will likely prefer seeing it in HTML (or possibly PDF, Excel, or some other human-readable form). The resource doesn’t change—only how it’s represented. Understanding how ContentNegotiatingViewResolverworks involves getting to know the content-negotiation two-step: Determine the requested media type(s). Find the best view for the requested media type(s). The @ResponseBodyannotation tells Spring that you want to send the returned object as a resource to the client, converted into some representational form that the client can accept. More specifically, DispatcherServletconsiders the request’s Acceptheader and looks for a message converter that can give the client the representation it wants. Just as @ResponseBodytells Spring to employ a message converter when sending data to a client, the @RequestBodytells Spring to find a message converter to convert a resource representation coming from a client into an object. For example, suppose that you need a way for a client to submit a new Spittle to be saved. You can write the controller method to handle such a request like this: The body of the POST request is expected to carry a resource representation for a Spittle. Because the Spittleparameter is annotated with @RequestBody, Spring will look at the Content-Typeheader of the request and try to find a message converter that can convert the request body into a Spittle. For example, if the client sent the Spittle data in a JSON representation, then the Content-Type header might be set to application/json. In that case, DispatcherServletwill look for a message converter that can convert JSON into Java objects. If the Jackson 2 library is on the classpath, then MappingJackson2Http-MessageConverterwill get the job and will convert the JSON representation into a Spittle that’s passed into the saveSpittle()method. The method is also annotated with @ResponseBodyso that the returned Spittle will be converted into a resource representation to be returned to the client. Notice that the @RequestMappinghas a consumesattribute set to application/json. The consumesattribute works much like the producesattribute, only with regard to the request’s Content-Typeheader. This tells Spring that this method will only handle POSTrequests to /spittles if the request’s Content-Typeheader is application/json. Otherwise, it will be up to some other method (if a suitable one exists) to handle the request. The key thing to notice in listing 16.3is what’s not in the code. Neither of the handler methods are annotated with @ResponseBody. But because the controller is annotated with @RestController, the objects returned from those methods will still go through message conversion to produce a resource representation for the client.The @ExceptionHandler annotation can be applied to controller methods to handle specific exceptions. Here, it’s indicating that if a SpittleNotFoundException is thrown from any of the handler methods in the same controller, the spittleNotFound() method should be called to handle that exception. @ExceptionHandler(SpittleNotFoundException.class)@ResponseStatus(HttpStatus.NOT_FOUND)public @ResponseBody Error spittleNotFound(SpittleNotFoundException e) { long spittleId = e.getSpittleId(); return new Error(4, “Spittle [“ + spittleId + “] not found”);} @ExceptionHandler(SpittleNotFoundException.class)@ResponseStatus(HttpStatus.NOT_FOUND)public @ResponseBody Error spittleNotFound(SpittleNotFoundException e) { long spittleId = e.getSpittleId(); return new Error(4, “Spittle [“ + spittleId + “] not found”);} Because spittleNotFound() always returns an Error, the only reason to keep Response-Entity around is so you can set the status code. But by annotating spittleNotFound() with @ResponseStatus(HttpStatus.NOT_FOUND), you can achieve the same effect and get rid of ResponseEntity. Again, if the controller class is annotated with @RestController, you can remove the @ResponseBody annotation and clean up the code a little more: @ExceptionHandler(SpittleNotFoundException.class)@ResponseStatus(HttpStatus.NOT_FOUND)public Error spittleNotFound(SpittleNotFoundException e) { long spittleId = e.getSpittleId(); return new Error(4, “Spittle [“ + spittleId + “] not found”);} public Spittle fetchSpittle(long id) { RestTemplate rest = new RestTemplate(); ResponseEntity response = rest.getForEntity( &quot;http://localhost:8080/spittr-api/spittles/{id}&quot;, Spittle.class, id); if(response.getStatusCode() == HttpStatus.NOT_MODIFIED) { throw new NotModifiedException(); } return response.getBody();} Just like the getForEntity() method, postForEntity() returns a Response-Entity object. From that object, you can call getBody() to get the resource object (a Spitter in this case). And the getHeaders() method gives you an HttpHeaders from which you can access the various HTTP headers returned in the response. Here, you’re calling getLocation() to retrieve the Location header as a java.net.URI. By passing in HttpMethod.GET as the HTTP verb, you’re asking exchange() to send a GET request. The third argument is for sending a resource on the request, but because this is a GET request, it can be null. The next argument indicates that you want the response converted into a Spitter object. An Used this way, the exchange() method is virtually identical to the previously used getForEntity(). But unlike getForEntity()—or getForObject()—exchange() lets you set headers on the request sent. Instead of passing null to exchange(), you pass in an HttpEntity created with the request headers you want. RESTful architecture uses web standards to integrate applications, keeping the interactions simple and natural. Resources in a system are identified by URLs, manipulated with HTTP methods, and represented in one or more forms suitable for the client. Spring’s philosophy of avoiding checked exceptions, you don’t want to let the JMSException escape this method, so you’ll catch it instead. In the catch block, you can use the convertJmsAccessException() method from Spring’s JmsUtils class to convert the checked JMSException to an unchecked JmsException. This is effectively the same thing JmsTemplate does for you in other cases. A message-listener container is a special bean that watches a JMS destination, waiting for a message to arrive. Once a message arrives, the bean retrieves the message and passes it on to any message listeners that are interested. &lt;jms:listener-container connection-factory=”connectionFactory”&gt; &lt;jms:listener destination=”spitter.alert.queue” ref=”spittleHandler” method=”handleSpittleAlert” /&gt; JmsInvokerServiceExporter is much like those other service exporters. In fact, note that there’s some symmetry in the names of JmsInvokerServiceExporter and HttpInvokerServiceExporter. If HttpInvokerServiceExporter exports services that communicate over HTTP, then JmsInvokerServiceExporter must export services that converse over JMS. As it turns out, AMQP offers several advantages over JMS. First, AMQP defines a wire-level protocol for messaging, whereas JMS defines an API specification. JMS’s API specification ensures that all JMS implementations can be used through a common API but doesn’t mandate that messages sent by one JMS implementation can be consumed by a different JMS implementation. AMQP’s wire-level protocol, on the other hand, specifies the format that messages will take when en route between the producer and consumer. Consequently, AMQP is more interoperable than JMS—not only across different AMQP implementations, but also across languages and platforms. In JMS, there are just three primary participants: the message producer, the message consumer(s), and a channel (either a queue or a topic) to carry the message between producers and consumers. These essentials of the JMS messaging model are illustrated in figures 17.3 and 17.4. In JMS, the channel helps to decouple the producer from the consumer, but both are still coupled to the channel. A producer publishes messages to a specific queue or topic, and the consumer receives those message from a specific queue or topic. The channel has the double duty of relaying messages and determining how those messages will be routed; queues route using a point-to-point algorithm, and topics route in publish/subscribe fashion. In contrast, AMQP producers don’t publish directly to a queue. Instead, AMQP introduces a new level of indirection between the producer and any queues that will carry the message: the exchange. This relationship is illustrated in figure 17.8. Figure 17.8. In AMQP, message producers are decoupled from message queues by an exchange that handles message routing. For example, to have a message routed to multiple queues with no regard for the routing key, you can configure a fanout exchange and several queues like this: As its name implies, the RabbitMQ connection factory is used to create connections with RabbitMQ. If you want to send messages via RabbitMQ, you could inject the connectionFactory bean into your AlertServiceImpl class, use it to create a Connection, use that Connection to create a Channel, and use that Channel to publish a message to an exchange. Yep, you could do that. you can configure different defaults using the exchange and routing-key attributes on the element: it was tricky to convert domain objects into Messages for sending, it’s messy to convert received Messages to domain objects. Therefore, consider using RabbitTemplate’s receiveAndConvert() method instead: Spittle spittle = (Spittle) rabbit.receiveAndConvert(“spittle.alert.queue”);Or you can leave the queue name out of the call parameters to fall back on the template’s default queue name: The first thing you’ll need in order to consume a Spittle object asynchronously in a message-driven POJO is the POJO itself. Here’s SpittleAlertHandler, which fills that role: package com.habuma.spittr.alerts;import com.habuma.spittr.domain.Spittle; public class SpittleAlertHandler { public void handleSpittleAlert(Spittle spittle) { // … implementation goes here … }} Do you see the difference? I’ll agree that it’s not obvious. The and elements appear to be similar to their JMS counterparts. These elements, however, come from the rabbit namespace instead of the JMS namespace. I said it wasn’t obvious. Regardless of whether you handle text messages, binary messages, or both, you might also be interested in handling the establishment and closing of connections. In that case, you can override afterConnectionEstablished() and afterConnectionClosed(): public void afterConnectionEstablished(WebSocketSession session) throws Exception { logger.info(“Connection established”);} @Overridepublic void afterConnectionClosed( WebSocketSession session, CloseStatus status) throws Exception { logger.info(“Connection closed. Status: “ + status);} Fortunately, you don’t have to work with raw WebSocket connections. Just as HTTP layers a request-response model on top of TCP sockets, STOMP layers a frame-based wire format to define messaging semantics on top of WebSocket. At a quick glance, STOMP message frames look very similar in structure to HTTP requests. Much like HTTP requests and responses, STOMP frames are comprised of a command, one or more headers, and a payload. For example, here’s a STOMP frame that sends data. SENDdestination:/app/marcocontent-length:20 {&quot;message&quot;:&quot;Marco!&quot;} In this simple example, the STOMP command is SEND, indicating that something is being sent. It’s followed by two headers: one indicates the destination where the message should be sent, and the other communicates the size of the payload. Following a blank line, the frame concludes with the payload; in this case, a JSON message. Taking advantage of Maven’s and Gradle’s transitive dependency resolution, the starters declare several dependencies in their own pom.xml file. When you add one of these starter dependencies to your Maven or Gradle build, the starter’s dependencies are resolved transitively. Whereas Spring Boot starters cut down the size of your build’s dependency list, Spring Boot autoconfiguration cuts down on the amount of Spring configuration. When Spring Boot’s web autoconfiguration detects Spring MVC in the classpath, it will automatically configure several beans to support Spring MVC, including view resolvers, resource handlers, and message converters (among others). All that’s left for you to do is write the controller classes to handle the requests. Spring Boot is an exciting new addition to the Spring family of projects. Where Spring aims to make Java development simpler, Spring Boot aims to make Spring itself simpler. Spring Boot employs two main tricks to eliminate boilerplate configuration in a Spring project: Spring Boot starters and automatic configuration. A single Spring Boot starter dependency can replace several common dependencies in a Maven or Gradle build. For example, adding only Spring Boot’s web starter as a dependency in a project pulls in Spring’s web and Spring MVC modules as well as the Jackson 2 databind module. Automatic configuration takes full advantage of Spring 4.0’s conditional configuration feature to automatically configure certain Spring beans to enable a certain feature. For example, Spring Boot can detect that Thymeleaf is in the application classpath and automatically configure the beans required to enable Thymeleaf templates as Spring MVC views. In addition to the MBean info assemblers you’ve seen thus far, Spring provides another assembler known as MetadataMBeanInfoAssembler that can be configured to use annotations to appoint bean methods as managed operations and attributes. I could show you JTA transaction management is resource-intensive; its exception handling is based on checked exceptions and so is not developer-friendly. Moreover, unit testing is hard with EJB CMT. Most applications just need local transactions since they do not deal with multiple servers or transactional resources such as databases, JMS, and JCA; hence, they do not need a full-blown application server. For distributed transactions spanned across multiple servers over remote calls, you need JTA, necessitating an application server, as JTA needs JNDI to look up the data source. JNDI is normally available only in an application server. Use JTATransactionManager inside application servers for JTA capabilities. TransactionDefinition defines the critical transaction attributes such as isolation, propagation, transaction timeout, and the read-only status of a given transaction instance. Transaction attributes determine the behavior of transaction instances. They can be set programmatically as well as declaratively. Transaction attributes are: Isolation level: Defines how much a transaction is isolated from (can see) other transactions running in parallel. Valid values are: None, Read committed, Read uncommitted, Repeatable reads, and Serializable. Read committed cannot see dirty reads from other transactions. Propagation: Determines the transactional scope of a database operation in relation to other operations before, after, and nested inside itself. Valid values are: REQUIRED, REQUIRES_NEW, NESTED, MANDATORY, SUPPORTS, NOT_SUPPORTED, and NEVER. Timeout: Maximum time period that a transaction can keep running or waiting before it completes. Once at timeout, it will roll back automatically. Read-only status: You cannot save the data read in this mode. These transaction attributes are not specific to Spring, but reflect standard transactional concepts. The TransactionDefinition interface specifies these attributes in the Spring Transaction Management context.For multiple DataSource objects or transactional resources, you need a JtaTransactionManager with JTA capabilities, which usually delegates to a container JTA provider. If you are using Hibernate and just a single DataSource (and no other transactional resource), then the best option is to use HibernateTransactionManager, which requires you to pass the session factory as a dependency. Spring Transactions supports two transactional modes: proxy mode and AspectJ mode. Proxy is the default and most popular mode. In proxy mode, Spring creates an AOP proxy object, wrapping the transactional beans, and applies transactional behavior transparently around the methods using transaction aspects based on the metadata. The AOP proxy created by Spring based on transactional metadata, with the help of the configured PlatformTransactionManager, performs transactions around the transactional methods. Spring offers two convenient approaches for declaratively defining the transactional behavior of your beans: AOP configuration for transactions in an XML metadata fileUsing the @Transactional annotation &lt;tx:advice id=”txAdvice” transaction-manager=”transactionManager”&gt; tx:attributes &lt;tx:method name=”find*” read-only=”true” /&gt; &lt;tx:method name=”*” isolation=”DEFAULT” propagation=”REQUIRED” /&gt; aop:config &lt;aop:pointcut id=”allServiceMethods” expression=”execution(* com.taskify.service..(..))” /&gt; &lt;aop:advisor advice-ref=”txAdvice” pointcut- ref=”allServiceMethods” /&gt;You can see that this AOP configuration instructs Spring how to weave transactional advices around the methods using pointcuts. It instructs TransactionManager to make all find methods of the entire service layer read-only, and to force other methods to have the transaction propagation: REQUIRED, which means that, if the caller of the method is already in a transactional context, this method joins the same transaction without creating a new one; otherwise, a new transaction is created. If you want to create a different transaction for this method, you should use the REQUIRES_NEW propagation.Also, note that the transaction isolation level is specified as DEFAULT, which means the default isolation of the database is to be used. Most databases default to READ_COMMITTED, which means a transactional thread cannot see the data of other transactions in progress (dirty reads). Note@Transactional can be applied only to public methods. If you want to annotate over protected, private, or package-visible methods, consider using AspectJ, which uses compile-time aspect weaving. Spring recommends annotating @Transactional only on concrete classes as opposed to interfaces, as it will not work in most cases such as when you use proxy-target-class=”true” or mode=”aspectj”. You need to first enable transaction management in your application before Spring can detect the @Transactional annotation for your bean methods. You enable transaction in your XML metadata using the following notation: &lt;tx:annotation-driven transaction-manager=”transactionManager” /&gt;The following is the Java configuration alternative for the preceding listing: @Configuration@EnableTransactionManagementpublic class JpaConfiguration {} Spring scans the application context for bean methods annotated with @Transactional when it sees either of the preceding settings. Implementing InitializingBean and DisposableBean The Spring IoC container invokes the callback methods afterPropertiesSet() of org.springframework.beans.factory.InitializingBean and destroy() of org.springframework.beans.factory.DisposableBean on any Spring bean and implements them: There are different bean scopes in Spring, such as singleton, prototype, request, session, and global session. We will understand each session one by one. By default, all Spring beans are singleton. Once ApplicationContext is initialized, it looks at all the beans in XML and initializes only one bean per bean definition in Spring Container. On each call to the getBean() method, Spring Container returns the same instance of the bean. Prototype The prototype is second bean scope in Spring, which returns a brand-new instance of a bean on each call to the getBean() method. When a bean is defined as a prototype, Spring waits for getBean() to happen and only then does it initialize the prototype. Spring doesn’t maintain the complete life cycle of the prototype. Here, the container instantiates and configures prototype beans and returns this bean to the client with no further record of this prototype instance. Request The third bean scope in Spring is request, which is available only in web applications that use Spring and create an instance of bean for every HTTP request. Here, a new bean is created per Servlet request. Spring will be aware of when a new request is happening because it ties well with the Servlet APIs, and depending on the request, Spring creates a new bean. So, if the reque Session The session is the fourth bean scope in Spring, which is available only in web applications that use Spring and create an instance of bean for every HTTP session. Here, a new bean is created per session. As long as there is one user accessing in a single session, each call to getBean() will return same instance of the bean. Global session The global session is the fifth bean scope in Spring, which works only in portlet environments that use Spring and create a bean for every new portlet session. Spring’s BeanFactory manages the life cycle of beans created through the Spring IoC container. The life cycle of beans consist of callback methods, which can be categorized broadly into the following two groups: Post-initialization callback methodsPre-destruction callback methods Initialization It represents a sequence of activities that take place between the bean instantiation and the handover of its reference to the client application: The bean container finds the definition of the Spring bean in the configuration file and creates an instance of the beanIf any properties are mentioned, populate the properties using settersIf the Bean class implements the BeanNameAware interface, then call the setBeanName() methodIf the Bean class implements the BeanFactoryAware interface, then call the setBeanFactory() methodIf the Bean class implements the ApplicationContextAware interface, then call the setApplicationContext() methodIf there are any BeanPostProcessors objects associated with the BeanFactory interface that loaded the bean, then Spring will call the postProcessBeforeInitialization() method before the properties for the bean are injectedIf the Bean class implements the InitializingBean interface, then call the afterPropertiesSet() method once all the bean properties defined in the configuration file are injectedIf the bean definition in the configuration file contains the init-method attribute, then call this method after resolving the value for the attribute to a method name in the Bean classThe postProcessAfterInitialization() method will be called if there are any bean post processors attached to the BeanFactory interface that loads the bean Destruction This represents the following sequence of activities: If the Bean class implements the DisposableBean interface, then call the destroy() method when the application no longer needs the bean referenceIf the bean definition in the configuration file contains the destroy-method attribute, then call this method after resolving the value for the attribute to a method name in the Bean class. The InitializingBean interface has afterPropertiesSet(), which needs to be implemented, and it will be called by Spring when this bean is initialized and all properties are set. This InitializingBean interface is a marker for the bean to know that the afterPropertiesSet() method of this bean needs to be called after initialization. Using init-method in the XML configurationIn the case of XML-based configuration metadata, you can use the init-method attribute to specify the name of the method that has a void no-argument signature, which is to be called on the bean immediately upon instantiation. In the beans.xml file, you’ll find the following code: ……In the EmployeeServiceImpl.java class, you’ll find the following code:]]></content>
      <tags>
        <tag>java</tag>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Oracle]]></title>
    <url>%2F2017-03-13-oracle%2F</url>
    <content type="text"><![CDATA[ORA-12899: Value Too Large for Column 123456789101112131415161718192021222324252627SQL&gt; SELECT value$ FROM sys.props$ WHERE name = 'NLS_CHARACTERSET' ;VALUE$-------------------------------------------------------------------------AL32UTF8SELECT * FROM NLS_DATABASE_PARAMETERSselect user from dual;u43888859@hkl105482$ lsnrctl services DHKCUSTDLSNRCTL for Linux: Version 12.1.0.2.0 - Production on 28-FEB-2017 06:40:53Copyright (c) 1991, 2014, Oracle. All rights reserved.Connecting to (DESCRIPTION=(ADDRESS=(PROTOCOL=TCP)(HOST=hkl105482.hk.hsbc)(PORT=2001))(CONNECT_DATA=(SERVER=DEDICATED)(SERVICE_NAME=DHKCUSTD.hk.hsbc)))Services Summary...Service "DHKCUSTD.hk.hsbc" has 1 instance(s). Instance "DHKCUSTD", status READY, has 1 handler(s) for this service... Handler(s): "DEDICATED" established:15 refused:0 state:ready LOCAL SERVERThe command completed successfully]]></content>
      <tags>
        <tag>database</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AWS Tips]]></title>
    <url>%2F2017-03-21-aws%2F</url>
    <content type="text"><![CDATA[After establishing a SSH session, you can install a default web server by executing sudo yum install httpd -y. To start the web server, type sudo service httpd start and press Return to execute the command. Your web browser should show a placeholder site if you open http://$PublicIp with $PublicIp replaced by the public IP address of your virtual server.]]></content>
      <tags>
        <tag>AWS</tag>
        <tag>Cloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Setup WebSphere profiles and application in command line]]></title>
    <url>%2F2017-03-30-WebSphere-setp-via-CLI%2F</url>
    <content type="text"><![CDATA[Setup WebSphere profiles and application in command lineBackground &amp; descriptions Beginning with V8.5, WebSphere Application Server provides two runtime profiles. Every WebSphere Application Server package includes both profile types. Full WebSphere Application Server Liberty profile What’s profile? Simply put, a profile contains an Application Server.When an Application Server is running, the server process may read and write data to the underlying configuration files and logs. So, by using profiles, transient data is kept away from the base product. This allows us to have more than one profile using the same base binaries, and also allows us to remove certain profiles without affecting other profiles. Another reason for separating the base binaries is that we can upgrade the product with maintenance updates and fix packs without having to re-create all profiles. Sometimes you do not want a specific profile to be updated. WAS profile management has been designed for flexibility. WAS has the ability to have multiple application server definitions using the same underlying base binaries. Each profile defines the attributes and configurations for a given application server.Each standalone application server can optionally have its own administrative console application, which you use to manage the application server.We will cover how to install a profile later in the chapter. On distributed platforms, profiles are created after you install the product by using either the Profile Management Tool or the manageprofiles command. WAS ConceptsNodesA node is an administrative grouping of application servers for configuration and operational management within one operating system instance. You can create multiple nodes inside one operating system instance, but a node cannot leave the operating system boundaries. A stand-alone application server configuration has only one node. With Network Deployment, you can configure a distributed server environment that consists of multiple nodes that are managed from one central administration server. Node agentsIn distributed server configurations, each node has a node agent that works with the deployment manager to manage administration processes. A node agent is created automatically when you add (federate) a stand-alone application server node to a cell. Node agents are not included in the Base and Express configurations because a deployment manager is not needed in these architectures. The node agent is an administrative server that runs on the same system as the node. It monitors the application servers on that node, routing administrative requests from the deployment manager to those application servers. Node groupsA node group is a collection of nodes within a cell that have similar capabilities in terms of installed software, available resources, and configuration. A node group is used to define a boundary for server cluster formation so that the servers on the same node group host the same applications.A DefaultNodeGroup is created automatically. The DefaultNodeGroup contains the deployment manager and any new nodes with the same platform type. A node can be a member of more than one node group. CellsA cell is a grouping of nodes into a single administrative domain. A cell encompasses the entire management domain. In the Base and Express configurations, a cell contains one node, and that node contains one server. The left side of Figure 3-11 illustrates a system with two cells that are each accessed by their own administrative console. Each cell has a node and a stand-alone application server.In a Network Deployment environment (the right side of Figure 3-11), a cell can consist of multiple nodes and node groups. These nodes and groups are all administered from a single point, the deployment manager. Figure 3-11 shows a single cell that spans two systems that are accessed by a single administrative console. The deployment manager is administering the nodes. A cell configuration that contains nodes that are running on the same operating system is called a homogeneous cell.It is also possible to configure a cell that consists of nodes on mixed operating systems. With this configuration, other operating systems can exist in the same WebSphere Application Server cell. For example, z/OS nodes, Linux nodes, UNIX nodes, and Windows system nodes can exist in the same WebSphere Application Server cell. This configuration is called a heterogeneous cell. A heterogeneous cell requires significant planning. Noteworthy pointsTools/utilities create profile 1/opt/IBM/WebSphere85/AppServer/bin/manageprofiles.sh check server status 1/opt/IBM/WebSphere85/AppServer/bin/serverStatus.sh -all start server 1/opt/IBM/WebSphere85/AppServer/bin/startServer.sh SERVER_NAME start server 1/opt/IBM/WebSphere85/AppServer/bin/stopServer.sh dmgr deploy application 1sudo -u wasadm /opt/IBM/WebSphere85/utilities/API/v1.0/AppMgmt -deploy -deployMechanism simpleDeploy -appId xxx_war -appEnvId xxx -file "/tmp/xxx.ear" stop&amp;start application 12sudo -u wasadm /opt/IBM/WebSphere85/utilities/API/v1.0/AppMgmt -operate -appId xxx_war -appEnvId xxx -stopsudo -u wasadm /opt/IBM/WebSphere85/utilities/API/v1.0/AppMgmt -operate -appId xxx_war -appEnvId xxx -start To create profile1sudo -u wasadm /opt/IBM/WebSphere85/AppServer/bin/manageprofiles.sh -create -profileName TEST_PROFILE -profilePath /opt/IBM/WebSphere85/AppServer/profiles/appprofiles/TEST_PROFILE -templatePath /opt/IBM/WebSphere85/AppServer/profileTemplates/default -serverName testSrv01 -nodeName testNode01 -hostName testserver.com -enableAdminSecurity true -adminUserName wasadmin -adminPassword wasadmin@12 Errors &amp; troubleshootingServer can’t started after profile creation You may find following errors if you try to bring up server after profile created1234567sudo -u wasadm /opt/IBM/WebSphere85/AppServer/bin/startServer.sh TEST_PROFILEADMU0116I: Tool information is being logged in file /opt/IBM/WebSphere85/AppServer/profiles/dmgrprofile/logs/TEST_PROFILE/startServer.logADMU0128I: Starting tool with the dmgrprofile profileADMU3100I: Reading configuration for server: TEST_PROFILEADMU0111E: Program exiting with error: java.io.FileNotFoundException: /opt/IBM/WebSphere85/AppServer/profiles/dmgrprofile/config/cells/wascell/nodes/dmgrnode/servers/TEST_PROFILE/server.xml Solution: Please use the server name, rather than the profile name , e.g. should be testSrv01, rather than TEST_PROFILE The command should be: 1sudo -u wasadm /opt/IBM/WebSphere85/AppServer/bin/startServer.sh testSrv01 On the other hand, you can troubleshoot this issue by double checking the server.xml by 1ll /opt/IBM/WebSphere85/AppServer/profiles/appprofiles/TEST_PROFILE/config/cells/xxxxx/nodes/testNode01/servers/testSrv01/server.xml WebServer console can’t opened There are few reasones, such as:DNS errors If the error as below:This webpage is not availableThe server at testserver.com can’t be found, because the DNS lookup failed. DNS is the network service that translates a website’s name to its Internet address.Solutions:That’s maybe something wrong for DNS as stated above, so please try to either udpate hosts file in you workstation or repalce the URL by IP address, e.g.replace https://testserver.com:9045/ibm/console/login.do?action=secure with https://123.123.112.123:9045/ibm/console/login.do?action=secure What’s the port number for WAS consoleYou should get to know it after profile createdFor example, in aforesaid profile creation step, you’ll get following in output: 1INSTCONFSUCCESS: Success: Profile TEST_PROFILE now exists. Please consult /opt/IBM/WebSphere85/AppServer/profiles/TEST_PROFILE/logs/AboutThisProfile.txt for more information about this profile. To check content of this file, you’ll find following snipets: 12Administrative console port: 9060Administrative console secure port: 9043 Check the port number from serverindex.xml1grep -a2 WC_adminhost /opt/IBM/WebSphere85/AppServer/profiles/appprofiles/TEST_PROFILE/config/cells/xxxx/nodes/testNode01/serverindex.xml You’ll find the port number listed as below: 1234567891011 &lt;endPoint xmi:id="EndPoint_1183122129645" host="testserver.com" port="9407"/&gt; &lt;/specialEndpoints&gt; &lt;specialEndpoints xmi:id="NamedEndPoint_1183122129646" endPointName="WC_adminhost"&gt; &lt;endPoint xmi:id="EndPoint_1183122129646" host="*" port="9062"/&gt; &lt;/specialEndpoints&gt;-- &lt;endPoint xmi:id="EndPoint_1183122129648" host="*" port="9355"/&gt; &lt;/specialEndpoints&gt; &lt;specialEndpoints xmi:id="NamedEndPoint_1183122129649" endPointName="WC_adminhost_secure"&gt; &lt;endPoint xmi:id="EndPoint_1183122129649" host="*" port="9045"/&gt; &lt;/specialEndpoints&gt; Check networking &amp; port on WAS server12345678910111213141516171819root@cn000tst1129 Thu Mar 30 17:07:11 / #netstat -nptlev | grep javatcp 0 0 133.14.16.2:20962 0.0.0.0:* LISTEN 200000000 938007 252552/javatcp 0 0 127.0.0.1:9635 0.0.0.0:* LISTEN 200000000 1048485 254902/javatcp 0 0 0.0.0.0:9445 0.0.0.0:* LISTEN 200000000 1048615 254902/javatcp 0 0 0.0.0.0:9062 0.0.0.0:* LISTEN 200000000 1048613 254902/javatcp 0 0 0.0.0.0:9102 0.0.0.0:* LISTEN 200000000 1048479 254902/javatcp 0 0 0.0.0.0:8882 0.0.0.0:* LISTEN 200000000 1048484 254902/javatcp 0 0 0.0.0.0:9045 0.0.0.0:* LISTEN 200000000 1048616 254902/javatcp 0 0 0.0.0.0:20950 0.0.0.0:* LISTEN 200000000 938036 252552/javatcp 0 0 0.0.0.0:20953 0.0.0.0:* LISTEN 200000000 937999 252552/javatcp 0 0 0.0.0.0:9082 0.0.0.0:* LISTEN 200000000 1048614 254902/javatcp 0 0 0.0.0.0:20954 0.0.0.0:* LISTEN 200000000 938118 252552/javatcp 0 0 0.0.0.0:2811 0.0.0.0:* LISTEN 200000000 1048482 254902/javatcp 0 0 0.0.0.0:20957 0.0.0.0:* LISTEN 200000000 937992 252552/javatcp 0 0 0.0.0.0:9407 0.0.0.0:* LISTEN 200000000 1048481 254902/javatcp 0 0 0.0.0.0:20959 0.0.0.0:* LISTEN 200000000 938088 252552/javatcp 0 0 0.0.0.0:9408 0.0.0.0:* LISTEN 200000000 1048480 254902/javatcp 0 0 127.0.0.1:20960 0.0.0.0:* LISTEN 200000000 938089 252552/javatcp 0 0 133.14.16.2:20961 0.0.0.0:* LISTEN 200000000 938047 252552/java]]></content>
      <tags>
        <tag>WAS</tag>
        <tag>WebSphere</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SSH SFTP]]></title>
    <url>%2F2017-04-06-SSH%2F</url>
    <content type="text"><![CDATA[Secure FTP SFTP over FTP is the equivalant of HTTPS over HTTP, the security version SFTP versus FTPS An increasing number of our customers are looking to move away from standard FTP for transferring data, so we are often asked which secure FTP protocol we recommend. In the next few paragraphs, we will explain what options are available and their main differences. The two mainstream protocols available for Secure FTP transfers are named SFTP (FTP over SSH) and FTPS (FTP over SSL). Both SFTP and FTPS offer a high level of protection since they implement strong algorithms such as AES and Triple DES to encrypt any data transferred. Both options also support a wide variety of functionality with a broad command set for transferring and working with files. So the most notable differences between SFTP and FTPS is how connections are authenticated and managed. Authentication: SFTP vs. FTPS With SFTP (FTP over SSH), a connection can be authenticated using a couple different techniques. For basic authentication, you (or your trading partner) may just require a user id and password to connect to the SFTP server. Its important to note that any user ids and passwords supplied over the SFTP connection will be encrypted, which is a big advantage over standard FTP. SSH keys can also be used to authenticate SFTP connections in addition to, or instead of, passwords. With key-based authentication, you will first need to generate a SSH private key and public key beforehand. If you need to connect to a trading partner’s SFTP server, you would send your SSH public key to them, which they will load onto their server and associate with your account. When you connect to their SFTP server, your client software will transmit your public key to the server for authentication. If the keys match, along with any user/password supplied, then the authentication will succeed. With FTPS (FTP over SSL), a connection is authenticated using a user id, password and certificate(s). Like SFTP, the users and passwords for FTPS connections will also be encrypted. When connecting to a trading partner’s FTPS server, your FTPS client will first check if the server’s certificate is trusted. The certificate is considered trusted if either the certificate was signed off by a known certificate authority (CA), like Verisign, or if the certificate was self-signed (by your partner) and you have a copy of their public certificate in your trusted key store. Your partner may also require that you supply a certificate when you connect to them. Your certificate may be signed off by a 3rd party CA or your partner may allow you to just self-sign your certificate, as long as you send them the public portion of your certificate beforehand (which they will load in their trusted key store). Implementation: SFTP vs. FTPS In regards to how easy each of the secure FTP protocols are to implement, SFTP is the clear winner since it is very firewall friendly. SFTP only needs a single port number (default of 22) to be opened through the firewall. This port will be used for all SFTP communications, including the initial authentication, any commands issued, as well as any data transferred. On the other hand, FTPS can be very difficult to patch through a tightly secured firewall since FTPS uses multiple port numbers. The initial port number (default of 21) is used for authentication and passing any commands. However, every time a file transfer request (get, put) or directory listing request is made, another port number needs to be opened. You and your trading partners will therefore have to open a range of ports in your firewalls to allow for FTPS connections, which can be a security risk for your network. In summary, SFTP and FTPS are both very secure with strong authentication options. However since SFTP is much easier to port through firewalls, and we are seeing an increasing percentage of trading partners adopting SFTP, we believe SFTP is the clear winner for your secure FTP needs. SSH There are several ways to use SSH; one is to use automatically generated public-private key pairs to simply encrypt a network connection, and then use password authentication to log on. Another is to use a manually generated public-private key pair to perform the authentication, allowing users or programs to log in without having to specify a password. In this scenario, anyone can produce a matching pair of different keys (public and private). The public key is placed on all computers that must allow access to the owner of the matching private key (the owner keeps the private key secret). While authentication is based on the private key, the key itself is never transferred through the network during authentication. SSH only verifies whether the same person offering the public key also owns the matching private key. In all versions of SSH it is important to verify unknown public keys, i.e. associate the public keys with identities, before accepting them as valid. Accepting an attacker’s public key without validation will authorize an unauthorized attacker as a valid user. Key management On Unix-like systems, the list of authorized public keys is typically stored in the home directory of the user that is allowed to log in remotely, in the file ~/.ssh/authorized_keys. WinFTP cryptographic protocol is SSH-2 SSH implementation is OpenSSH_5.3 Server fingerprint: File transfer protocol = SFTP-3Cryptographic protocol = SSH-2SSH implementation = OpenSSH_5.3Encryption algorithm = aesCompression = No Server host key fingerprint ssh-rsa 2048 86:54:d9:09:25:c0:9b:f8:17:8c:c0:52:13:0c:9c:ccReferences https://www.goanywhere.com/blog/2011/10/20/sftp-ftps-secure-ftp-transfers https://en.wikipedia.org/wiki/Secure_Shell]]></content>
      <tags>
        <tag>SFTP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JDK source]]></title>
    <url>%2F2017-06-24-JDK-sources%2F</url>
    <content type="text"><![CDATA[interface RandomAccessMarker interface used by List implementations to indicate that they support fast (generally constant time) random access. The primary purpose of this interface is to allow generic algorithms to alter their behavior to provide good performance when applied to either random or sequential access lists. Such a List implementation should generally implement this interface. As a rule of thumb, a List implementation should implement this interface if, for typical instances of the class, this loop: 123456 for (int i=0, n=list.size(); i &lt; n; i++) list.get(i); // runs faster than this loop: for (Iterator i=list.iterator(); i.hasNext(); ) i.next();]]></content>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[google analysis]]></title>
    <url>%2F2017-09-04-Google-Analytics%2F</url>
    <content type="text"><![CDATA[How Page Value is calculatedAt a glancePage Value is the average value for a page that a user visited before landing on the goal page or completing an Ecommerce transaction (or both). This value is intended to give you an idea of which page in your site contributed more to your site’s revenue. If the page wasn’t involved in an ecommerce transaction for your website in any way, then the Page Value for that page will be $0 since the page was never visited in a session where a transaction occurred. Below is the equation you can follow to calculate Page Value. Please note that the unique pageview statistic represents the number of individual users who have loaded a given page per session. Each user is counted only once per session, no matter how many pages are opened by the same user. Ecommerce Revenue + Total Goal ValueNumber of Unique Pageviews for Given Page In depthThe first example above illustrates how Page Value works. Let’s say you want to know the Page Value for Page B, and you know the following factors: Goal page D: $10 (Remember, you assign the value of the Goal page when you first create a goal in the Analytics Settings page)Receipt Page E: $100 (This page is where the user makes an ecommerce transaction of $100)Unique pageview for Page B: One You would then set up your Page Value equation like this: Ecommerce Revenue ($100) + Total Goal Value ($10)Number of Unique Pageviews for Page B (1) Page Value for Page B is $110 since a user visits Page B only once before the goal page during this session. Now let’s explore how Page Value for Page B is affected when we combine the data from two different sessions. You can see that Page B is viewed only once during Session 1, but during Session 2 it gets two pageviews (we’re assuming the two pageviews are from the same user). The total Ecommerce revenue stays the same during both sessions. Although there were two unique pageviews, there was still only one Ecommerce transaction total for both sessions. Goal page D: $10Receipt Page E: $100Unique pageview for Page B: Two Your Page Value calculation should be adjusted to look like this: Ecommerce Revenue ($100) + Total Goal Value ($10 x 2 sessions)Number of Unique Pageviews for Page B (2) Page Value for Page B across two sessions is then $60, or $120 divided by two sessions. Code snippet about setting up GA in your site1234567891011121314151617&lt;script&gt; window.ga=window.ga||function()&#123;(ga.q=ga.q||[]).push(arguments)&#125;;ga.l=+new Date; ga('create', 'UA-xxxx-1', 'auto'); var hostName = window.location.host; var ga_script = document.createElement('script'); ga_script.setAttribute('async',true); if (['localhost:9000','test','uat'].some(function(hostItem)&#123;return hostName.indexOf(hostItem)&gt;-1&#125;)) &#123; // non-production ga_script.setAttribute('src','https://www.google-analytics.com/analytics_debug.js'); window.ga_debug = &#123;trace: true&#125;; &#125;else &#123; ga_script.setAttribute('src','https://www.google-analytics.com/analytics.js'); &#125; document.head.appendChild(ga_script); ga('send', 'pageview'); &lt;/script&gt; 1234$rootScope.$on('$stateChangeSuccess', function() &#123; $window.ga('send', 'pageview', $location.path()); $window.scrollTo(0, 0); &#125;); Alternative async tracking snippet While the JavaScript tracking snippet described above ensures the script will be loaded and executed asynchronously on all browsers, it has the disadvantage of not allowing modern browsers to preload the script. The alternative async tracking snippet below adds support for preloading, which will provide a small performance boost on modern browsers, but can degrade to synchronous loading and execution on IE 9 and older mobile browsers that do not recognize the async script attribute. Only use this tracking snippet if your visitors primarily use modern browsers to access your site. &lt;!-- Google Analytics --&gt; &lt;script&gt; window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date; ga(&apos;create&apos;, &apos;UA-XXXXX-Y&apos;, &apos;auto&apos;); ga(&apos;send&apos;, &apos;pageview&apos;); &lt;/script&gt; &lt;script async src=&apos;https://www.google-analytics.com/analytics.js&apos;&gt;&lt;/script&gt; &lt;!-- End Google Analytics --&gt; From &lt;https://developers.google.com/analytics/devguides/collection/analyticsjs/&gt; What data does the tracking snippet capture? When you add either of these tracking snippets to your website, you send a pageview for each page your users visit. Google Analytics processes this data and can infer a great deal of information including: • The total time a user spends on your site. • The time a user spends on each page and in what order those pages were visited. • What internal links were clicked (based on the URL of the next pageview). In addition, the IP address, user agent string, and initial page inspection analytics.js does when creating a new tracker is used to determine things like the following: • The geographic location of the user. • What browser and operating system are being used. • Screen size and whether Flash or Java is installed. • The referring site. From &lt;https://developers.google.com/analytics/devguides/collection/analyticsjs/#alternative_async_tracking_snippet&gt; How analytics.js Works From &lt;https://developers.google.com/analytics/devguides/collection/analyticsjs/how-analyticsjs-works&gt; The ga command queue The JavaScript tracking snippet defines a global ga function known as the &quot;command queue&quot;. It&apos;s called the command queue because rather than executing the commands it receives immediately, it adds them to a queue that delays execution until the analytics.js library is fully loaded. In JavaScript, functions are also objects, which means they can contain properties. The tracking snippet defines a q property on the ga function object as an empty array. Prior to the analytics.js library being loaded, calling the ga() function appends the list of arguments passed to the ga()function to the end of the q array. For example, if you were to run the tracking snippet and then immediately log the contents of ga.qto the console, you&apos;d see an array, two items in length, containing the two sets of arguments already passed to the ga() function: console.log(ga.q); // Outputs the following: // [ // [&apos;create&apos;, &apos;UA-XXXXX-Y&apos;, &apos;auto&apos;], // [&apos;send&apos;, &apos;pageview&apos;] // ] Once the analytics.js library is loaded, it inspects the contents of the ga.q array and executes each command in order. After that, the ga() function is redefined, so all subsequent calls execute immediately. This pattern allows developers to use the ga() command queue without having to worry about whether or not the analytics.js library has finished loading. It provides a simple, synchronous-looking interface that abstracts away most of the complexities of asynchronous code. From &lt;https://developers.google.com/analytics/devguides/collection/analyticsjs/how-analyticsjs-works&gt; Adding commands to the queue All calls to the ga() command queue share a common signature. The first parameter, the &quot;command&quot;, is a string that identifies a particular analytics.js method. Any additional parameters are the arguments that get passed to that method. The method a particular command refers to can be a global method, like create, a method on the ga object, or it can be an instance method on a tracker object, like send. If the ga() command queue receives a command it doesn&apos;t recognize, it simply ignores it, making calls to the ga()function very safe, as they will almost never result in an error. For a comprehensive list of all commands that can be executed via the command queue, see the ga() command queue reference. From &lt;https://developers.google.com/analytics/devguides/collection/analyticsjs/how-analyticsjs-works&gt; Command parameters Most analytics.js commands (and their corresponding methods) accept parameters in a number of different formats. This is done as a convenience to make it easier to pass commonly used fields to certain methods. As an example, consider the two commands in the JavaScript tracking snippet: ga(&apos;create&apos;, &apos;UA-XXXXX-Y&apos;, &apos;auto&apos;); ga(&apos;send&apos;, &apos;pageview&apos;); In the first command, create accepts the fields trackingId, cookieDomain, and name to optionally be specified as the second, third, and fourth parameters, respectively. The sendcommand accepts an optional hitType second parameter. All commands accept a final fieldsObject parameter that can be used to specify any fields as well. For example, the above two commands in the tracking snippet could be rewritten as: ga(&apos;create&apos;, { trackingId: &apos;UA-XXXXX-Y&apos;, cookieDomain: &apos;auto&apos; }); ga(&apos;send&apos;, { hitType: &apos;pageview&apos; }); See the ga() command queue reference for a comprehensive list of the optional parameters allowed for each of the commands. From &lt;https://developers.google.com/analytics/devguides/collection/analyticsjs/how-analyticsjs-works&gt; Creating Trackers • Contents • The create method • Naming trackers • Specifying fields at creation time • Working with multiple trackers • Running commands for a specific tracker • Next steps Tracker objects (also known as &quot;trackers&quot;) are objects that can collect and store data and then send that data to Google Analytics. When creating a new tracker, you must specify a tracking ID (which is the same as the property ID that corresponds to one of your Google Analytics properties) as well as a cookie domain, which specifies how cookies are stored. (The recommended value &apos;auto&apos; specifies automatic cookie domain configuration.) If a cookie does not exist for the specified domain, a client ID is generated and stored in the cookie, and the user is identified as new. If a cookie exists containing a client ID value, that client ID is set on the tracker, and the user is identified as returning. Upon creation, tracker objects also gather information about the current browsing context such as the page title and URL, and information about the device such as screen resolution, viewport size, and document encoding. When it&apos;s time to send data to Google Analytics, all of the information currently stored on the tracker gets sent. From &lt;https://developers.google.com/analytics/devguides/collection/analyticsjs/creating-trackers&gt; Running commands for a specific tracker To run analytics.js commands for a specific tracker, you prefix the command name with the tracker name, followed by a dot. When you don&apos;t specify a tracker name, the command is run on the default tracker. To send pageviews for the above two trackers, you&apos;d run the following two commands: ga(&apos;send&apos;, &apos;pageview&apos;); ga(&apos;clientTracker.send&apos;, &apos;pageview&apos;); Future guides will go into more detail on the syntax for running specific commands. You can also refer to the command queue reference to see the full command syntax for all analytics.js commands. From &lt;https://developers.google.com/analytics/devguides/collection/analyticsjs/creating-trackers&gt; Getting trackers via ga Object methods If you&apos;re not using a default tracker, or if you have more than one tracker on the page, you can access those trackers via one of the ga object methods. Once the analytics.js library is fully loaded, it adds additional methods to the ga object itself. Two of those methods, getByName and getAll, are used to access tracker objects. Note: ga object methods are only available when analytics.js has fully loaded, so you should only reference them inside a ready callback. getByName If you know the name of the tracker you want to access, you can do so using the getByNamemethod: ga(&apos;create&apos;, &apos;UA-XXXXX-Y&apos;, &apos;auto&apos;, &apos;myTracker&apos;); ga(function() { // Logs the &quot;myTracker&quot; tracker object to the console. console.log(ga.getByName(&apos;myTracker&apos;)); }); From &lt;https://developers.google.com/analytics/devguides/collection/analyticsjs/accessing-trackers&gt; The last line of the JavaScript tracking snippet adds a send command to the ga() command queue to send a pageview to Google Analytics: ga(&apos;create&apos;, &apos;UA-XXXXX-Y&apos;, &apos;auto&apos;); ga(&apos;send&apos;, &apos;pageview&apos;); The object that is doing the sending is the tracker that was scheduled for creation in the previous line of code, and the data that gets sent is the data stored on that tracker. This guide describes the various ways to send data to Google Analytics and explains how to control what data gets sent. From &lt;https://developers.google.com/analytics/devguides/collection/analyticsjs/sending-hits&gt; Hits, hit types, and the Measurement Protocol When a tracker sends data to Google Analytics it&apos;s called sending a hit, and every hit must have a hit type. The JavaScript tracking snippet sends a hit of type pageview; other hit types include screenview, event, transaction, item, social, exception, and timing. This guide outlines the concepts and methods common to all hit types. Individual guides for each hit type can be found under the section Tracking common user interactions in the left-side navigation. The hit is an HTTP request, consisting of field and value pairs encoded as a query string, and sent to the Measurement Protocol. From &lt;https://developers.google.com/analytics/devguides/collection/analyticsjs/sending-hits&gt; The simplest way to use the send command, that works for all hit types, is to pass all fields using the fieldsObjectparameter. For example: ga(&apos;send&apos;, { hitType: &apos;event&apos;, eventCategory: &apos;Video&apos;, eventAction: &apos;play&apos;, eventLabel: &apos;cats.mp4&apos; }); For convenience, certain hit types allow commonly used fields to be passed directly as arguments to the sendcommand. For example, the above send command for the &quot;event&quot; hit type could be rewritten as: ga(&apos;send&apos;, &apos;event&apos;, &apos;Video&apos;, &apos;play&apos;, &apos;cats.mp4&apos;); For a complete list of what fields can be passed as arguments for the various hit types, see the &quot;parameters&quot; section of the send method reference. From &lt;https://developers.google.com/analytics/devguides/collection/analyticsjs/sending-hits&gt;]]></content>
  </entry>
  <entry>
    <title><![CDATA[TypeScript noteworthy notes]]></title>
    <url>%2F2017-11-27-TypeScript%2F</url>
    <content type="text"><![CDATA[Async Await keywordsAsync Await Support in TypeScriptAsync - Await has been supported by TypeScript since version 1.7. Asynchronous functions are prefixed with the async keyword; await suspends the execution until an asynchronous function return promise is fulfilled and unwraps the value from the Promise returned. It was only supported for target es6 transpiling directly to ES6 generators. TroubleshottingUnexpected token …That’s because your node version is lower (e.g. node v4.x), which don’t support spread operator. You’d firstly check your node version 1node -v If above result say node is v4.x, then you should run following commands to upgrade your node. Normally you can leverage Node Package Manager n as below: 12sudo npm install -g nsudo n stable]]></content>
      <tags>
        <tag>JavaScript</tag>
        <tag>Angular</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Openshift tips]]></title>
    <url>%2F2017-11-29-OpenShift%2F</url>
    <content type="text"><![CDATA[Commands bibleinstall cli in Mac1brew install openshift-cli Frequently used commandsStart mini shift1minishift start OC commands1oc config view Start a new application1oc new-app https://github.com/openshift/nodejs-ex -l name=toddapp To switch project1oc project Show a high level overview of the current project123456oc status# Export the overview of the current project in an svg file. oc status -o dot | dot -T svg -o project.svg # See an overview of the current project including details for any identified issues. oc status -v This command will show services, deployment configs, build configurations, and active deployments.If you have any misconfigured components information about them will be shown. For more informationabout individual items, use the describe command (e.g. oc describe buildConfig, oc describedeploymentConfig, oc describe service). You can specify an output format of “-o dot” to have this command output the generated status graphin DOT format that is suitable for use by the “dot” command. OpenShift command-line toolThe OpenShift command-line tool oc is the primary way most users interact with OpenShift. The command-line tool talks via a REST API exposed by the OpenShift cluster. PodThe most basic unit in OpenShift are pods. A pod is one or more containers guaranteed to be running on the same host. The containers within a pod share a unique IP address. They can communicate with each other via the “localhost” and also all share any volumes (persistent storage). The containers themselves are started from an image, which in our case is a Docker image. OpenShift Origin leverages the Kubernetes concept of a pod, which is one or more containers deployed together on one host, and the smallest compute unit that can be defined, deployed, and managed. Pods are the rough equivalent of a machine instance (physical or virtual) to a container. Each pod is allocated its own internal IP address, therefore owning its entire port space, and containers within pods can share their local storage and networking. Pods have a lifecycle; they are defined, then they are assigned to run on a node, then they run until their container(s) exit or they are removed for some other reason. Pods, depending on policy and exit code, may be removed after exiting, or may be retained in order to enable access to the logs of their containers. Pods have a lifecycle; they are defined, then they are assigned to run on a node, then they run until their container(s) exit or they are removed for some other reason. Pods, depending on policy and exit code, may be removed after exiting, or may be retained in order to enable access to the logs of their containers. OpenShift Origin treats pods as largely immutable; changes cannot be made to a pod definition while it is running. OpenShift Origin implements changes by terminating an existing pod and recreating it with modified configuration, base image(s), or both. Pods are also treated as expendable, and do not maintain state when recreated. Therefore pods should usually be managed by higher-level controllers, rather than directly by users. Scale upWhen scaled up, an application will have more than one copy of itself, and each copy will have its own local state. Each copy corresponds to a different instance of a pod with the pods being managed by the replication controller. As each pod has a unique IP, we need an easy way to address the set of all pods as a whole. This is where a service comes into play. The service gets its own IP and a DNS name. When making a connection to a service, OpenShift will automatically route the connection to one of the pods associated with that service. caling from the Web ConsoleScaling up the number of instances of your application running can be done from the Overview page for your application in the OpenShift web console (the page with those tell-tale up and down arrows we saw previously). Jump to that page and click the up arrow key twice to increase the replica count to 3If your application is a web application that adheres to the 12-factor methodology, or what might also be called a cloud native application, then it would generally be safe to scale up. Applications that can’t usually be able to be scaled up include traditional relational databases backed by persistent storage. Databases cannot be scaled in the traditional way as only the primary instance of the database should have the ability to update data. Scaling can still be performed, but usually only on read-only instances of the database. Kubernetes and OpenshiftThe basic concepts of Kubernetes and discuss how OpenShift builds on them. In general, you can view Kubernetes as being aimed at Ops teams, providing them with a tool for running containers at scale in production. OpenShift adds to this by also supporting the work of a Dev team and others by making the job of the Ops team easier, which helps to bridge the gap between Dev and Ops and thus enable the latest DevOps philosophy.OpenShift provides a number of different ways to interact with an OpenShift cluster. The OpenShift command-line tool oc is the primary way most users interact with OpenShift. The command-line tool talks via a REST API exposed by the OpenShift cluster.If you want to avoid using the command line tool, or you want to automate your interactions with the OpenShift cluster, you can always use the REST API directly. You may be wondering: “Is a namespace the same thing as an application?” OpenShift has no formal concept of an application, thereby allowing an application to be flexible depending on a user’s needs. You can dedicate one to everything related to just one application. Or, so long as you label all the resources making up an application so you know what goes with what, you can also use the namespace to hold more than one. SecretsThis topic discusses important properties of secrets and provides an overview on how developers can use them. The Secret object type provides a mechanism to hold sensitive information such as passwords, OpenShift Origin client configuration files, dockercfg files, private source repository credentials, and so on. Secrets decouple sensitive content from the pods. You can mount secrets into containers using a volume plug-in or the system can use secrets to perform actions on behalf of a pod. RouteAlthough a service has a DNS name, it is still only accessible within the OpenShift cluster and is not accessible externally. To make a service externally accessible, a route needs to be created. Creating a route automatically sets up haproxy or a hardware-based router, with an externally addressable DNS name, exposing the service and load-balancing inbound traffic across the pods. imageThe output of the build process is an image, which is stored in an integrated Docker registry ready for distribution out to nodes when the application is deployed. The image stream is how the image and its versions are tracked by OpenShift. If you already have an existing Docker image on an external registry such as Docker Hub, it can also be referenced by an image stream instead of building it locally. use the handy online cheat sheet available by running the oc types command. It gives you a quick summary of the different conceptual types and definitions used in OpenShift like those we covered here: VagrantVagrant is a software tool that allows users to create and configure lightweight, reproducible, and portable development environments. It works in conjunction with virtualization (both VMs and IaaS) to automate all the steps necessary to get your dev environment going One of the advantages of creating and deploying applications from the web console is that a route is automatically created for you. When deploying new containers while using the oc tool, you will need to expose the service manually VolumesOne of the great features of the OpenShift platform is the ability to provide persistent volumes for your running pods. This ensures that data in your database doesn’t suddenly disappear if the container is restarted. Another important aspect of persistent volumes is the ability to run both stateful and stateless applications on the platform. This is sometimes referred to as mode 1 and mode 2 applications as well as legacy and 12-factor applications. webhooksAutomatic Deployments Using WebhooksA webhook (also called a web callback or HTTP push API) is a way an application can provide other applications with real-time information or notifications. We can configure the GitHub code hosting service to trigger a webhook each time we push a set of changes to your project code repository. Using this tool, we can notify OpenShift when you have made code changes and thus initiate rebuild and redeployment of our application. Deployment StrategiesA deployment strategy defines the process by which a new version of your application is started and the existing instances shut down. By default OpenShift uses a rolling deployment strategy that enables you to perform an update with no apparent down time.]]></content>
      <tags>
        <tag>DevOps</tag>
        <tag>cloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[promise vs observiable]]></title>
    <url>%2F2017-12-01-Promise-vs-Observable%2F</url>
    <content type="text"><![CDATA[The drawback of using Promises is that they’re unable to handle data sources that produce more than one value, like mouse movements or sequences of bytes in a file stream. Also, they lack the ability to retry from failure—all present in RxJS. The most important downside, moreover, is that because Promises are immutable, they can’t be cancelled. So, for instance, if you use a Promise to wrap the value of a remote HTTP call, there’s no hook or mechanism for you to cancel that work. This is unfortunate because HTTP calls, based on the XmlHttpRequest object, can be aborted,[3] but this feature isn’t honored through the Promise interface]]></content>
      <tags>
        <tag>JavaScript</tag>
        <tag>Angular</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Container]]></title>
    <url>%2F2017-12-02-Container%2F</url>
    <content type="text"><![CDATA[The Docker project was responsible for popularizing container development in Linux systems. The original project defined a command and service (both named docker) and a format in which containers are structured. This chapter provides a hands-on approach to using the docker command and service to begin working with containers in Red Hat Enterprise Linux 7 and RHEL Atomic Host by getting and using container images and working with running containers. Containers provide a means of packaging applications in lightweight, portable entities. Running applications within containers offers the following advantages: Smaller than Virtual Machines: Because container images include only the content needed to run an application, saving and sharing is much more efficient with containers than it is with virtual machines (which include entire operating systems) Improved performance: Likewise, since you are not running an entirely separate operating system, a container will typically run faster than an application that carries with it the overhead of a whole new virtual machine. Secure: Because a container typically has its own network interfaces, file system, and memory, the application running in that container can be isolated and secured from other activities on a host computer. Flexible: With an application’s run time requirements included with the application in the container, a container is capable of being run in multiple environments. RHEL Atomic Host is a light-weight Linux operating system distribution that was designed specifically for running containers. It contains two different versions of the docker service, as well as some services that can be used to orchestrate and manage Docker containers, such as Kubernetes. Only one version of the docker service can be running at a time. ImagesContainers in OpenShift Origin are based on Docker-formatted container images. An image is a binary that includes all of the requirements for running a single container, as well as metadata describing its needs and capabilities. You can think of it as a packaging technology. Containers only have access to resources defined in the image unless you give the container additional access when creating it. By deploying the same image in multiple containers across multiple hosts and load balancing between them, OpenShift Origin can provide redundancy and horizontal scaling for a service packaged into an image. Container RegistriesA container registry is a service for storing and retrieving Docker-formatted container images. A registry contains a collection of one or more image repositories. Each image repository contains one or more tagged images. Docker provides its own registry, the Docker Hub, and you can also use private or third-party registries. Red Hat provides a registry at registry.access.redhat.com for subscribers. OpenShift Origin can also supply its own internal registry for managing custom container images. Reference https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux_atomic_host/7/single/getting_started_with_containers/index#introduction_to_linux_containers]]></content>
      <tags>
        <tag>DevOps</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[common errors in NPM or node]]></title>
    <url>%2F2017-12-07-Errors-In-NPM%2F</url>
    <content type="text"><![CDATA[code E503code E503 when run npm install packages, e.g. 1npm install pretty-data Get following error:` npm ERR! code E503npm ERR! 503 Service Unavailable: pretty-data@latest npm ERR! A complete log of this run can be found in:npm ERR! xxxx\nodejs\npm-cache_logs\2017-12-07T04_16_53_679Z-debug.log Solution:You maybe behind corporate proxy, so try execute following command 1npm config set proxy http://127.0.0.1:53128]]></content>
      <tags>
        <tag>nodejs</tag>
        <tag>javascript</tag>
        <tag>anugar</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RxJS reactive extension javascript]]></title>
    <url>%2F2017-12-08-Router-In-Angular%2F</url>
    <content type="text"><![CDATA[StreamsTraditionally, the term stream was used in programming languages as an abstract object related to I/O operations such as reading a file, reading a socket, or requesting data from an HTTP server. For instance, Node.js implements readable, writable, and duplex streams for doing just this. In the RP world, we expand the definition of a stream to mean any data source that can be consumed. 123456A$ = [20]; 1B$ = [22]; 2C$ = A$.concat(B$).reduce(adder); //-&gt; [42] 3A$.push(100); 4C$ = ? 1 Creates a stream initialized with the value 202 Creates a stream initialized with the value 223 Concatenates both streams and applies an adder function to get a new container with 424 Pushes a new value into A$First, we’ll explain some of the notation we use here. Streams are containers or wrappers of data very similar to arrays, so we used the array literal notation [] to symbolize this. Also, it’s common to use the $ suffix to qualify variables that point to streams. In the RxJS community, this is known as Finnish Notation, attributed to Andre Staltz, who is one of the main contributors of RxJS and Finnish. Array extrasJavaScript ES5 introduced new array methods, known as the array extras, which enable some level of native support for FP. These include map, reduce, filter, some, every, and othersReactive programming is oriented around data flows and propagation. In this case, you can think of C$ as an always-on variable that reacts to any change and causes actions to ripple through it when any constituent part changes. Now let’s see how RxJS implements this concept. If you were to visit the main website for the Reactive Extensions project (http://reactivex.io/), you’d find it defined as “an API for asynchronous programming with observable streams.” Definition A stream is nothing more than a sequence of events over time.Everything is a streamThe concept of a stream can be applied to any data point that holds a value; this ranges from a single integer to bytes of data received from a remote HTTP call. RxJS provides lightweight data types to subscribe to and manage streams as a whole that can be passed around as first-class objects and combined with other streams.RxJS provides lightweight data types to subscribe to and manage streams as a whole that can be passed around as first-class objects and combined with other streams. Learning how to manipulate and use streams is one of the central topics of this book. At this point, we haven’t talked about any specific RxJS objects; for now, we’ll assume that an abstract data type, a container called Stream, exists. You can create one from a single value as such: Stream(42);At this point, this stream remains dormant and nothing has actually happened, until there’s a subscriber (or observer) that listens for it. This is very different from Promises, which execute their operations as soon as they’re created. Instead, streams are lazy data types, which means that they execute only after a subscriber is attached. In this case, the value 42, which was lifted into the stream context, navigates or propagates out to at least one subscriber. After it receives the value, the stream is completed: 12345Stream(42).subscribe( val =&gt; &#123; 1 console.log(val); //-&gt; prints 42 &#125;); This creates two important challenges: scalability and latency. As more and more data is received, the amount of memory that your application consumes or requires will grow linearly or, in worst cases, exponentially; this is the classic problem of scalability, and trying to process it all at once will certainly cause the user interface (UI) to become unresponsive. Buttons may no longer appear to work, fancy animations will lag, and the browser may even flag the page to terminate, which is an unacceptable notion for modern web users. This problem is not new, though in recent years there has been exponential growth in the sheer scale of the number of events and data that JavaScript applications are required to process. This quantity of data is too big to be held readily available and stored in memory for use. Instead, we must create ways to fetch it from remote locations asynchronously, resulting in another big challenge of interconnected software systems: latency, which can be difficult to express in code.you’ll first learn about the fundamental principles of two emerging paradigms: functional programming (FP) and reactive programming (RP). This exhilarating composition is what gives rise to functional reactive programming (FRP), encoded in a library called RxJS (or rx.js), which is the best prescription to deal with asynchronous and event-based data sources effectively.By subscribing to a stream, your code expresses an interest in receiving the stream’s elements. During subscription, you specify the code to be invoked when the next element is emitted, and optionally the code for error processing and stream completion. Often you’ll specify a number of chained operators and then invoke the subscribe() method The subscribe() method creates the instance of Observer, which in this case passes each value from the stream generated by the searchInput to the getStockQuoteFromServer() method. In a real-world scenario, this method would issue a request to the server,No matter how many operators you chain together, none of them will be invoked on the stream until you invoke subscribe().If you prefer to generate an observable stream based on another event (such as on keyup), you can use the RxJS Observable.fromEvent() API (see the RxJSOne of the benefits of observables over promises is that the former can be canceled.Observable1 → switchMap(function) → Observable2 → subscribe() You’re switching over from the first observable to the second one. If Observable1 pushes the new value but the function that creates Observable2 hasn’t finished yet, it’s killed; switchMap() unsubscribes and resubscribes to Observable1 and starts handling the new value from this stream. If the observable stream from the UI pushes the next value before getWeather() has returned its observable value, switchMap() kills the running getWeather(), gets the new value for the city from the UI, and invokes getWeather() again. While killing getWeather(), it also aborts the HTTP request that was slow and didn’t complete in time. The first argument of subscribe() contains a callback for handling data coming from the server. The code in this arrow expression is specific to the API provided by the weather service. You just extract the temperature and humidity from the returned JSON. The API offered by this particular weather service stores the error codes in the response, so you manually handle the status 404 here and not in the error-handler callback. Now let’s verify that canceling previous requests works. Typing the word London takes more than the 200 milliseconds specified in debounceTime(), which means the valueChanges event will emit the observable data more than once. To ensure that the request to the server takes more than 200 milliseconds, you need a slow internet connection. Note Listing 5.5 has lots of code in the constructor, which may look like a red flag to developers who prefer using constructors only to initialize variables and not to execute any code that takes time to complete. If you take a closer look, though, you’ll notice that it just creates a subscription to two observable streams (UI events and HTTP service). No actual processing is done until the user starts entering the name of a city, which happens after the component is already rendered. We ran the preceding example and then turned on throttling in Chrome Developer Tools, emulating a slow GPRS connection. Typing the word London resulted in four getWeather() invocations: for Lo, Lon, Lond, and London. Accordingly, four HTTP requests were sent over the slow connection, and three of them were automatically canceled by the switchMap() operator, as shown in figure 5.10. Figure 5.10. Running observable_events_http.ts With very little programming, you saved bandwidth by eliminating the need for the server to send four HTTP responses for cities you’re not interested in and that may not even exist. As we stated in chapter 1, a good framework is one that allows you to write less code.Angular comes with a number of predefined pipes, and each pipe has a class that implements its functionality (such as DatePipe) as well as the name you can use in the template (such as date): UpperCasePipe allows you to convert an input string into uppercase by using | uppercase in the template.DatePipe lets you display a date in different formats by using | date.CurrencyPipe transforms a number into a desired currency by using | currency.AsyncPipe will unwrap the data from the provided observable stream by using | async. You’ll see a code sample that uses async in chapter 8.Some pipes don’t require input parameters (such as uppercase), and some do (such as date:’medium’). You can chain as many pipes as you want. The next code snippet shows how to display the value of the birthday variable in a medium date format and in uppercase (for example, JUN 15, 2001, 9:43:11 PM): Custom pipesIn addition to predefined pipes, Angular offers a simple way to create custom pipes, which can include code specific to your application. You need to create a @Pipe annotated class that implements the PipeTransform interface. The PipeTransform interface has the following signature: export interface PipeTransform { transform(value: any, …args: any[]): any;} This tells you that a custom pipe class must implement just one method with the preceding signature. The first parameter of transform takes a value to be transformed, and the second defines zero or more parameters required for your transformation algorithm. The @Pipe annotation is where you specify the name of the pipe to be used in the template. If your component uses custom pipes, they have to be explicitly listed in its @Component annotation in the pipes property. In the previous section, the weather example displayed the temperature in London in Fahrenheit. But most countries use the metric system and show temperature in Celsius. Let’s create a custom pipe that can convert the temperature from Fahrenheit to Celsius and back. The code of the custom TemperaturePipe pipe (see the following listing) can be used in a template as temperature. Listing 5.6. temperature-pipe.ts Next comes the code of the component (pipe-tester.ts) that uses the temperature pipe. Initially this program will convert the temperature from Fahrenheit to Celsius (the FtoC format). By clicking the toggle button, you can change the direction of the temperature conversionEvent emittersEvent emitters are popular mechanisms for asynchronous event-based architectures. The DOM, for instance, is probably one of the most widely known event emitters. On a server like Node.js, certain kinds of objects periodically produce events that cause functions to be called. In Node.js, the EventEmitter class is used to implement APIs for things like WebSocket I/O or file reading/writing so that if you’re iterating through directories and you find a file of interest, an object can emit an event referencing this file for you to execute any additional code.ajax(‘/items’, items =&gt; { for (let item of items) { ajax(&lt;host2&gt;/items/${item.getId()}/info, dataInfo =&gt; { ajax(&lt;host3&gt;/files/${dataInfo.files}, processFiles); }); }});—is known to be continuation-passing style (CPS), because none of the functions are explicitly waiting for a return value. But as we mentioned, abusing this makes code hard to reason about. What you can do is to make continuations first-class citizens and actually define a concrete interpretation of what it means to “continue.” So, we introduce the notion of then: “Do X, then do Y,” to create code that reads like this: Fetch all items, then 1 For-each item fetch all files, then 1 Process each file1 The key term “then” suggests time and sequence.This is where Promises come in. A Promise is a data type that wraps an asynchronous or long-running operation, a future value, with the ability for you to subscribe to its result or its error. A Promise is considered to be fulfilled when its underlying operation completes, at which point subscribers will receive the computed result. Because we can’t alter the value of a Promise once it’s been executed, it’s actually an immutable type, which is a functional quality we seek in our programs.ajax(‘/items’) .then(items =&gt; items.forEach(item =&gt; ajax(&lt;host2&gt;/data/${item.getId()}/info) .then(dataInfo =&gt; ajax(&lt;host3&gt;/data/files/${dataInfo.files}) ) .then(processFiles); ) );This looks similar to the previous statement! Being a more recent addition to the language with ES6 and inspired in FP design, Promises are more versatile and idiomatic than callbacks. Applying these functions declaratively—meaning your code expresses the what and not the how of what you’re trying to accomplish—into then blocks allows you to express side effects in a pure manner. let getItems = () =&gt; ajax(‘/items’);let getInfo = item =&gt; ajax(&lt;host2&gt;/data/${item.getId()}/info);let getFiles = dataInfo =&gt; ajax(&lt;host3&gt;/data/files/${dataInfo.files});and then use Promises to stitch together our asynchronous flow. We use the Promise.all() function to map an array of separate Promises into a single one containing an array of results: getItems() .then(items =&gt; items.map(getInfo)) .then(promises =&gt; Promise.all(promises)) .then(infos =&gt; infos.map(getFiles)) .then(promises =&gt; Promise.all(promises)) .then(processFiles);The use of then() explicitly implies that there’s time involved among these calls, which is a really good thing. If any step fails, we can also have matching catch() blocks to handle errors and potentially continue the chain of command if necessary, a Figure 1.7. Promises create a flow of calls chained by then methods. If the Promise is fulfilled, the chain of functions continues; otherwise, the error is delegated to the Promise catch block. The drawback of using Promises is that they’re unable to handle data sources that produce more than one value, like mouse movements or sequences of bytes in a file stream. Also, they lack the ability to retry from failure—all present in RxJS.The most important downside, moreover, is that because Promises are immutable, they can’t be cancelled. So, for instance, if you use a Promise to wrap the value of a remote HTTP call, there’s no hook or mechanism for you to cancel that work. This is unfortunate because HTTP calls, based on the XmlHttpRequest object, can be aborted,[3] but this feature isn’t honored through the Promise interface. It’s difficult to detect when events or long-running operations go rogue and need to be cancelled. Consider the case of a remote HTTP request that’s taking too long to process. Is the script unresponsive or is the server just slow? It would be ideal to have an easy mechanism to cancel events cleanly after some predetermined amount of time. Implementing your own cancellation mechanism can be very challenging and error prone even with the help of third-party libraries. One good quality of responsive design is to always throttle a user’s interaction with any UI components, so that the system isn’t unnecessarily overloaded. In chapter 4, you’ll learn how to use throttling and debouncing to your advantage. Manual solutions for achieving this are typically very hard to get right and involve functions that access data outside their local scope, which breaks the stability of your entire programYou learned that Promises certainly move the needle in the right direction (and RxJS integrates with Promises seamlessly if you feel the need to do so). But what you really need is a solution that abstracts out the notion of latency away from your code while allowing you to model your solutions using a linear sequence of steps through which data can flow over time THE REACTIVE EXTENSIONS FOR JAVASCRIPTReactive Extensions for JavaScript (RxJS) is an elegant replacement for callback or Promise-based libraries, using a single programming model that treats any ubiquitous source of events—whether it be reading a file, making an HTTP call, clicking a button, or moving the mouse—in the exact same manner. For example, instead of handling each mouse event independently with a callback, with RxJS you handle all of them combined. 12 Reference https://angular.io/guide/router]]></content>
      <tags>
        <tag>JavaScript</tag>
        <tag>Angular</tag>
        <tag>NodeJs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[reactive programing]]></title>
    <url>%2F2017-12-11-Why-Reactive-programming-is-outpermant%2F</url>
    <content type="text"><![CDATA[The second advantage to a lazy subscription is that the observable doesn’t hold onto data by default. In the previous example, each event generated by the interval will be processed and then dropped. This is what we mean when we say that the observable is streaming in nature rather than pooled. This discard-by-default semantic means that you never have to worry about unbounded memory growth sneaking up on you, causing memory leaks. When writing native event-driven JavaScript code, especially in older browsers, memory leaks can occur if you neglect event management and disposal.]]></content>
      <tags>
        <tag>JavaScript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Router in angular]]></title>
    <url>%2F2017-12-10-RxJS%2F</url>
    <content type="text"><![CDATA[Lettable operatorsRxJS 5.5, piping all the things So now we want a way to use those operators, how could we do that? Well, we said those operators are “lettable” that means we can use them by calling the let method on an observable: And if we want to chain multiple lettable operators we can keep dot chaining: import { Observable } from ‘rxjs/Rx’;import { filter, map, reduce } from ‘rxjs/operators’; const filterOutEvens = filter(x =&gt; x % 2);const sum = reduce((acc, next) =&gt; acc + next, 0);const doubleBy = x =&gt; map(value =&gt; value * x); const source$ = Observable.range(0, 10); source$ .let(filterOutEvens) .let(doubleBy(2)) .let(sum) .subscribe(x =&gt; console.log(x)); // 50 Meaning we can easily compose a bunch of pure function operators and pass them as a single operator to an observable! ConclusionWith those tools in hand, you can write RxJS code that is much more re-usable by just piping your (pure functions) operators together and easily re-use shared logic. import { Observable, pipe } from ‘rxjs/Rx’;import { filter, map, reduce } from ‘rxjs/operators’; const filterOutEvens = filter(x =&gt; x % 2);const sum = reduce((acc, next) =&gt; acc + next, 0);const doubleBy = x =&gt; map(value =&gt; value * x); const complicatedLogic = pipe( filterOutEvens, doubleBy(2), sum); const source$ = Observable.range(0, 10); source$.let(complicatedLogic).subscribe(x =&gt; console.log(x)); // 50 https://github.com/ReactiveX/rxjs/blob/master/doc/pipeable-operators.mdWhat?What is a pipeable operator? Simply put, a function that can be used with the current let operator. It used to be the origin of the name (“lettable”), but that was confusing and we call them “pipeable” now because they’re intended to be used with the pipe utility. A pipeable operator is basically any function that returns a function with the signature: &lt;T, R&gt;(source: Observable&lt;T&gt;) =&gt; Observable&lt;R&gt;. There is a pipe method built into Observable now at Observable.prototype.pipe that сan be used to compose the operators in similar manner to what you’re used to with dot-chaining (shown below). There is also a pipe utility function at rxjs/util/pipe that can be used to build reusable pipeable operators from other pipeable operators. UsageYou pull in any operator you need from one spot, under ‘rxjs/operators’ (plural!). It’s also recommended to pull in the Observable creation methods you need directly as shown below with range: 1234567891011import &#123; range &#125; from 'rxjs/observable/range';import &#123; map, filter, scan &#125; from 'rxjs/operators';const source$ = range(0, 10);source$.pipe( filter(x =&gt; x % 2 === 0), map(x =&gt; x + x), scan((acc, x) =&gt; acc + x, 0)).subscribe(x =&gt; console.log(x)) https://blog.angularindepth.com/rxjs-understanding-lettable-operators-fe74dda186d3RxJS: Understanding Lettable Operators What are lettable operators and what does lettable mean?If lettable operators are used with a method named pipe, you might wonder why they are referred to as lettable. The term is derived from RxJS’s let operator. The let operator is conceptually similar to the map operator, but instead of taking a projection function that receives and returns a value, let takes a function that receives and returns an observable. It’s unfortunate that let is one of the less-well-known operators, as it’s very useful for composing reusable functionality. 12345678910111213141516171819import * as Rx from "rxjs";export function retry&lt;T&gt;( count: number, wait: number): (source: Rx.Observable&lt;T&gt;) =&gt; Rx.Observable&lt;T&gt; &#123; return (source: Rx.Observable&lt;T&gt;) =&gt; source .retryWhen(errors =&gt; errors // Each time an error occurs, increment the accumulator. // When the maximum number of retries have been attempted, throw the error. .scan((acc, error) =&gt; &#123; if (acc &gt;= count) &#123; throw error; &#125; return acc + 1; &#125;, 0) // Wait the specified number of milliseconds between retries. .delay(wait) );&#125; When retry is called, it’s passed the number of retry attempts that should be made and the number of milliseconds to wait between attempts, and it returns a function that receives an observable and returns another observable into which the retry logic is composed. The returned function can be passed to the let operator, like this: 12345678910111213141516171819202122232425262728import * as Rx from "rxjs";import &#123; retry &#125; from "./retry";const name = Rx.Observable.ajax .getJSON&lt;&#123; name: string &#125;&gt;("/api/employees/alice") .let(retry(3, 1000)) .map(employee =&gt; employee.name) .catch(error =&gt; Rx.Observable.of(null)); ``` `Using the let operator, we’ve been able to create a reusable function much more simply than we would have been able to create a prototype-patching operator`. What we’ve created is a lettable operator.`Lettable operators are a higher-order functions. Lettable operators return functions that receive and return observables; and those functions can be passed to the let operator`.We can also use our lettable retry operator with pipe, like this:```typescriptimport &#123; ajax &#125; from "rxjs/observable/dom/ajax";import &#123; of &#125; from "rxjs/observable/of";import &#123; catchError, map &#125; from "rxjs/operators";import &#123; retry &#125; from "./retry";const name = ajax .getJSON&lt;&#123; name: string &#125;&gt;("/api/employees/alice") .pipe( retry(3, 1000), map(employee =&gt; employee.name), catchError(error =&gt; of(null)) ); Let’s return to our retry function and replace the chained methods with lettable operators and a pipe call, so that it looks like this: 12345678910111213141516171819import &#123; Observable &#125; from "rxjs/Observable";import &#123; delay, retryWhen, scan &#125; from "rxjs/operators";export function retry&lt;T&gt;( count: number, wait: number): (source: Observable&lt;T&gt;) =&gt; Observable&lt;T&gt; &#123; return retryWhen(errors =&gt; errors.pipe( // Each time an error occurs, increment the accumulator. // When the maximum number of retries have been attempted, throw the error. scan((acc, error) =&gt; &#123; if (acc &gt;= count) &#123; throw error; &#125; return acc + 1; &#125;, 0), // Wait the specified number of milliseconds between retries. delay(wait) ));&#125; With the chained methods replaced, we now have a proper, reusable lettable operator that imports only what it requires. Why should lettable operators should be preferred?For application developers, lettable operators are much easier to manage: Rather then relying upon operators being patched into Observable.prototype, lettable operators are explicitly imported into the modules in which they are used. It’s easy for TypeScript and bundlers to determine whether the lettable operators imported into a module are actually used. And if they are not, they can be left unbundled. If prototype patching is used, this task is manual and tedious. For library authors, lettable operators are much less verbose than call-based alternative, but it’s the correct inference of types that is — at least for me — the biggest advantage. Agreed, the pipe is awesome for composing custom rx operators. But why do we see more and more people using it even when not combining re-usable variables — instead of just chaining methods? Meaning, we use to write e.g… 123456const source$ = Observable.range(0, 10); source$ .filter(x =&gt; x % 2) .reduce((acc, next) =&gt; acc + next, 0) .map(value =&gt; value * 2) .subscribe(x =&gt; console.log(x)); Above is imho much cleaner than what I see more nowadays: 123456const source$ = Observable.range(0, 10);source$.pipe( filter(x =&gt; x % 2), reduce((acc, next) =&gt; acc + next, 0), map(value =&gt; value * 2)).subscribe(x =&gt; console.log(x)); Are there performance advantages by using the standalone operators instead of chaining? debounceInput is a function that takes and returns an observable, so it can be passed to the Observable.prototype.pipe function, like this: valueChanges.pipe(debounceInput). So, whenever you find yourself using the same combination of operators in many places, you could consider using the static pipe function to create a reusable operator combination. The static pipe function also makes something else much simpler: dealing with pipe-like overload signatures. Let’s look at that next. Tree Shakinghttps://webpack.js.org/guides/tree-shaking/Tree shaking is a term commonly used in the JavaScript context for dead-code elimination. It relies on the static structure of ES2015 module syntax, i.e. import and export. The name and concept have been popularized by the ES2015 module bundler rollup. So, what we’ve learned is that in order to take advantage of tree shaking, you must… Use ES2015 module syntax (i.e. import and export).Add a “sideEffects” entry to your project’s package.json file.Include a minifier that supports dead code removal (e.g. the UglifyJSPlugin). mergeMap vs flatMap vs concatMap vs switchMapToday we’re going to look at the difference between these four three RxJS operators. The simple part is that flatMap is just an alias for mergeMap. Other operators have a difference that might be important in some cases. When do you need them?All these operators are used with so-called higher order Observables. This is when items of an Observable are Observables themselves (or they are mapped to Observables) and we need to flatten all of them into one final Observable. You can easily identify this situation when you subscribe to an Observable inside subscription to another Observable (Note: this is not a recommended approach): 1234outerObservable.subscribe(outerItem =&gt; &#123; outerItem.subsribe(innerItem =&gt; &#123; foo(innerItem); &#125;)&#125;); In my examples, initial Observable is called outer Observable. And items of the outer Observable are called inner Observables. Technically inner and outer Observables are just plain Observables. A usage example for such operators can be a search box (search box text changes as outer Observable) with a request being sent to a server for each search text change (HTTP responses as inner Observables). Another example is mouse button clicks (outer Observable) that trigger an interval timer for each mouse click (timer events as inner Observables). Learning planTo tackle these operators you need to understand more basic ones first. In this article I won’t give any definitions or explanations per se, but rather just a small learning plan with links: Map, Merge, Concat. Basic Observable operators.MergeAll, ConcatAll, Switch. These are for higher order Observables already.After that MergeMap, ConcatMap, and SwitchMap should be easy for you.Operators from the third group are two step operators. First, they map outer Observable items to inner Observables. The second step is to merge a result set of inner Observables in some way. The way they are merged depends on the operator you use. 123mergeMap = map + mergeAllconcatMap = map + concatAllswitchMap = map + switch Here are some reworked reactivex.io diagrams. At first some explanations for the diagrams: MergeMapOuter (initial) Observable emits circles. Each circle is then mapped to its own inner Observable - collection of rhombuses. Collections are identified by color; each collection has its own color. All those inner Observables are then merged into one final Observable - resulting collection of rhombuses. MergeMapmergeMap emits items into the resulting Observable just as they are emitted from inner Observables. It doesn’t wait for anything.mergeMap doesn’t preserve the order from outer Observable. Collections of rhombuses interleave.mergeMap doesn’t cancel any inner Observables. All rhombuses from inner Observables get to final collection. ConcatMapconcatMap waits for inner Observable to complete before taking items from the next inner Observable.concatMap does preserve the order from outer Observable. Collections of rhombuses don’t interleave.Just as mergeMap, concatMap doesn’t cancel any inner Observables. All rhombuses from inner Observables get to the final collection. SwitchMapswitchMap emits items only from the most recent inner Observable.switchMap cancels previous inner Observables when a new inner Observable appears. Items of inner Observable that were emitted after the Observable was canceled will be lost (not included in the resulting Observable). “Talk is cheap. Show me the code.”I still wasn’t sure I had cracked the difference between the discussed operators even after reading all the results from Google’s first page :) I’ve set up a small example on JsFiddle to see the difference in practice. In the example, outer Observable emits three items with a one-second interval. Each item is then mapped to an inner Observable, which in its turn emits another three items with a one-second interval. The final result depends on the operator you use. Hope this small example will help you understand these operators without too much googling. Router1234567891011121314151617181920212223242526const appRoutes: Routes = [ &#123; path: 'crisis-center', component: CrisisListComponent &#125;, &#123; path: 'hero/:id', component: HeroDetailComponent &#125;, &#123; path: 'heroes', component: HeroListComponent, data: &#123; title: 'Heroes List' &#125; &#125;, &#123; path: '', redirectTo: '/heroes', pathMatch: 'full' &#125;, &#123; path: '**', component: PageNotFoundComponent &#125;];@NgModule(&#123; imports: [ RouterModule.forRoot( appRoutes, &#123; enableTracing: true &#125; // &lt;-- debugging purposes only ) // other imports here ], ...&#125;)export class AppModule &#123; &#125; The data property in the third route is a place to store arbitrary data associated with this specific route. The data property is accessible within each activated route. Use it to store items such as page titles, breadcrumb text, and other read-only, static data. You’ll use the resolve guard to retrieve dynamic data The empty path in the fourth route represents the default path for the application, the place to go when the path in the URL is empty, as it typically is at the start. This default route redirects to the route for the /heroes URL and, therefore, will display the HeroesListComponent. The ** path in the last route is a wildcard. The router will select this route if the requested URL doesn’t match any paths for routes defined earlier in the configuration. This is useful for displaying a “404 - Not Found” page or redirecting to another route. The order of the routes in the configuration matters and this is by design. The router uses a first-match wins strategy when matching routes, so more specific routes should be placed above less specific routes. Router outletThe router matches that URL to the route path and displays the Component after a RouterOutlet that you’ve placed in the host view’s HTML. 123content_copy&lt;router-outlet&gt;&lt;/router-outlet&gt;&lt;!-- Routed views go here --&gt; Define routesA router must be configured with a list of route definitions. mechanism When the application requests navigation to the path /crisis-center, the router activates an instance of CrisisListComponent, displays its view, and updates the browser’s address location and history with the URL for that path. Pass the array of routes, appRoutes, to the RouterModule.forRoot method. It returns a module, containing the configured Router service provider, plus other providers that the routing library requires. Once the application is bootstrapped, the Router performs the initial navigation based on the current browser URL Some key points of setting up router Load the router library. Add a nav bar to the shell template with anchor tags, routerLink and routerLinkActive directives. Add a router-outlet to the shell template where views will be displayed. Configure the router module with RouterModule.forRoot. Set the router to compose HTML5 browser URLs. handle invalid routes with a wildcard route. navigate to the default route when the app launches with an empty path. Difference between forRoot and forChildOnly call RouterModule.forRoot in the root AppRoutingModule (or the AppModule if that’s where you register top level application routes). In any other module, you must call the RouterModule.forChild method to register additional routes. Leave the default and the wildcard routes! These are concerns at the top level of the application itself. Reference https://angular.io/guide/router]]></content>
      <tags>
        <tag>javascript</tag>
        <tag>angular</tag>
        <tag>rxjs</tag>
        <tag>typescript</tag>
        <tag>node</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redux]]></title>
    <url>%2F2017-12-12-Redux%2F</url>
    <content type="text"><![CDATA[whats @EffectsYou can almost think of your Effects as special kinds of reducer functions that are meant to be a place for you to put your async calls in such a way that the returned data can then be easily inserted into the store’s internal state for the application. rule of a thumb for actionsAs a good rule of thumb, try not to make one reducer that handles all the actions, but also do not make a separate reducer for each action. Group them in a way that makes sense for the application structure.]]></content>
      <tags>
        <tag>javascript</tag>
        <tag>angular</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cloud computering]]></title>
    <url>%2F2017-12-18-Clouding%2F</url>
    <content type="text"><![CDATA[ConceptsCloud computing is the on-demand demand delivery of compute database storage applications and other IT resources through a cloud services platform via the Internet with pay-as-you-go pricing. The 6 advantages of cloud computeringTrade Capital Expense For Variable ExpenseBenefit from massive economies of scaleStop guessing about capacityIncrease speed and agilityStop spending money running and maintaining data centersGo global in minutes3 types of cloud computing IAAS PAAS SAAS “Certification is the beginning of your journey, not the end” Edge locationEdge locations are CDN endpoints for CloudFront. IAMIdentity Access Management, to control who can access the service. IAM is apply to “Global”, it’s not releated to any single region. MFAMultiple Factors Authentication, which can be virtual (software) or hardware devices. How to access AWS Web console CLI (command line) SDK AccountBy logging into webconsole with your email acess, you are in root account. The root account alwyas has full administrator access. You should not give these account credentials away to anyone. You should always secure this root account using multi factor authentication. GroupA group is simply a place to store your users. Your users will inherit all permissions that the group has. Examples of groups might be developers, system administrators, etc. PolicyTo set the permissions in a group you need to apply a policy to that group. Policies consist of JSON. S3 Its a safe place to store your files. They are object-based storage. S3 is a global name space, so you won’t have same name with others. There is unlimited storage. Files are stored in Buckets, you can think buckets as folder. There are versioning, encryptiong for file storage Secure your data using Access Control Lists(Object level) and Bucket policies(folder level) S3 solutions S3 (99.99% availability , 99.999999999% durability)-&gt; S3 IA (infrequently accessed) -&gt; Reduced Redundancy Storage -&gt; Glacier (very cheap , used for archival, it takes 3-5 hours to restore from Glacier, its $0.01/GB/Month) S3 is for current data while Glacier is for archived data. S3 Transfer AccelerationIt enable fast, easy and secure transfer of files over long distances between your end users and an S3 bucket. Transfer Acceleration takes advantage of Amazon CloudFront’s globally distributed edge locations. As data arrives at an edge location, data is routed to Amazon S3 over an optimized network path. CloudfrontIt’s the CDN services. Amazon CloudFront can be used to deliever your entire website, including dynamic, static contents using a global network of edge locations. There are two types of distributions available: Web Distribution: typically used for websites. RTMP distribution: used for media streaming Edge locationThis is the location where content will be cached. This is separate to an AWS region/AZ (Avaiability Zones) OriginThis is the origin of all the files that the CDN will distribute. This can be an S3 bucket, an EC2 instance, and Elastic Load Balancer or Route53. EC2Amazon Elastic Compute Cloud (EC2) is a web service that provies resizable compute capacity in the cloud. Basically EC2 are just virtual machines. EC2 reduces the time required to obtain and boot new server instances to minutes, allowing you to quickly scale capacity, both up and down, as your computing requirements changes. EC2 options On demand reserved Spot. Users with urgent computing needs for large amounts of additional capacity. Dedicated EBSElastic Block Storage. Amazon EBS allows you to create storage volumes and attach them to Amazon EC2 instances. Once attached, you can create a file system on top of these volumes, run a database, or use them in any other way you would use in a block device. It’s simply a virtual disk that you install your operating system on and all relevant files. openshift vs openstack“How does OpenShift relate to OpenStack?”, I answer “OpenShift Origin can run on top of OpenStack. They are complementary projects that work well together. OpenShift Origin is not presently part of OpenStack, and does not compete with OpenStack. If you stand up your own OpenStack system, you can make it even more useful by installing OpenShift Origin on top of it.” OpenStack provides “Infrastructure-as-a-Service”, or “IaaS”. It provides bootable virtual machines, networking, block storage, object storage, and so forth. Some IaaS service providers based on OpenStack are HP Cloud and Rackspace Cloud. The OpenShift hosted service provides “Platform-as-a-Service” or “PaaS”. It provides the necessary parts to quickly deploy and run a LAMP application: the web server, application server, application runtimes and libraries, database service, and so forth.]]></content>
      <tags>
        <tag>cloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cloud computering]]></title>
    <url>%2F2017-12-18-OpenStack-vs-OpenShift%2F</url>
    <content type="text"><![CDATA[openshift vs openstackThe shoft and direct answer is `OpenShift Origin can run on top of OpenStack. They are complementary projects that work well together. OpenShift Origin is not presently part of OpenStack, and does not compete with OpenStack. If you stand up your own OpenStack system, you can make it even more useful by installing OpenShift Origin on top of it.’ OpenStack is IAASOpenStack provides “Infrastructure-as-a-Service”, or “IaaS”. It provides bootable virtual machines, networking, block storage, object storage, and so forth. Some IaaS service providers based on OpenStack are HP Cloud and Rackspace Cloud. OpenShift is PAASThe OpenShift hosted service provides “Platform-as-a-Service” or “PaaS”. It provides the necessary parts to quickly deploy and run a LAMP application: the web server, application server, application runtimes and libraries, database service, and so forth.]]></content>
      <tags>
        <tag>cloud</tag>
        <tag>OpenStack</tag>
        <tag>OpenShift</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[iOS programming]]></title>
    <url>%2F2018-01-06-iOS%2F</url>
    <content type="text"><![CDATA[ViewA view is also a responder (UIView is a subclass of UIResponder). This means that a view is subject to user interactions, such as taps and swipes. Thus, views are the basis not only of the interface that the user sees, but also of the interface that the user touches The Window and Root ViewThe top of the view hierarchy is the app’s window. It is an instance of UIWindow (or your own subclass thereof), which is a UIView subclass. Your app should have exactly one main window. It is created at launch time and is never destroyed or replaced ObjectIn Swift, the syntax of message-sending is dot-notation. We start with the object; then there’s a dot (a period); then there’s the message. (Some messages are also followed by parentheses, but ignore them for now; the full syntax of message-sending is one of those details we’ll be filling in later.) This is valid Swift syntax: fido.bark()rover.sit() The idea of everything being an object is a way of suggesting that even “primitive” linguistic entities can be sent messages. Take, for example, 1. It appears to be a literal digit and no more. It will not surprise you, if you’ve ever used any programming language, that you can say things like this in Swift: let sum = 1 + 2 But it is surprising to find that 1 can be followed by a dot and a message. This is legal and meaningful in Swift (don’t worry about what it actually means): let s = 1.description Just as 1 is actually an object, + is actually a message; but it’s a message with special syntax (operator syntax). In Swift, every noun is an object, and every verb is a message. extension Int { func sayHello() { print(“Hello, I’m (self)”) }} 1.sayHello() // outputs: “Hello, I’m 1” In Swift, then, 1 is an object. In some languages, such as Objective-C, it clearly is not; it is a “primitive” or scalar built-in data type. So the distinction being drawn here is between object types on the one hand and scalars on the other. In Swift, there are no scalars; all types are ultimately object types. That’s what “everything is an object” really means. ClassSwift has classes, but 1 in Swift is not a class or an instance of a class: the type of 1, namely Int, is a struct, and 1 is an instance of a struct. And Swift has yet another kind of thing you can send messages to, called an enum. So Swift has three kinds of object type: classes, structs, and enums. I like to refer to these as the three flavors of object type. Exactly how they differ from one another will emerge in due course. But they are all very definitely object types, and their similarities to one another are far stronger than their differences. For now, just bear in mind that these three flavors exist. VariablesA variable is a name for an object. Technically, it refers to an object; it is an object reference. Nontechnically, you can think of it as a shoebox into which an object is placed. The object may undergo changes, or it may be replaced inside the shoebox by another object, but the name has an integrity all its own. The object to which the variable refers is the variable’s value. In Swift, no variable comes implicitly into existence; all variables must be declared. If you need a name for something, you must say “I’m creating a name.” You do this with one of two keywords: let or var. In Swift, declaration is usually accompanied by initialization — you use an equal sign to give the variable a value immediately, as part of the declaration. These are both variable declarations (and initializations): let one = 1var two = 2 main.swiftSwift also has a special rule that a file called main.swift, exceptionally, can have executable code at its top level, outside any function body, and this is the code that actually runs when the program runs. You can construct your app with a main.swift file, but in general you won’t need to.]]></content>
      <tags>
        <tag>iOS</tag>
        <tag>swift</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ngrx]]></title>
    <url>%2F2018-01-08-ngrx%2F</url>
    <content type="text"><![CDATA[Why @Effects?In a simple ngrx/store project without ngrx/effects there is really no good place to put your async calls. Suppose a user clicks on a button or types into an input box and then we need to make an asynchronous call. The dumb component will be the first to know about this action from the user, and it’s handler will be called when the button is actually clicked. However, we don’t want to put the logic to do our async call right in the dumb component since we want to keep it dumb! The only thing in the dumb component’s handler is it’s @Output emitter emitting an event to the smart component telling it that the button was clicked. Then the smart component gets the event and it’s handler function is triggered, but we don’t want to put the async login right in there because we want to keep it lean and only dipatching actions to our store so that the store can modify the state! Ok… but the store only handles actions in the reducer, and reducer are meant to be pure functions so where are we supposed to logically put our async calls so that we can put their response data in the store? The answer, friends, is @Effects! You can almost think of your Effects as special kinds of reducer functions that are meant to be a place for you to put your async calls in such a way that the returned data can then be easily inserted into the store’s internal state for the application. what’s effects At it’s core, the Effects Class in simply just an Angular 2 Service: In Angular 2 a service is just a regular old TypeScript class with the @Injectable metadata, and when working with @Effects you make a single “Effect Class” or “Effect Service” that then contains various @Effect functions, each corresponding to an action dispatched by your ngrx store. sample12345@Effect() update$ = this.action$ .ofType('SUPER_SIMPLE_EFFECT') .switchMap( () =&gt; Observable.of(&#123;type: "SUPER_SIMPLE_EFFECT_HAS_FINISHED"&#125;) ); Were using the TypeScript metadata to label our variable update$ (the $ is commonly used as a suffix for variables whose value is an observable) as an “ngrx effect” that will be triggered when we dispatch actions with the store (the same was we always send actions to the reducer or reducers). Then we see “this.action$.ofType(‘SUPER_SIMPLE_EFFECT’)”. Remeber, we’re translating the dispatched event into an observable, so .ofType means your taking in an observable and then returning the observable only if it is of that type.]]></content>
      <tags>
        <tag>Angular</tag>
        <tag>javascript</tag>
        <tag>ngrx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CORS :Cross-Origin Resource Sharing]]></title>
    <url>%2F2018-01-10-CORS%2F</url>
    <content type="text"><![CDATA[Cross-Origin Request Sharing - CORS (A.K.A. Cross-Domain AJAX request) is an issue that most web developers might encounter, according to Same-Origin-Policy, browsers restrict client JavaScript in a security sandbox, usually JS cannot directly communicate with a remote server from a different domain. In the past developers created many tricky ways to achieve Cross-Domain resource request, most commonly using ways are: Use Flash/Silverlight or server side as a “proxy” to communicate with remote.JSON With Padding (JSONP).Embeds remote server in an iframe and communicate through fragment or window.name, refer here.Those tricky ways have more or less some issues, for example JSONP might result in security hole if developers simply “eval” it, and #3 above, although it works, both domains should build strict contract between each other, it neither flexible nor elegant IMHO:) W3C had introduced Cross-Origin Resource Sharing (CORS) as a standard solution to provide a safe, flexible and a recommended standard way to solve this issue. The Mechanism From a high level we can simply deem CORS is a contract between client AJAX call from domain A and a page hosted on domain B, a typical Cross-Origin request/response would be: DomainA AJAX request headers Host DomainB.comUser-Agent Mozilla/5.0 (Windows NT 6.1; WOW64; rv:2.0) Gecko/20100101 Firefox/4.0Accept text/html,application/xhtml+xml,application/xml;q=0.9,/;q=0.8,application/jsonAccept-Language en-us;Accept-Encoding gzip, deflateKeep-Alive 115Origin http://DomainA.comDomainB response headers Cache-Control privateContent-Type application/json; charset=utf-8Access-Control-Allow-Origin DomainA.comContent-Length 87Proxy-Connection Keep-AliveConnection Keep-AliveThe blue parts I marked above were the kernal facts, “Origin” request header “indicates where the cross-origin request or preflight request originates from”, the “Access-Control-Allow-Origin” response header indicates this page allows remote request from DomainA (if the value is * indicate allows remote requests from any domain). As I mentioned above, W3 recommended browser to implement a “preflight request” before submiting the actually Cross-Origin HTTP request, in a nutshell it is an HTTP OPTIONS request: OPTIONS DomainB.com/foo.aspx HTTP/1.1If foo.aspx supports OPTIONS HTTP verb, it might return response like below: HTTP/1.1 200 OKDate: Wed, 01 Mar 2011 15:38:19 GMTAccess-Control-Allow-Origin: http://DomainA.comAccess-Control-Allow-Methods: POST, GET, OPTIONS, HEADAccess-Control-Allow-Headers: X-Requested-WithAccess-Control-Max-Age: 1728000Connection: Keep-AliveContent-Type: application/jsonOnly if the response contains “Access-Control-Allow-Origin” AND its value is “*” or contain the domain who submitted the CORS request, by satisfying this mandtory condition browser will submit the actual Cross-Domain request, and cache the result in “Preflight-Result-Cache”. I blogged about CORS three years ago: AJAX Cross-Origin HTTP request]]></content>
      <tags>
        <tag>DevOps</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[What is difference between declarations, providers and import in NgModule]]></title>
    <url>%2F2018-01-11-Angular-Module-Declaration-Import%2F</url>
    <content type="text"><![CDATA[What is difference between declarations, providers and import in NgModule imports: is used to import supporting modules likes FormsModule, RouterModule, CommonModule, or any other custom-made feature module. makes the exported declarations of other modules available in the current module declarations are to make directives (including components and pipes) from the current module available to other directives in the current module. Selectors of directives, components or pipes are only matched against the HTML if they are declared or imported. declaration is used to declare components, directives, pipes that belongs to the current module. Everything inside declarations knows each other. For example, if we have a component, say UsernameComponent, which display list of the usernames, and we also have a pipe, say toupperPipe, which transform string to uppercase letter string. Now If we want to show usernames in uppercase letters in our UsernameComponent, we can use the toupperPipe which we had created before but how UsernameComponent know that the toupperPipe exist and how we can access and use it, here comes the declarations, we can declare UsernameComponent and toupperPipe. providers are to make services and values known to DI. They are added to the root scope and they are injected to other services or directives that have them as dependency.provider is used to inject the services required by components, directives, pipes in our module.]]></content>
      <tags>
        <tag>JavaScript</tag>
        <tag>Angular</tag>
        <tag>NodeJs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Minium Viable Product]]></title>
    <url>%2F2018-02-27-MVP%2F</url>
    <content type="text"><![CDATA[https://blog.leanstack.com/minimum-viable-product-mvp-7e280b0b9418 What is a Minimum Viable Product (MVP)A Minimum Viable Product is the smallest thing you can build that delivers customer value (and as a bonus captures some of that value back i.e. gets you paid).]]></content>
  </entry>
  <entry>
    <title><![CDATA[NodeJs Notes]]></title>
    <url>%2F2018-03-18-NodeJs-Notes%2F</url>
    <content type="text"><![CDATA[commands to read files var lineReader = require(‘readline’).createInterface({ input: require(‘fs’).createReadStream(‘C:\dev\node\input\git_reset_files.txt’) }); lineReader.on(‘line’, function(line){ console.log(‘git checkout ‘+line); });]]></content>
      <tags>
        <tag>Angular</tag>
        <tag>NodeJs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Node errors troubleshooting]]></title>
    <url>%2F2018-05-11-Node-errors-DB%2F</url>
    <content type="text"><![CDATA[Here is the typical erros log: 12345node_modules/@types/node/index.d.ts(6202,55): error TS2304: Cannot find name 'Map'.node_modules/@types/node/index.d.ts(6209,55): error TS2304: Cannot find name 'Set'.node_modules/@types/node/index.d.ts(6213,64): error TS2304: Cannot find name 'Symbol'.node_modules/@types/node/index.d.ts(6219,59): error TS2304: Cannot find name 'WeakMap'.node_modules/@types/node/index.d.ts(6220,59): error TS2304: Cannot find name 'WeakSet'. The main reason is above stuff are new to ES6, which are unavaiable in ES5. Hold no, you don’t need to change your typescript target to ES6, which may break projects and leads to tons of new regression testing.Firsty, try to add following in tsconfig.json 1"lib": ["es2016", "dom"], If no luck, try following in command line, it should resolve this issue. 1$ tsc index.ts --lib "es6" certificate errorTypical errors 12345678910events.js:183 throw er; // Unhandled 'error' event ^Error: unable to verify the first certificate at TLSSocket.&lt;anonymous&gt; (_tls_wrap.js:1103:38) at emitNone (events.js:106:13) at TLSSocket.emit (events.js:208:7) at TLSSocket._finishInit (_tls_wrap.js:637:8) at TLSWrap.ssl.onhandshakedone (_tls_wrap.js:467:38) Solutionadd following to https request options , rejectUnauthorized: false, requestCert: true, agent: false error ECONNREFUSED1234Error: connect ECONNREFUSED 127.0.0.1:443 at Object._errnoException (util.js:1022:11) at _exceptionWithHostPort (util.js:1044:20) at TCPConnectWrap.afterConnect [as oncomplete] (net.js:1182:14) Solution:in the http request, do not use ‘url’ but ‘host’ and pathhost: xxx.com, port: 443, path: /login,]]></content>
      <tags>
        <tag>Angular</tag>
        <tag>NodeJs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Strategy-Of-Openshift-Releases]]></title>
    <url>%2F2018-05-14-Strategy-Of-Openshift-Releases%2F</url>
    <content type="text"><![CDATA[Release &amp; Testing StrategyThere are various methods for safely releasing changes to Production. Each team must select what is appropriate for their own use case with consideration to risk, rollback approaches and testing approaches. The following are options:Canary ReleaseThis is the lowest risk strategy since it allows for testing on a subset of users, and it allows for fast rollback: Build a new Environment Use a routing tool (eg. Apigee) to test with a specific set of users Bleed traffic across Remove old environment Blue Green This is the classic zero-downtime deployment model that involves flipping traffic between two environments: Ensure your router has two entry points, one for Production Testing and one for Production traffic Have two environments: blue and green If Production traffic is pointing to blue then deploy your changes on green Point your Production Testing traffic to green When verified, point Production traffic to green On the next release, deploy changes to blue]]></content>
      <tags>
        <tag>clud</tag>
        <tag>Openshift</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[How to setup nodejs to install package from intranet]]></title>
    <url>%2F2018-05-15-how-to-setup-node-config-in-intranet%2F</url>
    <content type="text"><![CDATA[Error of ‘ECONNRESET’You may face error ECONNRESET from intranet, even appropriate proxy tools (e.g. cntlm) is running. The errors may looks like 1234567891011121314$ npm install -g @angular/cli@latestnpm WARN registry Unexpected warning for http://registry.npmjs.org/: Miscellaneous Warning ECONNRESET: request to http://registry.npmjs.org/@angular%2fcli failed, reason: read ECONNRESETnpm WARN registry Using stale package data from http://registry.npmjs.org/ due to a request error during revalidation.npm ERR! code ECONNRESETnpm ERR! errno ECONNRESETnpm ERR! network request to http://registry.npmjs.org/@angular-devkit%2farchitect failed, reason: read ECONNRESETnpm ERR! network This is a problem related to network connectivity.npm ERR! network In most cases you are behind a proxy or have bad network settings.npm ERR! networknpm ERR! network If you are behind a proxy, please make sure that thenpm ERR! network 'proxy' config is set properly. See: 'npm help config'npm ERR! A complete log of this run can be found in:npm ERR! C:\Users\xxx\AppData\Roaming\npm-cache\_logs\2018-05-15T05_04_39_505Z-debug.log ressonYou need to update npm configuration to make sure below config presence registryand config https-proxy should NOT exist. You can run following command to remove it https-proxy You can check your current config as1npm config list To fix above issues, please run following commands1npm config delete https-proxy Error of ‘code E503’Sometimes when you run npm install, you can see error ‘E503’. 123456$ npm install -g @angular/cli@latest npm WARN registry Using stale package data from http://registry.npmjs.org/ due to a request error during revalidation.npm ERR! code E503npm ERR! 503 Service Unavailable: @angular-devkit/architect@0.6.0npm ERR! A complete log of this run can be found in:npm ERR! C:\Users\xxx\AppData\Roaming\npm-cache\_logs\2018-05-15T05_40_53_127Z-debug.log That’s because property ‘proxy’ missing for npm. Please run below command to check wether it’s exist or not. 1npm config list | grep 'proxy' If not exist, run below command to add it back. 1npm config set proxy http://127.0.0.1:53128]]></content>
      <tags>
        <tag>nodejs</tag>
        <tag>javascript</tag>
        <tag>angular</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[rxjs pipe in depth]]></title>
    <url>%2F2018-05-15-rxjs-pipe-in-depth%2F</url>
    <content type="text"><![CDATA[https://stormforger.com/blog/2016/07/08/types-of-performance-testing/ Learn more about load testing, scalability testing, stress, spike and soak testing, configuration testing as well as availability and resilience testing. ———–15/05/2018 notes —————https://blog.hackages.io/rxjs-5-5-piping-all-the-things-9d469d1b3f44RxJS 5.5, piping all the things So now we want a way to use those operators, how could we do that? Well, we said those operators are “lettable” that means we can use them by calling the let method on an observable: And if we want to chain multiple lettable operators we can keep dot chaining: import { Observable } from ‘rxjs/Rx’;import { filter, map, reduce } from ‘rxjs/operators’; const filterOutEvens = filter(x =&gt; x % 2);const sum = reduce((acc, next) =&gt; acc + next, 0);const doubleBy = x =&gt; map(value =&gt; value * x); const source$ = Observable.range(0, 10); source$ .let(filterOutEvens) .let(doubleBy(2)) .let(sum) .subscribe(x =&gt; console.log(x)); // 50 Meaning we can easily compose a bunch of pure function operators and pass them as a single operator to an observable! ConclusionWith those tools in hand, you can write RxJS code that is much more re-usable by just piping your (pure functions) operators together and easily re-use shared logic. import { Observable, pipe } from ‘rxjs/Rx’;import { filter, map, reduce } from ‘rxjs/operators’; const filterOutEvens = filter(x =&gt; x % 2);const sum = reduce((acc, next) =&gt; acc + next, 0);const doubleBy = x =&gt; map(value =&gt; value * x); const complicatedLogic = pipe( filterOutEvens, doubleBy(2), sum); const source$ = Observable.range(0, 10); source$.let(complicatedLogic).subscribe(x =&gt; console.log(x)); // 50 https://github.com/ReactiveX/rxjs/blob/master/doc/pipeable-operators.mdWhat?What is a pipeable operator? Simply put, a function that can be used with the current let operator. It used to be the origin of the name (“lettable”), but that was confusing and we call them “pipeable” now because they’re intended to be used with the pipe utility. A pipeable operator is basically any function that returns a function with the signature: &lt;T, R&gt;(source: Observable&lt;T&gt;) =&gt; Observable&lt;R&gt;. There is a pipe method built into Observable now at Observable.prototype.pipe that сan be used to compose the operators in similar manner to what you’re used to with dot-chaining (shown below). There is also a pipe utility function at rxjs/util/pipe that can be used to build reusable pipeable operators from other pipeable operators. UsageYou pull in any operator you need from one spot, under ‘rxjs/operators’ (plural!). It’s also recommended to pull in the Observable creation methods you need directly as shown below with range: 1234567891011import &#123; range &#125; from 'rxjs/observable/range';import &#123; map, filter, scan &#125; from 'rxjs/operators';const source$ = range(0, 10);source$.pipe( filter(x =&gt; x % 2 === 0), map(x =&gt; x + x), scan((acc, x) =&gt; acc + x, 0)).subscribe(x =&gt; console.log(x)) https://blog.angularindepth.com/rxjs-understanding-lettable-operators-fe74dda186d3RxJS: Understanding Lettable Operators What are lettable operators and what does lettable mean?If lettable operators are used with a method named pipe, you might wonder why they are referred to as lettable. The term is derived from RxJS’s let operator. The let operator is conceptually similar to the map operator, but instead of taking a projection function that receives and returns a value, let takes a function that receives and returns an observable. It’s unfortunate that let is one of the less-well-known operators, as it’s very useful for composing reusable functionality. 12345678910111213141516171819import * as Rx from "rxjs";export function retry&lt;T&gt;( count: number, wait: number): (source: Rx.Observable&lt;T&gt;) =&gt; Rx.Observable&lt;T&gt; &#123; return (source: Rx.Observable&lt;T&gt;) =&gt; source .retryWhen(errors =&gt; errors // Each time an error occurs, increment the accumulator. // When the maximum number of retries have been attempted, throw the error. .scan((acc, error) =&gt; &#123; if (acc &gt;= count) &#123; throw error; &#125; return acc + 1; &#125;, 0) // Wait the specified number of milliseconds between retries. .delay(wait) );&#125; When retry is called, it’s passed the number of retry attempts that should be made and the number of milliseconds to wait between attempts, and it returns a function that receives an observable and returns another observable into which the retry logic is composed. The returned function can be passed to the let operator, like this: 12345678910111213141516171819202122232425262728import * as Rx from "rxjs";import &#123; retry &#125; from "./retry";const name = Rx.Observable.ajax .getJSON&lt;&#123; name: string &#125;&gt;("/api/employees/alice") .let(retry(3, 1000)) .map(employee =&gt; employee.name) .catch(error =&gt; Rx.Observable.of(null)); ``` Using the let operator, we’ve been able to create a reusable function much more simply than we would have been able to create a prototype-patching operator. What we’ve created is a lettable operator.Lettable operators are a higher-order functions. Lettable operators return functions that receive and return observables; and those functions can be passed to the let operator.We can also use our lettable retry operator with pipe, like this:```typescriptimport &#123; ajax &#125; from "rxjs/observable/dom/ajax";import &#123; of &#125; from "rxjs/observable/of";import &#123; catchError, map &#125; from "rxjs/operators";import &#123; retry &#125; from "./retry";const name = ajax .getJSON&lt;&#123; name: string &#125;&gt;("/api/employees/alice") .pipe( retry(3, 1000), map(employee =&gt; employee.name), catchError(error =&gt; of(null)) ); Let’s return to our retry function and replace the chained methods with lettable operators and a pipe call, so that it looks like this: 12345678910111213141516171819import &#123; Observable &#125; from "rxjs/Observable";import &#123; delay, retryWhen, scan &#125; from "rxjs/operators";export function retry&lt;T&gt;( count: number, wait: number): (source: Observable&lt;T&gt;) =&gt; Observable&lt;T&gt; &#123; return retryWhen(errors =&gt; errors.pipe( // Each time an error occurs, increment the accumulator. // When the maximum number of retries have been attempted, throw the error. scan((acc, error) =&gt; &#123; if (acc &gt;= count) &#123; throw error; &#125; return acc + 1; &#125;, 0), // Wait the specified number of milliseconds between retries. delay(wait) ));&#125; With the chained methods replaced, we now have a proper, reusable lettable operator that imports only what it requires. Why should lettable operators should be preferred?For application developers, lettable operators are much easier to manage: Rather then relying upon operators being patched into Observable.prototype, lettable operators are explicitly imported into the modules in which they are used. It’s easy for TypeScript and bundlers to determine whether the lettable operators imported into a module are actually used. And if they are not, they can be left unbundled. If prototype patching is used, this task is manual and tedious. For library authors, lettable operators are much less verbose than call-based alternative, but it’s the correct inference of types that is — at least for me — the biggest advantage. Agreed, the pipe is awesome for composing custom rx operators. But why do we see more and more people using it even when not combining re-usable variables — instead of just chaining methods? Meaning, we use to write e.g… const source$ = Observable.range(0, 10);source$ .filter(x =&gt; x % 2) .reduce((acc, next) =&gt; acc + next, 0) .map(value =&gt; value * 2) .subscribe(x =&gt; console.log(x));Above is imho much cleaner than what I see more nowadays: const source$ = Observable.range(0, 10);source$.pipe( filter(x =&gt; x % 2), reduce((acc, next) =&gt; acc + next, 0), map(value =&gt; value * 2)).subscribe(x =&gt; console.log(x));Are there performance advantages by using the standalone operators instead of chaining? https://webpack.js.org/guides/tree-shaking/Tree shaking is a term commonly used in the JavaScript context for dead-code elimination. It relies on the static structure of ES2015 module syntax, i.e. import and export. The name and concept have been popularized by the ES2015 module bundler rollup. So, what we’ve learned is that in order to take advantage of tree shaking, you must… Use ES2015 module syntax (i.e. import and export).Add a “sideEffects” entry to your project’s package.json file.Include a minifier that supports dead code removal (e.g. the UglifyJSPlugin). — english—it can safely prune unused exports. Trim (a tree, shrub, or bush) by cutting away dead or overgrown branches or stems, especially to encourage growth. ‘now is the time to prune roses’]]></content>
      <tags>
        <tag>Angular</tag>
        <tag>NodeJs</tag>
        <tag>RxJS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Agile and SCRUM]]></title>
    <url>%2F2018-06-06-Scrum-Agile%2F</url>
    <content type="text"><![CDATA[Key conceptIn Scrum, a team is cross functional, meaning everyone is needed to take a feature from idea to implementation. Within agile development, Scrum teams are supported by two specific roles. The first is a ScrumMaster, who can be thought of as a coach for the team, helping team members use the Scrum process to perform at the highest level. The product owner (PO) is the other role, and in Scrum software development, represents the business, customers or users, and guides the team toward building the right product.]]></content>
  </entry>
  <entry>
    <title><![CDATA[Portactor]]></title>
    <url>%2F2018-06-07-Protractor%2F</url>
    <content type="text"><![CDATA[Better to use smart waitIf possible, you’d better to use smart wait in protractor e2e testing. Which could increase end to end testing efficiency. Normally dev tend to use sleep or wait to insert some stop during execution. What’s the difference of these two?The difference between browser.sleep() and browser.wait() is that browser.wait() expects a specific value/condition. This wait condition is valid of Protractor or any WebDriver framework. An Expectation for checking an element is visible and enabled such that you can click it. 1browser.wait(EC.elementToBeClickable($(‘#abc’)),5000); Here are some protractor functions utilities. textToBePresentInElementAn expectation for checking if the given text is present in the element. 1browser.wait(EC.textToBePresentInElement($(‘#abc’),’foo’),5000); presenceOfAn expectation for checking that an element is present on the DOM of a page. 1browser.wait(EC.presenceOf($(‘#abc’)),5000); 123456789101112131415161718192021222324import &#123; go, click, see, below, slow, type &#125; from 'blue-harvest';import &#123; browser, ExpectedConditions &#125; from 'protractor';const client = 'TEST CLIENT';const clientFullName = 'TEST CLIENT LTD';const timeOut = 3000;describe('Should show bell potter', () =&gt; &#123; beforeEach(async () =&gt; &#123; await go('http://localhost:4200/#/dashboard') &#125;); it('should be able to search bell potter', async () =&gt; &#123; await slow.click('Search clients or accounts') await type(`TEST`); browser.wait(see(client), timeOut) await click(client) browser.wait(see(clientFullName), timeOut) expect(await below(clientFullName).see('GO')).toBeTruthy(); &#125;)&#125;) Reference https://medium.com/@ited.ro/how-to-use-smart-waits-with-protractor-how-to-use-expected-conditions-with-protractor-10c545c670be]]></content>
  </entry>
  <entry>
    <title><![CDATA[Foreign Exchange]]></title>
    <url>%2F2018-06-09-FX%2F</url>
    <content type="text"><![CDATA[Foreign Exchange marketsGenerally, FX Swap is the biggest portion of FX market, followed by FX Spot, outright forward, Options. Forwardforward contract or simply a forward is a non-standardized contract between two parties to buy or to sell an asset at a specified future time at a price agreed upon today, making it a type of derivative instrument A foreign exchange swap has two legs - a spot transaction and a forward transaction - that are executed simultaneously for the same quantity, and therefore offset each other.]]></content>
      <tags>
        <tag>FX</tag>
        <tag>Financial</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Jboss tips]]></title>
    <url>%2F2018-06-14-JBoss-Console%2F</url>
    <content type="text"><![CDATA[commands:to list all deployed applicationsjboss\jboss-eap-6.4\bin\jboss-cli.bat –connect –controller=localhost:7373 –command=/deployment=*:read-attribute(name=name) to list JNDI tree:123456789101112131415161718192021cd /subsystem=maills/subsystem=mail/mail-session=java:jboss/mail/payment_mail /subsystem=naming:jndi-view() &#125;,"mail" =&gt; &#123; "class-name" =&gt; "javax.naming.Context", "children" =&gt; &#123; "Default" =&gt; &#123; "class-name" =&gt; "javax.mail.Session", "value" =&gt; "javax.mail.Session@22951e8f" &#125;, "payment_mail" =&gt; &#123; "class-name" =&gt; "javax.mail.Session", "value" =&gt; "javax.mail.Session@548df9e2" &#125; &#125;&#125;, 12345678910111213 &lt;management-interfaces&gt; &lt;native-interface security-realm="ManagementRealm"&gt; &lt;socket-binding native="management-native"/&gt; &lt;/native-interface&gt; &lt;http-interface security-realm="ManagementRealm"&gt; &lt;socket-binding http="management-http"/&gt; &lt;/http-interface&gt; &lt;/management-interfaces&gt; &lt;/management&gt;xxx&lt;socket-binding-group name="standard-sockets" default-interface="public" port-offset="$&#123;jboss.socket.binding.port-offset:0&#125;"&gt; &lt;socket-binding name="management-native" interface="management" port="$&#123;jboss.management.native.port:7373&#125;"/&gt; &lt;socket-binding name="management-http" interface="management" port="$&#123;jboss.management.http.port:7371&#125;"/&gt;]]></content>
      <tags>
        <tag>Java</tag>
        <tag>Jboss</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Locking and multithreading]]></title>
    <url>%2F2018-06-12-Locking-And-Multithreading%2F</url>
    <content type="text"><![CDATA[Single Writer principleThere is a lot of research in computer science for managing this contention that boils down to 2 basic approaches. One is to provide mutual exclusion to the contended resource while the mutation takes place; the other is to take an optimistic strategy and swap in the changes if the underlying resource has not changed while you created the new copy. Memory BarierMemory barriers, or fences, are a set of processor instructions used to apply ordering limitations on memory operations. The keyword volatile prevents this problem because it establishes a happens before relationship between the write to the turn variable and the write to the intentFirst variable. The compiler cannot re-order these write operations and if necessary it must forbid the processor from doing so with a memory barrier. A memory barrier, also known as a membar, memory fence or fence instruction, is a type of barrier instruction that causes a central processing unit (CPU) or compiler to enforce an ordering constraint on memory operations issued before and after the barrier instruction. This typically means that operations issued prior to the barrier are guaranteed to be performed before operations issued after the barrier. Memory barriers are necessary because most modern CPUs employ performance optimizations that can result in out-of-order execution. This reordering of memory operations (loads and stores) normally goes unnoticed within a single thread of execution, but can cause unpredictable behaviour in concurrent programs and device drivers unless carefully controlled. Non blocking programingImplementationWith few exceptions, non-blocking algorithms use atomic read-modify-write primitives that the hardware must provide, the most notable of which is compare and swap (CAS). Compare And SwapIt compares the contents of a memory location with a given value and, only if they are the same, modifies the contents of that memory location to a new given value. This is done as a single atomic operation. The atomicity guarantees that the new value is calculated based on up-to-date information; if the value had been updated by another thread in the meantime, the write would fail. The result of the operation must indicate whether it performed the substitution; this can be done either with a simple boolean response (this variant is often called compare-and-set), or by returning the value read from the memory location (not the value written to it). Here is the pseudo code 1234567function cas(p : pointer to int, old : int, new : int) returns bool &#123; if *p ≠ old &#123; return false &#125; *p ← new return true&#125; This operation is used to implement synchronization primitives like semaphores and mutexes, as well as more sophisticated lock-free and wait-free algorithms. Algorithms built around CAS typically read some key memory location and remember the old value. Based on that old value, they compute some new value. Then they try to swap in the new value using CAS, where the comparison checks for the location still being equal to the old value. If CAS indicates that the attempt has failed, it has to be repeated from the beginning: the location is re-read, a new value is re-computed and the CAS is tried again. A common workaround is to add extra “tag” or “stamp” bits to the quantity being considered. For example, an algorithm using compare and swap on a pointer might use the low bits of the address to indicate how many times the pointer has been successfully modified. Because of this, the next compare-and-swap will fail, even if the addresses are the same, because the tag bits will not match. This does not completely solve the problem, as the tag bits will eventually wrap around, but helps to avoid it. Some architectures provide a double-word compare and swap, which allows for a larger tag. This is sometimes called ABAʹ since the second A is made slightly different from the first. Such tagged state references are also used in transactional memory. Priority InversionConsider two tasks H and L, of high and low priority respectively, either of which can acquire exclusive use of a shared resource R. If H attempts to acquire R after L has acquired it, then H becomes blocked until L relinquishes the resource. Sharing an exclusive-use resource (R in this case) in a well-designed system typically involves L relinquishing R promptly so that H (a higher priority task) does not stay blocked for excessive periods of time. Despite good design, however, it is possible that a third task M of medium priority (p(L) &lt; p(M) &lt; p(H), where p(x) represents the priority for task (x)) becomes runnable during L’s use of R. At this point, M being higher in priority than L, preempts L, causing L to not be able to relinquish R promptly, in turn causing H—the highest priority process—to be unable to run. This is called priority inversion where a higher priority task is preempted by a lower priority one. RCUIn computer science, read-copy-update (RCU) is a synchronization mechanism based on mutual exclusion. It is used when performance of reads is crucial and is an example of space–time tradeoff, enabling fast operations at the cost of more space. Read-copy-update allows multiple threads to efficiently read from shared memory by deferring updates after pre-existing reads to a later time while simultaneously marking the data, ensuring new readers will read the updated data. This makes all readers proceed as if there were no synchronization involved, hence they will be fast, but also making updates more difficult. package java/util/concurrent/atomicA small toolkit of classes that support lock-free thread-safe programming on single variables. In essence, the classes in this package extend the notion of volatile values, fields, and array elements to those that also provide an atomic conditional update operation of the form: 12345678910111213141516 boolean compareAndSet(expectedValue, updateValue);``` This method (which varies in argument types across different classes) atomically sets a variable to the updateValue if it currently holds the expectedValue, reporting true on success. The classes in this package also contain methods to get and unconditionally set values, as well as a weaker conditional atomic update operation weakCompareAndSet described below.The specifications of these methods enable implementations to employ efficient machine-level atomic instructions that are available on contemporary processors. However on some platforms, support may entail some form of internal locking. Thus the methods are not strictly guaranteed to be non-blocking -- a thread may block transiently before performing the operation. Instances of classes AtomicBoolean, AtomicInteger, AtomicLong, and AtomicReference each provide access and updates to a single variable of the corresponding type. Each class also provides appropriate utility methods for that type. For example, classes AtomicLong and AtomicInteger provide atomic increment methods. One application is to generate sequence numbers, as in:```java class Sequencer &#123; private final AtomicLong sequenceNumber = new AtomicLong(0); public long next() &#123; return sequenceNumber.getAndIncrement(); &#125; &#125; The AtomicIntegerArray, AtomicLongArray, and AtomicReferenceArray classes further extend atomic operation support to arrays of these types. These classes are also notable in providing volatile access semantics for their array elements, which is not supported for ordinary arrays. Volatile The Java volatile keyword is used to mark a Java variable as “being stored in main memory”. More precisely that means, that every read of a volatile variable will be read from the computer’s main memory, and not from the CPU cache, and that every write to a volatile variable will be written to main memory, and not just to the CPU cache. What’s wrong to volatile?The Java volatile keyword guarantees visibility of changes to variables across threads. This may sound a bit abstract, so let me elaborate. In a multithreaded application where the threads operate on non-volatile variables, each thread may copy variables from main memory into a CPU cache while working on them, for performance reasons. If your computer contains more than one CPU, each thread may run on a different CPU. That means, that each thread may copy the variables into the CPU cache of different CPUs. With non-volatile variables there are no guarantees about when the Java Virtual Machine (JVM) reads data from main memory into CPU caches, or writes data from CPU caches to main memory. This can cause several problems. visibility problemThe problem with threads not seeing the latest value of a variable because it has not yet been written back to main memory by another thread, is called a “visibility” problem. The updates of one thread are not visible to other threads. The Java volatile Visibility Guarantee The Java volatile keyword is intended to address variable visibility problems. By declaring the counter variable volatile all writes to the counter variable will be written back to main memory immediately. Also, all reads of the counter variable will be read directly from main memory. Full volatile Visibility GuaranteeActually, the visibility guarantee of Java volatile goes beyond the volatile variable itself. The visibility guarantee is as follows: If Thread A writes to a volatile variable and Thread B subsequently reads the same volatile variable, then all variables visible to Thread A before writing the volatile variable, will also be visible to Thread B after it has read the volatile variable. If Thread A reads a volatile variable, then all all variables visible to Thread A when reading the volatile variable will also be re-read from main memory.The Java volatile Happens-Before GuaranteeTo address the instruction reordering challenge, the Java volatile keyword gives a “happens-before” guarantee, in addition to the visibility guarantee. The happens-before guarantee guarantees that: Reads from and writes to other variables cannot be reordered to occur after a write to a volatile variable, if the reads / writes originally occurred before the write to the volatile variable. The reads / writes before a write to a volatile variable are guaranteed to &quot;happen before&quot; the write to the volatile variable. Notice that it is still possible for e.g. reads / writes of other variables located after a write to a volatile to be reordered to occur before that write to the volatile. Just not the other way around. From after to before is allowed, but from before to after is not allowed. Reads from and writes to other variables cannot be reordered to occur before a read of a volatile variable, if the reads / writes originally occurred after the read of the volatile variable. Notice that it is possible for reads of other variables that occur before the read of a volatile variable can be reordered to occur after the read of the volatile. Just not the other way around. From before to after is allowed, but from after to before is not allowed. In short: ==before write, after read==. Limitations of volatileEven if the volatile keyword guarantees that all reads of a volatile variable are read directly from main memory, and all writes to a volatile variable are written directly to main memory, there are still situations where it is not enough to declare a variable volatile Performance Considerations of volatileReading and writing of volatile variables causes the variable to be read or written to main memory. Reading from and writing to main memory is more expensive than accessing the CPU cache. Accessing volatile variables also prevent instruction reordering which is a normal performance enhancement technique. Thus, you should only use volatile variables when you really need to enforce visibility of variables.]]></content>
      <tags>
        <tag>CAS</tag>
        <tag>Concurrent</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM wram up]]></title>
    <url>%2F2018-06-10-JVM-Warm-up%2F</url>
    <content type="text"><![CDATA[JVM wram upKeeping this in mind, for low-latency applications, we need to cache all classes beforehand – so that they’re available instantly when accessed at runtime. This process of tuning the JVM is known as warming up. Escape AnalysisEscape analysis is a technique by which the Java Hotspot Server Compiler can analyze the scope of a new object’s uses and decide whether to allocate it on the Java heap. Based on escape analysis, an object’s escape state might be one of the following: GlobalEscape – An object escapes the method and thread. For example, an object stored in a static field, or, stored in a field of an escaped object, or, returned as the result of the current method. ArgEscape – An object passed as an argument or referenced by an argument but does not globally escape during a call. This state is determined by analyzing the bytecode of called method. NoEscape – A scalar replaceable object, meaning its allocation could be removed from generated code. After escape analysis, the server compiler eliminates scalar replaceable object allocations and associated locks from generated code. The server compiler also eliminates locks for all non-globally escaping objects. It does not replace a heap allocation with a stack allocation for non-globally escaping objects. The JIT aggressively inlines methods, removing the overhead of method calls. Methods that can be inlined include static, private or final methods but also public methods if it can be determined that they are not overridden. Because of this, subsequent class loading can invalidate the previously generated code. Because inlining every method everywhere would take time and would generate an unreasonably big binary, the JIT compiler inlines the hot methods first until it reaches a threshold. To determine which methods are hot, the JVM keeps counters to see how many times a method is called and how many loop iterations it has executed. This means that inlining happens only after a steady state has been reached, so you need to repeat the operations a certain number of times before there is enough profiling information available for the JIT compiler to do its job. Rather than trying to guess what the JIT is doing, you can take a peek at what’s happening by turning on java command line flags: -XX:+PrintCompilation -XX:+UnlockDiagnosticVMOptions -XX:+PrintInlining Here is what they do: -XX:+PrintCompilation: logs when JIT compilation happens -XX:+UnlockDiagnosticVMOptions: enables other flags like -XX:+PrintInliningGlobalEscape and ArgEscape objects must be allocated on the heap, but for ArgEscape objects it is possible to remove some locking and memory synchronization overhead because these objects are only visible from the calling thread. The NoEscape objects may be allocated freely, for example on the stack instead of on the heap. In fact, under some circumstances, it is not even necessary to construct an object at all, but instead only the object’s scalar values, such as an int for the object Integer. Synchronization may be removed too, because we know that only this thread will use the objects. For example, if we were to use the somewhat ancient StringBuffer (which as opposed to StringBuilder has synchronized methods), then these synchronizations could safely be removed. EA is currently only available under the C2 HotSpot Compiler so we have to make sure that we run in -server mode. Why It MattersIn theory, NoEscape objects objects can be allocated on the stack or even in CPU registers using EA, giving very fast execution. When we allocate objects on the heap, we start to drain our CPU caches because objects are placed on different addresses on the heap possibly far away from each other. This way we will quickly deplete our L1 CPU cache and performance will decrease. With EA and stack allocation on the other hand, we are using memory that (most likely) is already in the L1 cache anyhow. So, EA and stack allocation will improve our localization of data. This is good from a performance standpoint. Obviously, the garbage collects needs to run much less frequently when we are using EA with stack allocation. This is perhaps the biggest performance advantage. Recall that each time the JVM runs a complete heap scan, we take performance out of our CPUs and the CPU caches will quickly deplete. Not to mention if we have virtual memory paged out on our server, whereby GC is devastating for performance. The most important advantage of EA is not performance though. EA allows us to use local abstractions like Lambdas, Functions, Streams, Iterators etc. without any significant performance penalty so that we can write better and more readable code. Code that describes what we are doing rather than how it is done. The GC cleans up the heap and not the stack. The stack is cleaned up automatically when methods return to their caller whereby the stack pointer is reset to its former value. So GC will clean up objects that ended up on the stack before EA/C2 compilation could be performed. The actual instances (or rather their corresponding representations) live on the stack, there are no referenced objects on the stack in the context of EA optimizations. JIT optimizationSome JIT Compilation Techniques One of the most common JIT compilation techniques used by Java HotSpot VM is inlining, which is the practice of substituting the body of a method into the places where that method is called. Inlining saves the cost of calling the method; no new stack frames need to be created. By default, Java HotSpot VM will try to inline methods that contain less than 35 bytes of JVM bytecode. Another common optimization that Java HotSpot VM makes is monomorphic dispatch, which relies on the observed fact that, usually, there aren’t paths through a method that cause an object reference to be of one type most of the time but of another type at other times. You might think that having different types via different code paths would be ruled out by Java’s static typing, but remember that an instance of a subtype is always a valid instance of a supertype (this principle is known as the Liskov substitution principle, after Barbara Liskov). This situation means that there could be two paths into a method—for example, one that passes an instance of a supertype and one that passes an instance of a subtype—which would be legal by the rules of Java’s static typing (and does occur in practice). In the usual case (the monomorphic case), however, having different, path-dependent types does not happen. So we know the exact method definitions that will be called when methods are called on the passed object, because we don’t need to check which override is actually being used. This means we can eliminate the overhead of doing virtual method lookup, so the JIT compiler can emit optimized machine code that is often faster than an equivalent C++ call (because in the C++ case, the virtual lookup cannot easily be eliminated).The two Java HotSpot VM compiler modes use different techniques for JIT compilation, and they can output very different machine code for the same Java method. Modern Java applications, however, can usually make use of both compilation modes. Java HotSpot VM uses many other techniques to optimize the code that JIT compilation produces. Loop optimization, type sharpening, dead-code elimination, and intrinsics are just some of the other ways that Java HotSpot VM tries to optimize code as much as it can. Techniques are frequently layered one on top of another, so that once one optimization has been applied, the compiler might be able to see more optimizations that can be performed. Compilation ModesInside Java HotSpot VM, there are actually two separate JIT compiler modes, which are known as C1 and C2. C1 is used for applications where quick startup and rock-solid optimization are required; GUI applications are often good candidates for this compiler. C2, on the other hand, was originally intended for long-running, predominantly server-side applications. Prior to some of the later Java SE 7 releases, these two modes were available using the -client and -server switches, respectively. The two compiler modes use different techniques for JIT compilation, and they can output very different machine code for the same Java method. Modern Java applications, however, can usually make use of both compilation modes. To take advantage of this fact, starting with some of the later Java SE 7 releases, a new feature called tiered compilation became available. This feature uses the C1 compiler mode at the start to provide better startup performance. Once the application is properly warmed up, the C2 compiler mode takes over to provide more-aggressive optimizations and, usually, better performance. With the arrival of Java SE 8, tiered compilation is now the default behavior. Java memory monitoring tools1234567891011121314pemi$ jps | grep Main50903 Mainpemi$ jmap -histo 50903 | head num #instances #bytes class name---------------------------------------------- 1: 95 42952184 [I 2: 1079 101120 [C 3: 485 55272 java.lang.Class 4: 526 25936 [Ljava.lang.Object; 5: 13 25664 [B 6: 1057 25368 java.lang.String 7: 74 5328 java.lang.reflect.Field jmap - Memory Map Tool or Option Description and Usage Java Mission Control Java Mission Control (JMC) is a new JDK profiling and diagnostic tools platform for HotSpot JVM. It s a tool suite basic monitoring, managing, and production time profiling and diagnostics with high performance. Java Mission Control minimizes the performance overhead that’s usually an issue with profiling tools. See Java Mission Control. jcmd utility The jcmd utility is used to send diagnostic command requests to the JVM, where these requests are useful for controlling Java Flight Recordings. The JFRs are used to troubleshoot and diagnose JVM and Java Applications with flight recording events. See The jcmd Utility. Java VisualVM This utility provides a visual interface for viewing detailed information about Java applications while they are running on a Java Virtual Machine. This information can be used in troubleshooting local and remote applications, as well as for profiling local applications. See Java VisualVM. JConsole utility This utility is a monitoring tool that is based on Java Management Extensions (JMX). The tool uses the built-in JMX instrumentation in the Java Virtual Machine to provide information about performance and resource consumption of running applications. See JConsole. jmap utility This utility can obtain memory map information, including a heap histogram, from a Java process, a core file, or a remote debug server. See The jmap Utility. jps utility This utility lists the instrumented Java HotSpot VMs on the target system. The utility is very useful in environments where the VM is embedded, that is, it is started using the JNI Invocation API rather than the java launcher. See The jps Utility. jstack utility This utility can obtain Java and native stack information from a Java process. On Oracle Solaris and Linux operating systems the utility can alos get the information from a core file or a remote debug server. See The jstack Utility. jstat utility This utility uses the built-in instrumentation in Java to provide information about performance and resource consumption of running applications. The tool can be used when diagnosing performance issues, especially those related to heap sizing and garbage collection. See The jstat Utility. jstatd daemon This tool is a Remote Method Invocation (RMI) server application that monitors the creation and termination of instrumented Java Virtual Machines and provides an interface to allow remote monitoring tools to attach to VMs running on the local host. See The jstatd Daemon. visualgc utility This utility provides a graphical view of the garbage collection system. As with jstat, it uses the built-in instrumentation of Java HotSpot VM. See The visualgc Tool. Native tools Each operating system has native tools and utilities that can be useful for monitoring purposes. For example, the dynamic tracing (DTrace) capability introduced in Oracle Solaris 10 operating system performs advanced monitoring. See Native Operating System Tools. 1234567$ jps16217 MyApplication16342 jpsThe utility lists the virtual machines for which the user has access rights. This is determined by access-control mechanisms specific to the operating system. On Oracle Solaris operating system, for example, if a non-root user executes the jps utility, then the output is a list of the virtual machines that were started with that user's uid.In addition to listing the PID, the utility provides options to output the arguments passed to the application's main method, the complete list of VM arguments, and the full package name of the application's main class. The jps utility can also list processes on a remote system if the remote system is running the jstatd daemon. GC-less JavaJava Development without GCAll products developed by Coral Blocks have the very important feature of leaving ZERO garbage behind. Because the latency imposed by the Java Garbage Collector (i.e. GC) is unacceptable for high-performance systems and because it is impossible to turn off the GC, the best option for real-time systems in Java is to not produce any garbage at all so that the GC never kicks in. Imagine a high-performance matching engine operating in the microsecond level, sending and receiving hundreds of thousands messages per second. If at any given time the GC decides to kick in with its 1+ millisecond latencies, the disruption in the system will be huge. Therefore, if you want to develop real-time systems in Java with minimal variance and latency, the best option is to do it right without creating any garbage for the GC. Warming up, Checking the GC and SamplingThe key to make sure your system is not creating any garbage is to warm up your critical path from start to finish a couple of million times and then check for memory allocation another couple of million times. If it is allocating memory linearly as the number of iterations increases, it is most likely creating garbage and you should use the stack trace]]></content>
  </entry>
  <entry>
    <title><![CDATA[flexbox]]></title>
    <url>%2F2018-06-14-flex-box%2F</url>
    <content type="text"><![CDATA[How Flexbox works — explained with big, colorful, animated gifs Flexbox promises to save us from the evils of plain CSS (like vertical alignment). Well, Flexbox does deliver on that goal. But mastering its new mental model can be challenging. So let’s take an animated look at how Flexbox works, so we can use it to build better layouts. Flexbox’s underlying principle is to make layouts flexible and intuitive. To accomplish this, it lets containers decide for themselves how to evenly distribute their children — including their size and the space between them. This all sounds good in principle. But let’s see what it looks like in practice. In this article, we’ll dive into the 5 most common Flexbox properties. We’ll explore what they do, how you can use them, and what their results will actually look like. Property #1: Display: FlexHere’s our example webpage: You have four colored divs of various sizes, held within a grey container div. As of now, each div has defaulted to display: block. Each square thus takes up the full width of its line. In order to get started with Flexbox, you need to make your container into a flex container. This is as easy as: #container { display: flex;} Not a lot has changed — your divs are displayed inline now, but that’s about it. But behind the scenes, you’ve done something powerful. You gave your squares something called a flex context. You can now start to position them within that context, with far less difficulty than traditional CSS. Property #2: Flex DirectionA Flexbox container has two axes: a main axis and a cross axis, which default to looking like this: By default, items are arranged along the main axis, from left to right. This is why your squares defaulted to a horizontal line once you applied display: flex. Flex-direction, however, let’s you rotate the main axis. #container { display: flex; flex-direction: column;} There’s an important distinction to make here: flex-direction: column doesn’t align the squares on the cross axis instead of the main axis. It makes the main axis itself go from horizontal to vertical. There are a couple of other options for flex-direction, as well: row-reverse and column-reverse. Property #3: Justify ContentJustify-content controls how you align items on the main axis. Here, you’ll dive a bit deeper into the main/cross axis distinction. First, let’s go back to flex-direction: row. #container { display: flex; flex-direction: row; justify-content: flex-start;} You have five commands at your disposal to use justify-content: Flex-startFlex-endCenterSpace-betweenSpace-around Space-around and space-between are the least intuitive. Space-between gives equal space between each square, but not between it and the container. Space-around puts an equal cushion of space on either side of the square — which means the space between the outermost squares and the container is half as much as the space between two squares (each square contributing a non-overlapping equal amount of margin, thus doubling the space). A final note: remember that justify-content works along the main-axis, and flex-direction switches the main-axis. This will be important as you move to… Property #4: Align ItemsIf you ‘get’ justify-content, align-items will be a breeze. As justify-content works along the main axis, align-items applies to the cross axis. Let’s reset our flex-direction to row, so our axes look the same as the above image. Then, let’s dive into the align-items commands. flex-startflex-endcenterstretchbaselineThe first three are exactly the same as justify-content, so nothing too fancy here. The next two are a bit different, however. You have stretch, in which the items take up the entirety of the cross-axis, and baseline, in which the bottom of the paragraph tags are aligned. (Note that for align-items: stretch, I had to set the height of the squares to auto. Otherwise the height property would override the stretch.) For baseline, be aware that if you take away the paragraph tags, it aligns the bottom of the squares instead, like so: To demonstrate the main and cross axes better, let’s combine justify-content and align-items and see how centering works different for the two flex-direction commands: With row, the squares are set up along a horizontal main axis. With column, they fall along a vertical main axis. Even if the squares are centered both vertically and horizontally in both cases, the two are not interchangeable! Property #5: Align SelfAlign-self allows you to manually manipulate the alignment of one particular element. It’s basically overriding align-items for one square. All the properties are the same, though it defaults to auto, in which it follows the align-items of the container. #container { align-items: flex-start;} .square#one { align-self: center;} // Only this square will be centered.Let’s see what this looks like. You’ll apply align-self to two squares, and for the rest apply align-items: center and flex-direction: row. ConclusionEven though we’ve just scratched the surface of Flexbox, these commands should be enough for you to handle most basic alignments — and to vertically align to your heart’s content. https://github.com/angular/flex-layout https://medium.freecodecamp.org/an-animated-guide-to-flexbox-d280cf6afc35]]></content>
  </entry>
  <entry>
    <title><![CDATA[Ansible]]></title>
    <url>%2F2018-06-21-Ansible%2F</url>
    <content type="text"><![CDATA[Ansible: What Is It Good For?Ansible is often described as a configuration management tool, and is typically mentioned in the same breath as Chef, Puppet, and Salt. When we talk about configuration management, we are typically talking about writing some kind of state description for our servers, and then using a tool to enforce that the servers are, indeed, in that state: the right packages are installed, configuration files contain the expected values and have the expected permissions, the right services are running, and so on. Like other configuration management tools, Ansible exposes a domain-specific language (DSL) that you use to describe the state of your servers. These tools also can be used for doing deployment as well. When people talk about deployment, they are usually referring to the process of taking software that was written in-house, generating binaries or static assets (if necessary), copying the required files to the server(s), and then starting up the services. Capistrano and Fabric are two examples of open-source deployment tools. Ansible is a great tool for doing deployment as well as configuration management. Using a single tool for both configuration management and deployment makes life simpler for the folks responsible for operations. Some people talk about the need for orchestration of deployment. This is where multiple remote servers are involved, and things have to happen in a specific order. For example, you need to bring up the database before bringing up the web servers, or you need to take web servers out of the load balancer one at a time in order to upgrade them without downtime. Ansible’s good at this as well, and is designed from the ground up for performing actions on multiple servers. Ansible has a refreshingly simple model for controlling the order that actions happen in. Finally, you’ll hear people talk about provisioning new servers. In the context of public clouds such as Amazon EC2, this refers to spinning up a new virtual machine instance. Ansible’s got you covered here, with a number of modules for talking to clouds, including EC2, Azure, Digital Ocean, Google Compute Engine, Linode, and Rackspace, as well as any clouds that support the OpenStack APIs. ArchitectureAs with most configuration management software, Ansible has two types of servers: controlling machines and nodes. First, there is a single controlling machine which is where orchestration begins. Nodes are managed by a controlling machine over SSH. The controlling machine describes the location of nodes through its inventory. AgentlessIn contrast with popular configuration management software — such as Chef, Puppet, and CFEngine — Ansible uses an agentless architecture.[14] With an agent-based architecture, nodes must have a locally installed daemon that communicates with a controlling machine. With an agentless architecture, nodes are not required to install and run background daemons to connect with a controlling machine. This type of architecture reduces the overhead on the network by preventing the nodes from polling the controlling machine. PlaybookPlaybooks are Ansible’s configuration, deployment, and orchestration language. They can describe a policy you want your remote systems to enforce, or a set of steps in a general IT process. I like to think of Ansible playbooks as executable documentation. It’s like the README file that describes the commands you had to type out to deploy your software, except that the instructions will never go out-of-date because they are also the code that gets executed directly. If Ansible modules are the tools in your workshop, playbooks are your instruction manuals, and your inventory of hosts are your raw material. In Ansible, a script is called a playbook. A playbook describes which hosts (what Ansible calls remote servers) to configure, and an ordered list of tasks to perform on those hosts. To execute the playbook using the ansible-playbook command. In the example, the playbook is named webservers.yml, and is executed by typing: 1$ ansible-playbook webservers.yml Ansible will make SSH connections in parallel to web1, web2, and web3. It will execute the first task on the list on all three hosts simultaneously. In this example, the first task is installing the nginx apt package (since Ubuntu uses the apt package manager), so the task in the playbook would look something like this: 12- name: install nginx apt: name=nginx Ansible will: Generate a Python script that installs the nginx package. Copy the script to web1, web2, and web3. Execute the script on web1, web2, web3. Wait for the script to complete execution on all hosts. Ansible will then move to the next task in the list, and go through these same four steps. It’s important to note that: Ansible runs each task in parallel across all hosts. Ansible waits until all hosts have completed a task before moving to the next task. Ansible runs the tasks in the order that you specify them. VariableVariable names should be letters, numbers, and underscores. Variables should always start with a letter. foo_port is a great variable. foo5 is fine too.foo-port, foo port, foo.port and 12 are not valid variable names. Defining Variables in PlaybooksThe simplest way to define variables is to put a vars section in your playbook with the names and values of variables. Ansible also allows you to put variables into one or more files, using a section called vars_files. We would replace the vars section with a vars_files that looks like this: 123456789vars_files: - nginx.yml## nginx.ymlkey_file: /etc/nginx/ssl/nginx.keycert_file: /etc/nginx/ssl/nginx.crtconf_file: /etc/nginx/sites-available/defaultserver_name: localhost To debug variable1- debug: var=myvarname Registering VariablesOften, you’ll find that you need to set the value of a variable based on the result of a task. To do so, we create a registered variable using the register clause when invoking a module. In order to use the login variable later, we need to know what type of value to expect. The value of a variable set using the register clause is always a dictionary, but the specific keys of the dictionary are different, depending on the module that was invoked. ACCESSING DICTIONARY KEYS IN A VARIABLEIf a variable contains a dictionary, then you can access the keys of the dictionary using either a dot (.) or a subscript ([]). factsWhen Ansible gathers facts, it connects to the host and queries the host for all kinds of details about the host: CPU architecture, operating system, IP addresses, memory info, disk info, and more. This information is stored in variables that are called facts, and they behave just like any other variable does. Here’s a simple playbook that will print out the operating system of each server: 12345- name: print out operating system hosts: all gather_facts: True tasks: - debug: var=ansible_distribution Viewing All Facts Associated with a ServerAnsible implements fact collecting through the use of a special module called the setup module. You don’t need to call this module in your playbooks because Ansible does that automatically when it gathers facts. However, if you invoke it manually with the ansible command-line tool, like this: $ ansible server1 -m setup interactive modeIf Ansible did not succeed, add the -vvvv flag to see more details about the error: $ ansible testserver -i hosts -m ping -vvvv We can see that the module succeeded. The “changed”: false part of the output tells us that executing the module did not change the state of the server. The “ping”: “pong” text is output that is specific to the ping module. Simplifying with the ansible.cfg FileWe had to type a lot of text in the inventory file to tell Ansible about our test server. Fortunately, Ansible has a number of ways you can specify these sorts of variables so we don’t have to put them all in one place. Right now, we’ll use one such mechanism, the ansible.cfg file, to set some defaults so we don’t need to type as much. WHERE SHOULD I PUT MY ANSIBLE.CFG FILE?Ansible looks for an ansible.cfg file in the following places, in this order: File specified by the ANSIBLE_CONFIG environment variable ./ansible.cfg (ansible.cfg in the current directory) ~/.ansible.cfg (.ansible.cfg in your home directory) /etc/ansible/ansible.cfg I typically put an ansible.cfg in the current directory, alongside my playbooks. That way, I can check it into the same version control repository my playbooks are in. Run command remotelyI like to use the ansible command-line tool to run arbitrary commands on remote machines, like parallel SSH. You can execute arbitrary commands with the command module. When invoking this module, you also need to pass an argument to the module with the -a flag, which is the command to run. For example, to check the uptime of our server, we can use: $ ansible testserver -m command -a uptime The command module is so commonly used that it’s the default module, so we can omit it: $ ansible testserver -a uptime$ ansible testserver -a “tail /var/log/dmesg” inventoryWARNINGAlthough Ansible adds the localhost to your inventory automatically, you have to have at least one other host in your inventory file; otherwise, ansible-playbook will terminate with the error: ERROR: provided hosts list is empty property “Changed”The changed key is present in the return value of all Ansible modules, and Ansible uses it to determine whether a state change has occurred. For the command and shell module, this will always be set to true unless overridden with the changed_when clause ignore errorIgnoring when a module returns an error 12345- name: Run myprog command: /opt/myprog register: result ignore_errors: True- debug: var=result Data typeAll members of a list are lines beginning at the same indentation level starting with a “- “ (a dash and a space): 12345678---# A list of tasty fruitsfruits: - Apple - Orange - Strawberry - Mango... A dictionary is represented in a simple key: value form (the colon must be followed by a space): 12345# An employee recordmartin: name: Martin D'vloper job: Developer skill: Elite More complicated data structures are possible, such as lists of dictionaries, dictionaries whose values are lists or a mix of both: 123456789101112131415# Employee records- martin: name: Martin D'vloper job: Developer skills: - python - perl - pascal- tabitha: name: Tabitha Bitumen job: Developer skills: - lisp - fortran - erlang Dictionaries and lists can also be represented in an abbreviated form if you really want to: 123---martin: &#123;name: Martin D'vloper, job: Developer, skill: Elite&#125;fruits: ['Apple', 'Orange', 'Strawberry', 'Mango'] These are called “Flow collections”. span multiple linesValues can span multiple lines using | or &gt;. Spanning multiple lines using a “Literal Block Scalar” | will include the newlines and any trailing spaces. Using a “Folded Block Scalar” &gt; will fold newlines to spaces; it’s used to make what would otherwise be a very long line easier to read and edit. In either case the indentation will be ignored. Examples are: 123456789include_newlines: | exactly as you see will appear these three lines of poetryfold_newlines: &gt; this is really a single line of text despite appearances commandsfilefile - Sets attributes of files Sets attributes of files, symlinks, and directories, or removes files/symlinks/directories. Many other modules support the same options as the file module - including copy, template, and assemble. 1234567891011# change file ownership, group and mode. When specifying mode using octal numbers, first digit should always be 0.- file: path: /etc/foo.conf owner: foo group: foo mode: 0644- file: path: /work owner: root group: root mode: 01777 delegationThis isn’t actually rolling update specific but comes up frequently in those cases. If you want to perform a task on one host with reference to other hosts, use the ‘delegate_to’ keyword on a task. This is ideal for placing nodes in a load balanced pool, or removing them. It is also very useful for controlling outage windows. Be aware that it does not make sense to delegate all tasks, debug, add_host, include, etc always get executed on the controller. Using this with the ‘serial’ keyword to control the number of hosts executing at one time is also a good idea: 12345678910111213141516171819---- hosts: webservers serial: 5 tasks: - name: take out of load balancer pool command: /usr/bin/take_out_of_pool &#123;&#123; inventory_hostname &#125;&#125; delegate_to: 127.0.0.1 - name: actual steps would go here yum: name: acme-web-stack state: latest - name: add back to load balancer pool command: /usr/bin/add_back_to_pool &#123;&#123; inventory_hostname &#125;&#125; delegate_to: 127.0.0.1 These commands will run on 127.0.0.1, which is the machine running Ansible. There is also a shorthand syntax that you can use on a per-task basis: ‘local_action’. Here is the same playbook as above, but using the shorthand syntax for delegating to 127.0.0.1: 12345678910111213---# ... tasks: - name: take out of load balancer pool local_action: command /usr/bin/take_out_of_pool &#123;&#123; inventory_hostname &#125;&#125;# ... - name: add back to load balancer pool local_action: command /usr/bin/add_back_to_pool &#123;&#123; inventory_hostname &#125;&#125; A common pattern is to use a local action to call ‘rsync’ to recursively copy files to the managed servers. Here is an example: 123456---# ... tasks: - name: recursively copy files from management server to target local_action: command rsync -a /path/to/files &#123;&#123; inventory_hostname &#125;&#125;:/path/to/target/ Note that you must have passphrase-less SSH keys or an ssh-agent configured for this to work, otherwise rsync will need to ask for a passphrase. dev experiencelead]]></content>
  </entry>
  <entry>
    <title><![CDATA[Apigee]]></title>
    <url>%2F2018-06-21-Apigee%2F</url>
    <content type="text"><![CDATA[App deployment, configuration management and orchestration - all from one system. Ansible is powerful IT automation that you can learn quickly. ArchitectureAPIs are the glue that connect apps and act as the foundation for what we call the digital economy. Through every connection there is at least one API interacting with other applications and exchanging data. A business needs API management solutions that can support diverse users and not compromise the stability and reliability of back-end systems. Apigees intelligent API management platform allows companies to provide connected, seamless, digital experiences and increase the speed at which they innovate and adapt. Apigee Edge, Apigee’s API Management platform, provides a unified solution that:makes your APIs fail-proofhelps you grow your developer and partner ecosystemenables you to run your APIs at scaleprovides deep insights into your APIs and your businessApigee Edge is the API management tool that offers the solutions today’s digital economy demands. For more information, download our Definitive Guide to API Management. Build RESTful APIsYou have data, you have services, and you want to develop new business solutions quickly, both internally and externally. With Apigee, you can build API proxies—RESTful, HTTP-based APIs that interact with your services. With easy-to-use APIs, developers can be more productive, increasing your speed to market. API proxies give you the full power of Apigee’s API platform to secure API calls, throttle traffic, mediate messages, control error handling, cache things, build developer portals, document APIs, analyze API traffic data, make money on the use of your APIs, protect against bad bots, and more. paybookplaybooks contains plays, plays contains tasks, while taks call modules. tasks run sequentiallyhandlers are trigered by tasks and are un once, at the end of plays. Configuration managementAnsible configurations are simple data descriptions of your infrastructure (both human-readable and machine-parsable) - ensuring everyone on your team will be able to understand the meaning of each configuration task. New team members will be able to quickly dive in and make an impact. Existing team members can get work done faster - freeing up cycles to attend to more critical and strategic work instead of configuration management. Ansible requires nothing more than a password or SSH key in order to start managing systems and can start managing them without installing any agent software, avoiding the problem of “managing the management” common in many automation systems. There’s no more wondering why configuration management daemons are down, when to upgrade management agents, or when to patch security vulnerabilities in those agents. GOAL-ORIENTED, NOT SCRIPTEDAnsible features an state-driven resource model that describes the desired state of computer systems and services, not the paths to get them to this state. No matter what state a system is in, Ansible understands how to transform it to the desired state (and also supports a “dry run” mode to preview needed changes). This allows reliable and repeatable IT infrastructure configuration, avoiding the potential failures from scripting and script-based solutions that describe explicit and often irreversible actions rather than the end goal. SECURE &amp; AGENTLESSAnsible relies on the most secure remote configuration management system available as its default transport layer: OpenSSH. OpenSSH is available for a wide variety of platforms, is very lightweight and when security issues in OpenSSH are discovered, they are patched quickly. Further, Ansible does not require any remote agents. Ansible delivers all modules to remote systems and executes tasks, as needed, to enact the desired configuration. These modules run with user-supplied credentials, including support for sudo and even Kerberos and clean up after themselves when complete. Ansible does not require root login privileges, specific SSH keys, or dedicated users and respects the security model of the system under management. As a result, Ansible has a very low attack surface area and is quite easy to deploy into new environments. EFFICIENT ARCHITECTUREAnsible works by connecting to your nodes and pushing out small programs, called “Ansible modules” to them. These programs are written to be resource models of the desired state of the system. Ansible then executes these modules (over SSH by default), and removes them when finished. Your library of modules can reside on any machine, and there are no servers, daemons, or databases required. Typically you’ll work with your favorite terminal program, a text editor, and probably a version control system to keep track of changes to your content. MANAGE YOUR INVENTORY IN SIMPLE TEXT FILESBy default, Ansible represents what machines it manages using a very simple INI file that puts all of your managed machines in groups of your own choosing. To add new machines, there is no additional SSL signing server involved, so there’s never any hassle deciding why a particular machine didn’t get linked up due to obscure NTP or DNS issues. 1234567[webservers]www1.example.comwww2.example.com[dbservers]db0.example.comdb1.example.com installationOnce Ansible is installed, it will not add a database, and there will be no daemons to start or keep running. You only need to install it on one machine (which could easily be a laptop) and it can manage an entire fleet of remote machines from that central point. When Ansible manages remote machines, it does not leave software installed or running on them, so there’s no real question about how to upgrade Ansible when moving to a new version.]]></content>
  </entry>
  <entry>
    <title><![CDATA[Core Java]]></title>
    <url>%2F2018-06-26-Core-Java%2F</url>
    <content type="text"><![CDATA[Annotation retention policyWhat is Retention policy in java annotations? A retention policy determines at what point annotation should be discarded.Java defined 3 types of retention policies through java.lang.annotation.RetentionPolicy enumeration. It has SOURCE, CLASS and RUNTIME.Annotation with retention policy SOURCE will be retained only with source code, and discarded during compile time.Annotation with retention policy CLASS will be retained till compiling the code, and discarded during runtime.Annotation with retention policy RUNTIME will be available to the JVM through runtime.The retention policy will be specified by using java built-in annotation @Retention, and we have to pass the retention policy type.The default retention policy type is CLASS. overload and override“selection among overloaded methods is static, while selection among overridden methods is dynamic. ” “a method is overridden when a subclass contains a method declaration with the same signature as a method declaration in an ancestor.” 12345678910111213141516171819202122232425“// Broken! - What does this program print?public class CollectionClassifier &#123; public static String classify(Set&lt;?&gt; s) &#123; return "Set"; &#125; public static String classify(List&lt;?&gt; lst) &#123; return "List"; &#125; public static String classify(Collection&lt;?&gt; c) &#123; return "Unknown Collection"; &#125; public static void main(String[] args) &#123; Collection&lt;?&gt;[] collections = &#123; new HashSet&lt;String&gt;(), new ArrayList&lt;BigInteger&gt;(), new HashMap&lt;String, String&gt;().values() &#125;; for (Collection&lt;?&gt; c : collections) System.out.println(classify(c)); &#125;&#125; You might expect this program to print Set, followed by List and Unknown Collection, but it doesn’t. It prints Unknown Collection three times. Why does this happen? Because the classify method is overloaded, and the choice of which overloading to invoke is made at compile time.” “the best way to fix the program is to replace all three overloadings of classify with a single method that does an explicit instanceof test: 1234public static String classify(Collection&lt;?&gt; c) &#123; return c instanceof Set ? "Set" : c instanceof List ? "List" : "Unknown Collection";&#125; Because overriding is the norm and overloading is the exception, overriding sets people’s expectations for the behavior of method invocation. As demonstrated by the CollectionClassifier example, overloading can easily confound these expectations. It is bad practice to write code whose behavior is likely to confuse programmers. This is especially true for APIs.” “Exactly what constitutes a confusing use of overloading is open to some debate. A safe, conservative policy is never to export two overloadings with the same number of parameters. If a method uses varargs, a conservative policy is not to overload it at all,” “Functional programming is a term that means different things to different people.”“At the heart of functional programming is thinking about your problem domain in terms of immutable values and functions that translate between them.” Target typing“What is implicit in all these examples is that a lambda expression’s type is context dependent. It gets inferred by the compiler. This target typing isn’t entirely new, either.” “This restriction is relaxed a bit in Java 8. It’s possible to refer to variables that aren’t final; however, they still have to be effectively final. Although you haven’t declared the variable(s) as final, you still cannot use them as nonfinal variable(s) if they are to be used in lambda expressions. If you do use them as nonfinal variables, then the compiler will show an error.” Functional interface“A functional interface is an interface with a single abstract method that is used as the type of a lambda expression.”]]></content>
      <tags>
        <tag>Java</tag>
        <tag>Coding</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Solace MQ]]></title>
    <url>%2F2018-07-04-Solace-MQ%2F</url>
    <content type="text"><![CDATA[Solace PubSub+It is a message broker that lets you establish event-driven interactions between applications and microservices across hybrid cloud environments using open APIs and protocols. DeploymentCloudPubSub+ Cloud is enterprise-grade messaging available as a fully managed service in your favorite public clouds. What do we mean by “enterprise grade?”Unrivaled ReliabilityIntelligent routing protocols always identify best path and adapt around network issues to keep your applications humming, and automatic message buffering keeps bursts of data from affecting slow consumers or your system as a whole. Built-in high availability and disaster recovery capabilities mean your system will bounce back in a flash, without ever losing a message, even in the event of major system or network failures. Serious SecurityPubSub+ supports authentication and authorization mechanisms ranging from username and password and one-time passwords to sophisticated access control lists and robust integration with existing security policies and systems such as LDAP, Radius, Kerberos. To protect messages in transit, Solace supports transport-layer TLS encryption using a variety of cypher suites. MessagingIn application development terms, messaging, which is also commonly known as message-oriented middleware or just middleware, refers to technology that lets computer systems share information without requiring direct connections or awareness of one another’s location. Docker on SolaceTo bounce solace server: 1234docker run -d -p 8080:8080 -p 55555:55555 --shm-size=2g --env 'username_admin_globalaccesslevel=admin' --env 'username_admin_password=admin' --name=solace solace-pubsub-standard:8.11.0.1029# or bring up an existing onedocker start solace Solace CLI management access:Enter the following docker exec command: 1docker exec -it solace /usr/sw/loads/currentload/bin/cli -A]]></content>
      <tags>
        <tag>Java</tag>
        <tag>MQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JXM]]></title>
    <url>%2F2018-07-10-JMX%2F</url>
    <content type="text"><![CDATA[Exporting your beans to JMXThe core class in Spring’s JMX framework is the MBeanExporter. This class is responsible for taking your Spring beans and registering them with a JMX MBeanServer. when running inside a container that does not provide an MBeanServer. To address this you can create an MBeanServer instance declaratively by adding an instance of the org.springframework.jmx.support.MBeanServerFactoryBean class to your configuration. You can also ensure that a specific MBeanServer is used by setting the value of the MBeanExporter’s server property to the MBeanServer value returned by an MBeanServerFactoryBean]]></content>
  </entry>
  <entry>
    <title><![CDATA[Camel]]></title>
    <url>%2F2018-07-13-Camel%2F</url>
    <content type="text"><![CDATA[EndpointsCamel supports the Message Endpoint pattern using the Endpoint interface. Endpoints are usually created by a Component and Endpoints are usually referred to in the DSL via their URIs. From an Endpoint you can use the following methods createProducer() will create a Producer for sending message exchanges to the endpointcreateConsumer() implements the Event Driven Consumer pattern for consuming message exchanges from the endpoint via a Processor when creating a ConsumercreatePollingConsumer() implements the Polling Consumer pattern for consuming message exchanges from the endpoint via a PollingConsumer implementationWhen using the DSL to create Routes you typically refer to Message Endpoints by their URIs rather than directly using the Endpoint interface. Its then a responsibility of the CamelContext to create and activate the necessary Endpoint instances using the available Component implementations. connectorSub interface including Initiator.abstract class SessionConnector implements Connector sessionclass Session.java FieldMapsWhich leverage TreeMap&lt;Integer, Field&lt;?&gt;&gt; fields; Field1234567public class Field&lt;T&gt; implements Serializable public Field(int field, T object) &#123; this.tag = field; this.object = object; &#125;]]></content>
  </entry>
  <entry>
    <title><![CDATA[QuickFixJ]]></title>
    <url>%2F2018-07-10-QuickFixJ%2F</url>
    <content type="text"><![CDATA[SettingsA settings file is set up with two types of heading, a [DEFAULT] and a [SESSION] heading. [SESSION] tells QuickFIX/J that a new Session is being defined. [DEFAULT] is a place that you can define settings which will be inherited by sessions that do not explicitly define them. If you do not provide a setting that QuickFIX/J needs, it will throw a ConfigError telling you what setting is missing or improperly formatted. SSL cipherAn SSL cipher specification in cipher-spec is composed of 4 major attributes plus a few extra minor ones. Key Exchange Algorithm:RSA or Diffie-Hellman variants. Authentication Algorithm:RSA, Diffie-Hellman, DSS or none. Cipher/Encryption Algorithm:DES, Triple-DES, RC4, RC2, IDEA or none. MAC Digest Algorithm:MD5, SHA or SHA1. InOut exchangeAlthough the FIX protocol is event-driven and asynchronous, there are specific pairs of messagesthat represent a request-reply message exchange. To use an InOut exchange pattern, there shouldbe a single request message and single reply message to the request. Examples include anOrderStatusRequest message and UserRequest. #FIX Sequence Number ManagementIf an application exception is thrown during synchronous exchange processing, this will cause QuickFIX/J to not increment incoming FIX message sequence numbers and will cause a resend of the counterparty message. This FIX protocol behavior is primarily intended to handle transport errors rather than application errors. There are risks associated with using this mechanism to handle application errors. The primary risk is that the message will repeatedly cause application errors each time it is re-received. A better solution is to persist the incoming message (database, JMS queue) immediately before processing it. This also allows the application to process messages asynchronously without losing messages when errors occur. Although it is possible to send messages to a FIX session before it is logged on (the messages will be sent at logon time), it is usually a better practice to wait until the session is logged on. This eliminates the required sequence number resynchronization steps at logon. Waiting for session logon can be done by setting up a route that processes the SessionLogon event category and signals the application to start sending messages. Source codeQuickFixJComponent.classif (configuration != null) { settings = configuration.createSessionSettings(); } else { settings = QuickfixjEngine.loadSettings(remaining); } MessageStoreThis interface Used by a Session to store and retrieve messages for resend purposes. boolean set(int sequence, String message) throws IOException; void get(int startSequence, int endSequence, Collection messages) throws IOException; Implementations such as MemoryStore.java, it use one HashMap&lt;Integer, String&gt; to keep messages (string) Parse bodyprivate void parseBody(DataDictionary dd, boolean doValidation) throws InvalidMessage { for(StringField field = this.extractField(dd, this); field != null; field = this.extractField(dd, this)) { if (isTrailerField(field.getField())) { this.pushBack(field); return; } validate check sumin message.class private void validateCheckSum(String messageData) throws InvalidMessage { try { int checksum = this.trailer.getInt(10); if (checksum != MessageUtils.checksum(messageData)) { throw new InvalidMessage(“Expected CheckSum=” + MessageUtils.checksum(messageData) + “, Received CheckSum=” + checksum + “ in “ + messageData); }} the first checksum is 131 in MessageUtils.checksumpublic static int checksum int end = isEntireMessage ? data.lastIndexOf(“\u000110=”) : -1; int len = end &gt; -1 ? end + 1 : data.length(); for(int i = 0; i &lt; len; ++i) { sum += data.charAt(i); } return sum &amp; 255;the checksum from above messageUtil is 87?? how to get and set this.trailer.10=131 ?]]></content>
  </entry>
  <entry>
    <title><![CDATA[File Util in Apache Camel]]></title>
    <url>%2F2018-07-16-FileUtils_Camel%2F</url>
    <content type="text"><![CDATA[FileUtil.classcompactPath(String path)To normalize path and join with provided separator if path is null, return nullif path.indexOf(47) == -1 &amp;&amp; path.indexOf(92) ==-1 (means /, ) return path normalizePathcheck whether it is windowsString osName = System.getProperty(“os.name”).toLowerCase(Locale.ENGLISH); return osName.contains(“windows”);for windows, replace / with \, for linux, replace \ with / split path by separator (\ or /) to get an array traverse array and check whether current is “..”, if so , call stack.popotherwise, stach.push(part) Then iterate stack and then combine them by a StringBuilder via provided addtional parameter separator]]></content>
  </entry>
  <entry>
    <title><![CDATA[Apache Camel]]></title>
    <url>%2F2018-07-11-Camel%2F</url>
    <content type="text"><![CDATA[Camel’s message modelIn Camel, there are two abstractions for modeling messages, both of which we’ll cover in this section. org.apache.camel.Message—The fundamental entity containing the data being carried and routed in Camel org.apache.camel.Exchange—The Camel abstraction for an exchange of mes- sages. This exchange of messages has an “in” message and as a reply, an “out” message MessageMessages are the entities used by systems to communicate with each other when using messaging channels. Messages flow in one direction from a sender to a receiver, Messages have a body (a payload), headers, and optional attachments, Messages are uniquely identified with an identifier of type java.lang.String. The identifier’s uniqueness is enforced and guaranteed by the message creator, it’s protocol depen- dent, and it doesn’t have a guaranteed format.For protocols that don’t define a unique message identification scheme, Camel uses its own UID generator.HEADERS AND ATTACHMENTSHeaders are values associated with the message, such as sender identifiers, hints about content encoding, authentication infor- mation, and so on. Headers are name-value pairs; the name is a unique, case-insensitive string, and the value is of type java. lang.Object. This means that Camel imposes no constraints on the type of the headers. Headers are stored as a map within the message. A message can also have optional attachments, which are typically used for the web service and email components. ExchangeAn exchange in Camel is the message’s container during routing. An exchange also provides support for the various types of interactions between systems, also known as message exchange patterns (MEPs). MEPs are used to differentiate between one-way and request-response messaging styles. The Camel exchange holds a pattern property that can be either InOnly—A one-way message (also known as an Event message). For example, JMS messaging is often one-way messaging. InOut—A request-response message. For example, HTTP-based transports are often request reply, where a client requests to retrieve a web page, waiting for the reply from the server. Start Camel applicationCamel doesn’t start magically by itself. Often it’s the server (container) that Camel is running inside that invokes the start method on CamelContext, starting up Camel. This is also what you saw in chapter 1, where you used Camel inside a standalone Java application. A standalone Java application isn’t the only deployment choice—you can also run Camel inside a container such as Spring or OSGi.Regardless of which container you use, the same principle applies. The container must prepare and create an instance of CamelContext up front, before Camel can be started. Spring containerCamel provides the CamelNamespaceHandler.When using Camel in the Spring XML file, you would define the tag as follows: The http://camel.apache.org/schema/spring namespace is the Camel custom namespace. To let Spring know about this custom namespace, it must be identified in the META-INF/spring.handlers, where you map the namespace to the class implementation:http://camel.apache.org/schema/spring= org.apache.camel.spring.handler.CamelNamespaceHandler The CamelNamespaceHandler is then responsible for parsing the XML and dele- gating to other factories for further pro- cessing. One of these factories is the Camel- ContextFactoryBean, which is responsible for creating the CamelContext that essen- tially is your Camel application.When Spring is finished initializing, it signals to third-party frameworks that they can start by broadcasting the Context- RefreshedEvent event. StartupAt this point, CamelContext is ready to be started. What happens next is the same regardless of which container or deploy- ment option you’re using with Camel. CamelContext is started by invoking its start method. The first step is to determines whether or not autostartup is enabled for Camel. If it’s disabled, the entire startup process is skipped. By default, Camel is set to autostart, which involves the following four steps. 1 Start internal services—Prepares and starts internal services used by Camel, such as the type-converter mechanism.2 Compute starting order—Computes the order in which the routes should be started. By default, Camel will start up all the routes in the order they are defined in the Spring XML files or the RouteBuilder classes. We’ll cover how to configure the order of routes in section 13.1.3.3 Prepare routes—Prepares the routes before they’re started.4 Start routes—Starts the routes by starting the consumers, which essentially opensthe gates to Camel and lets the messages start to flow in. After step 4, Camel writes a message to the log indicating that it has been started and that the startup process is complete. ConceptENDPOINTAn endpoint is the Camel abstraction that models the end of a channel through which a system can send or receive messages. In Camel, you configure endpoints using URIs, such as 1file:data/inbox?delay=5000 and you also refer to endpoints this way. At runtime, Camel will look up an endpoint based on the URI notation. The scheme denotes which Camel component handles that type of endpoint. In this case,the scheme of file selects the FileComponent.The FileComponent then works as a factory creat-ing the FileEndpoint based on the remaining parts of the URI.The context path data/ inbox tells the FileComponent that the starting folder is data/inbox.The option, delay=5000 indicates that files should be polled at a 5 second interval. There’s more to an endpoint than meets the eye. JMSQueues are strictly point-to-point, where each message has only one consumer. Topics operate on a publish/subscribe scheme; a single message may be delivered to many consumers if they have subscribed to the topic. JMS also provides a ConnectionFactory that clients (like Camel) can use to cre- ate a connection with a JMS provider. JMS providers are usually referred to as brokers because they manage the communication between a message producer and a mes- sage consumer. HOW TO CONFIGURE CAMEL TO USE A JMS PROVIDERTo connect Camel to a specific JMS provider, you need to configure Camel’s JMS com- ponent with an appropriate ConnectionFactory.Apache ActiveMQ is one of the most popular open source JMS providers, and it’s the primary JMS broker that the Camel team uses to test the JMS component. JSM destinationsThere are two types of JMS destinations: queues and topics. The queue is a point-to-point channel, where each message has only one recipient. A topic delivers a copy of the message to all clients who have subscribed to receive it. ActiveMQo in the case of Apache ActiveMQ, you can create an ActiveMQConnectionFactorythat points to the location of the running ActiveMQ broker: 12ConnectionFactory connectionFactory = new ActiveMQConnectionFactory("vm://localhost"); The vm://localhost URI means that you should connect to an embedded broker named “localhost” running inside the current JVM. The vm transport connector in ActiveMQ creates a broker on demand if one isn’t running already, so it’s very handy for quickly testing JMS applications; for production scenarios, it’s recommended that you connect to a broker that’s already running. Next, when you create your CamelContext, you can add the JMS component as follows:CamelContext context = new DefaultCamelContext();context.addComponent(“jms”, JmsComponent.jmsComponentAutoAcknowledge(connectionFactory));The JMS component and the ActiveMQ-specific connection factory aren’t part of the camel-core module. In order to use these, you’ll need to add some dependencies to your Maven-based project. For the plain JMS component, all you have to add is this: 1234567891011&lt;dependency&gt; &lt;groupId&gt;org.apache.camel&lt;/groupId&gt; &lt;artifactId&gt;camel-jms&lt;/artifactId&gt; &lt;version&gt;2.5.0&lt;/version&gt;&lt;/dependency&gt;The connection factory comes directly from ActiveMQ, so you’ll need the following dependency:&lt;dependency&gt; &lt;groupId&gt;org.apache.activemq&lt;/groupId&gt; &lt;artifactId&gt;activemq-core&lt;/artifactId&gt; &lt;version&gt;5.3.2&lt;/version&gt;&lt;/dependency&gt; USING URIS TO SPECIFY THE DESTINATIONOnce the JMS component is configured, you can start sending and receiving JMS mes- sages at your leisure. Because you’re using URIs, this is a real breeze to configure.Let’s say you want to send a JMS message to the queue named incomingOrders. The URI in this case would be 1jms:queue:incomingOrders This is pretty self-explanatory. The “jms” prefix indicates that you’re using the JMS component you configured before. By specifying “queue”, the JMS component knows to send to a queue named incomingOrders. You could even have omitted the queuequalifier, because the default behavior is to send to a queue rather than a topic. NOTE Some endpoints can have an intimidating list of endpoint URI proper- ties. For instance, the JMS component has about 60 options, many of which are only used in specific JMS scenarios. Camel always tries to provide built-in defaults that fit most cases, and you can always find out what the default values are by browsing to the component’s page in the online Camel documentation. Using Camel’s Java DSL, you can send a message to the incomingOrders queue by using the to keyword like this:…to(“jms:queue:incomingOrders”)This can be read as sending to the JMS queue named incomingOrders. FINDING ROUTE BUILDERSUsing the Spring CamelContext as a runtime and the Java DSL for route development is a great way of using Camel. In fact, it’s the most frequent usage of Camel.You saw before that you can explicitly tell the Spring CamelContext what route builders to load. You can do this by using the routerBuilder element: 123&lt;camelContext xmlns="http://camel.apache.org/schema/spring"&gt; &lt;routeBuilder ref="ftpToJmsRoute"/&gt;&lt;/camelContext&gt; Being this explicit results in a clean and concise definition of what is being loaded into Camel.Sometimes, though, you may need to be a bit more dynamic. This is where the packageScan and contextScan elements come in: 12345&lt;camelContext xmlns="http://camel.apache.org/schema/spring"&gt; &lt;packageScan&gt; &lt;package&gt;camelinaction.routes&lt;/package&gt; &lt;/packageScan&gt;&lt;/camelContext&gt; This packageScan element will load all RouteBuilder classes found in the camelinac- tion.routes package, including all subpackages.You can even be a bit more picky about what route builders are included: 1234567&lt;camelContext xmlns="http://camel.apache.org/schema/spring"&gt; &lt;packageScan&gt; &lt;package&gt;camelinaction.routes&lt;/package&gt; &lt;excludes&gt;**.*Test*&lt;/excludes&gt; &lt;includes&gt;**.*&lt;/includes&gt; &lt;/packageScan&gt;&lt;/camelContext&gt; In this case, you’re loading all route builders in the camelinaction.routes package, except for ones with “Test” in the class name. The matching syntax is similar to what is used in Apache Ant’s file pattern matchers.]]></content>
      <tags>
        <tag>Apache</tag>
        <tag>Camel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scala]]></title>
    <url>%2F2018-07-28-Scala%2F</url>
    <content type="text"><![CDATA[Scala StringScala offers the magic of implicit conver‐ sions, String instances also have access to all the methods of the StringOps class, so you can do many other things with them, such as treating a String instance as a sequence of characters. As a result, you can iterate over every character in the string using the foreach method: 12345678scala&gt; "hello".foreach(println) hell o//You can treat a String as a sequence of characters in a for loop:scala&gt; for (c &lt;- "hello") println(c)scala&gt; val result = "hello world".filter(_ != 'l') result: String = heo word]]></content>
  </entry>
  <entry>
    <title><![CDATA[Presto DB]]></title>
    <url>%2F2018-08-15-PrestoDB%2F</url>
    <content type="text"><![CDATA[WHAT IS PRESTO?Presto is an open source distributed SQL query engine for running interactive analytic queries against data sources of all sizes ranging from gigabytes to petabytes. Presto was designed and written from the ground up for interactive analytics and approaches the speed of commercial data warehouses while scaling to the size of organizations like Facebook. IntroductionPresto is a distributed system that runs on a cluster of machines. A full installation includes a coordinator and multiple workers. Queries are submitted from a client such as the Presto CLI to the coordinator. The coordinator parses, analyzes and plans the query execution, then distributes the processing to the workers. Presto does not use MapReduce and thus only requires HDFS. Separation of Storage and ComputeArchitected for separation of storage and compute, Presto can scale up and down based on your analytics demand to access this data. There’s no need to move your data and provisioning compute to the exact need results in significant cost savings. Config PropertiesThe config properties file, etc/config.properties, contains the configuration for the Presto server. Every Presto server can function as both a coordinator and a worker, but dedicating a single machine to only perform coordination work provides the best performance on larger clusters. Catalog PropertiesPresto accesses data via connectors, which are mounted in catalogs. The connector provides all of the schemas and tables inside of the catalog. For example, the Hive connector maps each Hive database to a schema, so if the Hive connector is mounted as the hive catalog, and Hive contains a table clicks in database web, that table would be accessed in Presto as hive.web.clicks. Catalogs are registered by creating a catalog properties file in the etc/catalog directory. For example, create etc/catalog/jmx.properties with the following contents to mount the jmx connector as the jmx catalog:]]></content>
  </entry>
  <entry>
    <title><![CDATA[Cucumber]]></title>
    <url>%2F2018-08-08-Cucumber%2F</url>
    <content type="text"><![CDATA[Acceptance testing vs unit testIt’s sometimes said that unit tests ensure you build the thing right, whereas acceptance tests ensure you build the right thing. Cucumber.The acceptance testing tool. Source of TruthFor many teams, the Cucumber feature files become the definitive source of truth as to what the system does. ScenariosScenarios are written before production code. They start their life as an executable specification. As the production code emerges, Scenarios take on a role as living documentation and automated tests. Each Cucumber test is called a scenario, and each scenario contains steps that tell Cucumber what to do. GherkinThe keywords Feature, Scenario, Given, When, and Then are the structure, and everything else is documentation.The structure is called Gherkin. Cucumber features are all about communicating with business users in their language, and it’s important that we don’t force them to sound like robots. StructureWe start with features, which contain our scenarios and steps. The steps of our scenarios call step definitions that provide the link between the Gherkin fea- tures and the application being built. This principle, deliberately doing the minimum useful work the tests will let us get away with, might seem lazy, but in fact it’s a discipline. It ensures that we make our tests thorough: if the test doesn’t drive us to write the right thing, then we need a better test. Step DefinitionsStep definitions are the glue that binds your Cucumber tests to the application you’re testing. A scenario that’s been executed can end up in any of the following states: Failed Pending Undefined Skipped PassedThese states are designed to help indicate the progress that you make as you develop your tests. Pending StepsWhen Cucumber discovers a step definition that’s halfway through being implemented, it marks the step as pending (yellow). Again, the scenario will be stopped, and the rest of the steps will be skipped or marked as undefined. 123456public class Steps &#123;@Given("^I have deposited \\$(\\d+) in my account$")public void iHaveDeposited$InMyAccount(int amount) throws Throwable &#123; // Write code here that turns the phrase above into concrete actionsthrow new PendingException(); &#125;&#125; Step definition recapThe pending status is a bit like those under construction signs you used to see all over the Internet in the 1990s. You can use it as a temporary signpost to your teammates that you’re in the middle of working on something. Because regular expressions can contain wildcards, this means you have the flexibility to make the Gherkin steps nice and readable, while keeping your Java step definition code clean and free of duplication.• Step definitions provide a mapping from the Gherkin scenarios’ plain- language descriptions of user actions into Java code, which simulates those actions.• Step definitions are registered with Cucumber by using @Given, @When, @Then, or one of the aliases for your spoken language.• Step definitions use regular expressions to declare the steps that they can handle. Because regular expressions can contain wildcards, one step definition can handle several different steps.• A step definition communicates its result to Cucumber by raising, or not raising, an exception. recap Readability should be your number-one goal when writing Gherkin fea- tures. Always try to sit together with a stakeholder when you write your scenarios, or at the very least pass them over for feedback once you’ve written them. Keep fine-tuning the language in your scenarios to make them more readable. Use a Background to factor out repeated steps from a feature and to help tell a story. Repetitive scenarios can be collapsed into a Scenario Outline. Steps can be extended with multiline strings or data tables. You can organize features into subfolders, like chapters in a book. Tags allow you to mark up scenarios and features so you select particular sets to run or report on. Samplecompile and run via CLI123javac -cp "jars/*" step_definitions/CheckoutSteps.javajava -cp "jars/*:." cucumber.api.cli.Main -p pretty --snippets camelcase \ -g step_definitions features Line 1 compiles the CheckoutSteps class that we’ve just created. Then line 2 invokes Cucumber. There are two slight additions to Cucumber’s invocation: We’ve added the current directory “.” to the classpath. We’ve added the -g step_definitions command-line argument to tell Cucumber where to look for the step definitions that it will need to “glue” the steps in the feature file to the checkout application (which we haven’t written yet). feature file1234567Feature: Is it Friday yet? Everybody wants to know when it&apos;s Friday Scenario: Sunday isn&apos;t Friday Given today is Sunday When I ask whether it&apos;s Friday yet Then I should be told &quot;Nope&quot; The first line of this file starts with the keyword Feature: followed by a name. It’s a good idea to use a name similar to the file name. The second line is a brief description of the feature. Cucumber does not execute this line, it’s just documentation. The fourth line, Scenario: Sunday is not Friday is a Scenario, which is a concrete example illustrating how the software should behave. The last three lines starting with Given, When and Then are the steps of our scenario. This is what Cucumber will execute. Notice how we go from Scenario to Scenario Outline when we start using Examples. 12345678910111213Feature: Is it Friday yet? Everybody wants to know when it&apos;s Friday Scenario Outline: Today is or is not Friday Given today is &lt;day&gt; When I ask whether it&apos;s Friday yet Then I should be told &lt;answer&gt; Examples: | day | answer | | &quot;Friday&quot; | &quot;TGIF&quot; | | &quot;Sunday&quot; | &quot;Nope&quot; | | &quot;anything else!&quot; | &quot;Nope&quot; | Scenario Outline: Withdraw fixed amountGiven I have in my accountWhen I choose to withdraw the fixed amount of Then I should receive cashAnd the balance of my account should be Examples: | Balance | Withdrawal | Received | Remaining || $500| $500| $500|$50 |$50 |$450 | | $100 | $100 | $400 | | $200 | $200 | $300 |We indicate placeholders within the scenario outline using angle brackets (&lt;..&gt;) where we want real values to be substituted. The scenario outline itself is useless without an Examples table, which lists rows of values to be substituted for each placeholder. Doc StringsDoc strings allow you to specify a larger piece of text than you could fit on a single line. For example, if you need to describe the precise content of an email message, you could do it like this:Scenario: Ban Unscrupulous UsersWhen I behave unscrupulouslyThen I should receive an email containing:“”” Dear Sir, Your account privileges have been revoked due to your unscrupulous behavior.Sincerely,The Management “””And my account should be lockedJust like a data table, the entire string between the “”” triple quotes is attached to the step above it. The indentation of the opening “”” is not important, although common practice is to indent two spaces from the enclosing step, as we’ve shown. The indentation inside the triple quotes, however, is signifi- cant: imagine the left margin running down from the start of the first “””. If you want to include indentation within your string, you need to indent it within this margin. TGIFThanks God It’s Friday Data table12345Given these Users:| name | date of birth | | Michael Jackson | August 29, 1958 | | Elvis | January 8, 1935 | | John Lennon | October 9, 1940 | That’s much clearer. The table starts on the line immediately following the step, and its cells are separated using the pipe character: |. You can line up the pipes using whitespace to make the table look tidy, although Cucumber doesn’t mind whether you do; it will strip out the values in each cell, ignoring the surrounding whitespace. public class BoardSteps {@Given(“^a board like this:$”)public void aBoardLikeThis(DataTable arg1) throws Throwable { // Write code here that turns the phrase above into concrete actions // For automatic transformation, change DataTable to one of // List, List&lt;List&gt;, List&lt;Map&lt;K,V&gt;&gt; or Map&lt;K,V&gt;. // E,K,V must be a scalar (String, Integer, Date, enum etc)throw new PendingException(); } BDDThe two main practices in the BDD approach are discovery workshops, which bridge the communication gap between business and IT, and executable specifications. Background123456789Background:Given I have been issued a new cardAnd I insert the card, entering the correct PIN And I choose "Change PIN" from the menuScenario: Change PIN successfullyWhen I change the PIN to 9876Then the system should remember my PIN is now 9876Scenario: Try to change PIN to the same as beforeWhen I try to change the PIN to the original PIN number Then I should see a warning messageAnd the system should not have changed my PIN Our refactoring hasn’t changed the behavior of the tests at all: at runtime, the steps in the background are executed at the beginning of each scenario, just as they were before. What we have done is made each individual scenario much easier to read. Using a Background element isn’t always necessary, but it’s often useful to improve the readability of your features by removing repetitive steps from individual scenarios. A good ‘background’ Make your Background section vivid. Use colorful names and try to tell a story, because your readers can keep track of stories much better than they can keep track of dull names like User A, User B, Site 1, and so on. If it’s worth mentioning at all, make it really stand out. Keep your scenarios short, and don’t have too many. If the Background is more than three or four steps long, think about using higher-level steps or splitting the feature file in two. You can use a background as a good indicator of when a feature is getting too long: if the new scenarios you want to add don’t fit with the existing background, consider splitting the feature. Avoid putting technical details such as clearing queues, starting back- end services, or opening browsers in a background. Backgrounds are useful for taking Given (and sometimes When) steps that are repeated in each scenario and moving them to a single place. This helps keep your scenarios clear and concise. Discovery WorkshopsDiscovery workshops (or Specification workshops) are short and frequent meetings where business and IT meet to gain a common understanding of how the software should behave. Relationship with TDDThe main difference is that Cucumber operates on a higher abstraction level, closer to the domain and farther away from classes and methods. BDD builds on TDD, while preserving a strong link between the business requirements and the technical solution. Outside inThis technique is called Outside-in because programmers typically start with the functionality that is closest to the user (the user interface, which is on the outside of the system) and gradually work towards the guts of the system (business logic, persistence, messaging and so on) as they discover more of what needs to be implemented. Your cucumber features should drive your implementation, not reflect it. This means Cucumber features should be written before the code implementing the feature. Notice that we’re just sketching out the interface to the class, rather than adding any implementation to it. This way of working is fundamental to out- side-in development. We try not to think about how the Account is going to work yet but concentrate on what it should be able to do. Keeping specifications, regression tests and documentation in a single place reduces the overhead of keeping multiple documents in sync - the Cucumber scenarios work as a shared source of truth for business and IT. While many people focus on the value added by the automated “tests” you get out of BDD, the real value is actually the shared understanding we get at the beginning. Cucumber is not a tool for testing software. It is a tool for testing people’s understanding of how software (yet to be written) should behave. The biggest advantage of BDD approach for software development might be that they describe a set of functions that a user expects from a system in a very concrete and direct manner. The sum of these behaviors essentially document a contract with the user/client. If any of the tests fail, this contract is not upheld. Processthe most important stage of BDD. Three amigos (business persons, developers, testers) get together and identify the expected behavior of our product by discussing examples. We can use feature mapping approach to effectively analyse and elaborate the product behavior. always make sure that your scenarios are not tightly coupled with your tests. Your BDD scenarios should change only when the requirement changes, not when the the implementation changes (i.e. your BDD scenarios must drive the implementation, not the other way around). Executable SpecificationAn Executable Specification is a Definition of Done that you can run as a test. In Behavior Driven Development (BDD), we refer to acceptance criteria as “executable specifications.” Executable Specifications are meant to be clear, unambiguous, written in business terms, and easy to automate. Each acceptance criteria is a concrete example of how a user interacts with the system to achieve some business goal. The most well-known format for BDD acceptance criteria uses the “Given-When-Then” structure: 123Given &lt;some precondition&gt;When &lt;something happens&gt;Then &lt;we expect some outcome&gt; This format is a great way to make sure that we are thinking in terms of the outcomes we want to achieve. After all, the outcomes of an application are where the value lies. These scenarios are also easy to automate with BDD tools like Cucumber and Specflow. No silver bullet“The hardest single part of building a software system is deciding precisely what to build.”We’ve all worked on projects where, because of a misunderstanding, code that we’d worked hard on for several days or more had to be thrown away. Better communication between developers and stakeholders is essential to help avoid this kind of wasted time. One technique that really helps facilitate this communication is the use of concrete examples to illustrate what we want the software to do. Concrete ExamplesBy using real-world examples to describe the desired behavior of the system we want to build, we stay grounded in language and terminology that makes sense to our stakeholders: we’re speaking their language. To illustrate this, let’s imagine you’re building a credit card payment system. One of the requirements is to make sure users can’t enter bad data. Here’s one way of expressing that:Customers should be prevented from entering invalid credit card details.This is an example of what Agile teams often call acceptance criteria or condi- tions of satisfaction.1 We use the word acceptance because they tell us what the system must be able to do in order for our stakeholders to find it acceptable. The previous requirements statement is useful, but it leaves far too much room for ambiguity and misunderstanding. It lacks precision. What exactly makes a set of details invalid? How exactly should the user be prevented from entering them? We’ve seen too many projects get dragged into the tar pit2 by these kind of worthy but vague statements. Let’s try illustrating this requirement with a concrete example:If a customer enters a credit card number that isn’t exactly 16 digits long, when they try to submit the form, it should be redisplayed with an error message advising them of the correct number of digits. Can you see how much more specific this second statement is? As a developer implementing this feature, we know almost everything we need to be able to sit down and start working on the code. As a stakeholder, we have a much clearer idea of what the developer is going to build.In fact, a stakeholder reading this might point out that there are certain types of cards that are valid with fewer than 16 digits and give us another example. This is the real power of examples: they stimulate our imagination, enabling us to explore and discover edge cases we might otherwise not have found until much later. By giving an example to illustrate our requirement, we’ve turned an acceptance criterion into an acceptance test. Now we have something unambiguous that we can use to test the behavior of the system, either manually or by using an automated test script. GherkinsGherkin use main keywords: Feature, Scenario, Given, When, Then, And, But, Background, Scenario Outline, Examplesand some extra syntax “”” (Doc strings), | (Data tables), @(Tags), # (Comments). dry run1$ java -cp ".:jars/*" cucumber.api.cli.Main -g step_definitions --dry-run features The –dry-run switch tells Cucumber to parse the file without executing it. Itwill tell you if your Gherkin isn’t valid. Replacing Given/When/Then with BulletsSome people find Given, When, Then, And, and But a little verbose. There is an additional keyword you can use to start a step: * (an asterisk). We could have written the previous scenario like this:Scenario: Attempt withdrawal using stolen card * I have $100 in my account my card is invalid I request $50 my card should not be returned I should be told to contact the bankTo Cucumber, this is exactly the same scenario. Do you find this version easier to read? Maybe. Did some of the meaning get lost? Maybe. It’s up to you and your team how you want to word things. The only thing that matters is that everybody understands what’s communicated. CucumberOptionsthe @CucumberOptions. One can define the location of features, glue files (step definitions), and formatter plugins inside this Cucumber options. 1234567891011@CucumberOptions( features = "src/test/resources/features", glue = &#123;"stepdefs"&#125;, tags = &#123;"~@Ignore"&#125;, format = &#123; "pretty", "html:target/cucumber-reports/cucumber-pretty", "json:target/cucumber-reports/CucumberTestReport.json", "rerun:target/cucumber-reports/rerun.txt" &#125;)public class TestRunner &#123; Step definitionsCucumber doesn’t know how to execute your scenarios out-of-the-box. It needs Step Definitions to translate plain text Gherkin steps into actionsthat will interact with the system. When Cucumber executes a Step in a Scenario, it will look for a matching Step Definition to execute. Afterone can implement initial configurations of the project in TestNG’s BeforeClass method. In cucumber’s Before hook, one can implement code to open web browser which is a prerequisite for all scenarios. In Background of each feature, one can implement steps to navigate to web site and/or login to account. In Cucumber’s After hook, one can take a snapshot of failure and close the browser. tagsGrouping Features, Scenarios, and Step Definitions using TagsTags is a great way made for Cucumber power users to organize their features and scenarios. In above example, by changing tags = {“~@Ignore”} line totags = {“@UpdateProfile”}, one can choose run only the features and scenarios tagged with @UpdateProfile tag. A Scenario or feature can have as many tags as you like. Just separate them with spaces: @important @maintenance @db @auth If subfolders are the chapters in your book of features, then tags are the sticky notes you’ve put on pages you want to be able to find easily. You tag a scenario by putting a word prefixed with the @ character on the line before the Scenario keyword, like this:@widgetsScenario: Generate reportGiven I am logged in There are three main reasons for tagging scenarios: Documentation: You want to use a tag to attach a label to certain scenarios, for example to label them with an ID from a project management tool. Filtering: Cucumber allows you to use tags as a filter to pick out specific scenarios to run or report on. You can even have Cucumber fail your test run if a certain tag appears too many times. Hooks: Run a block of code whenever a scenario with a particular tag is about to start or has just finished. config tagTags are a great way to organise your features and scenarios. Consider this example: @billingFeature: Verify billing @important Scenario: Missing product description Given hello Scenario: Several products Given helloA feature or scenario or can have as many tags as you like. Just separate them with spaces: @billing @bicker @annoyFeature: Verify billingTags can be placed above the following Gherkin elements: FeatureScenarioScenario OutlineExamplesIt is not possible to place tags above Background or steps (Given, When, Then, And and But). Cucumber for java 8 lambdaUsing Lambda Expressions for Step DefinitionsJava Step Definitions are written in regular classes which don’t need to extend or implement anything. They can be written either using lambda expressions or method annotations. In the above, we used the method annotations. To use lambda expressions, use cucumber-java8 module instead of cucumber-java module in your pom.xml file. When you use the cucumber-java8 module, you can write the Step Definitions using lambdas: 1234567891011121314151617181920212223package cucumber;import cucumber.api.java8.En; public class StepDefinitions implements En &#123; public StepDefinitions() &#123; Given("I have (\\d+) cukes in my belly", (Integer cukes) -&gt; &#123; System.out.format("Cukes: %n\n", cukes); &#125;); &#125;&#125;package steps; import cucumber.api.java8.En; public class MyStepdefs implements En &#123; public MyStepdefs() &#123; Given("I login as (.*)$",(String name)-&gt; System.out.println(name)); &#125;&#125; GherkinCucumber tests are expressed using a syntax called Gherkin. Gherkin files are plain text and have a .feature extension. Steps and Step DefinitionsLet’s start by clarifying the distinction between a step and a step definition.Each Gherkin scenario is made up of a series of steps, written in plain lan- guage. On its own, a step is just documentation; it needs a step definition to bring it to life. A step definition is a piece of code that says to Cucumber, “If you see a step that looks like this…, then here’s what I want you to do….”When Cucumber tries to execute each step, it looks for a matching step defi- nition to execute. So, how does Cucumber match a step definition to a step? Creating a Step DefinitionIf Cucumber sees a step definition with this regular expression, it will execute it when it comes to the first step of our scenario. So, how do we create a step definition? Step definitions live in ordinary files. To create a step definition in Java, you use a special Cucumber annotation, such as @Given, like this:@Given(“I have \$100 in my Account”)public void iHave$100InMyAccount() throws Throwable { // TODO: code that puts $100 into User’s Account goes here} Given, When, Then Are the SameIt doesn’t actually matter which of the three methods you use to register a step definition, because Cucumber ignores the keyword when matching a step. Under the hood, all of the annotations are aliases for StepDefAnnotation. The best way we’ve found to avoid this kind of problem is to pay careful attention to the precise wording in your steps. You could change both steps to be less ambiguous:Given I have deposited $100 in my Account Then the balance of my Account should be $100By rewording the steps like this, you’ve made them better at communicating exactly what they will do when executed. Learning to spot and remove this kind of ambiguity is something that takes practice. Paying attention to the distinction in wording between two steps like this can also give you hints about concepts that may not be expressed in your code but need to be. It might seem pedantic, but we’ve found that teams who pay this much careful attention to detail write much better software, faster. AlternationWe can specify a wildcard in a regular expression using a few different approaches. One of the simplest is alternation, where we express different options separated by a pipe character |, like this: 123@Given("I have deposited \\$(100|250) in my Account") public void iHaveDeposited$InMyAccount(int amount) &#123; // TODO: code goes here&#125; This step definition will now match a step with either of the two values 100 or 250 in it, and the number will be captured and passed to the method as an argument. Alternation can be useful if there are a fixed set of values that you want to accept in your step definition, but normally you’ll want something a little looser. The DotThe dot is a metacharacter, meaning it has magical powers in a regular expression. Literally, a dot means match any single character. So, we can try this instead: 123@Given("I have deposited \\$(...) in my Account") public void iHaveDeposited$InMyAccount(int amount) &#123; // TODO: code goes here&#125; That will now match a step with any three-figure dollar sum and send the matched amount into the method. What If I Actually Want to Match a Dot?Any of the metacharacters like the dot can be escaped by preceding them with a backslash. So, if you wanted to specifically match, say 3.14, you could use “3\.14”.You might have noticed that there’s a backslash in front of the dollar amount in the step definition we’re using. That’s because $ itself is a metacharacter (it’s an anchor, which we’ll explain later), so we need to escape to make it match a normal dollar sign. Star modifierThe star modifier means any number of times. So, with .* we’re capturing any character, any number of times. Now we’re getting somewhere—this will allow us to capture all those different amounts. But there’s still a problem.The star modifier is a bit of a blunt instrument. Because we’re using it with the dot that matches any character, it will gobble up any text at all up until the phrase in my Account. This is why, in regex terminology, the star modifier is known as a greedy operator. For example, it would happily match this step:Given I have deposited $1 and a cucumber in my AccountThe amount captured by our regular expression in this case would be 1 and a cucumber. We need to be more specific about the characters we want to match and just capture numbers. Instead of a dot, we can use something else. Character ClassesCharacter classes allow you to tell the regular expression engine to match one of a range of characters. You just place all of the characters you would accept inside square brackets: 1234567@Given("I have deposited \\$([01234567890]*) in my Account") public void iHaveDeposited$InMyAccount(int amount) &#123; // TODO: code goes here&#125;For a continuous range of characters like we have, you can use a hyphen:@Given("I have deposited \\$([0-9]*) in my Account") public void iHaveDeposited$InMyAccount(int amount) &#123; // TODO: code goes here&#125; Shorthand Character ClassesFor common patterns of characters like [0-9], there are a few shorthand char- acter classes that you can use instead. You may find this just makes your regular expressions more cryptic, but there are only a few to learn. For a digit, you can use \d as a shorthand for [0-9]: 123@Given("I have deposited \\$(\\d*) in my Account") public void iHaveDeposited$InMyAccount(int amount) &#123; // TODO: code goes here&#125; Here are the most useful shorthand character classes: \d stands for digit, or [0-9].\w stands for word character, specifically [A-Za-z0-9_]. Notice that underscores and digits are included but not hyphens.\s stands for whitespace character, specifically [ \t\r\n]. That means a space, a tab, or a line break.\b stands for word boundary, which is a lot like \s but actually means the opposite of \w. Anything that is not a word character is a word boundary.You can also negate shorthand character classes by capitalizing them, so for example, \D means any character except a digit.Back to matching our amount. It looks like we’re done, but there’s one last problem to fix. Can you see what it is? question markLike the star and the plus, the question mark modifies the character that precedes it, specifying how many times it can be repeated. The question mark modifier means zero or one times; in other words, it makes the preceding character optional. In step definitions, it’s particularly useful for plurals:@Given(“I have (\d+) cucumbers? in my basket”) public void iHaveCucumbersInMyBasket(int number) { // TODO: code goes here} noncapturing group@When(“I (?:visit|go to) the homepage”) public void iVisitTheHomepage() { // TODO: code goes here} Notice that we’ve had to prefix the list of alternates with another bit of regular expression magic. The ?: at the start of the group marks it as noncapturing, meaning Cucumber won’t pass it as an argument to our block. AnchorsThe undefined steps start with a ^ and end with a $. These two metacharacters are called anchors, because they’re used to tie down each end of the regular expression to the beginning and end of the string that they match on. Generally, it’s best to keep your regular expressions as tight as you can so that there’s less chance of two step definitions clashing with each other. Guides on how to write scenariosTry to avoid being guided by existing step definitions when you write your scenarios and just write down exactly what you want to happen, in plain English. In fact, try to avoid programmers or testers writing scenarios on their own. Instead, get nontechnical stakeholders or analysts to write the first draft of each scenario from a purely business-focused perspective or ideally in a pair with a programmer to help them share their mental model. With a well- engineered support layer, you can confidently and quickly write new step definitions to match the way the scenario has been expressed. Imperative StepsIn computer programming, there are two contrasting styles for expressing the instructions you give to a computer to make it do something for you. These styles are called imperative programming and declarative programming. Imperative programming means using a sequence of commands for the com- puter to perform in a particular order. Java is an example of an imperative language: you write a program as a series of statements that Java runs one at a time, in order. A declarative program tells the computer what it should do without prescribing precisely how to do it. CSS is an example of a declar- ative language: you tell the computer what you want the various elements on a web page to look like, and you leave it to take care of the rest. Use a Declarative Style InsteadLet’s raise the level of abstraction in this scenario and rewrite it using a more declarative style:Scenario: Redirect user to originally requested page after logging in Given I am an unauthenticated UserWhen I attempt to view some restricted contentThen I am shown a login formWhen I authenticate with valid credentials Then I should be shown the restricted contentThe beauty of this style is that it is not coupled to any specific implementation of the user interface. This same scenario could apply to a thick-client or mobile application. The words it uses aren’t technical and are instead written in a language (unauthenticated, restricted, credentials) that any stakeholder interested in security should be able to clearly understand. It’s by expressing every scenario at this level of abstraction that you discover your team’s ubiquitous language. DAMPHowever, when you are using examples to drive your code, there is another principle in play that I believe trumps the DRY principle: the examples should tell a good story. They are the docu- mentation narrative that will guide future programmers (including you when you come back to change this code in three months time and you’ve forgotten what it does). In this case, clarity of intent is found in the quality of the narrative, not necessarily in minimizing duplication. Some people refer to this as the DAMP principle: Descriptive and Meaningful Phrases. When you’re writing examples, readability is paramount, and DAMP trumps DRY. We consider fixture data to be an antipattern. We much prefer using Test Data Builders, on page 104, where the relevant data is created within the test itself, rather than being buried away in a big tangled set of fixture data. We find that teams that have a single humongous build also tend to have an architecture that could best be described as a big ball of mud. Because all of the behavior in the system is implemented in one place, all the tests have to live in one place, too, and have to all be run together as one big lump. This is a classic ailment of long-lived applications, which have grown organically without obvious interfaces between their subsystems. Defect PreventionToyota’s counterintuitive but hugely successful policy of stopping the line works because it’s part of a wider process, known as defect prevention, that focuses on continuously improving the manufacturing system. Without this wider process, stop the line itself would have very little effect. There are four steps to this process: Detect the abnormality. Stop what you’re doing. Fix or correct the immediate problem. Investigate the root cause and install a countermeasure.This fourth step is crucial because it seizes the opportunity offered by the problem at hand to understand something more fundamental about your process. It also means that fixing things becomes a habit, rather than some- thing you put off to do someday later when you’re not in such a hurry. Cucumber might just seem like a testing tool, but at its heart it’s really a collaboration tool. If you make a genuine effort to write features that work as documentation for the nontechnical stakeholders on your team, you’ll find you are forced to talk with them about details that you might never have otherwise made the time to talk about. Those conversations reveal insights about their understanding of the problem, insights that will help you build a much better solution than you would have otherwise. This is Cucumber’s big secret: the tests and documentation are just a happy side effect; the real value lies in the knowledge you discover during those conversations. Reference https://medium.com/agile-vision/cucumber-bdd-part-2-creating-a-sample-java-project-with-cucumber-testng-and-maven-127a1053c180 https://codoid.com/cucumber-lambda-expressions/]]></content>
  </entry>
  <entry>
    <title><![CDATA[akka framework of scala]]></title>
    <url>%2F2018-07-23-akka-scala%2F</url>
    <content type="text"><![CDATA[philosophyThe actor model adopts the philosophy that everything is an actor. This is similar to the everything is an object philosophy used by some object-oriented programming languages. Decoupling the sender from communications sent was a fundamental advance of the Actor model enabling asynchronous communication and control structures as patterns of passing messages. Recipients of messages are identified by address, sometimes called “mailing address”. Thus an actor can only communicate with actors whose addresses it has. RouteRoutes effectively are simply highly specialised functions that take a RequestContext and eventually complete it, which could (and often should) happen asynchronously. Directives create Routes. The Route is the central concept of Akka HTTP’s Routing DSL. All the structures you build with the DSL, no matter whether they consists of a single line or span several hundred lines, are type turning a RequestContext into a Future[RouteResult]. 1type Route = RequestContext =&gt; Future[RouteResult] Generally when a route receives a request (or rather a RequestContext for it) it can do one of these things: Complete the request by returning the value of requestContext.complete(…) Reject the request by returning the value of requestContext.reject(…) (see Rejections) Fail the request by returning the value of requestContext.fail(…) or by just throwing an exception (see Exception Handling) Do any kind of asynchronous processing and instantly return a Future[RouteResult] to be eventually completed later The Routing TreeEssentially, when you combine directives and custom routes via nesting and the ~ operator, you build a routing structure that forms a tree. When a request comes in it is injected into this tree at the root and flows down through all the branches in a depth-first manner until either some node completes it or it is fully rejected. In RouteDirective.scala 1234567/** * Completes the request using the given arguments. * * @group route */ def complete(m: ? ToResponseMarshallable): StandardRoute = StandardRoute(_.complete(m)) RouteResultRouteResult is a simple abstract data type (ADT) that models the possible non-error results of a Route. It is defined as such: 123456sealed trait RouteResultobject RouteResult &#123; final case class Complete(response: HttpResponse) extends RouteResult final case class Rejected(rejections: immutable.Seq[Rejection]) extends RouteResult&#125; Routing DSLIn addition to the Core Server API Akka HTTP provides a very flexible ,Routing DSL, for elegantly defining RESTful web services. Http().bindAndHandle(routes ~ abcRoute, host, port) 123456789101112131415/** * Returns a Route that chains two Routes. If the first Route rejects the request the second route is given a * chance to act upon the request. */ def ~(other: Route): Route = &#123; ctx ? import ctx.executionContext route(ctx).fast.flatMap &#123; case x: RouteResult.Complete ? FastFuture.successful(x) case RouteResult.Rejected(outerRejections) ? other(ctx).fast.map &#123; case x: RouteResult.Complete ? x case RouteResult.Rejected(innerRejections) ? RouteResult.Rejected(outerRejections ++ innerRejections) &#125; &#125; &#125; Path matching NoteThe path matching DSL describes what paths to accept after URL decoding. This is why the path-separating slashes have special status and cannot simply be specified as part of a string! The string ¡°foo/bar¡± would match the raw URI path ¡°foo%2Fbar¡±, which is most likely not what you want! SinkA Sink is a set of stream processing steps that has one open input. Can be used as a Subscriber supervisionWhat Supervision MeansAs described in Actor Systems supervision describes a dependency relationship between actors: the supervisor delegates tasks to subordinates and therefore must respond to their failures. When a subordinate detects a failure (i.e. throws an exception), it suspends itself and all its subordinates and sends a message to its supervisor, signaling failure. Depending on the nature of the work to be supervised and the nature of the failure, the supervisor has a choice of the following four options: Resume the subordinate, keeping its accumulated internal state Restart the subordinate, clearing out its accumulated internal state Stop the subordinate permanently Escalate the failure, thereby failing itself sealed class123object Supervision &#123; sealed trait Directive&#125; A sealed class may not be directly inherited, except if the inheriting template is defined in the same source file as the inherited class. However, subclasses of a sealed class can inherited anywhere. implicitA method can have an implicit parameter list, marked by the implicit keyword at the start of the parameter list. If the parameters in that parameter list are not passed as usual, Scala will look if it can get an implicit value of the correct type, and if it can, pass it automatically. The places Scala will look for these parameters fall into two categories: Scala will first look for implicit definitions and implicit parameters that can be accessed directly (without a prefix) at the point the method with the implicit parameter block is called.Then it looks for members marked implicit in all the companion objects associated with the implicit candidate type. KillSwitchA KillSwitch allows completion of Graphs from the outside by completing Graphs of FlowShape linked to the switch. Depending on whether the KillSwitch is a UniqueKillSwitch or a SharedKillSwitch one or multiple streams might be linked with the switch. 12345trait KillSwitch &#123;//After calling KillSwitch.shutdown() the linked Graphs of FlowShape are completed normally. def shutdown(): Unit def abort(ex:Throwable): Unit&#125; SourceA Source is a set of stream processing steps that has one open output. It can comprise any number of internal sources and transformations that are wired together, or it can be an atomic source, e.g. from a collection or a file. Materialization turns a Source into a Reactive Streams Publisher (at least conceptually). connect to Sink12345/** * Connect this [[akka.stream.scaladsl.Source]] to a [[akka.stream.scaladsl.Sink]], * concatenating the processing steps of both. */ def to[Mat2](sink: Graph[SinkShape[Out], Mat2]): RunnableGraph[Mat] = toMat(sink)(Keep.left) Futuretrait Future[+T] extends Awaitable[T]A Future represents a value which may or may not currently be available, but will be available at some point, or an exception if that value could not be made available. Asynchronous computations that yield futures are created with the Future.apply call and are computed using a supplied ExecutionContext, which can be backed by a Thread pool. import ExecutionContext.Implicits.global val s = “Hello” val f: Future[String] = Future { s + “ future!” } f foreach { msg =&gt; println(msg) } Future introductionFutures provide a way to reason about performing many operations in parallel¨C in an efficient and non-blocking way. A Future is a placeholder object for a value that may not yet exist. Generally, the value of the Future is supplied concurrently and can subsequently be used. Composing concurrent tasks in this way tends to result in faster, asynchronous, non-blocking parallel code. By default, futures and promises are non-blocking, making use of callbacks instead of typical blocking operations. To simplify the use of callbacks both syntactically and conceptually, Scala provides combinators such as flatMap, foreach, and filter used to compose futures in a non-blocking way. Blocking is still possible - for cases where it is absolutely necessary, futures can be blocked on (although this is discouraged). Scala scope protection:private[C] means that access is private “up to” C, where C is the corresponding package, class or singleton object. 123private[http] def build = &#123; // ...&#125; The modi?er can be quali?ed with an identi?er C (e.g. private[C]) that must denote a class or package enclosing the de?nition. Members labeled with such a modi?er are accessible respectively only from code inside the package C or only from code inside the class C and its companion module (¡ì5.4). Such members are also inherited only from templates inside C. Scala flexible importyou can import several classes the Scala way: 1import java.io.&#123;File, IOException, FileNotFoundException&#125; Use the following syntax to import everything from the java.io package: 1import java.io._ The _ character in this example is similar to the * wildcard character in Java. If the _ character feels unusual at first, it helps to know that it¡¯s used consistently throughout the Scala language as a wildcard character, and that consistency is very nice. DirectivesA ¡°Directive¡± is a small building block used for creating arbitrarily complex route structures. Akka HTTP already pre-defines a large number of directives and you can easily construct your own: Regular Expression1234val pattern = "Scala".r val str = "Scala is Scalable and cool" println(pattern findFirstIn str) We create a String and call the r( ) method on it. Scala implicitly converts the String to a RichString and invokes that method to get an instance of Regex. To find a first match of the regular expression, simply call the findFirstIn() method. If instead of finding only the first occurrence we would like to find all occurrences of the matching word, we can use the findAllIn( ) method and in case there are multiple Scala words available in the target string, this will return a collection of all matching words. PathMatcher Segment: PathMatcher1[String]Matches if the unmatched path starts with a path segment (i.e. not a slash). If so the path segment is extracted as a String instance. OptionIn short, if you have a value of type A that may be absent, Scala uses an instance of Option[A] as its container. An Intance of Option is either an instance of case class Some when it is present or case object None when it is not. Since both Some and None are children of Option, your function signature should declare that the returned value is an Option of some type, e.g. Option[A] SampleIt is very easy to create an Option in Scala, i.e. you can use a present/absent value directly. 123val optionalInt: Option[Int] = Some(1)// or// val optionalInt: Option[Int] = None To validate user login1234567def auth(user:String, pwd:String): AuthResult = (user, pwd) match &#123; case (u, _) if Option(u).exists(_.trim.isEmpty) =&gt; ErrorLogin case (_, p) if Option(p).exists(_.trim.isEmpty) =&gt; ErrorPwd case (u, p) =&gt; doAuth(u,p)&#125; isDefinedyou add a checker for the None value using the isDefined method and specify logic to handle each scenario accordingly. 123def addTwoWithDefault(a: Option[Int]): Int = &#123; if(a.isDefined) a.get + 2 else 2&#125; getOrElseIn many cases, you have a fallback or default value for your absent values, e.g. zero in the above example. With Option, you can easily provide a default value via the getOrElse method. def addTwoWithDefault(a: Option[Int]): Int = a.getOrElse(0) + 2 flattenAssume that we have a List of Option[Int]. 1val l: List[Option[Int]] = List(Some(3), Some(1), None, Some(5), Some(8), None) A common scenario is that we need to filter out the absent values and return a List of Int. A straightfoward approach is to combine filter with .isDefined. 12l.filter(_.isDefined).map(_.get) // res1: List[Int] = List(3, 1, 5, 8) However, Scala actually provides an elegent built-in function to achieve the same goal, which is often more preferred. 12l.flatten// res1: List[Int] = List(3, 1, 5, 8) underscoreIn Scala, pattern matching is somewhat similar to java switch statement. But it is more powerful. 12345def matchTest(x: Int): String = x match &#123; case 1 =&gt; "one" case 2 =&gt; "two" case _ =&gt; "anything other than one and two" &#125; _ acts like a wildcard. It will match anything. Scala allows nested patterns, so we can nest the _ also.Lets see another example that uses _ in nested pattern. 123456expr match &#123; case List(1,_,_) =&gt; " a list with three element and the first element is 1" case List(_*) =&gt; " a list with zero or more elements " case Map[_,_] =&gt; " matches a map with any key type and any value type " case _ =&gt; &#125; Anonymous FunctionsScala represents anonymous functions with a elegant syntax. The _ acts as a placeholder for parameters in the anonymous function. The _ should be used only once, But we can use two or more underscores to refer different parameters. 123456List(1,2,3,4,5).foreach(print(_))List(1,2,3,4,5).foreach( a =&gt; print(a))// Here the _ refers to the parameter. The first one is a short form of the second one. Lets look at another example which take two parameters.val sum = List(1,2,3,4,5).reduceLeft(_+_)val sum = List(1,2,3,4,5).reduceLeft((a, b) =&gt; a + b) FunctionsScala is a functional language. So we can treat function as a normal variable. If you try to assign a function to a new variable, the function will be invoked and the result will be assigned to the variable. This confusion occurs due to the optional braces for method invocation. We should use _ after the function name to assign it to another variable. 12345678List(1,2,3,4,5).foreach(print(_))class Test &#123; def fun = &#123; // some code &#125; val funLike = fun _&#125;List(1,2,3,4,5).foreach(print(_)) Either, Left, RightUsing Either, Left, and RightPrior to Scala 2.10, an approach similar to Try was available with the Either, Left, and Right classes. With these classes, Either is analogous to Try, Right is similar to Success, and Left is similar to Failure. The following method demonstrates how to implement the Either approach: 1234def divideXByY(x: Int, y: Int): Either[String, Int] = &#123; if (y == 0) Left("Dude, can't divide by 0") else Right(x / y)&#125; As shown, your method should be declared to return an Either, and the method body should return a Right on success and a Left on failure. The Right type is the type your method returns when it runs successfully (an Int in this case), and the Left type is typically a String, because that¡¯s how the error message is returned. As with Option and Try, a method returning an Either can be called in a variety of ways, including getOrElse or a match expression: 12345678val x = divideXByY(1, 1).right.getOrElse(0) // returns 1val x = divideXByY(1, 0).right.getOrElse(0) // returns 0// prints "Answer: Dude, can't divide by 0"divideXByY(1, 0) match &#123; case Left(s) =&gt; println("Answer: " + s) case Right(i) =&gt; println("Answer: " + i)&#125; You can also access the error message by testing the result with isLeft, and then accessing the left value, but this isn¡¯t really the Scala way: 123456789scala&gt; val x = divideXByY(1, 0)x: Either[String,Int] = Left(Dude, can't divide by 0)scala&gt; x.isLeftres0: Boolean = truescala&gt; x.leftres1: scala.util.Either.LeftProjection[String,Int] = LeftProjection(Left(Dude, can't divide by 0)) Although the Either classes offered a potential solution prior to Scala 2.10, I now use the Try classes in all of my code instead of Either. classOfRetrieve the runtime representation of a class type. classOf[T] is equivalent to the class literal T.class in Java. Operators// Keywords&lt;- // Used on for-comprehensions, to separate pattern from generator=&gt; // Used for function types, function literals and import renaming Akka frameworkAkka is a toolkit and runtime for building highly concurrent, distributed, and fault-tolerant event-driven applications on the JVM. Actors are the unit of execution in Akka. The Actor model is an abstraction that makes it easier to write correct concurrent, parallel and distributed systems. This will get your feet wet, and hopefully inspire you to dive deeper into the wonderful sea of Akka! The Akka team refers to their creation as a toolkit rather than a framework. Frameworks tend to be a mechanism for providing a discrete element of a stack (e.g. the ui, or the web services layer). Akka provides a set of tools to render any part of the stack, and to provide the interconnects between them. He two ways (shared mutable state/message passing (Akka)) to solve the problem of selling tickets. Actors do not share state, can only communicate through immutable messages and do not talk to each other directly but through actor references, similar to the addresses we talked about. This approach satisfies the three things we wanted to change. So why is this simpler than the shared mutable state approach? We don’t need to manage locks. We don’t have to think about how to protect the shared data. Inside an actor we’re safe. We are more protected from deadlocks caused by out of order access by multiple threads, that cause the system to wait forever, or other problems like race conditions and thread starvation. Use of Akka precludes most of these problems, relieving us of the burden. Performance tuning a shared mutable state solution is hard work and error prone and verification through tests is nearly impossible. Benefits of using the Actor ModelThe following characteristics of Akka allow you to solve difficult concurrency and scalability challenges in an intuitive way: Event-driven model — Actors perform work in response to messages. Communication between Actors is asynchronous, allowing Actors to send messages and continue their own work without blocking to wait for a reply. Strong isolation principles — Unlike regular objects in Java, an Actor does not have a public API in terms of methods that you can invoke. Instead, its public API is defined through messages that the actor handles. This prevents any sharing of state between Actors; the only way to observe another actor’s state is by sending it a message asking for it. Location transparency — The system constructs Actors from a factory and returns references to the instances. Because location doesn’t matter, Actor instances can start, stop, move, and restart to scale up and down as well as recover from unexpected failures. Lightweight — Each instance consumes only a few hundred bytes, which realistically allows millions of concurrent Actors to exist in a single application. Messagetwo special channels. The first is the Dead Letter channel, which contain message that couldn’t be delivered. This is sometimes also called a dead message queue. This channel can help when debugging, why some messages aren’t processed or to monitor where there are problems. EventStreamthe benefit of decoupling the receivers and the sender and the dynamic nature of the publish-subscribe channel, but because the EventStream is available for all actors is also a nice solution for messages which can be send from all over the system and needs to be collected at one or more Actors. A good example is logging. Logging can be done throughout the system and needs to be collected at one point and be written to a log file. Internally the ActorLogging is using the EventStream to collect the log lines from all over the system. Dead Letter MessageAkka is using the EventStream to implement the dead letter queue. This way only the actors which are interested in the failed messages are receiving them. When a message is queued in a mailbox of an actor that Terminates or is send after the Termination, the message is send to the EventStream of the ActorSystem. The message is wrapped into a DeadLetter object. This Object contains the original message, the sender of the message and the intended receiver. This way the Dead letter queue is integrated in the EventStream. To get these dead letter messages you only need to subscribe your actor to the EventStream with the DeadLetter class as the Classifier. Messages send to a Terminated Actor can’t be processed anymore and the ActorRef of this actor should not be used anymore. When there are messages send to a terminated Actor, these message will be send to the DeadLetter queue. Another use of the DeadLetter queue is when the processing fails. This is a Actor specific decision. An actor can decide that a received message couldn’t be processed and that it doesn’t know what to do with it. In this situation the messages can be send to the dead letter queue. Design recommendationsWhen defining Actors and their messages, keep these recommendations in mind: Since messages are the Actor’s public API, it is a good practice to define messages with good names and rich semantic and domain specific meaning, even if they just wrap your data type. This will make it easier to use, understand and debug actor-based systems. Messages should be immutable, since they are shared between different threads. It is a good practice to put an actor’s associated messages as static classes in the class of the Actor. This makes it easier to understand what type of messages the actor expects and handles. It is also a common pattern to use a static props method in the class of the Actor that describes how to construct the Actor. PropsThe static props method creates and returns a Props instance. Props is a configuration class to specify options for the creation of actors, think of it as an immutable and thus freely shareable recipe for creating an actor that can include associated deployment information. This example simply passes the parameters that the Actor requires when being constructed. We will see the props method in action later in this tutorial. configurationWhen using the default, the library will try to find the configuration file. Since the library supports a number of different configuration formats, it looks for different files, in the following order:application.propertiesThis file should contain the configuration properties in the java property file format.application.jsonThis file should contain the configuration properties in the json style application.confThis file should contain the configuration properties in the HOCON format. This is a format based on json but easier to read..It is possible to use all the different files at the same time. For the example below, in listing 7.2 we use the last file:MyAppl { version = 10 description = “My application” database { connect=”jdbc:mysql://localhost/mydata” user=”me” }} Nesting is done by simply grouping with {}s substitution hostname=”localhost”hostname=${?HOST_NAME}MyAppl { version = 10 description = “My application” database { connect=”jdbc:mysql://${hostname}/mydata”user=”me” }} define the variable first, if system environment do exist, override it, otherwise use default? means get a variable from system envrionment Default/fallback properiesDefault properties are configured in the file reference.conf and placed in the root of the jar file; the idea is that every library contains its own defaults. The configuration library will find all the reference.conf files and integrate these settings into the configuration fall-back structure. Order of propertiesSystem properties-&gt;application.conf-&gt;applicaiton.json-&gt;application.properties-&gt;reference.conf The power of location transparencyIn Akka you can’t create an instance of an Actor using the new keyword. Instead, you create Actor instances using a factory. The factory does not return an actor instance, but a reference, akka.actor.ActorRef, that points to the actor instance. This level of indirection adds a lot of power and flexibility in a distributed system. In Akka location doesn’t matter. Location transparency means that the ActorRef can, while retaining the same semantics, represent an instance of the running actor in-process or on a remote machine. If needed, the runtime can optimize the system by changing an Actor’s location or the entire application topology while it is running. This enables the “let it crash” model of failure management in which the system can heal itself by crashing faulty Actors and restarting healthy ones. The Akka ActorSystemThe akka.actor.ActorSystem factory is, to some extent, similar to Spring’s BeanFactory. It acts as a container for Actors and manages their life-cycles. The actorOf factory method creates Actors and takes two parameters, a configuration object called Props and a name. Asynchronous communicationActors are reactive and message driven. An Actor doesn’t do anything until it receives a message. Actors communicate using asynchronous messages. This ensures that the sender does not stick around waiting for their message to be processed by the recipient. Instead, the sender puts the message in the recipient’s mailbox and is free to do other work. The Actor’s mailbox is essentially a message queue with ordering semantics. The order of multiple messages sent from the same Actor is preserved, but can be interleaved with messages sent by another Actor. You might be wondering what the Actor is doing when it is not processing messages, i.e. doing actual work? It is in a suspended state in which it does not consume any resources apart from memory. Again, showing the lightweight, efficient nature of Actors. Sending messages to an ActorTo put a message into an Actor’s mailbox, use the tell method on the ActorRef. For example, the main class of Hello World sends messages to the Greeter Actor like this: 12howdyGreeter.tell(new WhoToGreet("Akka"), ActorRef.noSender());howdyGreeter.tell(new Greet(), ActorRef.noSender()); The test class is using akka.test.javadsl.TestKit, which is a module for integration testing of actors and actor systems. This class only uses a fraction of the functionality provided by TestKit. Akka under the hoodSo are there no concurrency primitives like locks used at all in Akka? Well, of course there are, it’s just that you don’t have to deal with them directly . Everything still eventually runs on threads and low level concurrency primitives. Akka uses the java.util.concurrent library to coordinate message processing and takes great care to minimize the number of locks used to an absolute bare minimum. It uses lock free and wait free algorithms where possible, for example compare-and-swap (CAS) techniques, which are beyond the scope of this book. And because nothing can be shared between actors, the shared locks that you would normally have between objects are not present at all. There are other benefits that stem from the message passing approach that Akka uses, which we will discuss in the next sections. We have touched on them briefly already: Even in this first, simple example, the message passing approach is clearly more fault tolerant, averting catastrophic failure if one component (no matter how key) fails. The shared mutable state is always in one place in the example (in one JVM if it is kept entirely in memory). If you need to scale beyond this constraint, you will have to (re)distribute the data somehow. Since the message passing style uses addresses, looking ahead, you can see that if local and remote addresses were interchangeable, scaling out would be possible without code changes of any kind. This scenario is one example of a fault tolerance strategy that Akka provides, which is called the Restart strategy. Other strategies that can be used are Resume, Stop and Escalate. Scale up and Scale outIn our Ticketing example, scaling up would mean getting more TicketingAgents running on our one server, scaling out would be bringing up TicketingAgents on a number of machines. Lockinglocks result in contention, which will mean the number of threads doing work at any one time is often less than the total number, as some will have to wait on each other to finish. Sharing as little as possible means locking as little as possible, which is the goal of the message passing approach. Every thread has a stack to store runtime data. The size of the stack differs per operating system, for instance on the linux x64 platform it is normally 256kB. The stack size is one of the factors that limits the number of threads that run at the same time on a server. Around 4096 threads can fit in 1GB of memory on the linux x64 platform. DispatcherActors run on an abstraction which is called a dispatcher. The dispatcher takes care of which threading model is used and processes the mailboxes. Actors are lightweight because they run on top of dispatchers, the actors are not necessarily directly proportional to the number of threads. Akka Actors take a lot less space than threads, around 2.7 million actors can fit in 1GB of memory. A big difference compared to 4096 threads, which means that you can create different types of actors more freely than you would when using threads directly. There are different types of dispatchers to choose from which can be tuned to specific needs. We identified that we had to make the following changes to get to a message passing style: No mutable shared data structure. Immutable message passing. Asynchronous message sending. Akka implements actors and which components compare to the concepts we’ve talked about so far: actors, addresses and mailboxes. Actor PathSo how do you get an actor reference to an actor in the hierarchy? This is where ActorPaths come in. You could compare the hierarchy of actors to a URL path structure. Every actor has a name. This name needs to be unique per level in the hierarchy, two sibling actors cannot have the same name (if you do not provide a name Akka generates one for you, but it is a good idea to name all your actors). All actor references can be located directly by an actor path, absolute or relative, and it has to follow the URI generic syntax. An actor path is built just like a URI, starting with a scheme followed by a scheme-specific part, Core Actor OperationsAnother way to look at an actor is to describe the operations that it supports. An Akka actor has four core operations : CREATE: An actor can create another actor. In Akka, actors are part of an actor system, which defines the root of the actor hierarchy and creates top-level actors. Every actor can create child actors. The topology of the actors is dynamic, it depends on which actors create other actors and which addresses are used to communicate with them. SEND: An actor can send a message to another actor. Messages are sent asynchronously, using an address to send to an Actor associated with a given Mailbox. BECOME: The behavior of an actor can be dynamically changed. Messages are received one at a time and an actor can designate that it wants to handle next messages in a different way, basically swapping its behavior, which we will look at in later chapters. SUPERVISE: An actor supervises and monitors its children in the actor hierarchy and manages the failures that happen. As we will see in chapter 3, this provides a clean separation between message processing and error handling. Akka concurrencyMessage passing enables an easier road to real concurrency With that concurrent approach, we will be able to scale up and outWe can scale both the request and the processing elements of our applicationMessages also unlock greater fault tolerance capabilitiesSupervision provides a means of modeling for both concurrency and fault toleranceAkka infuses our code with these powers in a lightweight, unobtrusive manner Build Akkausing TypeSafe’s Simple Build Tool (SBT) to create a single jar file that can be used to run the app If you have not worked with the SBT DSL before it is important to note that you need to put en empty line between lines in the file (this is the price we pay for not telling Scala where each expression ends). tell vs askMessages are sent to an Actor through one of the following methods. ! means “fire-and-forget”, e.g. send a message asynchronously and return immediately. Also known as tell. ? sends a message asynchronously and returns a Future representing a possible reply. Also known as ask. So below line is equivalent to tell 123456class ReceiveActor extends Actor &#123; def receive = &#123; case "Hello" =&gt; sender ! "And Hello to you!" // same as sender.tell("And Hello to you!") &#125;&#125; Sample of actor123456789101112131415161718192021222324252627282930package com.goticksimport akka.actor.&#123;PoisonPill, Actor&#125;class TicketSeller extends Actor &#123; import TicketProtocol._ var tickets = Vector[Ticket]() def receive = &#123; case GetEvents =&gt; sender ! tickets.size case Tickets(newTickets) =&gt; tickets = tickets ++ newTickets case BuyTicket =&gt; if (tickets.isEmpty) &#123; sender ! SoldOut self ! PoisonPill&#125; tickets.headOption.foreach &#123; ticket =&gt; tickets = tickets.tail sender ! ticket&#125; &#125;&#125;case Event(name, nrOfTickets) =&gt; if(context.child(name).isEmpty) &#123;If TicketSellers have not been val ticketSeller = context.actorOf(Props[TicketSeller], name) val tickets = Tickets((1 to nrOfTickets).map&#123; nr=&gt; Ticket(name, nr)).toList&#125; ticketSeller ! tickets&#125;sender ! EventCreated The BoxOffice creates TicketSellers for each event. Notice that it uses it’s context instead of the actor system to create the actor; Actors created with the context of another Actor are its children and subject to the parent Actor’s supervision TestRight now it always fails since it is not implemented yet, as is expected in Red-Green-Refactor style, where you first make sure the test fails (Red), then implement the code to make it pass (Green), after which you might refactor the code to make it nicer. source codeUntypedActorThis class is the Java cousin to the akka.actor.Actor Scala interface. Subclass this abstract class to create a MDB-style untyped actor. An actor has a well-defined (non-cyclic) life-cycle. RUNNING (created and started actor) - can receive messages SHUTDOWN (when &apos;stop&apos; or &apos;exit&apos; is invoked) - can&apos;t do anythingThe Actor’s own akka.actor.ActorRef is available as getSelf(), the current message’s sender as getSender() and the akka.actor.UntypedActorContext as getContext(). The only abstract method is onReceive() which is invoked for each processed message unless dynamically overridden using getContext().become(). Annotations @Deprecated @deprecatedDeprecated (Since version 2.5.0) Use AbstractActor instead of UntypedActor.loggingAdaptortrait LoggingAdapter extends AnyRef Logging wrapper to make nicer and optimize: provide template versions which evaluate .toString only if the log level is actually enabled. Typically used by obtaining an implementation from the Logging object:]]></content>
  </entry>
  <entry>
    <title><![CDATA[Zoo-keeper]]></title>
    <url>%2F2018-08-19-ZooKeeper%2F</url>
    <content type="text"><![CDATA[ZK Motto the motto “ZooKeeper: Because Coordinating Distributed Systems is a Zoo.” Features of Zookeeper Synchronization − Mutual exclusion and co-operation between server processes. Ordered Messages - The strict ordering means that sophisticated synchronization primitives can be implemented at the client. Reliability - The reliability aspects keep it from being a single point of failure. Atomicity − Data transfer either succeeds or fails completely, but no transaction is partial. High performant - The performance aspects of Zookeeper means it can be used in large, distributed systems. Distributed. High avaliablity. Fault-tolerant. Loose coupling. Partial failure. High throughput and low latency - data is stored data in memory and on disk as well. Replicated. Automatic failover: When a Zookeeper dies, the session is automatically migrated over to another Zookeeper. Different type of dataWhen designing an application with ZooKeeper, one ideally separates application data from control or coordination data. For example, the users of a web-mail service are interested in their mailbox content, but not on which server is handling the requests of a particular mailbox. The mailbox content is application data, whereas the mapping of the mailbox to a specific mail server is part of the coordination data (or metadata). A ZooKeeper ensemble manages the latter. The multiple processes consequently need to implement mutual exclusion. We can actually think of the task of acquiring mastership as the one of acquiring a lock: the process that acquires the mastership lock exercises the role of master. Coordination does not always take the form of synchronization primitives like leader election or locks. Configuration metadata is often used as a way for a process to convey what others should be doing. For example, in a master-worker system, workers need to know the tasks that have been assigned to them, and this information must be available even if the master crashes. How world work without ZooKeeperIt is certainly possible to build distributed systems without using ZooKeeper. ZooKeep‐ er, however, offers developers the possibility of focusing more on application logic rather than on arcane distributed systems concepts. Programming distributed systems without ZooKeeper is possible, but more difficult. What does ZooKeeper does not doThe ensemble of ZooKeeper servers manages critical application data related to coor‐ dination. ZooKeeper is not for bulk storage. For bulk storage of application data, there are a number of options available, such as databases and distributed file systems. When designing an application with ZooKeeper, one ideally separates application data from control or coordination data. ZooKeeper, however, does not implement the tasks for you. It does not elect a master or track live processes for the application out of the box. Instead, it provides the tools for implementing such tasks. The developer decides what coordination tasks to implement. Processes in a distributed system have two broad options for communication: they can exchange messages directly through a network, or read and write to some shared storage. ZooKeeper uses the shared storage model to let applications implement coordination and synchronization primitives. But shared storage itself requires network communi‐ cation between the processes and the storage. It is important to stress the role of network communication because it is an important source of complications in the design of a distributed system. This scenario leads to a problem commonly called split-brain: two or more parts of the system make progress independ‐ ently, leading to inconsistent behavior. As part of coming up with a way to cope with master failures, it is critical that we avoid split-brain scenarios. TasksThe following requirements for our master-worker architecture:Master electionIt is critical for progress to have a master available to assign tasks to workers.Crash detectionThe master must be able to detect when workers crash or disconnect.Group membership managementThe master must be able to figure out which workers are available to execute tasks.Metadata managementThe master and the workers must be able to store assignments and execution sta‐ tuses in a reliable manner. CAPknown as CAP, which stands for Consistency, Availability, and Partition-tolerance, says that when designing a distributed system we may want all three of those properties, but that no system can handle all three.2 Zoo‐ Keeper has been designed with mostly consistency and availability in mind, although it also provides read-only capability in the presence of network partitions. ZooKeeper BasicsSeveral primitives used for coordination are commonly shared across many applica‐ tions. Consequently, one way of designing a service used for coordination is to come up with a list of primitives, expose calls to create instances of each primitive, and ma‐ nipulate these instances directly. For example, we could say that distributed locks con‐ stitute an important primitive and expose calls to create, acquire, and release locks.Such a design, however, suffers from a couple of important shortcomings. First, we need to either come up with an exhaustive list of primitives used beforehand, or keep ex‐ tending the API to introduce new primitives. Second, it does not give flexibility to the application using the service to implement primitives in the way that is most suitable for it.We consequently have taken a different path with ZooKeeper. ZooKeeper does not ex‐ pose primitives directly. Instead, it exposes a file system-like API comprised of a small set of calls that enables applications to implement their own primitives. We typically use recipes to denote these implementations of primitives. Recipes include ZooKeeper operations that manipulate small data nodes, called znodes, that are organized hier‐ archically as a tree, just like in a file system. znodesa few other znodes that could be useful in a master- worker configuration: The /workers znode is the parent znode to all znodes representing a worker avail‐ able in the system. Figure 2-1 shows that one worker (foo.com:2181) is available. If a worker becomes unavailable, its znode should be removed from /workers. The /tasks znode is the parent of all tasks created and waiting for workers to execute them. Clients of the master-worker application add new znodes as children of /tasks to represent new tasks and wait for znodes representing the status of the task. The /assign znode is the parent of all znodes representing an assignment of a task to a worker. When a master assigns a task to a worker, it adds a child znode to /assign.APIAPI OverviewZnodes may or may not contain data. If a znode contains any data, the data is stored as a byte array. The exact format of the byte array is specific to each application, and ZooKeeper does not directly provide support to parse it. Serialization packages such as Protocol Buffers, Thrift, Avro, and MessagePack may be handy for dealing with the format of the data stored in znodes, but sometimes string encodings such as UTF-8 or ASCII suffice. The ZooKeeper API exposes the following operations:12345678910111213create /path dataCreates a znode named with /path and containing datadelete /pathDeletes the znode /pathexists /pathChecks whether /path existssetData /path dataSets the data of znode /path to datagetData /pathReturns the data in /pathgetChildren /pathReturns the list of children under /pathOne important note is that ZooKeeper does not allow partial writes or reads of the znode data. When setting the data of a znode or reading it, the content of the znode is replaced or read entirely. ZooKeeper clients connect to a ZooKeeper service and establish a session through which they make API calls. If a worker becomes unavailable, its session expires and its znode in /workers disappears automatically. An ephemeral znode can be deleted in two situations: When the session of the client creator ends, either by expiration or because it ex‐ plicitly closed. When a client, not necessarily the creator, deletes it. Sequential znodesA znode can also be set to be sequential. A sequential znode is assigned a unique, mo‐ notonically increasing integer. This sequence number is appended to the path used to create the znode. For example, if a client creates a sequential znode with the path /tasks/ task-, ZooKeeper assigns a sequence number, say 1, and appends it to the path. The path of the znode becomes /tasks/task-1. Sequential znodes provide an easy way to create znodes with unique names. They also provide a way to easily see the creation order of znodes. watchThis is a common problem with polling. To replace the client polling, we have opted for a mechanism based on notifications: clients register with ZooKeeper to receive notifi‐ cations of changes to znodes. Registering to receive a notification for a given znode consists of setting a watch. A watch is a one-shot operation, which means that it triggers one notification. To receive multiple notifications over time, the client must set a new watch upon receiving each notification. VersionsEach znode has a version number associated with it that is incremented every time its data changes. A couple of operations in the API can be executed conditionally: setDa ta and delete. Both calls take a version as an input parameter, and the operation suc‐ ceeds only if the version passed by the client matches the current version on the server. The use of versions is important when multiple ZooKeeper clients might be trying to perform operations over the same znode. For example, suppose that a client c1 writes a znode /config containing some configuration. If another client c2 concurrently updates the znode, the version c1 has is stale and the setData of c1 must not succeed. Using versions avoids such situations. In this case, the version that c1 uses when writing back doesn’t match and the operation fails. ZooKeeper ArchitectureNow that we have discussed at a high level the operations that ZooKeeper exposes to applications, we need to understand more of how the service actually works. Applica‐ tions make calls to ZooKeeper through a client library. The client library is responsible for the interaction with ZooKeeper servers. ZooKeeper servers run in two modes: standalone and quorum. Standalone mode is pretty much what the term says: there is a single server, and ZooKeeper state is not replicated. In quorum mode, a group of ZooKeeper servers, which we call a ZooKeeper ensemble, replicates the state, and together they serve client requests. From this point on, we use the term “ZooKeeper ensemble” to denote an installation of servers. This installation could contain a single server and operate in standalone mode or contain a group of servers and operate in quorum mode. ZooKeeper QuorumsIn quorum mode, ZooKeeper replicates its data tree across all servers in the ensemble. But if a client had to wait for every server to store its data before continuing, the delays might be unacceptable. In public administration, a quorum is the minimum number of legislators required to be present for a vote. In ZooKeeper, it is the minimum number of servers that have to be running and available in order for ZooKeeper to work. This number is also the minimum number of servers that have to store a client’s data before telling the client it is safely stored. For instance, we might have five ZooKeeper servers in total, but a quorum of three. So long as any three servers have stored the data, the client can continue, and the other two servers will eventually catch up and store the data. It is important to choose an adequate size for the quorum. Quorums must guarantee that, regardless of delays and crashes in the system, any update request the service pos‐ itively acknowledges will persist until another request supersedes it. The bottom line is that we should always shoot for an odd number of servers. sessionsAll operations a client submits to ZooKeeper are associated to a session. When a session ends for any reason, the ephemeral nodes created during that session disappear. the session may be moved to a different server if the client has not heard from its current server for some time. Moving a session to a different server is handled transparently by the ZooKeeper client library. Sessions offer order guarantees, which means that requests in a session are executed in FIFO (first in, first out) order. Typically, a client has only a single session open, so its requests are all executed in FIFO order. If a client has multiple concurrent sessions, FIFO ordering is not necessarily preserved across the sessions. Commands12345678910111213141516171819202122[zk: 127.0.0.1:2181,127.0.0.1:2182(CONNECTED) 5] stat /mastercZxid = 0x4ctime = Mon Aug 20 21:10:23 AEST 2018mZxid = 0x4mtime = Mon Aug 20 21:10:23 AEST 2018pZxid = 0x4cversion = 0dataVersion = 0aclVersion = 0ephemeralOwner = 0x10003c70e250001dataLength = 11numChildren = 0[zk: 127.0.0.1:2181,127.0.0.1:2182(CONNECTED) 8] create /workers ""Created /workers[zk: 127.0.0.1:2181,127.0.0.1:2182(CONNECTED) 9] create /tasks ""Created /tasks[zk: 127.0.0.1:2181,127.0.0.1:2182(CONNECTED) 10] create /assign ""Created /assign[zk: 127.0.0.1:2181,127.0.0.1:2182(CONNECTED) 11] ls /[assign, master, tasks, workers, zookeeper] link master and workersIn a real application, these znodes need to be created either by a primary process before it starts assigning tasks or by some bootstrap procedure. Regardless of how they are created, once they exist, the master needs to watch for changes in the children of /workers and /tasks: [zk: localhost:2181(CONNECTED) 4] ls /workers true [] [zk: localhost:2181(CONNECTED) 5] ls /tasks true [] [zk: localhost:2181(CONNECTED) 6]Note that we have used the optional true parameter with ls, as we did before with stat on the master. The true parameter, in this case, creates a watch for changes to the set of children of the corresponding znode. 1234567[zk: 127.0.0.1:2181,127.0.0.1:2182(CONNECTED) 14] create -e /workers/todd-worker1 ""Created /workers/todd-worker1WATCHER::WatchedEvent state:SyncConnected type:NodeChildrenChanged path:/workers[zk: 127.0.0.1:2181,127.0.0.1:2182(CONNECTED) 15] Recall that the master has set a watch for changes to the children of /workers. Once the worker creates a znode under /workers, the master observes the following notification: WATCHER:: WatchedEvent state:SyncConnected type:NodeChildrenChanged path:/workers Tasks workflows Clients add tasks to the system. Here we assume that the client asks the master-worker system to run a command cmd. To add a task to the system, a client executes the following: [zk: localhost:2181(CONNECTED) 0] create -s /tasks/task- “cmd” Created /tasks/task-0000000000 The client now has to wait until the task is executed. The worker that executes the task creates a status znode for the task once the task completes. The client determines that the task has been executed when it sees that a status znode for the task has been created; the client consequently must watch for the creation of the status znode: 123456789101112131415161718192021222324252627282930[zk: 127.0.0.1:2181,127.0.0.1:2182(CONNECTED) 18] create -s /tasks/task- "cmd"Created /tasks/task-0000000000[zk: 127.0.0.1:2181,127.0.0.1:2182(CONNECTED) 19] ls /tasks[task-0000000000][zk: 127.0.0.1:2181,127.0.0.1:2182(CONNECTED) 20] ls -w /tasks/task-0000000000 [][zk: 127.0.0.1:2181,127.0.0.1:2182(CONNECTED) 21] ls -w /workers[todd-worker1][zk: 127.0.0.1:2181,127.0.0.1:2182(CONNECTED) 22] create /assign/todd-worker1/task-0000000000 ""Ephemerals cannot have children: /assign/todd-worker1/task-0000000000[zk: 127.0.0.1:2181,127.0.0.1:2182(CONNECTED) 23] delete /assign/todd-worker1[zk: 127.0.0.1:2181,127.0.0.1:2182(CONNECTED) 24] create /assign/todd-worker1Created /assign/todd-worker1[zk: 127.0.0.1:2181,127.0.0.1:2182(CONNECTED) 25] create /assign/todd-worker1/task-0000000000 ""Created /assign/todd-worker1/task-0000000000[zk: 127.0.0.1:2181,127.0.0.1:2182(CONNECTED) 26] ls /assign/todd-worker1[task-0000000000]# Once the worker finishes executing the task, it adds a status znode to /tasks: [zk: localhost:2181(CONNECTED) 4] create /tasks/task-0000000000/status "done" Created /tasks/task-0000000000/status [zk: localhost:2181(CONNECTED) 5]# and the client receives a notification and checks the result:WATCHER:: WatchedEvent state:SyncConnected type:NodeChildrenChanged path:/tasks/task-0000000000 [zk: localhost:2181(CONNECTED) 2] get /tasks/task-0000000000 "cmd" ZooKeeper APISetting the ZooKeeper CLASSPATH ZOOBINDIR=”/bin” . “$ZOOBINDIR”/zkEnv.sh handleThe ZooKeeper API is built around a ZooKeeper handle that is passed to every API call. This handle represents a session with ZooKeeper. A session that is established with one ZooKeeper server will migrate to another ZooKeeper server if its connection is broken. As long as the session is alive, the handle will remain valid, and the ZooKeeper client library will continually try to keep an active connection to a ZooKeeper server to keep the session alive. If the handle is closed, the ZooKeeper client library will tell the ZooKeeper servers to kill the session. If ZooKeeper decides that a client has died, it will invalidate the session. If a client later tries to reconnect to a Zoo‐ Keeper server using the handle that corresponds to the invalidated session, the Zoo‐ Keeper server informs the client library that the session is no longer valid and the handle returns errors for all operations. The constructor that creates a ZooKeeper handle usually looks like:ZooKeeper(String connectString, int sessionTimeout, Watcher watcher) Implementing a WatcherTo receive notifications from ZooKeeper, we need to implement watchers. Let’s look a bit more closely at the Watcher interface. It has the following declaration:public interface Watcher {void process(WatchedEvent event);} Sample ZooKeeper handle12345678910111213141516171819202122import org.apache.zookeeper.ZooKeeper; import org.apache.zookeeper.Watcher;public class Master implements Watcher &#123; ZooKeeper zk; String hostPort;Master(String hostPort) &#123; this.hostPort = hostPort;&#125;void startZK() &#123;zk = new ZooKeeper(hostPort, 15000, this);&#125;public void process(WatchedEvent e) &#123; System.out.println(e);&#125;public static void main(String args[]) throws Exception &#123;Master m = new Master(args[0]); m.startZK(); // wait for a bit Thread.sleep(60000); &#125;&#125; nce we have connected to ZooKeeper, there will be a background thread that will maintain the ZooKeeper session. This thread is a daemon thread, which means that the program may exit even if the thread is still active. Here we sleep for a bit so that we can see some events come in before the program exits.We can compile this simple example using the following:$ javac -cp $CLASSPATH Master.javaOnce we have compiled Master.java, we run it and see the following:$ java -cp $CLASSPATH Master 127.0.0.1:2181 disconnectWhen developers see the Disconnected event, some think they need to create a new ZooKeeper handle to reconnect to the service. Do not do that! See what happens when you start the server, start the Master, and then stop and start the server while the Master is still running. You should see the SyncConnected event followed by the Disconnec ted event and then another SyncConnected event. The ZooKeeper client library takes care of reconnecting to the service for you. Unfortunately, network outages and server failures happen. Usually, ZooKeeper can deal with these failures.]]></content>
  </entry>
  <entry>
    <title><![CDATA[Chronicle]]></title>
    <url>%2F2018-08-09-Chronicle%2F</url>
    <content type="text"><![CDATA[OverviewChronicle Software is about simplifying fast data. It is a suite of libraries to make it easier to write, monitor and tune data processing systems where performance and scalability are concerned. Writing to a QueueIn Chronicle Queue we refer to the act of writing your data to the Chronicle queue, as storing an excerpt. This data could be made up from any data type, including text, numbers, or serialised blobs. Ultimately, all your data, regardless of what it is, is stored as a series of bytes. Just before storing your excerpt, Chronicle Queue reserves an 8-byte header. Chronicle Queue writes the length of your data into this header. This way, when Chronicle Queue comes to read your excerpt, it knows how long each blob of data is. We refer to this 8-byte header, along with your excerpt, as a document. So strictly speaking Chronicle Queue can be used to read and write documents. Within this 8-byte header we also reserve a few bits for a number of internal operations, such as locking, to make Chronicle Queue thread-safe across both processors and threads. The important thing to note is that because of this, you can’t strictly convert the 8 bytes to an integer to find the length of your data blob. To write data to a Chronicle-Queue, you must first create an Appender 123try (ChronicleQueue queue = SingleChronicleQueueBuilder.binary(path + "/trades").build()) &#123; final ExcerptAppender appender = queue.acquireAppender();&#125; Chronicle Queue uses the following low-level interface to write the data: 123try (final DocumentContext dc = appender.writingDocument()) &#123; dc.wire().write().text(“your text data“);&#125; So, Chronicle Queue uses an Appender to write to the queue and a Tailer to read from the queue. Unlike other java queuing solutions, messages are not lost when they are read with a Tailer. Each Chronicle Queue excerpt has a unique index. 1234try (final DocumentContext dc = appender.writingDocument()) &#123; dc.wire().write().text(“your text data“); System.out.println("your data was store to index="+ dc.index());&#125; The high-level methods below such as writeText() are convenience methods on calling appender.writingDocument(), but both approaches essentially do the same thing. The actual code of writeText(CharSequence text) looks like this: 12345678/** * @param text to write a message */void writeText(CharSequence text) &#123; try (DocumentContext dc = writingDocument()) &#123; dc.wire().bytes().append8bit(text); &#125;&#125; This is the highest-level API which hides the fact you are writing to messaging at all. The benefit is that you can swap calls to the interface with a real component, or an interface to a different protocol. 1234// using the method writer interface.RiskMonitor riskMonitor = appender.methodWriter(RiskMonitor.class);final LocalDateTime now = LocalDateTime.now(Clock.systemUTC());riskMonitor.trade(new TradeDetails(now, "GBPUSD", 1.3095, 10e6, Side.Buy, "peter")); You can write a “self-describing message”. Such messages can support schema changes. They are also easier to understand when debugging or diagnosing problems. 12345678// writing a self describing messageappender.writeDocument(w -&gt; w.write("trade").marshallable( m -&gt; m.write("timestamp").dateTime(now) .write("symbol").text("EURUSD") .write("price").float64(1.1101) .write("quantity").float64(15e6) .write("side").object(Side.class, Side.Sell) .write("trader").text("peter"))); You can write “raw data” which is self-describing. The types will always be correct; position is the only indication as to the meaning of those values. 12345// writing just dataappender.writeDocument(w -&gt; w .getValueOut().int32(0x123456) .getValueOut().int64(0x999000999000L) .getValueOut().text("Hello World")); You can write “raw data” which is not self-describing. Your reader must know what this data means, and the types that were used. 123456// writing raw dataappender.writeBytes(b -&gt; b .writeByte((byte) 0x12) .writeInt(0x345678) .writeLong(0x999000999000L) .writeUtf8("Hello World")); This is the lowest level way to write data. You get an address to raw memory and you can write what you want. 1234567891011121314// Unsafe low levelappender.writeBytes(b -&gt; &#123; long address = b.address(b.writePosition()); Unsafe unsafe = UnsafeMemory.UNSAFE; unsafe.putByte(address, (byte) 0x12); address += 1; unsafe.putInt(address, 0x345678); address += 4; unsafe.putLong(address, 0x999000999000L); address += 8; byte[] bytes = "Hello World".getBytes(StandardCharsets.ISO_8859_1); unsafe.copyMemory(bytes, Unsafe.ARRAY_BYTE_BASE_OFFSET, null, address, bytes.length); b.writeSkip(1 + 4 + 8 + bytes.length);&#125;); You can print the contents of the queue. You can see the first two, and last two messages store the same data. // dump the content of the queueSystem.out.println(queue.dump()); position: 262568, header: 0— !!data #binarytrade: { timestamp: 2016-07-17T15:18:41.141, symbol: GBPUSD, price: 1.3095, quantity: 10000000.0, side: Buy, trader: peter} position: 262684, header: 1— !!data #binarytrade: { timestamp: 2016-07-17T15:18:41.141, symbol: EURUSD, price: 1.1101, quantity: 15000000.0, side: Sell, trader: peter} position: 262800, header: 2— !!data #binary!int 1193046168843764404224Hello World position: 262830, header: 3— !!data #binary000402b0 12 78 56 34 00 00 90 99 00 90 99 00 00 0B ·xV4·· ········000402c0 48 65 6C 6C 6F 20 57 6F 72 6C 64 Hello Wo rld position: 262859, header: 4— !!data #binary000402c0 12 ·000402d0 78 56 34 00 00 90 99 00 90 99 00 00 0B 48 65 6C xV4····· ·····Hel000402e0 6C 6F 20 57 6F 72 6C 64 Finding the index at the end of a Chronicle QueueChronicle Queue appenders are thread-local. In fact when you ask for: 1final ExcerptAppender appender = queue.acquireAppender(); the acquireAppender() uses a thread-local pool to give you an appender which will be reused to reduce object creation. As such, the method call to: 1long index = appender.lastIndexAppended(); will only give you the last index appended by this appender; not the last index appended by any appender. If you wish to find the index of the last record written, then you have to call: 1long index = queue.createTailer().toEnd().index(); Dumping a Chronicle Queue, cq4 file as text to the Command LineChronicle Queue stores its data in binary format, with a file extension of cq4: \��@π�header∂�SCQStoreÇE���»wireType∂�WireTypeÊBINARYÕwritePositionèèèèß��������ƒroll∂�SCQSRollÇ*���∆length¶ÄÓ6�∆formatÎyyyyMMdd-HH≈epoch¶ÄÓ6�»indexing∂ SCQSIndexingÇN��� indexCount•��ÃindexSpacing�Àindex2Indexé����ß��������…lastIndexé����ß��������ﬂlastAcknowledgedIndexReplicatedé������ßˇˇˇˇˇˇˇˇ»recovery∂�TimedStoreRecoveryÇ����…timeStampèèèß����������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������This can often be a bit difficult to read, so it is better to dump the cq4 files as text. This can also help you fix your production issues, as it gives you the visibility as to what has been stored in the queue, and in what order. You have to use the chronicle-queue.jar, from any version 4.5.3 or later, and set up the dependent files in the class path. $ java -cp chronicle-queue-4.5.5.jar net.openhft.chronicle.queue.DumpQueueMain 19700101-02.cq4 this will dump the 19700101-02.cq4 file out as text, as shown below: — !!meta-data #binaryheader: !SCQStore { wireType: !WireType BINARY, writePosition: 0, roll: !SCQSRoll { length: !int 3600000, format: yyyyMMdd-HH, epoch: !int 3600000 }, indexing: !SCQSIndexing { indexCount: !short 4096, indexSpacing: 4, index2Index: 0, lastIndex: 0 }, lastAcknowledgedIndexReplicated: -1, recovery: !TimedStoreRecovery { timeStamp: 0 }} …4198044 bytes remaining Reading from a Queue using a TailerReading the queue follows the same pattern as writing, except there is a possibility there is not a message when you attempt to read it. Start Reading 123try (ChronicleQueue queue = SingleChronicleQueueBuilder.binary(path + "/trades").build()) &#123; final ExcerptTailer tailer = queue.createTailer();&#125; You can turn each message into a method call based on the content of the message. 12345// reading using method callsRiskMonitor monitor = System.out::println;MethodReader reader = tailer.methodReader(monitor);// read one messageassertTrue(reader.readOne()); You can decode the message yourself. 12345678910assertTrue(tailer.readDocument(w -&gt; w.read("trade").marshallable( m -&gt; &#123; LocalDateTime timestamp = m.read("timestamp").dateTime(); String symbol = m.read("symbol").text(); double price = m.read("price").float64(); double quantity = m.read("quantity").float64(); Side side = m.read("side").object(Side.class); String trader = m.read("trader").text(); // do something with values. &#125;))); You can read self-describing data values. This will check the types are correct, and convert as required. 12345678assertTrue(tailer.readDocument(w -&gt; &#123; ValueIn in = w.getValueIn(); int num = in.int32(); long num2 = in.int64(); String text = in.text(); // do something with values&#125;)); You can read raw data as primitives and strings. 12345678assertTrue(tailer.readBytes(in -&gt; &#123; int code = in.readByte(); int num = in.readInt(); long num2 = in.readLong(); String text = in.readUtf8(); assertEquals("Hello World", text); // do something with values&#125;)); or, you can get the underlying memory address and access the native memory. 1234567891011121314151617assertTrue(tailer.readBytes(b -&gt; &#123; long address = b.address(b.readPosition()); Unsafe unsafe = UnsafeMemory.UNSAFE; int code = unsafe.getByte(address); address++; int num = unsafe.getInt(address); address += 4; long num2 = unsafe.getLong(address); address += 8; int length = unsafe.getByte(address); address++; byte[] bytes = new byte[length]; unsafe.copyMemory(null, address, bytes, Unsafe.ARRAY_BYTE_BASE_OFFSET, bytes.length); String text = new String(bytes, StandardCharsets.UTF_8); assertEquals("Hello World", text); // do something with values&#125;)); Tailers and File Handlers Clean upChronicle queue tailers may create file handlers, the file handlers are cleaned up whenever the associated chronicle queue is close() or whenever the Jvm runs a Garbage Collection. ExcerptTailer.toEnd()In some applications, it may be necessary to start reading from the end of the queue (e.g. in a restart scenario). For this use-case, ExcerptTailer provides the toEnd() method. If it is necessary to read backwards through the queue from the end, then the tailer can be set to read backwards: 12ExcerptTailer tailer = queue.createTailer();tailer.direction(TailerDirection.BACKWARD).toEnd(); When reading backwards, then the toEnd() method will move the tailer to the last record in the queue. If the queue is not empty, then there will be a DocumentContext available for reading: 123// this will be true if there is at least one message in the queueboolean messageAvailable = tailer.toEnd().direction(TailerDirection.BACKWARD). readingDocument().isPresent(); Low GCUltra low GC means less than one minor collection per day. the principles of Zero-copy eliminating unnecessary garbage collection and increased speed. Runtime code generation that reduces code size for efficient CPU cache usage and increased speed. Smart ordering for optimal parsing and you guessed it increased speed. All these combine to allow Chronicle FIX to achieve excellent performance results. Low garbage rateMinimising garbage is key to avoiding GC pauses. To use your L1 and L2 cache efficiently, you need to keep your garbage rates very low. If you are not using these cache efficiently your application can be 2-5x slower. The garbage from Chronicle is low enough that you can process one million events without jstat detecting you have created any garbage. jstat only displays multiples of 4 KB, and only when a new TLAB is allocated. Chronicle does create garbage, but it is extremely low. i.e. a few objects per million events processes. Once you make the GC pauses manageable, or non-existent, you start to see other sources of delay in your system. Take away the boulders and you start to see the rocks. Take away the rocks and you start to see the pebbles. Chronicle has minimal interaction with the Operating System.System calls are slow, and if you can avoid call the OS, you can save significant amounts of latency. For example, if you send a message over TCP on loopback, this can add a 10 micro-seconds latency between writing and reading the data. You can write to a chronicle, which is a plain write to memory, and read from chronicle, which is also a read from memory with a latency of 0.2 micro-seconds. (And as I mentioned before, you get persistence as well) No need to worry about running out of heap.A common problem with unbounded queues and this uses an open ended amount of heap. Chronicle solves this by not using the heap to store data, but instead using memory mapped files. This improve memory utilisation by making the data more compact but also means a 1 GB JVM can stream 1 TB of data over a day without worrying about the heap or how much main memory you have. In this case, an unbounded queue becomes easier to manage. how it worksChronicle uses a memory mapped file to continuously journal messages, chronicles file-based storage will slowly grow in size as more data is written to the queue, the size of the queue can exceed your available memory, you are only constrained by the amount of disk space you have on your server. Chronicle writes data directly into off-heap memory which is shared between java processes on the same server. Chronicle is very fast, it is able to write and read a message in just two microseconds with no garbage. Typically at the end of each day, you archive the queue and start the next day with a fresh empty queue. Chronicle Queue is a distributed unbounded persisted queue.Chronicle Queue: supports asynchronous RMI and Publish/Subscribe interfaces with microsecond latencies. passes messages between JVMs in under a microsecond (in optimised examples) passes messages between JVMs on different machines via replication in under 10 microseconds (in optimised examples) provides stable, soft, real time latencies into the millions of messages per second for a single thread to one queue; with total ordering of every event. Queue introductionChronicle Queue is a Java project focused on building a persisted low-latency messaging framework for high performance and critical applications. Chronicle diagram 005At first glance Chronicle Queue can be seen as simply another queue implementation. However, it has major design choices that should be emphasised. Using non-heap storage options (RandomAccessFile), Chronicle Queue provides a processing environment where applications do not suffer from Garbage Collection (GC). When implementing high-performance and memory-intensive applications (you heard the fancy term “bigdata”?) in Java, one of the biggest problems is garbage collection. Garbage collection may slow down your critical operations non-deterministically at any time. In order to avoid non-determinism, and escape from garbage collection delays, off-heap memory solutions are ideal. The main idea is to manage your memory manually so it does not suffer from garbage collection. Chronicle Queue behaves like a management interface over off-heap memory so you can build your own solutions over it. Chronicle Queue uses RandomAccessFiles while managing memory and this choice brings lots of possibilities. RandomAccessFiles permit non-sequential, or random, access to a file’s contents. To access a file randomly, you open the file, seek a particular location, and read from or write to that file. RandomAccessFiles can be seen as “large” C-type byte arrays that you can access at any random index “directly” using pointers. File portions can be used as ByteBuffers if the portion is mapped into memory. This memory mapped file is also used for exceptionally fast interprocess communication (IPC) without affecting your system performance. There is no garbage collection as everything is done off-heap. Message type TCP: Stream-oriented UDP, SCTP: message-oriented . On heap vs off heap memory usageOverview I was recently asked about the benefits and wisdom of using off heap memory in Java. The answers may be of interest to others facing the same choices. Off heap memory is nothing special. The thread stacks, application code, NIO buffers are all off heap. In fact in C and C++, you only have unmanaged memory as it does not have a managed heap by default. The use of managed memory or “heap” in Java is a special feature of the language. Note: Java is not the only language to do this.new Object() vs Object pool vs Off Heap memory. new Object() Before Java 5.0, using object pools was very popular. Creating objects was still very expensive. However, from Java 5.0, object allocation and garbage cleanup was made much cheaper, and developers found they got a performance speed up and a simplification of their code by removing object pools and just creating new objects whenever needed. Before Java 5.0, almost any object pool, even an object pool which used objects provided an improvement, from Java 5.0 pooling only expensive objects obviously made sense e.g. threads, socket and database connections. Object pools In the low latency space it was still apparent that recycling mutable objects improved performance by reduced pressure on your CPU caches. These objects have to have simple life cycles and have a simple structure, but you could see significant improvements in performance and jitter by using them.Another area where it made sense to use object pools is when loading large amounts of data with many duplicate objects. With a significant reduction in memory usage and a reduction in the number of objects the GC had to manage, you saw a reduction in GC times and an increase in throughput.These object pools were designed to be more light weight than say using a synchronized HashMap, and so they still helped. Take this StringInterner class as an example. You pass it a recycled mutable StringBuilder of the text you want as a String and it will provide a String which matches. Passing a String would be inefficient as you would have already created the object. The StringBuilder can be recycled.Note: this structure has an interesting property that requires no additional thread safety features, like volatile or synchronized, other than is provided by the minimum Java guarantees. i.e. you can see the final fields in a String correctly and only read consistent references. public class StringInterner { private final String[] interner; private final int mask; public StringInterner(int capacity) { int n = Maths.nextPower2(capacity, 128); interner = new String[n]; mask = n - 1; } private static boolean isEqual(@Nullable CharSequence s, @NotNull CharSequence cs) { if (s == null) return false; if (s.length() != cs.length()) return false; for (int i = 0; i &lt; cs.length(); i++) if (s.charAt(i) != cs.charAt(i)) return false; return true; } @NotNull public String intern(@NotNull CharSequence cs) { long hash = 0; for (int i = 0; i &lt; cs.length(); i++) hash = 57 * hash + cs.charAt(i); int h = (int) Maths.hash(hash) &amp; mask; String s = interner[h]; if (isEqual(s, cs)) return s; String s2 = cs.toString(); return interner[h] = s2; }} Off heap memory usage Using off heap memory and using object pools both help reduce GC pauses, this is their only similarity. Object pools are good for short lived mutable objects, expensive to create objects and long live immutable objects where there is a lot of duplication. Medium lived mutable objects, or complex objects are more likely to be better left to the GC to handle. However, medium to long lived mutable objects suffer in a number of ways which off heap memory solves. Off heap memory provides; Scalability to large memory sizes e.g. over 1 TB and larger than main memory.Notional impact on GC pause times.Sharing between processes, reducing duplication between JVMs, and making it easier to split JVMs.Persistence for faster restarts or replying of production data in test.The use of off heap memory gives you more options in terms of how you design your system. The most important improvement is not performance, but determinism. Off heap and testing One of the biggest challenges in high performance computing is reproducing obscure bugs and being able to prove you have fixed them. By storing all your input events and data off heap in a persisted way you can turn your critical systems into a series of complex state machines. (Or in simple cases, just one state machine) In this way you get reproducible behaviour and performance between test and production. A number of investment banks use this technique to replay a system reliably to any event in the day and work out exactly why that event was processed the way it was. More importantly, once you have a fix you can show that you have fixed the issue which occurred in production, instead of finding an issue and hoping this was the issue. Along with deterministic behaviour comes deterministic performance. In test environments, you can replay the events with realistic timings and show the latency distribution you expect to get in production. Some system jitter can’t be reproduce esp if the hardware is not the same, but you can get pretty close when you take a statistical view. To avoid taking a day to replay a day of data you can add a threshold. e.g. if the time between events is more than 10 ms you might only wait 10 ms. This can allow you to replay a day of events with realistic timing in under an hour and see whether your changes have improved your latency distribution or not. By going more low level don’t you lose some of “compile once, run anywhere”? To some degree this is true, but it is far less than you might think. When you are working closer the processor and so you are more dependant on how the processor, or OS behaves. Fortunately, most systems use AMD/Intel processors and even ARM processors are becoming more compatible in terms of the low level guarantees they provide. There is also differences in the OSes, and these techniques tend to work better on Linux than Windows. However, if you develop on MacOSX or Windows and use Linux for production, you shouldn’t have any issues. This is what we do at Higher Frequency Trading. What new problems are we creating by using off heap? Nothing comes for free, and this is the case with off heap. The biggest issue with off heap is your data structures become less natural. You either need a simple data structure which can be mapped directly to off heap, or you have a complex data structure which serializes and deserializes to put it off heap. Obvious using serialization has its own headaches and performance hit. Using serialization thus much slower than on heap objects. In the financial world, most high ticking data structure are flat and simple, full of primitives which maps nicely off heap with little overhead. How does Chronicle Queue workTerminology Messages are grouped by topics. A topic can contain any number of sub-topics which are logically stored together under the queue/topic. An appender is the source of messages. A tailer is a receiver of messages. Chronicle Queue is broker-less by default. You can use Chronicle Engine to act as a broker for remote access. NoteWe deliberately avoid the term consumer as messages are not consumed/destroyed by reading. At a high level: appenders write to the end of a queue. There is no way to insert, or delete excerpts. tailers read the next available message each time they are called. By using Chronicle Engine, a Java or C# client can publish to a queue to act as a remote appender, and you subscribe to a queue to act as a remote tailer. Topics and Queue filesEach topic is a directory of queues. There is a file for each roll cycle. If you have a topic called mytopic, the layout could look like this: mytopic/ 20160710.cq4 20160711.cq4 20160712.cq4 20160713.cq4To copy all the data for a single day (or cycle), you can copy the file for that day on to your development machine for replay testing. Appenders and tailers are cheap as they don’t even require a TCP connection; they are just a few Java objects. File RetentionYou can add a StoreFileListener to notify you when a file is added, or no longer used. This can be used to delete files after a period of time. However, by default, files are retained forever. Our largest users have over 100 TB of data stored in queues. Every Tailer sees every message.An abstraction can be added to filter messages, or assign messages to just one message processor. However, in general you only need one main tailer for a topic, with possibly, some supporting tailers for monitoring etc. As Chronicle Queue doesn’t partition its topics, you get total ordering of all messages within that topic. Across topics, there is no guarantee of ordering; if you want to replay deterministically from a system which consumes from multiple topics, we suggest replaying from that system’s output. GuaranteesChronicle Queue provides the following guarantees; for each appender, messages are written in the order the appender wrote them. Messages by different appenders are interleaved, for each tailer, it will see every message for a topic in the same order as every other tailer, when replicated, every replica has a copy of every message. Use CasesChronicle Queue is most often used for producer-centric systems where you need to retain a lot of data for days or years. What is a producer-centric system?Most messaging systems are consumer-centric. Flow control is implemented to avoid the consumer ever getting overloaded; even momentarily. A common example is a server supporting multiple GUI users. Those users might be on different machines (OS and hardware), different qualities of network (latency and bandwidth), doing a variety of other things at different times. For this reason it makes sense for the client consumer to tell the producer when to back off, delaying any data until the consumer is ready to take more data. Chronicle Queue is a producer-centric solution and does everything possible to never push back on the producer, or tell it to slow down. This makes it a powerful tool, providing a big buffer between your system, and an upstream producer over which you have little, or no, control. For market data in particular, real time means in a few microseconds; it doesn’t mean intra-day (during the day). Chronicle Queue is fast and efficient, and has been used to increase the speed that data is passed between threads. In addition, it also keeps a record of every message passed allowing you to significantly reduce the amount of logging that you need to do. Latency Sensitive Micro-servicesChronicle Queue supports low latency IPC (Inter Process Communication) between JVMs on the same machine in the order of magnitude of 1 microsecond; as well as between machines with a typical latency of 10 microseconds for modest throughputs of a few hundred thousands. Chronicle Queue supports throughputs of millions of events per second, with stable microsecond latencies. Log ReplacementAs Chronicle Queue can be used to build state machines. All the information about the state of those components can be reproduced externally, without direct access to the components, or to their state. This significantly reduces the need for additional logging. However, any logging you do need can be recorded in great detail. This makes enabling DEBUG logging in production practical. This is because the cost of logging is very low; less than 10 microseconds. Logs can be replicated centrally for log consolidation. Chronicle Queue is being used to store 100+ TB of data, which can be replayed from any point in time. Source codeMappedFile123456789101112131415161718192021222324252627282930313233343536package net.openhft.chronicle.bytes;import net.openhft.chronicle.core.Jvm;import net.openhft.chronicle.core.OS;import net.openhft.chronicle.core.ReferenceCounted;import net.openhft.chronicle.core.ReferenceCounter;import net.openhft.chronicle.core.io.IORuntimeException;import org.jetbrains.annotations.NotNull;import org.jetbrains.annotations.Nullable;import java.io.File;import java.io.FileNotFoundException;import java.io.IOException;import java.io.RandomAccessFile;import java.lang.ref.WeakReference;import java.nio.channels.FileChannel;import java.nio.channels.FileChannel.MapMode;import java.nio.channels.FileLock;import java.nio.file.Files;import java.util.ArrayList;import java.util.List;import java.util.concurrent.atomic.AtomicBoolean;import static net.openhft.chronicle.core.io.Closeable.closeQuietly;/** * A memory mapped files which can be randomly accessed in chunks. It has overlapping regions to * avoid wasting bytes at the end of chunks. */public class MappedFile implements ReferenceCounted &#123; private static final long DEFAULT_CAPACITY = 128L &lt;&lt; 40; // A single JVM cannot lock a file more than once. private static final Object GLOBAL_FILE_LOCK = new Object(); @NotNull private final RandomAccessFile raf; private final FileChannel fileChannel; public interface BytesStore extends RandomDataInput, RandomDataOutput, ReferencedCount, CharSequence public interface Memory { default long heapUsed() { Runtime runtime = Runtime.getRuntime(); return runtime.totalMemory() - runtime.freeMemory(); } @Override @ForceInline public void writeByte(long address, byte b) { UNSAFE.putByte(address, b); }/** Marker annotation for some methods and constructors in the JSR 292 implementation. To utilise this annotation se Chronicle Enterprise Warmup module. / @Target({ElementType.METHOD, ElementType.CONSTRUCTOR})@Retention(RetentionPolicy.RUNTIME)public @interface ForceInline {} Reference https://github.com/OpenHFT/Chronicle-Queue http://vanillajava.blogspot.com/2015/08/what-does-chronicle-software-do.html]]></content>
  </entry>
  <entry>
    <title><![CDATA[Sudo in a Nutshell]]></title>
    <url>%2F2018-08-21-sudo%2F</url>
    <content type="text"><![CDATA[Sudo in a NutshellSudo (su “do”) allows a system administrator to give certain users (or groups of users) the ability to run some (or all) commands as root while logging all commands and arguments. Sudo operates on a per-command basis, it is not a replacement for the shell. Its features include: The ability to restrict what commands a user may run on a per-host basis.Sudo does copious logging of each command, providing a clear audit trail of who did what. When used in tandem with syslogd, the system log daemon, sudo can log all commands to a central host (as well as on the local host). At CU, all admins use sudo in lieu of a root shell to take advantage of this logging.Sudo uses timestamp files to implement a “ticketing” system. When a user invokes sudo and enters their password, they are granted a ticket for 5 minutes (this timeout is configurable at compile-time). Each subsequent sudo command updates the ticket for another 5 minutes. This avoids the problem of leaving a root shell where others can physically get to your keyboard. There is also an easy way for a user to remove their ticket file, useful for placing in a .logout file.Sudo’s configuration file, the sudoers file, is setup in such a way that the same sudoers file may be used on many machines. This allows for central administration while keeping the flexibility to define a user’s privileges on a per-host basis. Please see the samples sudoers file below for a real-world example. sudo.confThe sudo.conf file is used to configure the sudo front end. It specifies the security policy and I/O logging plugins, debug flags as well as plugin-agnostic path names and settings. sudo supports a plugin architecture for security policies and input/output logging. Third parties can develop and distribute their own policy and I/O logging plugins to work seamlessly with the sudo front end. Plugins are dynamically loaded based on the contents of sudo.conf.A Plugin line consists of the Plugin keyword, followed by the symbol_name and the path to the dynamic shared object that contains the plugin. The symbol_name is the name of the struct policy_plugin or struct io_plugin symbol contained in the plugin. The path may be fully qualified or relative. If not fully qualified, it is relative to the directory specified by the plugin_dir Path setting, which defaults to /usr/local/libexec/sudo. In other words:Plugin sudoers_policy sudoers.sois equivalent to:Plugin sudoers_policy /usr/local/libexec/sudo/sudoers.so Configurations sudoers_file=pathnameThe sudoers_file argument can be used to override the default path to the sudoers file. sudoers_uid=uidThe sudoers_uid argument can be used to override the default owner of the sudoers file. It should be specified as a numeric user ID. email notificationIf a user who is not listed in the policy tries to run a command via sudo, mail is sent to the proper authorities. The address used for such mail is configurable via the mailto Defaults entry (described later) and defaults to root. Note that no mail will be sent if an unauthorized user tries to run sudo with the -l or -v option unless there is an authentication error and either the mail_always or mail_badpass flags are enabled. This allows users to determine for themselves whether or not they are allowed to use sudo. All attempts to run sudo (successful or not) will be logged, regardless of whether or not mail is sent. sudoers uses per-user time stamp files for credential caching. Once a user has been authenticated, a record is written containing the user ID that was used to authenticate, the terminal session ID, the start time of the session leader (or parent process) and a time stamp (using a monotonic clock if one is available). The user may then use sudo without a password for a short period of time (5 minutes unless overridden by the timestamp_timeout option). By default, sudoers uses a separate record for each terminal, which means that a user’s login sessions are authenticated separately. The timestamp_type option can be used to select the type of time stamp record sudoers will use. File formatThe sudoers file is composed of two types of entries: aliases (basically variables) and user specifications (which specify who may run what). When multiple entries match for a user, they are applied in order. Where there are multiple matches, the last match is used (which is not necessarily the most specific match).The sudoers file grammar will be described below in Extended Backus-Naur Form (EBNF). Don’t despair if you are unfamiliar with EBNF; it is fairly simple, and the definitions below are annotated. environmentBy default, the env_reset option is enabled. This causes commands to be executed with a new, minimal environment. Lists have two additional assignment operators, += and -=. These operators are used to add to and delete from a list respectively. It is not an error to use the -= operator to remove an element that does not exist in a list. 12345678910111213141535 Defaults env_reset36 Defaults env_keep += "BLOCKSIZE"37 Defaults env_keep += "COLORFGBG COLORTERM"38 Defaults env_keep += "__CF_USER_TEXT_ENCODING"39 Defaults env_keep += "CHARSET LANG LANGUAGE LC_ALL LC_COLLATE LC_CTYPE"40 Defaults env_keep += "LC_MESSAGES LC_MONETARY LC_NUMERIC LC_TIME"41 Defaults env_keep += "LINES COLUMNS"42 Defaults env_keep += "LSCOLORS"43 Defaults env_keep += "SSH_AUTH_SOCK"44 Defaults env_keep += "TZ"45 Defaults env_keep += "DISPLAY XAUTHORIZATION XAUTHORITY"46 Defaults env_keep += "EDITOR VISUAL"47 Defaults env_keep += "HOME MAIL"48 49 Defaults lecture_file = "/etc/sudo_lecture" lecturelectureThis option controls when a short lecture will be printed along with the password prompt. It has the following possible values: alwaysAlways lecture the user. neverNever lecture the user. onceOnly lecture the user the first time they run sudo.If no value is specified, a value of once is implied. Negating the option results in a value of never being used. The default value is once.lecture_filePath to a file containing an alternate sudo lecture that will be used in place of the standard lecture if the named file exists. By default, sudo uses a built-in lecture. Everything is fileA fundamental and very powerful, consistent abstraction provided in UNIX and compatible operating systems is the file abstraction. Many OS services and device interfaces are implemented to provide a file or file system metaphor to applications. manage user group12345678910111213141516171819202122232425# Alternatively, gpasswd may be used. Though the username can only be added (or removed) from one group at a time:gpasswd --add username group# Add users to a group with the gpasswd command:gpasswd -a user group#To remove users from a group:gpasswd -d user groupgpasswd - administer the /etc/group fileEXAMPLES1. Add user (tracy) to the group (hrd)$ gpasswd -a tracy hrd2. Add multiper users to the group (developer)$ gpasswd -a pavan,john developer3. Remove user (rakesh) from group (sqa)$ gpasswd -d rakesh sqa4. Remove multiple users from group (managers)$ gpasswd -d shane,ron,ram managers5. Set user (joy) and group administrator for (managers)$ gpasswd -A joy managers to show user details1id todzhang Display group membership with the groups command:1$ groups user To change the user’s login shell:1# usermod -s /bin/bash username reference https://www.sudo.ws/man/sudo.conf.man.html]]></content>
  </entry>
  <entry>
    <title><![CDATA[Protobuf]]></title>
    <url>%2F2018-08-23-protobuf%2F</url>
    <content type="text"><![CDATA[What are protocol buffers?Protocol buffers are a flexible, efficient, automated mechanism for serializing structured data – think XML, but smaller, faster, and simpler. You define how you want your data to be structured once, then you can use special generated source code to easily write and read your structured data to and from a variety of data streams and using a variety of languages. You can even update your data structure without breaking deployed programs that are compiled against the “old” format. How do they work?You specify how you want the information you’re serializing to be structured by defining protocol buffer message types in .proto files. Each protocol buffer message is a small logical record of information, containing a series of name-value pairs. 123456789101112131415161718message Person &#123; required string name = 1; required int32 id = 2; optional string email = 3; enum PhoneType &#123; MOBILE = 0; HOME = 1; WORK = 2; &#125; message PhoneNumber &#123; required string number = 1; optional PhoneType type = 2 [default = HOME]; &#125; repeated PhoneNumber phone = 4;&#125; As you can see, the message format is simple – each message type has one or more uniquely numbered fields, and each field has a name and a value type, where value types can be numbers (integer or floating-point), booleans, strings, raw bytes, or even (as in the example above) other protocol buffer message types, allowing you to structure your data hierarchically. You can specify optional fields, required fields, and repeated fields. Why not just use XML?Protocol buffers have many advantages over XML for serializing structured data. Protocol buffers: are simpler are 3 to 10 times smaller are 20 to 100 times faster are less ambiguous generate data access classes that are easier to use programmatically Assigning Field NumbersAs you can see, each field in the message definition has a unique number. These numbers are used to identify your fields in the message binary format, and should not be changed once your message type is in use. Note that field numbers in the range 1 through 15 take one byte to encode, including the field number and the field’s type (you can find out more about this in Protocol Buffer Encoding). Field numbers in the range 16 through 2047 take two bytes. So you should reserve the field numbers 1 through 15 for very frequently occurring message elements. Remember to leave some room for frequently occurring elements that might be added in the future. Field RulesYou specify that message fields are one of the following: required: a well-formed message must have exactly one of this field. optional: a well-formed message can have zero or one of this field (but not more than one). repeated: this field can be repeated any number of times (including zero) in a well-formed message. The order of the repeated values will be preserved. Some engineers at Google have come to the conclusion that using required does more harm than good; they prefer to use only optional and repeated. However, this view is not universal. Combining Messages leads to bloat While multiple message types (such as message, enum, and service) can be defined in a single .proto file, it can also lead to dependency bloat when large numbers of messages with varying dependencies are defined in a single file. It’s recommended to include as few message types per .proto file as possible. Reserved fieldsto make sure this doesn’t happen is to specify that the field numbers (and/or names, which can also cause issues for JSON serialization) of your deleted fields are reserved. The protocol buffer compiler will complain if any future users try to use these field identifiers. 1234message Foo &#123; reserved 2, 15, 9 to 11; reserved "foo", "bar";&#125; Note that you can’t mix field names and field numbers in the same reserved statement. Optional fields and default1optional int32 result_per_page = 3 [default = 10]; Importing DefinitionsYou can use definitions from other .proto files by importing them. To import another .proto’s definitions, you add an import statement to the top of your file: 1import "myproject/other_protos.proto"; To generate class1../protoc-3/bin/protoc --java_out=./ ticket.proto Builders vs. MessagesThe message classes generated by the protocol buffer compiler are all immutable. Once a message object is constructed, it cannot be modified, just like a Java String. To construct a message, you must first construct a builder, set any fields you want to set to your chosen values, then call the builder’s build() method. You may have noticed that each method of the builder which modifies the message returns another builder. The returned object is actually the same builder on which you called the method. It is returned for convenience so that you can string several setters together on a single line of code. Here’s an example of how you would create an instance of Person: 12345678910Person john = Person.newBuilder() .setId(1234) .setName("John Doe") .setEmail("jdoe@example.com") .addPhones( Person.PhoneNumber.newBuilder() .setNumber("555-4321") .setType(Person.PhoneType.HOME)) .build(); Parsing and SerializationFinally, each protocol buffer class has methods for writing and reading messages of your chosen type using the protocol buffer binary format. These include: byte[] toByteArray();: serializes the message and returns a byte array containing its raw bytes. static Person parseFrom(byte[] data);: parses a message from the given byte array. void writeTo(OutputStream output);: serializes the message and writes it to an OutputStream. pstatic Person parseFrom(InputStream input);: reads and parses a message from an InputStream. Write a message// Write the new address book back to disk. FileOutputStream output = new FileOutputStream(args[0]); addressBook.build().writeTo(output); Reading A MessageUse message’s parseFrom method on stream: AddressBook.parseFrom(new FileInputStream(args[0])); 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import com.example.tutorial.AddressBookProtos.AddressBook;import com.example.tutorial.AddressBookProtos.Person;import java.io.FileInputStream;import java.io.IOException;import java.io.PrintStream;class ListPeople &#123; // Iterates though all people in the AddressBook and prints info about them. static void Print(AddressBook addressBook) &#123; for (Person person: addressBook.getPeopleList()) &#123; System.out.println("Person ID: " + person.getId()); System.out.println(" Name: " + person.getName()); if (person.hasEmail()) &#123; System.out.println(" E-mail address: " + person.getEmail()); &#125; for (Person.PhoneNumber phoneNumber : person.getPhonesList()) &#123; switch (phoneNumber.getType()) &#123; case MOBILE: System.out.print(" Mobile phone #: "); break; case HOME: System.out.print(" Home phone #: "); break; case WORK: System.out.print(" Work phone #: "); break; &#125; System.out.println(phoneNumber.getNumber()); &#125; &#125; &#125; // Main function: Reads the entire address book from a file and prints all // the information inside. public static void main(String[] args) throws Exception &#123; if (args.length != 1) &#123; System.err.println("Usage: ListPeople ADDRESS_BOOK_FILE"); System.exit(-1); &#125; // Read the existing address book. AddressBook addressBook = AddressBook.parseFrom(new FileInputStream(args[0])); Print(addressBook); &#125;&#125; References https://developers.google.com/protocol-buffers/docs/overview]]></content>
  </entry>
  <entry>
    <title><![CDATA[Mockito]]></title>
    <url>%2F2018-09-03-distruptor%2F</url>
    <content type="text"><![CDATA[FeatureThere only 2 things you can do with Mockito mocks - verify or stub. Stubbing goes before execution and verification afterwards. References https://github.com/mockito/mockito/wiki/Mockito-vs-EasyMock]]></content>
  </entry>
  <entry>
    <title><![CDATA[Distruptor]]></title>
    <url>%2F2018-09-06-mockito%2F</url>
    <content type="text"><![CDATA[multithreadingConcurrent execution of code ia bout two things: mutal exclusion and visibility of change. Mutual exclusion is about managing contented updtes to some resources. Visibiliyt of change is about controlling when such changes are made visible to other threads. mutal exclusionIt is possible to avoid the need for mutal exclusion if you can eliminate the need for contented updates. If your algorithm can guarantee that any given resource is modified by only one thread then utal exclusion is unnecessary. Read and write operations require that all changes are made visible to other threads. The most costly operation in any concurrent environment is a contended write access. locksLock provide mutual exclusion and ensure that the visibility of change occurs in an ordered manner. Locks are incredibly expensive because they require arbitration when contended. This arbitration is achieved by a context switch to the OS kernerl which will suspend threads waiting on a lock until it’s released. During such a context switch , as well as releasing control to the OS which may decided to do other house-keeping tasks which it has control, execution context can lose previously cached data nad instrucionts. This can have a serious performance impact on modern CPU. References https://developers.google.com/protocol-buffers/docs/overview]]></content>
  </entry>
  <entry>
    <title><![CDATA[YAML]]></title>
    <url>%2F2018-09-16-yaml-config%2F</url>
    <content type="text"><![CDATA[Key pointsAll YAML files (regardless of their association with Ansible or not) can optionally begin with — and end with …. This is part of the YAML format and indicates the start and end of a document. In a way, YAML is to JSON what Markdown is to HTML. # References https://developers.google.com/protocol-buffers/docs/overview]]></content>
  </entry>
  <entry>
    <title><![CDATA[Citrix receiver]]></title>
    <url>%2F2018-09-26-Citrix%2F</url>
    <content type="text"><![CDATA[“Cannot connect to remote desktop” with Citrix Receiver1cd /opt/Citrix/ICAClient/keystore/ then: rm -rf cacerts and finally: ln -s /etc/ssl/certs cacerts Cann’t show full sreen in linux citrix receiverThere is workaround, i.e. Press Alt and drag RDP window, then maximum it.]]></content>
  </entry>
  <entry>
    <title><![CDATA[Guice]]></title>
    <url>%2F2018-09-18-Google-Guice%2F</url>
    <content type="text"><![CDATA[A new type of JuicePut simply, Guice alleviates the need for factories and the use of new in your Java code. Think of Guice’s @Inject as the new new. You will still need to write factories in some cases, but your code will not depend directly on them. Your code will be easier to change, unit test and reuse in other contexts. Guice embraces Java’s type safe nature, especially when it comes to features introduced in Java 5 such as generics and annotations. You might think of Guice as filling in missing features for core Java. Ideally, the language itself would provide most of the same features, but until such a language comes along, we have Guice. Guice helps you design better APIs, and the Guice API itself sets a good example. Guice is not a kitchen sink. We justify each feature with at least three use cases. When in doubt, we leave it out. We build general functionality which enables you to extend Guice rather than adding every feature to the core framework. Guice vs SpringSpring and Google Guice are two powerful dependency injection frameworks in use today. Both frameworks fully embrace the concepts of dependency injection, but each has its own way of implementing them. Although Spring provides many benefits, it was created in a pre-Java-5 world. The Guice framework takes DI to the next level. The advent of Java 5 brought significant changes to the language like generics and annotations: features that enhance the power of Java static typing. Guice is a DI framework that was built from the ground up with the intent to take full advantage of these new features and that has focused on one primary goal: to do dependency injection well. Guice aims to make development and debugging easier and faster, not harder and slower. In that vein, Guice steers clear of surprises and magic. You should be able to understand code with or without tools, though tools can make things even easier. When errors do occur, Guice goes the extra mile to generate helpful messages. core differences between the two, and see why I prefer to use Guice. Living in XML Hell Eliminating reliance on String identifiers Preferring Constructor Injection Although Spring and Guice both support constructor and setter injection, each framework has a preference. Spring has long favored setter injection. Back in the early days of Spring, the authors believed the lack of argument names and default arguments in constructor injection reduced clarity. In addition, constructor injection makes it difficult to have optional dependencies, requires dependencies to be configured in a specific order, and forces subclasses to deal with superclass dependencies. Using setter injection eliminates these problems, and so Spring favors that approach. The Guice authors saw difficulties with setter injection. One problem is immutability: it is impossible to make immutable a class that uses setter injection. Constructor injection, on the other hand, makes the creation of immutable classes easy, an important consideration in writing multi-threaded applications. In addition, optional dependencies, while perhaps convenient, can introduce confusion about how a class is configured at runtime. Configuring a class through setter injection can often lead to missed required dependencies. Though Spring does provide a @Required annotation to solve this problem, using constructor injection eliminates it by default. Constructor injection also makes a class’s dependencies immediately clear at a glance. If you’re writing or modifying a unit test, it’s easy to read what the system-under-test needs. Lastly, because Guice uses types to wire classes together, constructor argument order isn’t an issue. You can feel free to reorder them how you want without needing to modify configuration code at all. The potential drawbacks posed by setter injection outweigh the benefits in many common scenarios, and so Guice established a best practice of favoring constructor injection instead. Its API is well-suited to that approach. But if you should choose to switch from one form of injection to the other, Guice makes that easy too. Changing from setter to constructor injection or vice versa is simply a matter of modifying the class in question. Unlike in Spring, you need never touch a configuration file. Nullifying NullPointerExceptions Null is easily one of the most non-communicative return values possible from a method call.Guice hates nulls as much as I do. By default, Guice refuses to inject a null into any object, and if an accidental null shows up during wiring, it fails-fast with a ProvisionException. Guice does allow for the exception case by permitting fields to be annotated with @Nullable, but this is discouraged. Intruding into the domain Guice aims to eliminate all of this boilerplate without sacrificing maintainability.With Guice, you implement modules. Guice passes a binder to your module, and your module uses the binder to map interfaces to implementations. The following module tells Guice to map Service to ServiceImpl in singleton scope: 123456 public class MyModule implements Module &#123; public void configure(Binder binder) &#123; binder.bind(Service.class) .to(ServiceImpl.class) .in(Scopes.SINGLETON);&#125; &#125; A module tells Guice what we want to inject. Now, how do we tell Guice where we want it injected? With Guice, you annotate constructors, methods and fields with @Inject. 123456789 public class Client &#123; private final Service service;@Inject public Client(Service service) &#123; this.service = service;&#125; public void go() &#123; service.go();&#125; &#125; Guice vs. Dependency Injection By HandAs you can see, Guice saves you from having to write factory classes. You don’t have to write explicit code wiring clients to their dependencies. If you forget to provide a dependency, Guice fails at startup. Guice handles circular dependencies automatically.Guice enables you to specify scopes declaratively. For example, you don’t have to write the same code to store an object in the HttpSession over and over.In the real world, you often don’t know an implementation class until runtime. You need meta factories or service locators for your factories. Guice addresses these problems with minimal effort.When injecting dependencies by hand, you can easily slip back into old habits and introduce direct dependencies, especially if you’re new to the concept of dependency injection. Using Guice turns the tables and makes doing the right thing easier. Guice helps keep you on track. Guice annotationsWhen possible, Guice enables you to use annotations in lieu of explicit bindings and eliminate even more boilerplate code. Back to our example, if you need an interface to simplify unit testing but you don’t care about compile time dependencies, you can point to a default implementation directly from your interface. 1234 @ImplementedBy(ServiceImpl.class) public interface Service &#123; void go();&#125; If a client needs a Service and Guice can’t find an explicit binding, Guice willinject an instance of ServiceImpl. By default, Guice injects a new instance every time. If you want to specify adifferent scope, you can annotate the implementation class, too. 12345@Singleton public class ServiceImpl implements Service &#123; public void go() &#123;... &#125;&#125; Architectural OverviewWe can break Guice’s architecture down into two distinct stages: startup andruntime. You build an Injector during startup and use it to inject objects at runtime. StartupYou configure Guice by implementing Module. You pass Guice a module, Guice passes your module a Binder, and your module uses the binder to configure bindings. A binding most commonly consists of a mapping between an interface and a concrete implementation. For example: 123456789 public class MyModule implements Module &#123; public void configure(Binder binder) &#123; // Bind Foo to FooImpl. Guice will create a new // instance of FooImpl for every injection. binder.bind(Foo.class).to(FooImpl.class); // Bind Bar to an instance of Bar. Bar bar = new Bar(); binder.bind(Bar.class).toInstance(bar);&#125; Injecting ProvidersWith normal dependency injection, each type gets exactly one instance of each of its dependent types. The RealBillingService gets one CreditCardProcessor and one TransactionLog. Sometimes you want more than one instance of your dependent types. When this flexibility is necessary, Guice binds a provider. Providers produce a value when the get() method is invoked: 123public interface Provider&lt;T&gt; &#123; T get();&#125; @Provides MethodsWhen you need code to create an object, use an @Provides method. The method must be defined within a module, and it must have an @Provides annotation. The method’s return type is the bound type. Whenever the injector needs an instance of that type, it will invoke the method. 1234567891011121314public class BillingModule extends AbstractModule &#123; @Override protected void configure() &#123; ... &#125; @Provides TransactionLog provideTransactionLog() &#123; DatabaseTransactionLog transactionLog = new DatabaseTransactionLog(); transactionLog.setJdbcUrl("jdbc:mysql://localhost/pizza"); transactionLog.setThreadPoolSize(30); return transactionLog; &#125;&#125; If the @Provides method has a binding annotation like @PayPal or @Named(“Checkout”), Guice binds the annotated type. Dependencies can be passed in as parameters to the method. The injector will exercise the bindings for each of these before invoking the method. 1234567@Provides @PayPalCreditCardProcessor providePayPalCreditCardProcessor( @Named("PayPal API key") String apiKey) &#123; PayPalCreditCardProcessor processor = new PayPalCreditCardProcessor(); processor.setApiKey(apiKey); return processor;&#125; References]]></content>
  </entry>
  <entry>
    <title><![CDATA[Citrix receiver]]></title>
    <url>%2F2018-10-13-SBE%2F</url>
    <content type="text"><![CDATA[Simple Binary Encoding (SBE)SBE is an OSI layer 6 presentation for encoding and decoding binary application messages for low-latency financial applications.]]></content>
  </entry>
  <entry>
    <title><![CDATA[Inter Processes Communication]]></title>
    <url>%2F2019-01-03-IPC%2F</url>
    <content type="text"><![CDATA[IPCinterprocess communication (IPC) Posted by: Margaret RouseWhatIs.com Interprocess communication (IPC) is a set of programming interfaces that allow a programmer to coordinate activities among different program processes that can run concurrently in an operating system. This allows a program to handle many user requests at the same time. Since even a single user request may result in multiple processes running in the operating system on the user’s behalf, the processes need to communicate with each other. The IPC interfaces make this possible. Each IPC method has its own advantages and limitations so it is not unusual for a single program to use all of the IPC methods. Inter process communication (IPC) is a mechanism which allows processes to communicate each other and synchronize their actions. The communication between these processes can be seen as a method of co-operation between them. Processes can communicate with each other using these two ways:Shared MemoryMessage passing]]></content>
  </entry>
  <entry>
    <title><![CDATA[Foreign Exchange]]></title>
    <url>%2F2019-01-17-FX-ForeignExchange%2F</url>
    <content type="text"><![CDATA[currency pairs Direct ccy: means USD is part of currency pair Cross ccy: means ccy wihtout USD, so except NDF, the deal will be split to legs, both with USD. e.g. EUR/GBP will split to EURUSD and USDGBP non-convention ccy pair: that’s depends on where you sit on. e.g. for Aussie traders, they would trade AUDNZD, but for kiwi traders, they would trade for NZDAUD, then AUDNZD would be non-convention]]></content>
  </entry>
  <entry>
    <title><![CDATA[Mifid]]></title>
    <url>%2F2019-01-17-Mifid%2F</url>
    <content type="text"><![CDATA[FX Spot is not covered by the regulation, as it is not considered to be a financial instrument by ESMA, the European Union (EU) regulator. As FX is considered “illiquid” it does not have pre-trade reporting requirements. Recordkeeping – MiFID II requires firms to keep extensive records of all transactions, communications, services and activities for 10 years, in order for them to be able to provide transparency into the trade life cycle. This is to support trade reconstruction if required. Overall there are 3 points in the text which we think will be of particular interest to foreign exchange brokers. The EC has determined that FX Forward contracts remain outside the scope of MiFID II if they satisfy all of the following conditions:The contract for deliverable FX is physically settledAt least one of the parties to the contract is a non-financial counterpartyThe purpose of the contract is to facilitate payment for identifiable goods, services or direct investmentThe contract is not traded on a trading venue FX Forwards will qualify for the means of payment exclusion if they meet the following criteria: The counterparty is a corporate entity (a non-financial counterparty (‘NFC’) as defined under EMIR); The FX forwards are traded for the purpose of facilitating payment for identifiable goods or services (for example, entering into an FX forward in order to pay an upcoming invoice in a foreign currency, or in preparation of an upcoming purchase in a foreign currency, as opposed to trading FX forwards for speculative purposes); and The FX forwards are traded bilaterally, as opposed to on a regulated trading venue (note that Agile Markets is not a regulated trading venue and does not affect eligibility); The Financial Conduct Authority has provided some examples of scenarios that would fit within the exclusion. Please find the examples provided on the link here.]]></content>
      <tags>
        <tag>MTF</tag>
        <tag>Mifid</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 8 Tips]]></title>
    <url>%2F2017-01-14-Java-8%2F</url>
    <content type="text"><![CDATA[This blog is listing key new features introduced in Java 8It is best to think of a lambda expression as a function, not an object, and to accept that it can be converted to a functional interface. New comparator method in Java 8123456789List&lt;Track&gt; tracks = asList(new Track("Bakai", 524), new Track("Violets for Your Furs", 378), new Track("Time Was", 451));Track shortestTrack = tracks.stream() .min(Comparator.comparing(track -&gt; track.getLength())) .get();assertEquals(tracks.get(1), shortestTrack); When we think about maximum and minimum elements, the first thing we need to think about is the ordering that we’re going to be using. When it comes to finding the shortest track, the ordering is provided by the length of the ” In order to inform the Stream that we’re using the length of the track, we give it a Comparator. Conveniently, Java 8 has added a static method called comparing that lets us build a comparator using keys. Previously, we always encountered an ugly pattern in which we had to write code that got a field out of both the objects being compared, then compare these field values. Now, to get the same element out of both elements being compared, we just provide a getter function for the value. In this case we’ll use length, which is a getter function in disguise.It’s worth reflecting on the comparing method for a moment. This is actually a function that takes a function and returns a function. Pretty meta, I know, but also incredibly useful. At any point in the past, this method could have been added to the Java standard library, but the poor readability and verbosity issues surrounding anonymous inner classes would have made it impractical. Now, with lambda expressions, it’s convenient and concise.” But thinking of passing code to methods as a mere consequence of Streamsdownplays its range of uses within Java 8. It gives you a new concise way to express behavior parameterization. It might sound surprising, but interfaces in Java 8 can now declare methods with implementation code; this can happen in two ways. First, Java 8 allows static methods inside interfaces. Second, Java 8 introduces a new feature called default methods that allows you to provide a default implementation for methods in an interface. In other words, interfaces can provide concrete implementation for methods. As a result, existing classes implementing an interface will automatically inherit the default implementations if they don’t provide one explicitly. This allows you to evolve interfaces nonintrusively. You’ve been using several default methods all along. Two examples you’ve seen are sort in the List interface and stream in the Collection interface.Wow! Are interfaces like abstract classes now? Yes and no; there are fundamental differences, which we explain in this chapter. But more important, why should you care about default methods? The main users of default methods are library designers. As we explain later, default methods were introduced to evolve libraries such as the Java API in a compatible way, Now that static methods can exist inside interfaces, such utility classes in your code can go away and their static methods can be moved inside an interface. These companion classes will remain in the Java API in order to preserve backward compatibility.Adding a new method to an interface is binary compatible; this means existing class file implementations will still run without the implementation of the new method, if there’s no attempt to recompile them. In this case the game will still run (unless it’s recompiled) despite adding the method setRelativeSize to the Resizable interface Abstract classes vs. interfaces in Java 8So what’s the difference between an abstract class and an interface? They both can contain abstract methods and methods with a body. First, a class can extend only from one abstract class, but a class can implement multiple interfaces. Second, an abstract class can enforce a common state through instance variables (fields). An interface can’t have instance variables. Keeping interfaces minimal and orthogonal lets you achieve great reuse and composition of behavior inside your codebase. Minimal interfaces with orthogonal functionalitiesInheritance considered harmfulInheritance shouldn’t be your answer to everything when it comes down to reusing code. For example, inheriting from a class that has 100 methods and fields just to reuse one method is a bad idea, because it adds unnecessary complexity. You’d be better off using delegation: create a method that calls directly the method of the class you need via a member variable. This is why you’ll sometime find classes that are declared “final” intentionally: they can’t be inherited from to prevent this kind of antipattern or have their core behavior messed with. Note that sometimes final classes have a place; for example, String is final because we don’t want anybody to be able to interfere with such core functionality. Three resolution rules to knowThere are three rules to follow when a class inherits a method with the same signature from multiple places (such as another class or interface): Classes always win. A method declaration in the class or a superclass takes priority over any default method declaration. Otherwise, sub-interfaces win: the method with the same signature in the most specific default-providing interface is selected. (If B extends A, B is more specific than A). Finally, if the choice is still ambiguous, the class inheriting from multiple interfaces has to explicitly select which default method implementation to use by overriding it and calling the desired method explicitly. These are the only rules you need to know! Lambda InterfacesThis conversion to interfaces is what makes lambda expressions so compelling. The syntax is short and simple. 12BiFunction&lt;String, String, Integer&gt; comp = (first, second) -&gt; Integer.compare(first.length(), second.length()); The expression System.out::printlnis a method reference that is equivalent to the lambda expression x -&gt; System.out.println(x). There are three principal cases: object::instanceMethod Class::staticMethod Class::instanceMethod In the third case, the first parameter becomes the target of the method. For example, String::compareToIgnoreCaseis the same as (x, y) -&gt; x.compareToIgnoreCase(y). Just like lambda expressions, method references don’t live in isolation. They are always turned into instances of functional interfaces. Constructor ReferencesConstructor references are just like method references, except that the name of the method is newå. For example, Button::new is a reference to a Button constructor. Which constructor? It depends on the context. 123List&lt;String&gt; labels = ...;Stream&lt;Button&gt; stream = labels.stream().map(Button::new);List&lt;Button&gt; buttons = stream.collect(Collectors.toList()); For example, suppose we want to have an array of buttons. The Stream interface has a toArraymethod that returns an Object array: 1Object[] buttons = stream.toArray(); we need to refine our understanding of a lambda expression. A lambda expression has three ingredients: A block of code Parameters Values for the free variables, that is, the variables that are not parameters and not defined inside the code The technical term for a block of code together with the values of the free variables is a closure. If someone gloats that their language has closures, rest assured that Java has them as well. In Java, lambda expressions are closures. In fact, inner classes have been closures all along. Java 8 gives us closures with an attractive syntax. Inner classes can also capture values from an enclosing scope. Before Java 8, inner classes were only allowed to access finallocal variables. This rule has now been relaxed to match that for lambda expressions. An inner class can access any effectively final local variable—that is, any variable whose value does not change. When you use the this keyword in a lambda expression, you refer to the this parameter of the method that creates the lambda. For example, consider 123456public class Application &#123; public void doWork() &#123; Runnable runner = () -&gt; &#123; ...; System.out.println(this.toString()); ... &#125;; ... &#125;&#125; The expression this.toString()calls the toString method of the Application object, not the Runnable instance. There is nothing special about the use of this in a lambda expression. The scope of the lambda expression is nested inside the doWork method, and this has the same meaning anywhere in that method. default methodsThe Java designers decided to solve this problem once and for all by allowing interface methods with concrete implementations (called default methods). Those methods can be safely added to existing interfaces. 1234interface Person &#123; long getId(); default String getName() &#123; return "John Q. Public"; &#125;&#125; The interface has two methods: getId, which is an abstract method, and the default method getName. A concrete class that implements the Person interface must, of course, provide an implementation of getId, but it can choose to keep the implementation of getName or to override it. Default methods put an end to the classic pattern of providing an interface and an abstract class that implements most or all of its methods, such as Collection/AbstractCollectionor/WindowListener/WindowAdapter. Now you can just implement the methods in the interface. To compare Person objects by name, use Comparator.comparing(Person::getName). we have compared strings by length with the lambda expression 1(first, second) -&gt; Integer.compare(first.length(), second.length()). But with the static compare method, we can do much better and simply use 1Comparator.comparing(String::length). In Java 8, static methods have been added to quite a few interfaces. For example, the Comparator interface has a very useful static comparing method that accepts a “key extraction” function and yields a comparator that compares the extracted keys. Stream vs collectionsA stream seems superficially similar to a collection, allowing you to transform and retrieve data. But there are significant differences: A stream does not store its elements. They may be stored in an underlying collection or generated on demand. Stream operations don’t mutate their source. Instead, they return new streams that hold the result. Stream operations are lazy when possible. This means they are not executed until their result is needed. For example, if you only ask for the first five long words instead of counting them all, then the filter method will stop filtering after the fifth match. As a consequence, you can even have infinite streams! Streams follow the “what, not how” principle. In our stream example, we describe what needs to be done: get the long words and count them. We don’t specify in which order, or in which thread, this should happen. Work with streamsWhen you work with streams, you set up a pipeline of operations in three stages. You create a stream. You specify intermediate operations for transforming the initial stream into others, in one or more steps. You apply a terminal operation to produce a result. This operation forces the execution of the lazy operations that precede it. Afterwards, the stream can no longer be used. 1long count = words.parallelStream().filter(w -&gt; w.length() &gt; 12).count(); Stream operations are not executed on the elements in the order in which they are invoked on the streams. In our example, nothing happens until count is called. When the count method asks for the first element, then the filter method starts requesting elements, until it finds one that has length &gt; 12. To produce infinite sequences such as 0 1 2 3 …, use the iterate method instead. It takes a “seed” value and a function (technically, a UnaryOperator), and repeatedly applies the function to the previous result. For example, 12Stream&lt;BigInteger&gt; integers = Stream.iterate(BigInteger.ZERO, n -&gt; n.add(BigInteger.ONE)); The first element in the sequence is the seed You can use the following statement to split a string into words: 12Stream&lt;String&gt; words = Pattern.compile("[\\P&#123;L&#125;]+").splitAsStream(contents); The static Files.linesmethod returns a Stream of all lines in a file. The Stream interface has AutoCloseableas a superinterface. When the close method is called on the stream, the underlying file is also closed. To make sure that this happens, it is best to use the Java 7 try-with-resources statement: 123try (Stream&lt;String&gt; lines = Files.lines(path)) &#123; Do something with lines&#125; The stream, and the underlying file with it, will be closed when the try block exits normally or through an exception. The filter, map, and flatMapMethodsA stream transformation reads data from a stream and puts the transformed data into another stream. You have already seen the filter transformation that yields a new stream with all elements that match a certain condition. 2.3. The filter, map, and flatMap MethodsA stream transformation reads data from a stream and puts the transformed data into another stream. You have already seen the filtertransformation that yields a new stream with all elements that match a certain condition. Here, we transform a stream of strings into another stream containing only long words: 123List&lt;String&gt; wordList = ...;Stream&lt;String&gt; words = wordList.stream();Stream&lt;String&gt; longWords = words.filter(w -&gt; w.length() &gt; 12); The argument of filter is a Predicate—that is, a function from T to boolean. Often, you want to transform the values in a stream in some way. Use the map method and pass the function that carries out the transformation. For example, you can transform all words to lowercase like this: 1Stream&lt;String&gt; lowercaseWords = words.map(String::toLowerCase); Here, we used mapwith a method reference. Often, you will use a lambda expression instead: 1Stream&lt;Character&gt; firstChars = words.map(s -&gt; s.charAt(0)); The resulting stream contains the first character of each word. When you use map, a function is applied to each element, and the return values are collected in a new stream. Now suppose that you have a function that returns not just one value but a stream of values, such as this one: 12345public static Stream&lt;Character&gt; characterStream(String s) &#123; List&lt;Character&gt; result = new ArrayList&lt;&gt;(); for (char c : s.toCharArray()) result.add(c); return result.stream();&#125; For example, characterStream(“boat”)is the stream [‘b’, ‘o’, ‘a’, ‘t’]. Suppose you map this method on a stream of strings: 1Stream&lt;Stream&lt;Character&gt;&gt; result = words.map(w -&gt; characterStream(w)); You will get a stream of streams, like [… [‘y’, ‘o’, ‘u’, ‘r’], [‘b’, ‘o’, ‘a’, ‘t’], …] To flatten it out to a stream of characters [… ‘y’, ‘o’, ‘u’, ‘r’, ‘b’, ‘o’, ‘a’, ‘t’, …], use the flatMapmethod instead of map: 12Stream&lt;Character&gt; letters = words.flatMap(w -&gt; characterStream(w)) // CallscharacterStream on each word and flattens the results NOTE You may find a flatMap method in classes other than streams. It is a general concept in computer science. Suppose you have a generic type G (such as Stream) and functions ffrom some type T to Gand g from U to G. Then you can compose them, that is, first apply f and then g, by using flatMap. This is a key idea in the theory of monads. But don’t worry—you can use flatMapwithout knowing anything about monads. This method is particularly useful for cutting infinite streams down to size. For example, 1Stream&lt;Double&gt; randoms = Stream.generate(Math::random).limit(100); yields a stream with 100 random numbers. The peek method yields another stream with the same elements as the original, but a function is invoked every time an element is retrieved. That is handy for debugging: 123Object[] powers = Stream.iterate(1.0, p -&gt; p * 2) .peek(e -&gt; System.out.println("Fetching " + e)) .limit(20).toArray(); When an element is actually accessed, a message is printed. This way you can verify that the infinite stream returned by iterate is processed lazily. The stream transformations of the preceding sections were stateless. When an element is retrieved from a filtered or mapped stream, the answer does not depend on the previous elements. There are also a few stateful transformations. For example, the distinct method returns a stream that yields elements from the original stream, in the same order, except that duplicates are suppressed. The stream must obviously remember the elements that it has already seen. 123Stream&lt;String&gt; uniqueWords = Stream.of("merrily", "merrily", "merrily", "gently").distinct(); // Only one"merrily" is retained The sorted method must see the entire stream and sort it before it can give out any elements—after all, the smallest one might be the last one. Clearly, you can’t sort an infinite stream. There are several sorted methods. One works for streams of Comparableelements, and another accepts a Comparator. Here, we sort strings so that the longest ones come first: Click here to view code image Stream longestFirst = words.sorted(Comparator.comparing(String::length).reversed());Of course, you can sort a collection without using streams. The sorted method is useful when the sorting process is a part of a stream pipeline. NOTE The Collections.sortmethod sorts a collection in place, whereas Stream.sortedreturns a new sorted stream. The methods that we cover in this section are called reductions. They reduce the stream to a value that can be used in your program. Reductions are terminal operations. After a terminal operation has been applied, the stream ceases to be usable. In Java 8, the Optional type is the preferred way of indicating a missing return value. We discuss the Optional type in detail in the next section. Here is how you can get the maximum of a stream: Click here to view code image Optional largest = words.max(String::compareToIgnoreCase);if (largest.isPresent()) System.out.println(“largest: “ + largest.get()); reduce. Each segment needs to start out with its own empty hash set, and reduce only lets you supply one identity value. Instead, use collect. It takes three arguments: A supplier to make new instances of the target object, for example, a constructor for a hash set An accumulatorthat adds an element to the target, for example, an addmethod A combiner that merges two objects into one, such as addAll NOTE The target object need not be a collection. It could be a StringBuilderor an object that tracks a count and a sum. Here is how the collect method works for a hash set: Click here to view code image HashSet result = stream.collect(HashSet::new, HashSet::add, HashSet::addAll); In practice, you don’t have to do that because there is a convenient Collector interface for these three functions, and a Collectors class with factory methods for common collectors. To collect a stream into a list or set, you can simply call Click here to view code image List result = stream.collect(Collectors.toList()); or Click here to view code image Set result = stream.collect(Collectors.toSet()); If you want to control which kind of set you get, use the following call instead: Click here to view code image TreeSet result = stream.collect(Collectors.toCollection(TreeSet::new)); Suppose you want to collect all strings in a stream by concatenating them. You can call String result = stream.collect(Collectors.joining()); If you want a delimiter between elements, pass it to the joiningmethod: Click here to view code image String result = stream.collect(Collectors.joining(“, “)); If your stream contains objects other than strings, you need to first convert them to strings, like this: Click here to view code image String result = stream.map(Object::toString).collect(Collectors.joining(“, “)); If you want to reduce the stream results to a sum, average, maximum, or minimum, then use one of the methods summarizing(Int|Long|Double). These methods take a function that maps the stream objects to a number and yield a result of type (Int|Long|Double)SummaryStatistics, with methods for obtaining the sum, average, maximum, and minumum. Click here to view code image IntSummaryStatistics summary = words.collect( Collectors.summarizingInt(String::length));double averageWordLength = summary.getAverage();double maxWordLength = summary.getMax(); NOTE So far, you have seen how to reduce or collect stream values. But perhaps you just want to print them or put them in a database. Then you can use the forEachmethod: stream.forEach(System.out::println); The function that you pass is applied to each element. On a parallel stream, it’s your responsibility to ensure that the function can be executed concurrently. We discuss this in Section 2.13, “Parallel Streams,” on page 40. On a parallel stream, the elements can be traversed in arbitrary order. If you want to execute them in stream order, call forEachOrderedinstead. Of course, you might then give up most or all of the benefits of parallelism. The forEachand forEachOrderedmethods are terminal operations. You cannot use the stream again after calling them. If you want to continue using the stream, use peekinstead—see In the common case that the values should be the actual elements, use Function.identity()for the second function. Click here to view code image Map&lt;Integer, Person&gt; idToPerson = people.collect( Collectors.toMap(Person::getId, Function.identity()));If there is more than one element with the same key, the collector will throw an IllegalStateException. You can override that behavior by supplying a third function argument that determines the value for the key, given the existing and the new value. Your function could return the existing value, the new value, or a combination of them. Here, we construct a map that contains, for each language in the available locales, as key its name in your default locale (such as “German”), and as value its localized name (such as “Deutsch”). Click here to view code image Stream locales = Stream.of(Locale.getAvailableLocales());Map&lt;String, String&gt; languageNames = locales.collect( Collectors.toMap( l -&gt; l.getDisplayLanguage(), l -&gt; l.getDisplayLanguage(l), (existingValue, newValue) -&gt; existingValue));We don’t care that the same language might occur twice—for example, German in Germany and in Switzerland, and we just keep the first entry. However, suppose we want to know all languages in a given country. Then we need a Map&lt;String, Set&gt;. For example, the value for “Switzerland”is the set [French, German, Italian]. At first, we store a singleton set for each language. Whenever a new language is found for a given country, we form the union of the existing and the new set. Click here to view code image Map&lt;String, Set&gt; countryLanguageSets = locales.collect( Collectors.toMap( l -&gt; l.getDisplayCountry(), l -&gt; Collections.singleton(l.getDisplayLanguage()), (a, b) -&gt; { // Union of a and b Set r = new HashSet&lt;&gt;(a); r.addAll(b); return r; }));You will see a simpler way of obtaining this map in the next section. If you want a TreeMap, then you supply the constructor as the fourth argument. You must provide a merge function. Here is one of the examples from the beginning of the section, now yielding a TreeMap: Click here to view code image Map&lt;Integer, Person&gt; idToPerson = people.collect( Collectors.toMap( Person::getId, Function.identity(), (existingValue, newValue) -&gt; { throw new IllegalStateException(); }, TreeMap::new)); For example, if you want sets instead of lists, you can use the Collectors.toSetcollector that you saw in the preceding section: Click here to view code image Map&lt;String, Set&gt; countryToLocaleSet = locales.collect( groupingBy(Locale::getCountry, toSet())); Several other collectors are provided for downstream processing of grouped elements: • countingproduces a count of the collected elements. For example, Click here to view code image Map&lt;String, Long&gt; countryToLocaleCounts = locales.collect( groupingBy(Locale::getCountry, counting()));counts how many locales there are for each country. • summing(Int|Long|Double) takes a function argument, applies the function to the downstream elements, and produces their sum. For example, Click here to view code image Map&lt;String, Integer&gt; stateToCityPopulation = cities.collect( groupingBy(City::getState, summingInt(City::getPopulation)));computes the sum of populations per state in a stream of cities. • maxBy and minBytake a comparator and produce maximum and minimum of the downstream elements. For example, Click here to view code image Map&lt;String, City&gt; stateToLargestCity = cities.collect( groupingBy(City::getState, maxBy(Comparator.comparing(City::getPopulation))));produces the largest city per state. • mapping applies a function to downstream results, and it requires yet another collector for processing its results. For example, Click here to view code image Map&lt;String, Optional&gt; stateToLongestCityName = cities.collect( groupingBy(City::getState, mapping(City::getName, maxBy(Comparator.comparing(String::length)))));Here, we group cities by state. Within each state, we produce the names of the cities and reduce by maximum length. The mappingmethod also yields a nicer solution to a problem from the preceding section, to gather a set of all languages in a country. Click here to view code image Map&lt;String, Set&gt; countryToLanguages = locales.collect( groupingBy(l -&gt; l.getDisplayCountry(), mapping(l -&gt; l.getDisplayLanguage(), toSet())));In the preceding section, I used toMap instead of groupingBy. In this form, you don’t need to worry about combining the individual sets. • If the grouping or mapping function has return type int, long, or double, you can collect elements into a summary statistics object, as discussed in Section 2.9, “Collecting Results,” on page 33. For example, Click here to view code image Map&lt;String, IntSummaryStatistics&gt; stateToCityPopulationSummary = cities.collect( groupingBy(City::getState, summarizingInt(City::getPopulation)));Then you can get the sum, count, average, minimum, and maximum of the function values from the summary statistics objects of each group. • Finally, the reducingmethods apply a general reduction to downstream elements. There are three forms: reducing(binaryOperator), reducing(identity, binaryOperator), and reducing(identity, mapper, binaryOperator). In the first form, the identity is null. (Note that this is different from the forms of Stream::reduce, where the method without an identity parameter yields an Optional result.) In the third form, the mapperfunction is applied and its values are reduced. Here is an example that gets a comma-separated string of all city names in each state. We map each city to its name and then concatenate them. Click here to view code image Map&lt;String, String&gt; stateToCityNames = cities.collect( groupingBy(City::getState, reducing(“”, City::getName, (s, t) -&gt; s.length() == 0 ? t : s + “, “ + t)));As with Stream.reduce, Collectors.reducingis rarely necessary. In this case, you can achieve the same result more naturally as Click here to view code image Map&lt;String, String&gt; stateToCityNames = cities.collect( groupingBy(City::getState, mapping(City::getName, joining(“, “))));Frankly, the downstream collectors can yield very convoluted expressions. You should only use them in connection with groupingBy or partitioningBy to process the “downstream” map values. Otherwise, simply apply methods such as map, reduce, count, max, or mindirectly on streams. 2.12. Primitive Type StreamsSo far, we have collected integers in a Stream, even though it is clearly inefficient to wrap each integer into a wrapper object. The same is true for the other primitive types double, float, long, short, char, byte, and boolean. The stream library has specialized types IntStream, LongStream, and DoubleStream that store primitive values directly, without using wrappers. If you want to store short, char, byte, and boolean, use an IntStream, and for float, use a DoubleStream. The library designers didn’t think it was worth adding another five stream types. To create an IntStream, you can call the IntStream.of and Arrays.streammethods: Click here to view code image IntStream stream = IntStream.of(1, 1, 2, 3, 5);stream = Arrays.stream(values, from, to); // values is an int[] arrayAs with object streams, you can also use the static generate and iterate methods. In addition, IntStreamand LongStreamhave static methods range and rangeClosed that generate integer ranges with step size one: Click here to view code image IntStream zeroToNinetyNine = IntStream.range(0, 100); // Upper bound is excludedIntStream zeroToHundred = IntStream.rangeClosed(0, 100); // Upper bound is includedThe CharSequenceinterface has methods codePoints and chars that yield an IntStream of the Unicode codes of the characters or of the code units in the UTF-16 encoding. (If you don’t know what code units are, you probably shouldn’t use the chars method. Read up on the sordid details in Core Java, 9th Edition, Volume 1, Section 3.3.3.) Click here to view code image String sentence = “\uD835\uDD46 is the set of octonions.”; // \uD835\uDD46 is the UTF-16 encoding of the letter , unicode U+1D546 IntStream codes = sentence.codePoints(); // The stream with hex values 1D546 20 69 73 20 …When you have a stream of objects, you can transform it to a primitive type stream with the mapToInt, mapToLong, or mapToDoublemethods. For example, if you have a stream of strings and want to process their lengths as integers, you might as well do it in an IntStream: Click here to view code image Stream words = …;IntStream lengths = words.mapToInt(String::length);To convert a primitive type stream to an object stream, use the boxed method: Click here to view code image Stream integers = IntStream.range(0, 100).boxed(); Generally, the methods on primitive type streams are analogous to those on object streams. Here are the most notable differences: • The toArraymethods return primitive type arrays. • Methods that yield an optional result return an OptionalInt, OptionalLong, or OptionalDouble. These classes are analogous to the Optional class, but they have methods getAsInt, getAsLong, and getAsDoubleinstead of the getmethod. • There are methods sum, average, max, and min that return the sum, average, maximum, and minimum. These methods are not defined for object streams. • The summaryStatisticsmethod yields an object of type IntSummaryStatistics, LongSummaryStatistics, or DoubleSummaryStatisticsthat can simultaneously report the sum, average, maximum, and minimum of the stream. NOTE The Randomclass has methods ints, longs, and doubles that return primitive type streams of random numbers. 2.13. Parallel StreamsStreams make it easy to parallelize bulk operations. The process is mostly automatic, but you need to follow a few rules. First of all, you must have a parallel stream. By default, stream operations create sequential streams, except for Collection.parallelStream(). The parallelmethod converts any sequential stream into a parallel one. For example: Click here to view code image Stream parallelWords = Stream.of(wordArray).parallel(); As long as the stream is in parallel mode when the terminal method executes, all lazy intermediate stream operations will be parallelized. When stream operations run in parallel, the intent is that the same result is returned as if they had run serially. It is important that the operations are stateless and can be executed in an arbitrary order. Here is an example of something you cannot do. Suppose you want to count all short words in a stream of strings: Click here to view code image int[] shortWords = new int[12];words.parallel().forEach( s -&gt; { if (s.length() &lt; 12) shortWords[s.length()]++; }); // Error—race condition! System.out.println(Arrays.toString(shortWords));This is very, very bad code. The function passed to forEachruns concurrently in multiple threads, updating a shared array. That’s a classic race condition. If you run this program multiple times, you are quite likely to get a different sequence of counts in each run, each of them wrong. It is your responsibility to ensure that any functions that you pass to parallel stream operations are threadsafe. In our example, you could use an array of AtomicIntegerobjects for the counters (see Exercise 12). Or you could simply use the facilities of the streams library and group strings by length (see Exercise 13). By default, streams that arise from ordered collections (arrays and lists), from ranges, generators, and iterators, or from calling Stream.sorted, are ordered. Results are accumulated in the order of the original elements, and are entirely predictable. If you run the same operations twice, you will get exactly the same results. Ordering does not preclude parallelization. For example, when computing stream.map(fun), the stream can be partitioned into nsegments, each of which is concurrently processed. Then the results are reassembled in order. Some operations can be more effectively parallelized when the ordering requirement is dropped. By calling the Stream.unorderedmethod, you indicate that you are not interested in ordering. One operation that can benefit from this is Stream.distinct. On an ordered stream, distinct retains the first of all equal elements. That impedes parallelization—the thread processing a segment can’t know which elements to discard until the preceding segment has been processed. If it is acceptable to retain any of the unique elements, all segments can be processed concurrently (using a shared set to track duplicates). You can also speed up the limit method by dropping ordering. If you just want any nelements from a stream and you don’t care which ones you get, call Click here to view code image Stream sample = stream.parallel().unordered().limit(n); As discussed in Section 2.10, “Collecting into Maps,” on page 34, merging maps is expensive. For that reason, the Collectors.groupingByConcurrentmethod uses a shared concurrent map. Clearly, to benefit from parallelism, the order of the map values will not be the same as the stream order. Even on an ordered stream, that collector has a “characteristic” of being unordered, so that it can be used efficiently without having to make the stream unordered. You still need to make the stream parallel, though: Click here to view code image Map&lt;String, List&gt; result = cities.parallel().collect( Collectors.groupingByConcurrent(City::getState)); // Values aren’t collected in stream order CAUTION It is very important that you don’t modify the collection that is backing a stream while carrying out a stream operation (even if the modification is threadsafe). Remember that streams don’t collect their own data—the data is always in a separate collection. If you were to modify that collection, the outcome of the stream operations would be undefined. The JDK documentation refers to this requirement as noninterference. It applies both to sequential and parallel streams. To be exact, since intermediate stream operations are lazy, it is possible to mutate the collection up to the point when the terminal operation executes. For example, the following is correct: Click here to view code image List wordList = …;Stream words = wordList.stream();wordList.add(“END”); // Oklong n = words.distinct().count();But this code is not: Click here to view code image Stream words = wordList.stream();words.forEach(s -&gt; if (s.length() &lt; 12) wordList.remove(s)); // Error—interference Exercises Write a parallel version of the forloop in Section 2.1, “From Iteration to Stream Operations,” on page 22. Obtain the number of processors. Make that many separate threads, each working on a segment of the list, and total up the results as they come in. (You don’t want the threads to update a single counter. Why?) Verify that asking for the first five long words does not call the filter method once the fifth long word has been found. Simply log each method call. Measure the difference when counting long words with a parallelStreaminstead of a stream. Call System.nanoTimebefore and after the call, and print the difference. Switch to a larger document (such as War and Peace) if you have a fast computer. Suppose you have an array int[] values = { 1, 4, 9, 16 }. What is Stream.of(values)? How do you get a stream of intinstead? Using Stream.iterate, make an infinite stream of random numbers—not by calling Math.random but by directly implementing a linear congruential generator. In such a generator, you start with x0 = seedand then produce xn + 1 = (a xn + c) %m, for appropriate values of a, c, and m. You should implement a method with parameters a, c, m, and seed that yields a Stream. Try out a = 25214903917, c = 11, and m = 248. The characterStreammethod in Section 2.3, “The filter, map, and flatMapMethods,” on page 25, was a bit clumsy, first filling an array list and then turning it into a stream. Write a stream-based one-liner instead. One approach is to make a stream of integers from 0 to s.length() - 1and map that with the s::charAtmethod reference. Your manager asks you to write a method public static boolean isFinite(Stream stream). Why isn’t that such a good idea? Go ahead and write it anyway. Write a method public static Stream zip(Stream first, Stream second) that alternates elements from the streams first and second, stopping when one of them runs out of elements. Join all elements in a Stream&lt;ArrayList&gt;to one ArrayList. Show how to do this with the three forms of reduce. Write a call to reduce that can be used to compute the average of a Stream. Why can’t you simply compute the sum and divide by count()? It should be possible to concurrently collect stream results in a single ArrayList, instead of merging multiple array lists, provided it has been constructed with the stream’s size, since concurrent setoperations at disjoint positions are threadsafe. How can you achieve that? Count all short words in a parallel Stream, as described in Section 2.13, “Parallel Streams,” on page 40, by updating an array of AtomicInteger. Use the atomic getAndIncrementmethod to safely increment each counter. Repeat the preceding exercise, but filter out the short strings and use the collectmethod with Collectors.groupingByand Collectors.counting. A function type is alwayscontravariant in its arguments and covariant in its return value. For example, if you have a Function&lt;Person, Employee&gt;, you can safely pass it on to someone who needs a Function&lt;Employee, Person&gt;. They will only call it with employees, whereas your function can handle any person. They will expect the function to return a person, and you give them something even better. For example, look at the javadoc for Stream: Click here to view code image void forEach(Consumer&lt;? super T&gt; action)Stream filter(Predicate&lt;? super T&gt; predicate) Stream map(Function&lt;? super T, ? extends R&gt; mapper)The general rule is that you use superfor argument types, extends for return types. That way, you can pass a Consumerto forEach on a Stream. If it is willing to consume any object, surely it can consume strings. But the wildcards are not always there. Look at For example, consider the doInOrderAsyncmethod of the preceding section. Instead of Click here to view code image public static void doInOrderAsync(Supplier first, Consumer second, Consumer handler)it should be Click here to view code image public static void doInOrderAsync(Supplier&lt;? extends T&gt; first, Consumer&lt;? super T&gt; second, Consumer&lt;? superThrowable&gt; handler) In our example, we can call Click here to view code image largest.updateAndGet(x -&gt; Math.max(x, observed)); or Click here to view code image largest.accumulateAndGet(observed, Math::max); The accumulateAndGetmethod takes a binary operator that is used to combine the atomic value and the supplied argument. default in JavaThere are three rules about defaultRegarding how to handle the situation of same default method in multiple inheritance. class win, any class wins over any interfaces.So if there’s a method with a body, or an abstract declaration, in the superclass chain, we can ignore the interfaces completely. subtype win supertype, “which two interfaces are competing to provide a default method and one interface extends the other, the subclass wins.” No rule 3. if the previous two rules don’t give us the answer, the subclass must either implement the method or declare it abstract. “Interfaces give you multiple inheritance but no fields, while abstract classes let you “inherit fields but you don’t get multiple inheritance.” Static method in InterfaceWe’ve seen a lot of calling of Stream.of but haven’t gotten into its details yet. You may recall that Stream is an interface, but this is a static method on an interface. 1stream.collect(toCollection(TreeSet::new)); Optional is a better of null123456789101112131415161718192021//Optional can be created via factory method 'of', Optional&lt;String&gt; a = Optional.of("a");// Optional is just a container, you can get the underlying value by 'get' methodassertEquals("a", a.get());// at the meanwhile, Optional can represent 'absent'// factory method empty or ofNullable from a nullable object can be used@Test public void testOptional()&#123; Optional&lt;String&gt; optA=Optional.of("a"); Assert.assertEquals("a", optA.get()); &#125; @Test public void testEmpty()&#123; Optional emp=Optional.ofNullable(null); Assert.assertEquals(Optional.empty(), emp); Assert.assertFalse(emp.isPresent()); Assert.assertEquals("b", emp.orElse("b")); Assert.assertEquals("c", emp.orElseGet(()-&gt;"c")); &#125; method reference ** Classname::methodname** , such as Artist::getName is equivalant to artist-&gt;artist.getName() For constructors can be used Artist::new You can alos to create new array, String[]::new Stream“The purpose of streams isn’t just to convert from one collection to another; it’s to be able to provide a common set of operations over data.” partitioningByTo split a stream into two groups, one for ‘trueGroup’ and another group Lanmbda It is best to think of a lambda expression as a function, not an object, and to accept that it can be converted to a functional interface. This conversion to interfaces is what makes lambda expressions so compelling. The syntax is short and simple. 12BiFunction&lt;String, String, Integer&gt; comp = (first, second) -&gt; Integer.compare(first.length(), second.length()); The expression System.out::printlnis a method reference that is equivalent to the lambda expression x -&gt; System.out.println(x). There are three principal cases: object::instanceMethod Class::staticMethod Class::instanceMethod In the third case, the first parameter becomes the target of the method. For example, String::compareToIgnoreCaseis the same as (x, y) -&gt; x.compareToIgnoreCase(y). Just like lambda expressions, method references don’t live in isolation. They are always turned into instances of functional interfaces. Constructor ReferencesConstructor references are just like method references, except that the name of the method is new. For example, Button::new is a reference to a Button constructor. Which constructor? It depends on the context. 123List&lt;String&gt; labels = ...;Stream&lt;Button&gt; stream = labels.stream().map(Button::new);List&lt;Button&gt; buttons = stream.collect(Collectors.toList()); For example, suppose we want to have an array of buttons. The Stream interface has a toArraymethod that returns an Object array: 12Object[] buttons = stream.toArray(); we need to refine our understanding of a lambda expression. A lambda expression has three ingredients: A block of code Parameters Values for the free variables, that is, the variables that are not parameters and not defined inside the code The technical term for a block of code together with the values of the free variables is a closure. If someone gloats that their language has closures, rest assured that Java has them as well. In Java, lambda expressions are closures. In fact, inner classes have been closures all along. Java 8 gives us closures with an attractive syntax. Inner classes can also capture values from an enclosing scope. Before Java 8, inner classes were only allowed to access final local variables. This rule has now been relaxed to match that for lambda expressions. An inner class can access any effectively final local variable—that is, any variable whose value does not change. When you use the this keyword in a lambda expression, you refer to the this parameter of the method that creates the lambda. For example, consider 1234567public class Application &#123; public void doWork() &#123; Runnable runner = () -&gt; &#123; ...; System.out.println(this.toString()); ... &#125;; ... &#125;&#125; The expression this.toString()calls the toString method of the Application object, not the Runnable instance. There is nothing special about the use of this in a lambda expression. The scope of the lambda expression is nested inside the doWorkmethod, and thishas the same meaning anywhere in that method. The Java designers decided to solve this problem once and for all by allowing interface methods with concrete implementations (called default methods). Those methods can be safely added to existing interfaces. 1234interface Person &#123; long getId(); default String getName() &#123; return "John Q. Public"; &#125;&#125; The interface has two methods: getId, which is an abstract method, and the default method getName. A concrete class that implements the Person interface must, of course, provide an implementation of getId, but it can choose to keep the implementation of getName or to override it. Default methods put an end to the classic pattern of providing an interface and an abstract class that implements most or all of its methods, such as Collection/AbstractCollectionor WindowListener/WindowAdapter. Now you can just implement the methods in the interface. To compare Person objects by name, use Comparator.comparing(Person::getName). In this chapter, we have compared strings by length with the lambda expression (first, second) -&gt; Integer.compare(first.length(), second.length()). But But with the static compare method, we can do much better and simply use Comparator.comparing(String::length). In Java 8, static methods have been added to quite a few interfaces. For example, the Comparator interface has a very useful static comparing method that accepts a “key extraction” function and yields a comparator that compares the extracted keys. Exercises Is the comparator code in the Arrays.sort method called in the same thread as the call to sortor a different thread? Using the listFiles(FileFilter)and isDirectorymethods of the java.io.Fileclass, write a method that returns all subdirectories of a given directory. Use a lambda expression instead of a FileFilterobject. Repeat with a method reference. Using the list(FilenameFilter)method of the java.io.Fileclass, write a method that returns all files in a given directory with a given extension. Use a lambda expression, not a FilenameFilter. Which variables from the enclosing scope does it capture? Given an array of File objects, sort it so that the directories come before the files, and within each group, elements are sorted by path name. Use a lambda expression, not a Comparator. Take a file from one of your projects that contains a number of ActionListener, Runnable, or the like. Replace them with lambda expressions. How many lines did it save? Was the code easier to read? Were you able to use method references? Didn’t you always hate it that you had to deal with checked exceptions in a Runnable? Write a method uncheck that catches all checked exceptions and turns them into unchecked exceptions. For example, 1234new Thread(uncheck( () -&gt; &#123; System.out.println("Zzz"); Thread.sleep(1000); &#125;)).start(); // Look, nocatch (InterruptedException)! Hint: Define an interface RunnableExwhose runmethod may throw any exceptions. Then implement public static Runnable uncheck(RunnableEx runner). Use a lambda expression inside the uncheckmethod. Why can’t you just use Callableinstead of RunnableEx? Write a static method andThenthat takes as parameters two Runnableinstances and returns a Runnable that runs the first, then the second. In the main method, pass two lambda expressions into a call to andThen, and run the returned instance. What happens when a lambda expression captures values in an enhanced forloop such as this one? 12345String[] names = &#123; "Peter", "Paul", "Mary" &#125;;List&lt;Runnable&gt; runners = new ArrayList&lt;&gt;();for (String name : names) runners.add(() -&gt; System.out.println(name)); Is it legal? Does each lambda expression capture a different value, or do they all get the last value? What happens if you use a traditional loop for (int i = 0; i &lt; names.length; i++)? Form a subinterface Collection2from Collectionand add a default method void forEachIf(Consumer action, Predicate filter) that applies action to each element for which filterreturns true. How could you use it? Go through the methods of the Collectionsclass. If you were king for a day, into which interface would you place each method? Would it be a default method or a static method? Suppose you have a class that implements two interfaces I and J, each of which has a method void f(). Exactly what happens if f is an abstract, default, or static method of I and an abstract, default, or static method of J? Repeat where a class extends a superclass S and implements an interface I, each of which has a method void f(). In the past, you were told that it’s bad form to add methods to an interface because it would break existing code. Now you are told that it’s okay to add new methods, provided you also supply a default implementation. How safe is that? Describe a scenario where the new streammethod of the Collectioninterface causes legacy code to fail compilation. What about binary compatibility? Will legacy code from a JAR file still run? Stream vs CollectionsA stream seems superficially similar to a collection, allowing you to transform and retrieve data. But there are significant differences: A stream does not store its elements. They may be stored in an underlying collection or generated on demand. Stream operations don’t mutate their source. Instead, they return new streams that hold the result. Stream operations are lazy when possible. This means they are not executed until their result is needed. For example, if you only ask for the first five long words instead of counting them all, then the filter method will stop filtering after the fifth match. As a consequence, you can even have infinite streams! Streams follow the “what, not how” principle. In our stream example, we describe what needs to be done: get the long words and count them. We don’t specify in which order, or in which thread, this should happen. When you work with streams, you set up a pipeline of operations in three stages. You create a stream. You specify intermediate operations for transforming the initial stream into others, in one or more steps. You apply a terminal operation to produce a result. This operation forces the execution of the lazy operations that precede it. Afterwards, the stream can no longer be used.1long count = words.parallelStream().filter(w -&gt; w.length() &gt; 12).count(); Stream operations are not executed on the elements in the order in which they are invoked on the streams. In our example, nothing happens until count is called. When the count method asks for the first element, then the filter method starts requesting elements, until it finds one that has length &gt; 12. To produce infinite sequences such as 0 1 2 3 …, use the iterate method instead. It takes a “seed” value and a function (technically, a UnaryOperator), and repeatedly applies the function to the previous result. For example, 12Stream&lt;BigInteger&gt; integers = Stream.iterate(BigInteger.ZERO, n -&gt; n.add(BigInteger.ONE)); The first element in the sequence is the seed You can use the following statement to split a string into words: Stream words = Pattern.compile(“[\P{L}]+”).splitAsStream(contents);The static Files.linesmethod returns a Stream of all lines in a file. The Streaminterface has AutoCloseableas a superinterface. When the close method is called on the stream, the underlying file is also closed. To make sure that this happens, it is best to use the Java 7 try-with-resources statement: 123try (Stream&lt;String&gt; lines = Files.lines(path)) &#123; Do something with lines&#125; The stream, and the underlying file with it, will be closed when the try block exits normally or through an exception. The filter, map, and flatMapMethodsA stream transformation reads data from a stream and puts the transformed data into another stream. You have already seen the filtertransformation that yields a new stream with all elements that match a certain condition. 2.3. The filter, map, and flatMapMethodsA stream transformation reads data from a stream and puts the transformed data into another stream. You have already seen the filtertransformation that yields a new stream with all elements that match a certain condition. Here, we transform a stream of strings into another stream containing only long words: 123List&lt;String&gt; wordList = ...;Stream&lt;String&gt; words = wordList.stream();Stream&lt;String&gt; longWords = words.filter(w -&gt; w.length() &gt; 12); The argument of filter is a Predicate—that is, a function from T to boolean. Often, you want to transform the values in a stream in some way. Use the map method and pass the function that carries out the transformation. For example, you can transform all words to lowercase like this: 1Stream&lt;String&gt; lowercaseWords = words.map(String::toLowerCase); Here, we used map with a method reference. Often, you will use a lambda expression instead: 1Stream&lt;Character&gt; firstChars = words.map(s -&gt; s.charAt(0)); The resulting stream contains the first character of each word. When you use map, a function is applied to each element, and the return values are collected in a new stream. Now suppose that you have a function that returns not just one value but a stream of values, such as this one: 12345public static Stream&lt;Character&gt; characterStream(String s) &#123; List&lt;Character&gt; result = new ArrayList&lt;&gt;(); for (char c : s.toCharArray()) result.add(c); return result.stream();&#125; For example, characterStream(“boat”)is the stream [‘b’, ‘o’, ‘a’, ‘t’]. Suppose you map this method on a stream of strings: 1Stream&lt;Stream&lt;Character&gt;&gt; result = words.map(w -&gt; characterStream(w)); You will get a stream of streams, like [… [‘y’, ‘o’, ‘u’, ‘r’], [‘b’, ‘o’, ‘a’, ‘t’], …] To flatten it out to a stream of characters [… ‘y’, ‘o’, ‘u’, ‘r’, ‘b’, ‘o’, ‘a’, ‘t’, …], use the flatMapmethod instead of map: 123Stream&lt;Character&gt; letters = words.flatMap(w -&gt; characterStream(w)) // CallscharacterStream on each word and flattens the results NOTE You may find a flatMap method in classes other than streams. It is a general concept in computer science. Suppose you have a generic type G (such as Stream) and functions ffrom some type T to Gand g from U to G. Then you can compose them, that is, first apply f and then g, by using flatMap. This is a key idea in the theory of monads. But don’t worry—you can use flatMapwithout knowing anything about monads. This method is particularly useful for cutting infinite streams down to size. For example, 1Stream&lt;Double&gt; randoms = Stream.generate(Math::random).limit(100); yields a stream with 100 random numbers. The peek method yields another stream with the same elements as the original, but a function is invoked every time an element is retrieved. That is handy for debugging: 123Object[] powers = Stream.iterate(1.0, p -&gt; p * 2) .peek(e -&gt; System.out.println("Fetching " + e)) .limit(20).toArray(); When an element is actually accessed, a message is printed. This way you can verify that the infinite stream returned by iterate is processed lazily. The stream transformations of the preceding sections were stateless. When an element is retrieved from a filtered or mapped stream, the answer does not depend on the previous elements. There are also a few stateful transformations. For example, the distinct method returns a stream that yields elements from the original stream, in the same order, except that duplicates are suppressed. The stream must obviously remember the elements that it has already seen. 123Stream&lt;String&gt; uniqueWords = Stream.of("merrily", "merrily", "merrily", "gently").distinct(); // Only one"merrily" is retained The sorted method must see the entire stream and sort it before it can give out any elements—after all, the smallest one might be the last one. Clearly, you can’t sort an infinite stream. There are several sorted methods. One works for streams of Comparableelements, and another accepts a Comparator. Here, we sort strings so that the longest ones come first: 12Stream&lt;String&gt; longestFirst = words.sorted(Comparator.comparing(String::length).reversed()); Of course, you can sort a collection without using streams. The sorted method is useful when the sorting process is a part of a stream pipeline. The Collections.sort method sorts a collection in place, whereas Stream.sorted returns a new sorted stream. The methods that we cover in this section are called reductions. They reduce the stream to a value that can be used in your program. Reductions are terminal operations. After a terminal operation has been applied, the stream ceases to be usable. In Java 8, the Optional type is the preferred way of indicating a missing return value. We discuss the Optional type in detail in the next section. Here is how you can get the maximum of a stream: 123Optional&lt;String&gt; largest = words.max(String::compareToIgnoreCase);if (largest.isPresent()) System.out.println("largest: " + largest.get()); reduce. Each segment needs to start out with its own empty hash set, and reduce only lets you supply one identity value. Instead, use collect. It takes three arguments: A supplier to make new instances of the target object, for example, a constructor for a hash set An accumulatorthat adds an element to the target, for example, an addmethod A combiner that merges two objects into one, such as addAll The target object need not be a collection. It could be a StringBuilder or an object that tracks a count and a sum. Here is how the collect method works for a hash set: 1HashSet&lt;String&gt; result = stream.collect(HashSet::new, HashSet::add, HashSet::addAll); In practice, you don’t have to do that because there is a convenient Collector interface for these three functions, and a Collectors class with factory methods for common collectors. To collect a stream into a list or set, you can simply call 1234List&lt;String&gt; result = stream.collect(Collectors.toList());// orSet&lt;String&gt; result = stream.collect(Collectors.toSet()); If you want to control which kind of set you get, use the following call instead: 1TreeSet&lt;String&gt; result = stream.collect(Collectors.toCollection(TreeSet::new)); Suppose you want to collect all strings in a stream by concatenating them. You can call 1String result = stream.collect(Collectors.joining()); If you want a delimiter between elements, pass it to the joiningmethod: 1String result = stream.collect(Collectors.joining(", ")); If your stream contains objects other than strings, you need to first convert them to strings, like this: 1String result = stream.map(Object::toString).collect(Collectors.joining(", ")); If you want to reduce the stream results to a sum, average, maximum, or minimum, then use one of the methods summarizing(Int|Long|Double). These methods take a function that maps the stream objects to a number and yield a result of type (Int|Long|Double)SummaryStatistics, with methods for obtaining the sum, average, maximum, and minumum. 1234IntSummaryStatistics summary = words.collect( Collectors.summarizingInt(String::length));double averageWordLength = summary.getAverage();double maxWordLength = summary.getMax(); So far, you have seen how to reduce or collect stream values. But perhaps you just want to print them or put them in a database. Then you can use the forEach method: 1stream.forEach(System.out::println); The function that you pass is applied to each element. On a parallel stream, it’s your responsibility to ensure that the function can be executed concurrently. We discuss this in Section 2.13, “Parallel Streams,”. On a parallel stream, the elements can be traversed in arbitrary order. If you want to execute them in stream order, call forEachOrderedinstead. Of course, you might then give up most or all of the benefits of parallelism. The forEach and forEachOrdered methods are terminal operations. You cannot use the stream again after calling them. If you want to continue using the stream, use peek instead—see In the common case that the values should be the actual elements, use Function.identity() for the second function. 12Map&lt;Integer, Person&gt; idToPerson = people.collect( Collectors.toMap(Person::getId, Function.identity())); If there is more than one element with the same key, the collector will throw an IllegalStateException. You can override that behavior by supplying a third function argument that determines the value for the key, given the existing and the new value. Your function could return the existing value, the new value, or a combination of them. Here, we construct a map that contains, for each language in the available locales, as key its name in your default locale (such as “German”), and as value its localized name (such as “Deutsch”). 123456Stream&lt;Locale&gt; locales = Stream.of(Locale.getAvailableLocales());Map&lt;String, String&gt; languageNames = locales.collect( Collectors.toMap( l -&gt; l.getDisplayLanguage(), l -&gt; l.getDisplayLanguage(l), (existingValue, newValue) -&gt; existingValue)); We don’t care that the same language might occur twice—for example, German in Germany and in Switzerland, and we just keep the first entry. However, suppose we want to know all languages in a given country. Then we need a Map&lt;String, Set&gt;. For example, the value for “Switzerland”is the set [French, German, Italian]. At first, we store a singleton set for each language. Whenever a new language is found for a given country, we form the union of the existing and the new set. 12345678Map&lt;String, Set&lt;String&gt;&gt; countryLanguageSets = locales.collect( Collectors.toMap( l -&gt; l.getDisplayCountry(), l -&gt; Collections.singleton(l.getDisplayLanguage()), (a, b) -&gt; &#123; // Union of a and b Set&lt;String&gt; r = new HashSet&lt;&gt;(a); r.addAll(b); return r; &#125;)); You will see a simpler way of obtaining this map in the next section. If you want a TreeMap, then you supply the constructor as the fourth argument. You must provide a merge function. Here is one of the examples from the beginning of the section, now yielding a TreeMap: 123456Map&lt;Integer, Person&gt; idToPerson = people.collect( Collectors.toMap( Person::getId, Function.identity(), (existingValue, newValue) -&gt; &#123; throw new IllegalStateException(); &#125;, TreeMap::new)); For example, if you want sets instead of lists, you can use the Collectors.toSetcollector that you saw in the preceding section: 12Map&lt;String, Set&lt;Locale&gt;&gt; countryToLocaleSet = locales.collect( groupingBy(Locale::getCountry, toSet())); Several other collectors are provided for downstream processing of grouped elements: • counting produces a count of the collected elements. For example, 12Map&lt;String, Long&gt; countryToLocaleCounts = locales.collect( groupingBy(Locale::getCountry, counting())); counts how many locales there are for each country. • summing(Int|Long|Double) takes a function argument, applies the function to the downstream elements, and produces their sum. For example, 12Map&lt;String, Integer&gt; stateToCityPopulation = cities.collect( groupingBy(City::getState, summingInt(City::getPopulation))); computes the sum of populations per state in a stream of cities. • maxBy and minBy take a comparator and produce maximum and minimum of the downstream elements. For example, 123Map&lt;String, City&gt; stateToLargestCity = cities.collect( groupingBy(City::getState, maxBy(Comparator.comparing(City::getPopulation)))); produces the largest city per state. • mapping applies a function to downstream results, and it requires yet another collector for processing its results. For example, 1234Map&lt;String, Optional&lt;String&gt;&gt; stateToLongestCityName = cities.collect( groupingBy(City::getState, mapping(City::getName, maxBy(Comparator.comparing(String::length))))); Here, we group cities by state. Within each state, we produce the names of the cities and reduce by maximum length. The mapping method also yields a nicer solution to a problem from the preceding section, to gather a set of all languages in a country. 1234Map&lt;String, Set&lt;String&gt;&gt; countryToLanguages = locales.collect( groupingBy(l -&gt; l.getDisplayCountry(), mapping(l -&gt; l.getDisplayLanguage(), toSet()))); In the preceding section, I used toMap instead of groupingBy. In this form, you don’t need to worry about combining the individual sets. • If the grouping or mapping function has return type int, long, or double, you can collect elements into a summary statistics object, as discussed in Section 2.9, “Collecting Results,” on page 33. For example, 123Map&lt;String, IntSummaryStatistics&gt; stateToCityPopulationSummary = cities.collect( groupingBy(City::getState, summarizingInt(City::getPopulation))); Then you can get the sum, count, average, minimum, and maximum of the function values from the summary statistics objects of each group. • Finally, the reducingmethods apply a general reduction to downstream elements. There are three forms: reducing(binaryOperator), reducing(identity, binaryOperator), and reducing(identity, mapper, binaryOperator). In the first form, the identity is null. (Note that this is different from the forms of Stream::reduce, where the method without an identity parameter yields an Optional result.) In the third form, the mapperfunction is applied and its values are reduced. Here is an example that gets a comma-separated string of all city names in each state. We map each city to its name and then concatenate them. Click here to view code image Map&lt;String, String&gt; stateToCityNames = cities.collect( groupingBy(City::getState, reducing(“”, City::getName, (s, t) -&gt; s.length() == 0 ? t : s + “, “ + t)));As with Stream.reduce, Collectors.reducingis rarely necessary. In this case, you can achieve the same result more naturally as Click here to view code image Map&lt;String, String&gt; stateToCityNames = cities.collect( groupingBy(City::getState, mapping(City::getName, joining(“, “))));Frankly, the downstream collectors can yield very convoluted expressions. You should only use them in connection with groupingBy or partitioningBy to process the “downstream” map values. Otherwise, simply apply methods such as map, reduce, count, max, or mindirectly on streams. 2.12. Primitive Type StreamsSo far, we have collected integers in a Stream, even though it is clearly inefficient to wrap each integer into a wrapper object. The same is true for the other primitive types double, float, long, short, char, byte, and boolean. The stream library has specialized types IntStream, LongStream, and DoubleStream that store primitive values directly, without using wrappers. If you want to store short, char, byte, and boolean, use an IntStream, and for float, use a DoubleStream. The library designers didn’t think it was worth adding another five stream types. To create an IntStream, you can call the IntStream.of and Arrays.streammethods: Click here to view code image IntStream stream = IntStream.of(1, 1, 2, 3, 5);stream = Arrays.stream(values, from, to); // values is an int[] arrayAs with object streams, you can also use the static generate and iterate methods. In addition, IntStreamand LongStreamhave static methods range and rangeClosed that generate integer ranges with step size one: Click here to view code image IntStream zeroToNinetyNine = IntStream.range(0, 100); // Upper bound is excludedIntStream zeroToHundred = IntStream.rangeClosed(0, 100); // Upper bound is includedThe CharSequenceinterface has methods codePoints and chars that yield an IntStream of the Unicode codes of the characters or of the code units in the UTF-16 encoding. (If you don’t know what code units are, you probably shouldn’t use the chars method. Read up on the sordid details in Core Java, 9th Edition, Volume 1, Section 3.3.3.) Click here to view code image String sentence = “\uD835\uDD46 is the set of octonions.”; // \uD835\uDD46 is the UTF-16 encoding of the letter , unicode U+1D546 IntStream codes = sentence.codePoints(); // The stream with hex values 1D546 20 69 73 20 …When you have a stream of objects, you can transform it to a primitive type stream with the mapToInt, mapToLong, or mapToDoublemethods. For example, if you have a stream of strings and want to process their lengths as integers, you might as well do it in an IntStream: Click here to view code image Stream words = …;IntStream lengths = words.mapToInt(String::length);To convert a primitive type stream to an object stream, use the boxed method: Click here to view code image Stream integers = IntStream.range(0, 100).boxed(); Generally, the methods on primitive type streams are analogous to those on object streams. Here are the most notable differences: • The toArraymethods return primitive type arrays. • Methods that yield an optional result return an OptionalInt, OptionalLong, or OptionalDouble. These classes are analogous to the Optional class, but they have methods getAsInt, getAsLong, and getAsDoubleinstead of the getmethod. • There are methods sum, average, max, and min that return the sum, average, maximum, and minimum. These methods are not defined for object streams. • The summaryStatisticsmethod yields an object of type IntSummaryStatistics, LongSummaryStatistics, or DoubleSummaryStatisticsthat can simultaneously report the sum, average, maximum, and minimum of the stream. NOTE The Randomclass has methods ints, longs, and doubles that return primitive type streams of random numbers. 2.13. Parallel StreamsStreams make it easy to parallelize bulk operations. The process is mostly automatic, but you need to follow a few rules. First of all, you must have a parallel stream. By default, stream operations create sequential streams, except for Collection.parallelStream(). The parallelmethod converts any sequential stream into a parallel one. For example: Click here to view code image Stream parallelWords = Stream.of(wordArray).parallel(); As long as the stream is in parallel mode when the terminal method executes, all lazy intermediate stream operations will be parallelized. When stream operations run in parallel, the intent is that the same result is returned as if they had run serially. It is important that the operations are stateless and can be executed in an arbitrary order. Here is an example of something you cannot do. Suppose you want to count all short words in a stream of strings: Click here to view code image int[] shortWords = new int[12];words.parallel().forEach( s -&gt; { if (s.length() &lt; 12) shortWords[s.length()]++; }); // Error—race condition! System.out.println(Arrays.toString(shortWords));This is very, very bad code. The function passed to forEachruns concurrently in multiple threads, updating a shared array. That’s a classic race condition. If you run this program multiple times, you are quite likely to get a different sequence of counts in each run, each of them wrong. It is your responsibility to ensure that any functions that you pass to parallel stream operations are threadsafe. In our example, you could use an array of AtomicIntegerobjects for the counters (see Exercise 12). Or you could simply use the facilities of the streams library and group strings by length (see Exercise 13). By default, streams that arise from ordered collections (arrays and lists), from ranges, generators, and iterators, or from calling Stream.sorted, are ordered. Results are accumulated in the order of the original elements, and are entirely predictable. If you run the same operations twice, you will get exactly the same results. Ordering does not preclude parallelization. For example, when computing stream.map(fun), the stream can be partitioned into nsegments, each of which is concurrently processed. Then the results are reassembled in order. Some operations can be more effectively parallelized when the ordering requirement is dropped. By calling the Stream.unorderedmethod, you indicate that you are not interested in ordering. One operation that can benefit from this is Stream.distinct. On an ordered stream, distinct retains the first of all equal elements. That impedes parallelization—the thread processing a segment can’t know which elements to discard until the preceding segment has been processed. If it is acceptable to retain any of the unique elements, all segments can be processed concurrently (using a shared set to track duplicates). You can also speed up the limit method by dropping ordering. If you just want any nelements from a stream and you don’t care which ones you get, call Click here to view code image Stream sample = stream.parallel().unordered().limit(n); As discussed in Section 2.10, “Collecting into Maps,” on page 34, merging maps is expensive. For that reason, the Collectors.groupingByConcurrentmethod uses a shared concurrent map. Clearly, to benefit from parallelism, the order of the map values will not be the same as the stream order. Even on an ordered stream, that collector has a “characteristic” of being unordered, so that it can be used efficiently without having to make the stream unordered. You still need to make the stream parallel, though: Click here to view code image Map&lt;String, List&gt; result = cities.parallel().collect( Collectors.groupingByConcurrent(City::getState)); // Values aren’t collected in stream order CAUTION It is very important that you don’t modify the collection that is backing a stream while carrying out a stream operation (even if the modification is threadsafe). Remember that streams don’t collect their own data—the data is always in a separate collection. If you were to modify that collection, the outcome of the stream operations would be undefined. The JDK documentation refers to this requirement as noninterference. It applies both to sequential and parallel streams. To be exact, since intermediate stream operations are lazy, it is possible to mutate the collection up to the point when the terminal operation executes. For example, the following is correct: Click here to view code image List wordList = …;Stream words = wordList.stream();wordList.add(“END”); // Oklong n = words.distinct().count();But this code is not: Click here to view code image Stream words = wordList.stream();words.forEach(s -&gt; if (s.length() &lt; 12) wordList.remove(s)); // Error—interference Exercises Write a parallel version of the forloop in Section 2.1, “From Iteration to Stream Operations,” on page 22. Obtain the number of processors. Make that many separate threads, each working on a segment of the list, and total up the results as they come in. (You don’t want the threads to update a single counter. Why?) Verify that asking for the first five long words does not call the filter method once the fifth long word has been found. Simply log each method call. Measure the difference when counting long words with a parallelStreaminstead of a stream. Call System.nanoTimebefore and after the call, and print the difference. Switch to a larger document (such as War and Peace) if you have a fast computer. Suppose you have an array int[] values = { 1, 4, 9, 16 }. What is Stream.of(values)? How do you get a stream of intinstead? Using Stream.iterate, make an infinite stream of random numbers—not by calling Math.random but by directly implementing a linear congruential generator. In such a generator, you start with x0 = seedand then produce xn + 1 = (a xn + c) %m, for appropriate values of a, c, and m. You should implement a method with parameters a, c, m, and seed that yields a Stream. Try out a = 25214903917, c = 11, and m = 248. The characterStreammethod in Section 2.3, “The filter, map, and flatMapMethods,” on page 25, was a bit clumsy, first filling an array list and then turning it into a stream. Write a stream-based one-liner instead. One approach is to make a stream of integers from 0 to s.length() - 1and map that with the s::charAtmethod reference. Your manager asks you to write a method public static boolean isFinite(Stream stream). Why isn’t that such a good idea? Go ahead and write it anyway. Write a method public static Stream zip(Stream first, Stream second) that alternates elements from the streams first and second, stopping when one of them runs out of elements. Join all elements in a Stream&lt;ArrayList&gt;to one ArrayList. Show how to do this with the three forms of reduce. Write a call to reduce that can be used to compute the average of a Stream. Why can’t you simply compute the sum and divide by count()? It should be possible to concurrently collect stream results in a single ArrayList, instead of merging multiple array lists, provided it has been constructed with the stream’s size, since concurrent setoperations at disjoint positions are threadsafe. How can you achieve that? Count all short words in a parallel Stream, as described in Section 2.13, “Parallel Streams,” on page 40, by updating an array of AtomicInteger. Use the atomic getAndIncrementmethod to safely increment each counter. Repeat the preceding exercise, but filter out the short strings and use the collectmethod with Collectors.groupingByand Collectors.counting. A function type is alwayscontravariant in its arguments and covariant in its return value. For example, if you have a Function&lt;Person, Employee&gt;, you can safely pass it on to someone who needs a Function&lt;Employee, Person&gt;. They will only call it with employees, whereas your function can handle any person. They will expect the function to return a person, and you give them something even better. For example, look at the javadoc for Stream: Click here to view code image void forEach(Consumer&lt;? super T&gt; action)Stream filter(Predicate&lt;? super T&gt; predicate) Stream map(Function&lt;? super T, ? extends R&gt; mapper)The general rule is that you use superfor argument types, extends for return types. That way, you can pass a Consumerto forEach on a Stream. If it is willing to consume any object, surely it can consume strings. But the wildcards are not always there. Look at For example, consider the doInOrderAsyncmethod of the preceding section. Instead of Click here to view code image public static void doInOrderAsync(Supplier first, Consumer second, Consumer handler)it should be Click here to view code image public static void doInOrderAsync(Supplier&lt;? extends T&gt; first, Consumer&lt;? super T&gt; second, Consumer&lt;? superThrowable&gt; handler) In our example, we can call Click here to view code image largest.updateAndGet(x -&gt; Math.max(x, observed)); or Click here to view code image largest.accumulateAndGet(observed, Math::max); The accumulateAndGetmethod takes a binary operator that is used to combine the atomic value and the supplied argument. If you anticipate high contention, you should simply use a LongAdder instead of an AtomicLong. The method names are slightly different. Call increment to increment a counter or add to add a quantity, and sum to retrieve the total. Click here to view code image final LongAdder adder = new LongAdder();for (…) pool.submit(() -&gt; { while (…) { … if (…) adder.increment(); } });…long total = adder.sum()); NOTE The Arrays class now has a number of parallelized operations. The static Arrays.parallelSortmethod can sort an array of primitive values or objects. For example, Click here to view code image String contents = new String(Files.readAllBytes( Paths.get(“alice.txt”)), StandardCharsets.UTF_8); // Read file into stringString[] words = contents.split(“[\P{L}]+”); // Split along nonlettersArrays.parallelSort(words);When you sort objects, you can supply a Comparator. With all methods, you can supply the bounds of a range, such as Click here to view code image Arrays.parallelSort(values, values.length / 2, values.length); // Sort the upper half There was no easy way of saying: “When the result becomes available, here is how to process it.” This is the crucial feature that the new CompletableFutureclass provides. This composability is the key aspect of the CompletableFutureclass. Composing future actions solves a serious problem in programming asynchronous applications. The traditional approach for dealing with nonblocking calls is to use event handlers. The programmer registers a handler for the next action after completion. Of course, if the next action is also asynchronous, then the next action after that is in a different event handler. Even though the programmer thinks in terms of “first do step 1, then step 2, then step 3,” the program logic becomes dispersed in different places. For example, here is a two-stage pipeline for reading and processing the web page: Click here to view code image CompletableFuture&lt;List&gt; links = CompletableFuture.supplyAsync(() -&gt; blockingReadPage(url)) .thenApply(Parser::getLinks);You can have additional processing steps. Eventually, you’ll be done, and you will need to save the results somewhere. Here, we just print the result. Click here to view code image CompletableFuture links = CompletableFuture.supplyAsync(() -&gt; blockingReadPage(url)) .thenApply(Parser::getLinks) .thenAccept(System.out::println);The thenAcceptmethod takes a Consumer—that is, a function with return type void. Ideally, you would never call get on a future. The last step in the pipeline simply deposits the result where it belongs. Files.lineslazily reads a stream of lines. • Files.listlazily lists the entries of a directory, and Files.walktraverses them recursively. 8.4.2. ComparatorsThe Comparatorinterface has a number of useful new methods, taking advantage of the fact that interfaces can now have concrete methods. The static comparingmethod takes a “key extractor” function that maps a type T to a comparable type (such as String). The function is applied to the objects to be compared, and the comparison is then made on the returned keys. For example, suppose you have an array of Personobjects. Here is how you can sort them by name: Click here to view code image Arrays.sort(people, Comparator.comparing(Person::getName)); You can chain comparators with the thenComparingmethod for breaking ties. For example, Click here to view code image Arrays.sort(people, Comparator.comparing(Person::getLastName) .thenComparing(Person::getFirstName));If two people have the same last name, then the second comparator is used. There are a few variations of these methods. You can specify a comparator to be used for the keys that the comparingand thenComparingmethods extract. For example, here we sort people by the length of their names: Click here to view code image Arrays.sort(people, Comparator.comparing(Person::getName, (s, t) -&gt; Integer.compare(s.length(), t.length())));Moreover, both the comparing and thenComparingmethods have variants that avoid boxing of int, long, or doublevalues. An easier way of producing the preceding operation would be Click here to view code image Arrays.sort(people, Comparator.comparingInt(p -&gt; p.getName().length())); If your key function can return null, you will like the nullsFirst and nullsLast adapters. These static methods take an existing comparator and modify it so that it doesn’t throw an exception when encountering nullvalues but ranks them as smaller or larger than regular values. For example, suppose getMiddleNamereturns a null when a person has no middle name. Then you can use Comparator.comparing(Person::getMiddleName(), Comparator.nullsFirst(…)). The nullsFirstmethod needs a comparator—in this case, one that compares two strings. The naturalOrdermethod makes a comparator for any class implementing Comparable. A Comparator.naturalOrder()is what we need. Here is the complete call for sorting by potentially null middle names. I use a static import of java.util.Comparator.*, to make the expression more legible. Note that the type for naturalOrder is inferred. Click here to view code image Arrays.sort(people, comparing(Person::getMiddleName, nullsFirst(naturalOrder())));The static reverseOrdermethod gives the reverse of the natural order. To reverse any comparator, use the reversed instance method. For example, naturalOrder().reversed()is the same as reverseOrder(). To read the lines of a file lazily, use the Files.linesmethod. It yields a stream of strings, one per line of input: Click here to view code image Stream lines = Files.lines(path);Optional passwordEntry = lines.filter(s -&gt; s.contains(“password”)).findFirst();As soon as the first line containing password is found, no further lines are read from the underlying file. You will want to close the underlying file. Fortunately, the Stream interface extends AutoCloseable. The streams that you have seen in Chapter 2didn’t need to close any resources. But the Files.linesmethod produces a stream whose closemethod closes the file. The easiest way to make sure the file is indeed closed is to use a Java 7 try-with-resources block: Click here to view code image try (Stream lines = Files.lines(path)) { Optional passwordEntry = lines.filter(s -&gt; s.contains(“password”)).findFirst(); …} // The stream, and hence the file, will be closed hereWhen a stream spawns another, the close methods are chained. Therefore, you can also write Click here to view code image try (Stream filteredLines = Files.lines(path).filter(s -&gt; s.contains(“password”))) { Optional passwordEntry = filteredLines.findFirst(); …} When filteredLines is closed, it closes the underlying stream, which closes the underlying file. If you want to be notified when the stream is closed, you can attach an onClosehandler. Here is how you can verify that closing filteredLinesactually closes the underlying stream: Click here to view code image try (Stream filteredLines = Files.lines(path).onClose(() -&gt; System.out.println(“Closing”)) .filter(s -&gt; s.contains(“password”))) { … }If an IOExceptionoccurs as the stream fetches the lines, that exception is wrapped into an UncheckedIOExceptionwhich is thrown out of the stream operation. In Java 8, just use Files.list. The list method does not enter subdirectories. To process all descendants of a directory, use the Files.walk method instead. Click here to view code image try (Stream entries = Files.walk(pathToRoot)) { // Contains all descendants, visited in depth-first order}]]></content>
      <tags>
        <tag>Java</tag>
        <tag>Coding</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java Concurrent]]></title>
    <url>%2F2017-04-20-Concurrent-In-Java%2F</url>
    <content type="text"><![CDATA[This blog is about noteworthy pivot points about Java Concurrent Framework Back to Java old days there were wait()/notify() which is error prone, while from Java 5.0 there was Concurrent framework being introduced, this page list some pivot points. CountDownLatch CountDownLatch in Java is a kind of synchronizer which allows one Thread to wait for one or more Threads before starts processing. You can also implement same functionality using wait and notify mechanism in Java but it requires lot of code and getting it write in first attempt is tricky, With CountDownLatch it can be done in just few lines. One of the disadvantage of CountDownLatch is that its not reusable once count reaches to zero you can not use CountDownLatch any more, but don’t worry Java concurrency API has another concurrent utility called CyclicBarrier for such requirements.When to use CountDownLatchClassical example of using CountDownLatch in Java is any server side core Java application which uses services architecture, where multiple services is provided by multiple threads and application can not start processing until all services have started successfully as shown in our CountDownLatch example. Summary Main Thread wait on Latch by calling CountDownLatch.await() method while other thread calls CountDownLatch.countDown() to inform that they have completed. CyclicBarrier there is different you can not reuse CountDownLatch once the count reaches zero while you can reuse CyclicBarrier by calling reset() method which resets Barrier to its initial State. What it implies that CountDownLatch is a good for one-time events like application start-up time and CyclicBarrier can be used to in case of the recurrent event e.g. concurrently calculating a solution of the big problem etc. a simple example of CyclicBarrier in Java on which we initialize CyclicBarrier with 3 parties, means in order to cross barrier, 3 thread needs to call await() method. each thread calls await method in short duration but they don’t proceed until all 3 threads reached the barrier, once all thread reach the barrier, barrier gets broker and each thread started their execution from that point. Sample can be found at CyclicBarrierDemo.javaUse cases: To implement multi player game which can not begin until all player has joined. Perform lenghty calculation by breaking it into smaller individual tasks. In general, to implement Map-Reduce technique. CyclicBarrier can perform a completion task once all thread reaches to the barrier, This can be provided while creating CyclicBarrier. If CyclicBarrier is initialized with 3 parties means 3 thread needs to call await method to break the barrier. The thread will block on await() until all parties reach to the barrier, another thread interrupt or await timed out. CyclicBarrier.reset() put Barrier on its initial state, other thread which is waiting or not yet reached barrier will terminate with java.util.concurrent.BrokenBarrierException. ThreadLocal ThreadLocal in Java is another way to achieve thread-safety apart from writing immutable classes. ThreadLocal in Java is a different way to achieve thread-safety, it doesn’t address synchronization requirement, instead it eliminates sharing by providing explicitly copy of Object to each thread. Since Object is no more shared there is no requirement of Synchronization which can improve scalability and performance of application. One of the classic example of ThreadLocal is sharing SimpleDateForamt. Since SimpleDateFormat is not thread safe, having a global formatter may not work but having per Thread formatter will certainly work. but it can be source of severe memory leak and java.lang.OutOfMemoryError if not used carefully. so avoid until you don’t have any other option. Semaphone Semaphore provides two main method acquire() and release() for getting permits and releasing permits. acquire() method blocks until permit is available. Semaphore provides both blocking method as well as unblocking method to acquire permits. This Java concurrency tutorial focus on a very simple example of Binary Semaphore and demonstrate how mutual exclusion can be achieved using Semaphore in Java.Binary Semaphonea Counting semaphore with one permit is known as binary semaphore because it has only two state permit available or permit unavailable. Binary semaphore can be used to implement mutual exclusion or critical section where only one thread is allowed to execute. Thread will wait on acquire() until Thread inside critical section release permit by calling release() on semaphore. Scenarios usage1) To implement better Database connection pool which will block if no more connection is available instead of failing and handover Connection as soon as its available.2) To put a bound on collection classes. by using semaphore you can implement bounded collection whose bound is specified by counting semaphore.3) That’s all on Counting semaphore example in Java. Semaphore is real nice concurrent utility which can greatly simply design and implementation of bounded resource pool. Java 5 has added several useful concurrent utility and deserve a better attention than casual look. Race condition Race conditions occurs when two thread operate on same object without proper synchronization and there operation interleaves on each other. Classical example of Race condition is incrementing a counter since increment is not an atomic operation and can be further divided into three steps like read, update and write. if two threads tries to increment count at same time and if they read same value because of interleaving of read operation of one thread to update operation of another thread, one count will be lost when one thread overwrite increment done by other thread. I found that two code patterns namely “check and act” and “read modify write” can suffer race condition if not synchronized properly. classical example of “check and act” race condition in Java is getInstance() method of Singleton Class, put if absent scenario. consider below code if(!hashtable.contains(key)){ hashtable.put(key,value); } Fix race condition:-In order to fix this race condition in Java you need to wrap this code inside synchronized block which makes them atomic together because no thread can go inside synchronized block if one thread is already there. IllegalMonitorStateException in Java which will occur if we don’t call wait (), notify () or notifyAll () method from synchronized context. Any potential race condition between wait and notify method in Java Thread in Javadetails: A thread is essentialy a subdivision of a process, or LWP: lightweight process. Crucially, each process has its own memory space. A thread is a subdivision that shares the memory space of its parent process. Threads belonging to a process usually share a few other key resources as well, such as their working directory, environment variables, file handles etc. On the other hand, each thread has its own private stack and registers, including program counter. program counter (PC) register keeps track of the current instruction executing at any moment. That is like a pointer to the current instruction in sequence of instructions in a program. Method area: In general, method area is a logical part of heap area. But that is left to the JVM implementers to decide. Method area has per class structures and fields. Nothing but static fields and structures. Depending on the OS, threads may have some other private resources too, such as thread-local storage (effectively, a way of referring to “variable number X”, where each thread has its own private value of X). Wait &amp; Notify Since wait method is not defined in Thread class, you cannot simply call Thread.wait(), that won’t work but since many Java developers are used to calling Thread.sleep() they try the same thing with wait() method and stuck. You need to call wait() method on the object which is shared between two threads, in producer-consumer problem its the queue which is shared between producer and consumer threads.12345synchronized(lock)&#123; while(!someCondition)&#123; lock.wait(); &#125;&#125; Tips Always call wait(), notify() and notifyAll() methods from synchronized method or synchronized block otherwise JVM will throw IllegalMonitorStateException. Always call wait and notify method from a loop and never from if() block, because loop test waiting condition before and after sleeping and handles notification even if waiting for the condition is not changed. Always call wait in shared object e.g. shared queue in this example. Prefer notifyAll() over notify() method due to reasons given in this article. Fork-Join Fork/join tasks is “pure” in-memory algorithms in which no I/O operations come into picture.it is based on a work-stealing algorithm. Java’s most attractive part is it makes things easier and easier. its really challenging where several threads are working together to accomplish a large task so again java has tried to make things easy and simplifies this concurrency using Executors and Thread Queue. it work on divide and conquer algorithm and create sub-tasks and communicate with each other to complete. New fork-join executor framework has been created which is responsible for creating one new task object which is again responsible for creating new sub-task object and waiting for sub-task to be completed.internally it maintains a thread pool and executor assign pending task to this thread pool to complete when one task is waiting for another task to complete. whole Idea of fork-join framework is to leverage multiple processors of advanced machine. Thread.yield() This static method is essentially used to notify the system that the current thread is willing to “give up the CPU” for a while. The general idea is that: The thread scheduler will select a different thread to run instead of the current one. However, the details of how yielding is implemented by the thread scheduler differ from platform to platform. In general, you shouldn’t rely on it behaving in a particular way. Things that differ include: when, after yielding, the thread will get an opportunity to run again; whether or not the thread foregoes its remaining quantum. Windows In the Hotspot implementation, the way that Thread.yield() works has changed between Java 5 and Java 6. In Java 5, Thread.yield() calls the Windows API call Sleep(0). This has the special effect of clearing the current thread’s quantum and putting it to the end of the queue for its priority level. In other words, all runnable threads of the same priority (and those of greater priority) will get a chance to run before the yielded thread is next given CPU time. When it is eventually re-scheduled, it will come back with a full quantum, but doesn’t “carry over” any of the remaining quantum from the time of yielding. This behaviour is a little different from a non-zero sleep where the sleeping thread generally loses 1 quantum value (in effect, 1/3 of a 10 or 15ms tick). In Java 6, this behaviour was changed. The Hotspot VM now implements Thread.yield() using the Windows SwitchToThread() API call. This call makes the current thread give up its current timeslice, but not its entire quantum. This means that depending on the priorities of other threads, the yielding thread can be scheduled back in one interrupt period later. Linux Under Linux, Hotspot simply calls sched_yield(). The consequences of this call are a little different, and possibly more severe than under Windows: a yielded thread will not get another slice of CPU until all other threads have had a slice of CPU; (at least in kernel 2.6.8 onwards), the fact that the thread has yielded is implicitly taken into account by the scheduler’s heuristics on its recent CPU allocation — thus, implicitly, a thread that has yielded could be given more CPU when scheduled in the future. When to use yield()? I would say practically never. Its behaviour isn’t standardly defined and there are generally better ways to perform the tasks that you might want to perform with yield(): if you’re trying to use only a portion of the CPU, you can do this in a more controllable way by estimating how much CPU the thread has used in its last chunk of processing, then sleeping for some amount of time to compensate: see the sleep() method; if you’re waiting for a process or resource to complete or become available, there are more efficient ways to accomplish this, such as by using join() to wait for another thread to complete, using the wait/notify mechanism to allow one thread to signal to another that a task is complete, or ideally by using one of the Java 5 concurrency constructs such as a Semaphore or blocking queue. Thread Scheduling thread scheduler, part of the OS (usually) that is responsible for sharing the available CPUs out between the various threads. How exactly the scheduler works depends on the individual platform, but various modern operating systems (notably Windows and Linux) use largely similar techniques that we’ll describe here. Note that we’ll continue to talk about a single thread scheduler. On multiprocessor systems, there is generally some kind of scheduler per processor, which then need to be coordinated in some way. Across platforms, thread scheduling tends to be based on at least the following criteria: a priority, or in fact usually multiple “priority” settings that we’ll discuss below; a quantum, or number of allocated timeslices of CPU, which essentially determines the amount of CPU time a thread is allotted before it is forced to yield the CPU to another thread of the same or lower priority (the system will keep track of the remaining quantum at any given time, plus its default quantum, which could depend on thread type and/or system configuration); a state, notably “runnable” vs “waiting”; metrics about the behaviour of threads, such as recent CPU usage or the time since it last ran (i.e. had a share of CPU), or the fact that it has “just received an event it was waiting for”. Most systems use what we might dub priority-based round-robin scheduling to some extent. The general principles are: a thread of higher priority (which is a function of base and local priorities) will preempt a thread of lower priority; otherwise, threads of equal priority will essentially take turns at getting an allocated slice or quantum of CPU; there are a few extra “tweaks” to make things work. StatesDepending on the system, there are various states that a thread can be in. Probably the two most interesting are: runnable, which essentially means “ready to consume CPU”; being runnable is generally the minimum requirement for a thread to actually be scheduled on to a CPU; waiting, meaning that the thread currently cannot continue as it is waiting for a resource such as a lock or I/O, for memory to be paged in, for a signal from another thread, or simply for a period of time to elapse (sleep). Other states include terminated, which means the thread’s code has finished running but not all of the thread’s resources have been cleared up, and a new state, in which the thread has been created, but not all resources necessary for it to be runnable have been created. Quanta and clock ticks Each thread has a quantum, which is effectively how long it is allowed to keep hold of the CPU if: it remains runnable; the scheduler determines that no other thread needs to run on that CPU instead. Thread quanta are generally defined in terms of some number of clock ticks. If it doesn’t otherwise cease to be runnable, the scheduler decides whether to preempt the currently running thread every clock tick. As a rough guide: a clock tick is typically 10-15 ms under Windows; under Linux, it is 1ms (kernel 2.6.8 onwards); a quantum is usually a small number of clock ticks, depending on the OS:either 2, 6 or 12 clock ticks on Windows, depending on whether Windows is running in “server” mode: Windows mode Foreground process Non-foreground process Normal 6 ticks 2 ticks Server 12 ticks 12 ticks between 10-200 clock ticks (i.e. 10-200 ms) under Linux, though some granularity is introduced in the calculation— see below.a thread is usually allowed to “save up” unused quantum, up to some limit and granularity. In Windows, a thread’s quantum allocation is fairly stable. In Linux, on the other hand, a thread’s quantum is dynamically adjusted when it is scheduled, depending partly on heuristics about its recent resource usage and partly on a nice value Switching and scheduling algorithms At key moments, the thread scheduler considers whether to switch the thread that is currently running on a CPU. These key moments are usually: periodically, via an interrupt routine, the scheduler will consider whether the currently running thread on each CPU has reached the end of its allotted quantum; at any time, a currently running thread could cease to be runnable (e.g. by needing to wait, reaching the end of its execution or being forcibly killed); when some other attribute of the thread changes (e.g. its priority or processor affinity4) which means that which threads are running needs to be re-assessed. At these decision points, the scheduler’s job is essentially to decide, of all the runnable threads, which are the most appropriate to actually be running on the available CPUs. Potentially, this is quite a complex task. But we don’t want the scheduler to waste too much time deciding “what to do next”. So in practice, a few simple heuristics are used each time the scheduler needs to decide which thread to let run next: there’s usually a fast path for determining that the currently running thread is still the most appropriate one to continue running (e.g. storing a bitmask of which priorities have runnable threads, so the scheduler can quickly determine that there’s none of a higher priority than that currently running); if there is a runnable thread of higher priority than the currently running one, then the higher priority one will be scheduled in3; if a thread is “preempted” in this way, it is generally allowed to keep its remaining quantum and continue running when the higher-priority thread is scheduled out again; when a thread’s quantum runs out, the thread is “put to the back of the queue” of runnable threads with the given priority and if there’s no queued (runnable) thread of higher priority, then next thread of the same priority will be scheduled in; at the end of its quantum, if there’s “nothing better to run”, then a thread could immediately get a new quantum and continue running; a thread typically gets a temporary boost to its quantum and/or priority at strategic points. Quantum and priority boostingBoth Windows and Linux (kernel 2.6.8 onwards) implement temporary boosting. Strategic points at which a thread may be given a “boost” include: when it has just finished waiting for a lock/signal or I/O5; when it has not run for a long time (in Windows, this appears to be a simple priority boost after a certain time; in Linux, there is an ongoing calculation based on the thread’s nice value and its recent resource usage); when a GUI event occurs; while it owns the focussed window (recent versions of Windows give threads of the owning process a larger quantum; earlier versions give them a priority boost). Context switching context switching. Roughly speaking, this is the procedure that takes place when the system switches between threads running on the available CPUs. the thread scheduler must actually manage the various thread structures and make decisions about which thread to schedule next where, and every time the thread running on a CPU actually changes— often referred to as a context switch switching between threads of different processes (that is, switching to a thread that belongs to a different process from the one last running on that CPU) will carry a higher cost, since the address-to-memory mappings must be changed, and the contents of the cache almost certainly will be irrelevant to the next process. Context switches appear to typically have a cost somewhere between 1 and 10 microseconds (i.e. between a thousandth and a hundredth of a millisecond) between the fastest and slowest cases (same-process threads with little memory contention vs different processes). So the following are acceptable:1 nanoseconds is billionth of one second,1 microsecond is millionth of one second,1 millisecond is thousandth of one second What causes too many slow context switches in Java? Every time we deliberately change a thread’s status or attributes (e.g. by sleeping, waiting on an object, changing the thread’s priority etc), we will cause a context switch. But usually we don’t do those things so many times in a second to matter. Typically, the cause of excessive context switching comes from contention on shared resources, particularly synchronized locks: rarely, a single object very frequently synchronized on could become a bottleneck; more frequently, a complex application has several different objects that are each synchronized on with moderate frequency, but overall, threads find it difficult to make progress because they keep hitting different contended locks at regular intervals. Avoiding contention and context switches in Java Firstly, before hacking with your code, a first course of action is upgrading your JVM, particularly if you are not yet using Java 6. Most new Java JVM releases have come with improved synchronization optimisation. Then, a high-level solution to avoiding synchronized lock contention is generally to use the various classes from the Java 5 concurrency framework (see the java.util.concurrent package). For example, instead of using a HashMap with appropriate synchronization, a ConcurrentHashMap can easily double the throughput with 4 threads and treble it with 8 threads (see the aforementioned link for some ConcurrentHashMap performance measurements). A replacement to synchronized with often better concurrency is offered with various explicit lock classes (such as ReentrantLock). Java thread priority Lower-priority threads are given CPU when all higher priority threads are waiting (or otherwise unable to run) at that given moment. Thread priority isn’t very meaningful when all threads are competing for CPU. The number should lie in the range of two constants MIN_PRIORITY and MAX_PRIORITY defined on Thread, and will typically reference NORM_PRIORITY, the default priority of a thread if we don’t set it to anything else. For example, to give a thread a priority that is “half way between normal and maximum”, we could call:1thr.setPriority((Thread.MAX_PRIORITY - Thread.NORM_PRIORITY) / 2); ####### Some points about thread property depending on your OS and VM version, Thread.setPriority() may actually do nothing at all (see below for details); what thread priorities mean to the thread scheduler differs from scheduler to scheduler, and may not be what you intuitively presume; in particular: Priority may not indicate “share of the CPU”. As we’ll see below, it turns out that “priority” is more or less an indication of CPU distribution on UNIX systems, but not under Windows. thread priorities are usually a combination of “global” and “local” priority settings, and Java’s setPriority() method typically works only on the local priority— in other words, you can’t set priorities across the entire range possible (this is actually a form of protection— you generally don’t want, say, the mouse pointer thread or a thread handling audio data to be preempted by some random user thread); the number of distinct priorities available differs from system to system, but Java defines 10 (numbered 1-10 inclusive), so you could end up with threads that have different priorities under one OS, but the same priority (and hence unexpected behaviour) on another; most operating systems’ thread schedulers actually perform temporary manipulations to thread priorities at strategic points (e.g. when a thread receives an event or I/O it was waiting for), and often “the OS knows best”; trying to manually manipulate priorities could just interfere with this system; your application doesn’t generally know what threads are running in other processes, so the effect on the overall system of changing the priority of a thread may be hard to predict. So you might find, for example, that your low-priority thread designed to “run sporadically in the background” hardly runs at all due to a virus dection program running at a slightly higher (but still ‘lower-than-normal’) priority, and that the performance unpredictably varies depending on which antivirus program your customer is using. Of course, effects like these will always happen to some extent or other on modern systems. Thread scheduling implications in JavaThread Control the granularity and responsiveness of the Thread.sleep() method is largely determined by the scheduler’s interrupt period and by how quickly the slept thread becomes the “chosen” thread again; the precise function of the setPriority() method depends on the specific OS’s interpretation of priority (and which underlying API call Java actually uses when several are available): for more information, see the more detailed section on thread priority; the behaviour of the Thread.yield() method is similarly determined by what particuar underlying API calls do, and which is actually chosen by the VM implementation. “Granularity” of threads Although our introduction to threading focussed on how to create a thread, it turns out that it isn’t appropriate to create a brand new thread just for a very small task. Threads are actually quite a “coarse-grained” unit of execution, for reasons that are hopefully becoming clear from the previous sections. Overhead and limits of creating and destroying threads creating and tearing down threads isn’t free: there’ll be some CPU overhead each time we do so; there may be some moderate limit on the number of threads that can be created, determined by the resources that a thread needs to have allocated (if a process has 2GB of address space, and each thread as 512K of stack, that means a maximum of a few thousands threads per process). Avoiding thread overhead in Java In applications such as servers that need to continually execute short, multithreaded tasks, the usual way to avoid the overhead of repeated thread creation is to create a thread pool. Dinnig Philosophers problem The problem was designed to illustrate the challenges of avoiding deadlock, a system state in which no progress is possible. To see that a proper solution to this problem is not obvious, consider a proposal in which each philosopher is instructed to behave as follows: think until the left fork is available; when it is, pick it up; think until the right fork is available; when it is, pick it up; when both forks are held, eat for a fixed amount of time; then, put the right fork down; then, put the left fork down; repeat from the beginning. This attempted solution fails because it allows the system to reach a deadlock state, in which no progress is possible. This is a state in which each philosopher has picked up the fork to the left, and is waiting for the fork to the right to become available, vice versa. With the given instructions, this state can be reached, and when it is reached, the philosophers will eternally wait for each other to release a fork Resource starvation might also occur independently of deadlock if a particular philosopher is unable to acquire both forks because of a timing problem. For example, there might be a rule that the philosophers put down a fork after waiting ten minutes for the other fork to become available and wait a further ten minutes before making their next attempt. This scheme eliminates the possibility of deadlock (the system can always advance to a different state) but still suffers from the problem of livelock. If all five philosophers appear in the dining room at exactly the same time and each picks up the left fork at the same time the philosophers will wait ten minutes until they all put their forks down and then wait a further ten minutes before they all pick them up again.SolutionsArbitrator solutionAnother approach is to guarantee that a philosopher can only pick up both forks or none by introducing an arbitrator, e.g., a waiter. In order to pick up the forks, a philosopher must ask permission of the waiter. The waiter gives permission to only one philosopher at a time until the philosopher has picked up both of their forks. Putting down a fork is always allowed. The waiter can be implemented as a mutex. In addition to introducing a new central entity (the waiter), this approach can result in reduced parallelism. if a philosopher is eating and one of their neighbors is requesting the forks, all other philosophers must wait until this request has been fulfilled even if forks for them are still available. QueueWhat is the difference between poll() and remove() method of Queue interface? (answer) Though both poll() and remove() method from Queue is used to remove the object and returns the head of the queue, there is a subtle difference between them. If Queue is empty() then a call to remove() method will throw Exception, while a call to poll() method returns null. What is the difference between fail-fast and fail-safe Iterators? Fail-fast Iterators throws ConcurrentModificationException when one Thread is iterating over collection object and other thread structurally modify Collection either by adding, removing or modifying objects on underlying collection. They are called fail-fast because they try to immediately throw Exception when they encounter failure. On the other hand fail-safe Iterators works on copy of collection instead of original collection To remove entry from collection you need to use Iterator’s remove() method. This method removes current element from Iterator’s perspective. If you use Collection’s or List’s remove() method during iteration then your code will throw ConcurrentModificationException. That’s why it’s advised to use Iterator remove() method to remove objects from Collection. What is the difference between Synchronized Collection and Concurrent Collection? One Significant difference is that Concurrent Collections has better performance than synchronized Collection ** because they **lock only a portion of Map to achieve concurrency and Synchronization. When do you use ConcurrentHashMap in Java ConcurrentHashMap is better suited for situation where you have multiple readers and oneWriter or fewer writers since Map gets locked only during the write operation. If you have an equal number of reader and writer than ConcurrentHashMap will perform in the line of Hashtable or synchronized HashMap. Sorting collections Sorting is implemented using Comparable and Comparator in Java and when you call Collections.sort() it gets sorted based on the natural order specified in compareTo() method while Collections.sort(Comparator) will sort objects based on compare() method of Comparator. Hashmap vs Hasset HashSet implements java.util.Set interface and that’s why only contains unique elements, while HashMap allows duplicate values. In fact, HashSet is actually implemented on top of java.util.HashMap. What is NavigableMap in Java NavigableMap Map was added in Java 1.6, it adds navigation capability to Map data structure. It provides methods like lowerKey() to get keys which is less than specified key, floorKey() to return keys which is less than or equal to specified key, ceilingKey() to get keys which is greater than or equal to specified key and higherKey() to return keys which is greater specified key from a Map. It also provide similar methods to get entries e.g. lowerEntry(), floorEntry(), ceilingEntry() and higherEntry(). Apart from navigation methods, it also provides utilities to create sub-Map e.g. creating a Map from entries of an exsiting Map like tailMap, headMap and subMap. headMap() method returns a NavigableMap whose keys are less than specified, tailMap() returns a NavigableMap whose keys are greater than the specified and subMap() gives a NavigableMap between a range, specified by toKey to fromKey Array vs ArrayList Array is fixed length data structure, once created you can not change it’s length. On the other hand, ArrayList is dynamic, it automatically allocate a new array and copies content of old array, when it resize. Another reason of using ArrayList over Array is support of Generics. Can we replace Hashtable with ConcurrentHashMap? Since Hashtable locks whole Map instead of a portion of Map, compound operations like if(Hashtable.get(key) == null) put(key, value) works in Hashtable but not in concurrentHashMap. instead of this use putIfAbsent() method of ConcurrentHashMap What is CopyOnWriteArrayList, how it is different than ArrayList and Vector CopyOnWriteArrayList is new List implementation introduced in Java 1.5 which provides better concurrent access than Synchronized List. better concurrency is achieved by Copying ArrayList over each write and replace with original instead of locking. Also CopyOnWriteArrayList doesn’t throw any ConcurrentModification Exception. Its different than ArrayList because its thread-safe and ArrayList is not thread-safe and it’s different than Vector in terms of Concurrency. CopyOnWriteArrayList provides better Concurrency by reducing contention among readers and writers. Why ListIterator has added() method but Iterator doesn’t or Why to add() method is declared in ListIterator and not on Iterator. (answer) ListIterator has added() method because of its ability to traverse or iterate in both direction of the collection. it maintains two pointers in terms of previous and next call and in a position to add a new element without affecting current iteration. What is BlockingQueue, how it is different than other collection classes? (answer) BlockingQueue is a Queue implementation available in java.util.concurrent package. It’s one of the concurrent Collection class added on Java 1.5, main difference between BlockingQueue and other collection classes is that apart from storage, it also provides flow control. It can be used in inter-thread communication and also provides built-in thread-safety by using happens-before guarantee. You can use BlockingQueue to solve Producer Consumer problem, which is what is needed in most of concurrent applications. You have thread T1, T2 and T3, how will you ensure that thread T2 run after T1 and thread T3 run after T2 To use join method. Happen before In computer science, the happened-before relation (denoted: → {\displaystyle \to ;} \to ;) is a relation between the result of two events, such that if one event should happen before another event, the result must reflect that, even if those events are in reality executed out of order (usually to optimize program flow). In Java specifically, a happens-before relationship is a guarantee that memory written to by statement A is visible to statement B, that is, that statement A completes its write before statement B starts its read Concurrent framework The advantage of using Callable over Runnable is that Callable can explicitly return a value. Executors are a big step forward compared to plain old threads because executors ease the management of concurrent tasks. Some types of algorithms exist that require tasks to create subtasks and communicate with each other to complete. Those are the “divide and conquer” algorithms, which are also referred to as “map and reduce,” in reference to the eponymous functions in functional languages. The fork/join framework added to the java.util.concurrent package in Java SE 7 through Doug Lea’s efforts fills that gap. The Java SE 5 and Java SE 6 versions of java.util.concurrent helped in dealing with concurrency, and the additions in Java SE 7 help with parallelism. First and foremost, fork/join tasks should operate as “pure” in-memory algorithms in which no I/O operations come into play. Also, communication between tasks through shared state should be avoided as much as possible, because that implies that locking might have to be performed. The core addition is a new ForkJoinPool executor that is dedicated to running instances implementing ForkJoinTask. ForkJoinTask objects support the creation of subtasks plus waiting for the subtasks to complete. With those clear semantics, the executor is able to dispatch tasks among its internal threads pool by “stealing” jobs when a task is waiting for another task to complete and there are pending tasks to be run. ForkJoinTask objects feature two specific methods: The fork() method allows a ForkJoinTask to be planned for asynchronous execution. This allows a new ForkJoinTask to be launched from an existing one. In turn, the join() method allows a ForkJoinTask to wait for the completion of another one. There are two types of ForkJoinTask specializations: Instances of RecursiveAction represent executions that do not yield a return value. In contrast, instances of RecursiveTask yield return values. In general, RecursiveTask is preferred because most divide-and-conquer algorithms return a value from a computation over a data set. The fork and join principle consists of two steps which are performed recursively. These two steps are the fork step and the join step. A task that uses the fork and join principle can fork (split) itself into smaller subtasks which can be executed concurrently. This is illustrated in the diagram below: By splitting itself up into subtasks, each subtask can be executed in parallel by different CPUs, or different threads on the same CPU. The limit for when it makes sense to fork a task into subtasks is also called a threshold. It is up to each task to decide on a sensible threshold. It depends very much on the kind of work being done. Once the subtasks have finished executing, the task may join (merge) all the results into one result. Of course, not all types of tasks may return a result. If the tasks do not return a result then a task just waits for its subtasks to complete. No result merging takes place then. The ForkJoinPool is a special thread pool which is designed to work well with fork-and-join task splitting. The ForkJoinPool located in the java.util.concurrent package, so the full class name is java.util.concurrent.ForkJoinPool. You create a ForkJoinPool using its constructor. As a parameter to the ForkJoinPool constructor you pass the indicated level of parallelism you desire. The parallelism level indicates how many threads or CPUs you want to work concurrently on on tasks passed to the ForkJoinPool. You submit tasks to a ForkJoinPool similarly to how you submit tasks to an ExecutorService. You can submit two types of tasks. A task that does not return any result (an “action”), and a task which does return a result (a “task”). Fork/Join framework details ForkJoinPool is consists of ForkJoinTask array and ForkJoinWorkerThread array. ForkJoinTask array contains tasks submitted to ForkJoinPool ForkJoinWorkerThread array in charge of executing those tasks When you call fork method on ForkJoinTask, program will call “pushTask” asynchronously of ForkJoinWorkerThread, and then return result right away. “pushTask” will put current task into ForkJoinTask array queue, then execute “signalWork()” of ForkJoinPool to create a new thread to execute task. 123456789101112 final void pushTask(ForkJoinTask t) &#123; ForkJoinTask[] q; int s, m; if ((q = queue) != null) &#123; // ignore if queue removed long u = (((s = queueTop) &amp; (m = q.length - 1)) &lt;&lt; ASHIFT) + ABASE; UNSAFE.putOrderedObject(q, u, t); queueTop = s + 1; // or use putOrderedInt if ((s -= queueBase) &lt;= 2) pool.signalWork();else if (s == m) growQueue(); &#125; &#125; “join” method main functionality is blocking current thread and wait for resutls. 1234567891011121314public final V join() &#123; if (doJoin() != NORMAL) return reportResult(); else return getRawResult();&#125;private V reportResult() &#123; int s; Throwable ex; if ((s = status) == CANCELLED) throw new CancellationException();if (s == EXCEPTIONAL &amp;&amp; (ex = getThrowableException()) != null) UNSAFE.throwException(ex); return getRawResult();&#125; When do call doJoin(), you can get status of curent thread. There are 4 status: NORMAL: completed CANCELLED SIGNAL EXCEPTIONAL The method of doJoin()12345678910111213141516171819private int doJoin() &#123; Thread t; ForkJoinWorkerThread w; int s; boolean completed; if ((t = Thread.currentThread()) instanceof ForkJoinWorkerThread) &#123; if ((s = status) &lt; 0) return s; if ((w = (ForkJoinWorkerThread)t).unpushTask(this)) &#123; try &#123; completed = exec(); &#125; catch (Throwable rex) &#123; return setExceptionalCompletion(rex); &#125; if (completed) return setCompletion(NORMAL); &#125; return w.joinTask(this); &#125; else return externalAwaitDone(); &#125; newTaskForIf a SocketUsingTask is cancelled through its Future, the socket is closed and the As of Java 6, ExecutorService implementations can override newTaskFor in AbstractExecutorService to control instantiation of the Future corresponding to a submitted Callable or Runnable. The default implementation just creates a new FutureTask, as shown in Listing 6.12. 123protected &lt;T&gt; RunnableFuture&lt;T&gt; newTaskFor(Callable&lt;T&gt; task) &#123; return new FutureTask&lt;T&gt;(task); &#125; Listing 6.12. Default implementation of newTaskFor in ThreadPoolExecutor. Thread shutdown Sensible encapsulation practices dictate that you should not manipulate a thread—interrupt it, modify its priority, etc.—unless you own it. The thread API has no formal concept of thread ownership: a thread is represented with a Thread object that can be freely shared like any other object. However, it makes sense to think of a thread as having an owner, and this is usually the class that created the thread. So a thread pool owns its worker threads, and if those threads need to be interrupted, the thread pool should take care of it. As with any other encapsulated object, thread ownership is not transitive: the application may own the service and the service may own the worker threads, but the application doesn’t own the worker threads and therefore should not attempt to stop them directly. Instead, the service should provide lifecycle methods for shutting itself down that also shut down the owned threads; then the application can shut down the service, and the service can shut down the threads. Executor- Service provides the shutdown and shutdownNow methods; other thread-owning services should provide a similar shutdown mechanism. Log service implemented by blocking queue If you are logging multiple lines as part of a single log message, you may need to use additional client-side locking to prevent undesirable interleaving of output from multiple threads. If two threads logged multiline stack traces to the same stream with one println call per line, the results would be interleaved unpredictably, and could easily look like one large but meaningless stack trace. 12345678910111213141516171819public class LogWriter &#123;private final BlockingQueue&lt;String&gt; queue; private final LoggerThread logger;public LogWriter(Writer writer) &#123;this.queue = new LinkedBlockingQueue&lt;String&gt;(CAPACITY); this.logger = new LoggerThread(writer);&#125;public void start() &#123; logger.start(); &#125;public void log(String msg) throws InterruptedException &#123; queue.put(msg);&#125;private class LoggerThread extends Thread &#123; private final PrintWriter writer;...public void run() &#123; try &#123; while (true)writer.println(queue.take());&#125; catch(InterruptedException ignored) &#123; &#125; finally &#123; writer.close();&#125;&#125; &#125;&#125; Stop logging However, this approach has race conditions that make it unreliable. The implementation of log is a check-then-act sequence: producers could observe that the service has not yet been shut down but still queue messages after the shutdown, again with the risk that the producer might get blocked in log and never become unblocked. There are tricks that reduce the likelihood of this (like having the consumer wait several seconds before declaring the queue drained), but these do not change the fundamental problem, merely the likelihood that it will cause a failure. 123456public void log(String msg) throws InterruptedException &#123; if (!shutdownRequested) queue.put(msg); else throw new IllegalStateException("logger is shut down");&#125; The way to provide reliable shutdown for LogWriter is to fix the race con- dition, which means making the submission of a new log message atomic. But we don’t want to hold a lock while trying to enqueue the message, since put could block. Instead, we can atomically check for shutdown and conditionally increment a counter to “reserve” the right to submit a message, as shown in Log- Service in Listing 7.15. Delegate shutdown to high level service1234567891011121314public class LogService &#123;private final ExecutorService exec = newSingleThreadExecutor(); ...public void start() &#123; &#125;public void stop() throws InterruptedException &#123; try &#123;exec.shutdown();exec.awaitTermination(TIMEOUT, UNIT); &#125; finally &#123; writer.close();&#125; &#125;public void log(String msg) &#123; try &#123; &#125; &#125;exec.execute(new WriteTask(msg));&#125; catch (RejectedExecutionException ignored) &#123; &#125; It can even delegate to one shot Executor, OneShotExecutionService.java 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748import java.util.Set;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;import java.util.concurrent.TimeUnit;import java.util.concurrent.atomic.AtomicBoolean;/** * Created by todzhang on 2017/1/30. * If a method needs to process a batch of tasks and does not return * until all the tasks are finished, it can simplify service lifecycle management * by using a private Executor whose lifetime is bounded by that method. * * * The checkMail method in Listing checks for new mail in parallel * on a number of hosts. It creates a private executor and submits * a task for each host: it then shuts down the executor and waits * for termination, which occurs when all */public class OneShotExecutionService &#123; boolean checkMail(Set&lt;String&gt; hosts, long timeout, TimeUnit unit) throws InterruptedException&#123; ExecutorService exec= Executors.newCachedThreadPool(); final AtomicBoolean hasNewMail=new AtomicBoolean(false); try &#123; for (final String host : hosts ) &#123; exec.execute(new Runnable() &#123; @Override public void run() &#123; if (checkMail(host)) &#123; hasNewMail.set(true); &#125; &#125; &#125;); &#125; &#125; finally&#123; exec.shutdown(); exec.awaitTermination(timeout,unit); &#125; return hasNewMail.get(); &#125; boolean checkMail(String host)&#123; return true; &#125;&#125; When an ExecutorService is shut down abruptly with shutdownNow, it attempts to cancel the tasks currently in progress and returns a list of tasks that were sub- mitted but never started so that they can be logged or saved for later processing. Detailed logic can be found at CancelledTaskTrackingExecutor.java The leading cause of premature thread death is RuntimeException. JVM shutdown The JVM can shut down in either an orderly or abrupt manner. An orderly shut- down is initiated when the last “normal” (nondaemon) thread terminates, some- one calls System.exit, or by other platform-specific means (such as sending a SIGINT or hitting Ctrl-C). While this is the standard and preferred way for the JVM to shut down, it can also be shut down abruptly by calling Runtime.halt or by killing the JVM process through the operating system (such as sending a SIGKILL). Shutdown hooks In an orderly shutdown, the JVM first starts all registered shutdown hooks. Shutdown hooks are unstarted threads that are registered with Runtime.addShutdownHook. The JVM makes no guarantees on the order in which shutdown hooks are started. If any application threads (daemon or nondaemon) are still running at shutdown time, they continue to run concurrently with the shutdown process. When all shutdown hooks have completed, the JVM may choose to run finalizers if runFinalizersOnExit is true, and then halts. The JVM makes no attempt to stop or interrupt any application threads that are still running at shutdown time; they are abruptly terminated when the JVM eventually halts. If the shutdown hooks or finalizers don’t complete, then the orderly shutdown process “hangs” and the JVM must be shut down abruptly. In an abrupt shutdown, the JVM is not required to do anything other than halt the JVM; shutdown hooks will not run. Shutdown hooks should be thread-safe: they must use synchronization when accessing shared data and should be careful to avoid deadlock, just like any other concurrent code. Further, they should not make assumptions about the state of the application (such as whether other services have shut down already or all normal threads have completed) or about why the JVM is shutting down, and must therefore be coded extremely defensively. Finally, they should exit as quickly as possible, since their existence delays JVM termination at a time when the user may be expecting the JVM to terminate quickly. Shutdown hooks can be used for service or application cleanup, such as deleting temporary files or cleaning up resources that are not automatically cleaned up by the OS. Listing 7.26 shows how LogService in Listing 7.16 could register a shutdown hook from its start method to ensure the log file is closed on exit. Because shutdown hooks all run concurrently, closing the log file could cause trouble for other shutdown hooks who want to use the logger. To avoid this problem, shutdown hooks should not rely on services that can be shut down by the application or other shutdown hooks. One way to accomplish this is to use a single shutdown hook for all services, rather than one for each service, and have it call a series of shutdown actions. This ensures that shutdown actions execute sequentially in a single thread, thus avoiding the possibility of race conditions or deadlock between shutdown actions. This technique can be used whether or not you use shutdown hooks; executing shutdown actions sequentially rather than concurrently eliminates many potential sources of failure. 1234567public void start() &#123; Runtime.getRuntime().addShutdownHook(new Thread() &#123; public void run() &#123; try &#123; LogService.this.stop(); &#125; catch (InterruptedException ignored) &#123;&#125;&#125; &#125;);&#125; Daemon thread Threads are divided into two types: normal threads and daemon threads. When the JVM starts up, all the threads it creates (such as garbage collector and other housekeeping threads) are daemon threads, except the main thread. When a new thread is created, it inherits the daemon status of the thread that created it, so by default any threads created by the main thread are also normal threads. Normal threads and daemon threads differ only in what happens when they exit. When a thread exits, the JVM performs an inventory of running threads, and if the only threads that are left are daemon threads, it initiates an orderly shutdown. When the JVM halts, any remaining daemon threads are abandoned— finally blocks are not executed, stacks are not unwound—the JVM just exits. Daemon threads should be used sparingly—few processing activities can be safely abandoned at any time with no cleanup. In particular, it is dangerous to use daemon threads for tasks that might perform any sort of I/O. Daemon threads are best saved for “housekeeping” tasks, such as a background thread that periodically removes expired entries from an in-memory cache.Daemon threads are not a good substitute for properly managing the life- cycle of services within an application. Finalizer Finalizers offer no guarantees on when or even if they run, and they impose a significant performance cost on objects with nontrivial finalizers. They are also extremely difficult to write correctly.9 In most cases, the combination of finally blocks and explicit close methods does a better job of resource management than finalizers; the sole exception is when you need to manage objects that hold resources acquired by native methods. Java does not provide a preemptive mechanism for cancelling activities or terminating threads. Instead, it provides a cooperative interruption mechanism that can be used to facilitate cancellation, but it is up to you to construct protocols for cancellation and use them consistently. Using FutureTask and the Executor framework simplifies building cancellable tasks and services. Thread Pool Thread pools work best when tasks are homogeneous and independent. Mix- ing long-running and short-running tasks risks “clogging” the pool unless it is very large; submitting tasks that depend on other tasks risks deadlock unless the pool is unbounded. Fortunately, requests in typical network-based server applications—web servers, mail servers, file servers—usually meet these guide- lines. Some tasks have characteristics that require or preclude a specific exe- cution policy. Tasks that depend on other tasks require that the thread pool be large enough that tasks are never queued or rejected; tasks that exploit thread confinement require sequential execution. Document these requirements so that future maintainers do not undermine safety or live- ness by substituting an incompatible execution policy. In a single-threaded executor, a task that submits another task to the same executor and waits for its result will always deadlock. The same thing can happen in larger thread pools if all threads are executing tasks that are blocked waiting for other tasks still on the work queue. This is called thread starvation deadlock, and can occur whenever a pool task initiates an unbounded blocking wait for some resource or condition that can succeed only through the action of another pool task, such as waiting for the return value or side effect of another task, unless you can guarantee that the pool is large enough. Whenever you submit to an Executor tasks that are not independent, be aware of the possibility of thread starvation deadlock, and document any pool sizing or configuration constraints in the code or configuration file where the Executor is configured. Task that deadlocks in a single-threaded Executor. Don’t do this.12345Future&lt;String&gt; header,footer; header=exec.submit(new LoadFileTask("header.html")); footer=exec.submit(new LoadFileTask("footer.html")); String body=renderBody(); return header.get()+body+footer.get(); Long running tasks Thread pools can have responsiveness problems if tasks can block for extended periods of time, even if deadlock is not a possibility. A thread pool can become clogged with long-running tasks, increasing the service time even for short tasks. If the pool size is too small relative to the expected steady-state number of long- running tasks, eventually all the pool threads will be running long-running tasks and responsiveness will suffer. One technique that can mitigate the ill effects of long-running tasks is for tasks to use timed resource waits instead of unbounded waits. Most blocking methods in the plaform libraries come in both untimed and timed versions, such as Thread.join, BlockingQueue.put, CountDownLatch.await, and Selector.sel- ect. If the wait times out, you can mark the task as failed and abort it or requeue it for execution later. This guarantees that each task eventually makes progress towards either successful or failed completion, freeing up threads for tasks that might complete more quickly. If a thread pool is frequently full of blocked tasks, this may also be a sign that the pool is too small. size the thread pool The ideal size for a thread pool depends on the types of tasks that will be submitted and the characteristics of the deployment system. Thread pool sizes should rarely be hard-coded; instead pool sizes should be provided by a configuration mechanism or computed dynamically by consulting Runtime.availableProcessors. If you have different categories of tasks with very different behaviors, consider using multiple thread pools so each can be tuned according to its workload. The optimal pool size for keeping the processors at the desired utilization is:Nthreads=Ncpu∗Ucpu∗ (1+((W/C)Ncpu: Number of CPUUcpu: target CPU utilization , 0&lt;Ucpu&lt;1W/C: ratio of wait time to compute time ThreadPoolExecutor ThreadPoolExecutor provides the base implementation for the executors re- turned by the newCachedThreadPool, newFixedThreadPool, and newScheduled- ThreadExecutor factories in Executors. Implementation of ThreadPoolExecutor1public ThreadPoolExecutor(int corePoolSize,int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue,ThreadFactory threadFactory,RejectedExecutionHandler handler)&#123;...&#125; corePoolSize is the target size, the implementation attempts to maintain the pool at this size when there are no tasks to execute. and will not create more threads than this unless the work queue is full. When a ThreadPoolExecutor is initially created, the core threads are not started immediately, but instead as tasks are submitted. Unless you call prestartAllCoreThreads The maximum pool size is the upper bound on how many threads can be active at once. A thread that has been idel for longer than the keep-alive time becomes a candidate for reaping and can be terminated if the current pool size exceed the core size. By tuning the core pool size and keep-alive times, you can encourage the pool to reclaim resources used by otherwise idle threads, making them available for more useful work. (Like everything else, this is a tradeoff: reaping idle threads incurs additional latency due to thread creation if threads must later be created when demand increases.) The newFixedThreadPool factory sets both the core pool size and the maxi- mum pool size to the requested pool size, creating the effect of infinite timeout; the newCachedThreadPool factory sets the maximum pool size to Integer.MAX_VALUE and the core pool size to zero with a timeout of one minute, creating the effect of an infinitely expandable thread pool that will contract again when demand decreases. Other combinations are possible using the explicit ThreadPool- Executor constructor. ThreadPoolExecutor allows you to supply a BlockingQueue to hold tasks awaiting execution. There are three basic approaches to task queueing: un- bounded queue, bounded queue, and synchronous handoff. The choice of queue interacts with other configuration parameters such as pool size. The default for newFixedThreadPool and newSingleThreadExecutor is to use an unbounded LinkedBlockingQueue. Tasks will queue up if all worker threads are busy, but the queue could grow without bound if the tasks keep arriving faster than they can be executed. A more stable resource management strategy is to use a bounded queue, such as an ArrayBlockingQueue or a bounded LinkedBlockingQueue or Priority- BlockingQueue. Bounded queues help prevent resource exhaustion but introduce the question of what to do with new tasks when the queue is full. (There are a number of possible saturation policies for addressing this problem; For very large or unbounded pools, you can also bypass queueing entirely and instead hand off tasks directly from producers to worker threads using a SynchronousQueue. A SynchronousQueue is not really a queue at all, but a mechanism for managing handoffs between threads. In order to put an element on a SynchronousQueue, another thread must already be waiting to accept the handoff. If no thread is waiting but the current pool size is less than the maximum, ThreadPoolExecutor creates a new thread; otherwise the task is rejected according to the saturation policy. Using a direct handoff is more efficient because the task can be handed right to the thread that will execute it, rather than first placing it on a queue and then having the worker thread fetch it from the queue. Synchron- ousQueue is a practical choice only if the pool is unbounded or if rejecting excess tasks is acceptable. The newCachedThreadPool factory uses a SynchronousQueue. Using a FIFO queue like LinkedBlockingQueue or ArrayBlockingQueue causes tasks to be started in the order in which they arrived. For more con- trol over task execution order, you can use a PriorityBlockingQueue, which orders tasks according to priority. Priority can be defined by natural order (iftasks implement Comparable) or by a Comparator. The newCachedThreadPool factory is a good default choice for an Executor, providing better queuing performance than a fixed thread pool.5 A fixed size thread pool is a good choice when you need to limit the number of concurrent tasks for resource-management purposes, as in a server application that accepts requests from network clients and would otherwise be vulnerable to overload. ith tasks that depend on other tasks, bounded thread pools or queues can cause thread starvation deadlock; instead, use an unbounded pool configuration like newCachedThreadPool. Saturation policies When a bounded work queue fills up, the saturation policy comes into play. The saturation policy for a ThreadPoolExecutor can be modified by calling setRejectedExecutionHandler. Several implementations of RejectedExecutionHandler are provided, each implementing a different saturation policy: AbortPolicy, CallerRunsPolicy, DiscardPolicy, and DiscardOldestPolicy. The default policy, abort, causes execute to throw the unchecked Rejected- ExecutionException; the caller can catch this exception and implement its own overflow handling as it sees fit. The discard policy silently discards the newly submitted task if it cannot be queued for execution; the discard-oldest policy discards the task that would otherwise be executed next and tries to resubmit the new task. (If the work queue is a priority queue, this discards the highest-priority element, so the combination of a discard-oldest saturation policy and a priority queue is not a good one.) The caller-runs policy implements a form of throttling that neither discards tasks nor throws an exception, but instead tries to slow down the flow of new tasks by pushing some of the work back to the caller. It executes the newly submitted task not in a pool thread, but in the thread that calls execute. If we modified our WebServer example to use a bounded queue and the caller-runs policy, after all the pool threads were occupied and the work queue filled up the next task would be executed in the main thread during the call to execute. Thread Factory Whenever a thread pool needs to create a thread, it does so through a thread factory (see Listing 8.5). The default thread factory creates a new, nondaemon thread with no special configuration. Specifying a thread factory allows you to customize the configuration of pool threads. ThreadFactory has a single method, newThread, that is called whenever a thread pool needs to create a new thread. There are a number of reasons to use a custom thread factory. You might want to specify an UncaughtExceptionHandler for pool threads, or instantiate an instance of a custom Thread class, such as one that performs debug logging. 123public interface ThreadFactory&#123; Thread newThread(Runnable r);&#125; BoundedExecutor.java is using semaphore and Executor for bounded executor service. MyThreadFactory.java and MyAppThread.java are used to customize ThreadFactory, a customized Thread. MyExtendedThreadPool.java implemented beforeExecute, afterExecute, etc method to add statistics, such as log and timing for each operations in the thread pool Process sequential processing to parallel1234567void processSequentially(List&lt;Element&gt; elements) &#123; for (Element e : elements)process(e);&#125;void processInParallel(Executor exec, List&lt;Element&gt; elements) &#123; for (final Element e : elements)exec.execute(new Runnable() &#123;public void run() &#123; process(e); &#125;&#125;); &#125; If you want to submit a set of tasks and wait for them all to complete, you can use ExecutorService.invokeAll; to retrieve the results as they become available, you can use a CompletionService. Deadlocks There is often a tension between safety and liveness. We use locking to ensure thread safety, but indiscriminate use of locking can cause lock-ordering deadlocks. Similarly, we use thread pools and semaphores to bound resource consumption, but failure to understand the activities being bounded can cause resource deadlocks. Java applications do not recover from deadlock, so it is worthwhile to ensure that your design precludes the conditions that could cause it. When a thread holds a lock forever, other threads attempting to acquire that lock will block forever waiting. When thread A holds lock L and tries to acquire lock M, but at the same time thread B holds M and tries to acquire L, both threads will wait forever. This situation is the simplest case of deadlock (or deadly embrace), Database systems are designed to detect and recover from deadlock. A trans- action may acquire many locks, and locks are held until the transaction commits. So it is quite possible, and in fact not uncommon, for two transactions to deadlock. Without intervention, they would wait forever (holding locks that are probably re- quired by other transactions as well). But the database server is not going to let this happen. When it detects that a set of transactions is deadlocked (which it does by searching the is-waiting-for graph for cycles), it picks a victim and aborts that transaction. This releases the locks held by the victim, allowing the other transactions to proceed. The application can then retry the aborted transaction, which may be able to complete now that any competing transactions have com- pleted. A program will be free of lock-ordering deadlocks if all threads acquire the locks they need in a fixed global order. To break deadlock by ensuring lock order uses System.identityHashCode to induce a lock ordering. It involves a few extra lines of code, but eliminates the possibility of deadlock. 1public static native int identityHashCode(Object x); In the rare case that two objects have the same hash code, we must use an arbitrary means of ordering the lock acquisitions, and this reintroduces the pos- sibility of deadlock. To prevent inconsistent lock ordering in this case, a third “tie breaking” lock is used. By acquiring the tie-breaking lock before acquiring either Account lock, we ensure that only one thread at a time performs the risky task of acquiring two locks in an arbitrary order, eliminating the possibility of deadlock (so long as this mechanism is used consistently). If hash collisions were common, this technique might become a concurrency bottleneck (just as having a single, program-wide lock would), but because hash collisions with System.identity- HashCode are vanishingly infrequent, this technique provides that last bit of safety at little cost. two locks are acquired by two threads in different orders, risking deadlock. Calling a method with no locks held is called an open call [CPJ 2.4.1.3], and classes that rely on open calls are more well-behaved and composable than classes that make calls with locks held. Using open calls to avoid deadlock is analogous to using encapsulation to provide thread safety: while one can certainly construct a thread-safe program without any encapsulation, the thread safety analysis of a program that makes effective use of encapsulation is far easier than that of one that does not. Avoiding and diagnosing deadlocks A program that never acquires more than one lock at a time cannot experience lock-ordering deadlock. Of course, this is not always practical, but if you can get away with it, it’s a lot less work. If you must acquire multiple locks, lock ordering must be a part of your design: try to minimize the number of potential locking interactions, and follow and document a lock-ordering protocol for locks that may be acquired together. In programs that use fine-grained locking, audit your code for deadlock free- dom using a two-part strategy: first, identify where multiple locks could be ac- quired (try to make this a small set), and then perform a global analysis of all such instances to ensure that lock ordering is consistent across your entire pro- gram. Using open calls wherever possible simplifies this analysis substantially. With no non-open calls, finding instances where multiple locks are acquired is fairly easy, either by code review or by automated bytecode or source code anal- ysis. Timed lock attempts Another technique for detecting and recovering from deadlocks is to use the timed tryLock feature of the explicit Lock classes (see Chapter 13) instead of intrinsic locking. Where intrinsic locks wait forever if they cannot acquire the lock, explicit locks let you specify a timeout after which tryLock returns failure. JVM Thread dump including dead lock There are two threads trying to accquire two locks in different orders 123456789Java stack information for the threads listed above: "ApplicationServerThread ":at MumbleDBConnection.remove_statement- waiting to lock &lt;0x650f7f30&gt; (a MumbleDBConnection) at MumbleDBStatement.close- locked &lt;0x6024ffb0&gt; (a MumbleDBCallableStatement)..."ApplicationServerThread ":at MumbleDBCallableStatement.sendBatch- waiting to lock &lt;0x6024ffb0&gt; (a MumbleDBCallableStatement) at MumbleDBConnection.commit- locked &lt;0x650f7f30&gt; (a MumbleDBConnection) Other liveness hazards While deadlock is the most widely encountered liveness hazard, there are sev- eral other liveness hazards you may encounter in concurrent programs including starvation, missed signals, and livelock. Reference http://www.javamex.com/tutorials/threads/thread_scheduling.shtml http://www.javamex.com/tutorials/threads/priority.shtml http://www.javamex.com/tutorials/threads/how_threads_work.shtml http://www.javamex.com/tutorials/threads/thread_scheduling_2.shtml http://www.javamex.com/tutorials/threads/yield.shtml http://javarevisited.blogspot.in/2012/07/countdownlatch-example-in-java.html http://javarevisited.blogspot.sg/2012/05/how-to-use-threadlocal-in-java-benefits.html http://javarevisited.blogspot.com/2012/03/simpledateformat-in-java-is-not-thread.html http://javarevisited.blogspot.com/2012/05/counting-semaphore-example-in-java-5.html#ixzz4WRuTQFDF http://javarevisited.blogspot.in/2012/02/what-is-race-condition-in.html http://javarevisited.blogspot.in/2011/05/wait-notify-and-notifyall-in-java.html http://javarevisited.blogspot.in/2015/07/how-to-use-wait-notify-and-notifyall-in.html http://javarevisited.blogspot.in/2012/07/cyclicbarrier-example-java-5-concurrency-tutorial.html http://javarevisited.blogspot.in/2011/09/fork-join-task-java7-tutorial.html https://en.wikipedia.org/wiki/Dining_philosophers_problem http://javarevisited.blogspot.com/2011/11/collection-interview-questions-answers.html#ixzz4WTN72QPa http://javarevisited.blogspot.in/2011/11/collection-interview-questions-answers.html http://www.oracle.com/technetwork/articles/java/fork-join-422606.html http://tutorials.jenkov.com/java-util-concurrent/java-fork-and-join-forkjoinpool.html http://coopsoft.com/ar/CalamityArticle.html http://www.infoq.com/cn/articles/fork-join-introduction]]></content>
      <tags>
        <tag>Java</tag>
        <tag>Concurrent</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DevOps-Philosophy]]></title>
    <url>%2F2018-06-01-DevOps-Philosophy%2F</url>
    <content type="text"><![CDATA[DevOps Model DefinedDevOps is the combination of cultural philosophies, practices, and tools that increases an organization’s ability to deliver applications and services at high velocity: evolving and improving products at a faster pace than organizations using traditional software development and infrastructure management processes. This speed enables organizations to better serve their customers and compete more effectively in the market. What’s means to team These two teams are merged into a single team where the engineers work across the entire application lifecycle, from development and test to deployment to operations, and develop a range of skills not limited to a single function. DevSecOps In some DevOps models, quality assurance and security teams may also become more tightly integrated with development and operations and throughout the application lifecycle. When security is the focus of everyone on a DevOps team, this is sometimes referred to as DevSecOps. Benefits Velocity: microservices and continuous delivery let teams take ownership of services and then release updates to them quicker. Reliablity: CI, CD, automated testing Governmence: using infrastructure as code and policy as code, you can define and then track compliance at scale. mindsetDevOps Cultural PhilosophyTransitioning to DevOps requires a change in culture and mindset. At its simplest, DevOps is about removing the barriers between two traditionally siloed teams, development and operations. In some organizations, there may not even be separate development and operations teams; engineers may do both. Practicevery frequent but small releaseOne fundamental practice is to perform very frequent but small updates. This is how organizations innovate faster for their customers. These updates are usually more incremental in nature than the occasional updates performed under traditional release practices. Frequent but small updates make each deployment less risky. They help teams address bugs faster because teams can identify the last deployment that caused the error. microservicesuse a microservices architecture to make their applications more flexible and enable quicker innovation.This architecture reduces the coordination overhead of updating applications, and when each service is paired with small, agile teams who take ownership of each service, organizations can move more quickly. Tech ConceptsContinuous IntegrationContinuous integration is a software development practice where developers regularly merge their code changes into a central repository, after which automated builds and tests are run. The key goals of continuous integration are to find and address bugs quicker, improve software quality, and reduce the time it takes to validate and release new software updates. Continuous DeliveryContinuous delivery is a software development practice where code changes are automatically built, tested, and prepared for a release to production. It expands upon continuous integration by deploying all code changes to a testing environment and/or a production environment after the build stage. When continuous delivery is implemented properly, developers will always have a deployment-ready build artifact that has passed through a standardized test process. Monitoring and LoggingOrganizations monitor metrics and logs to see how application and infrastructure performance impacts the experience of their product’s end user. By capturing, categorizing, and then analyzing data and logs generated by applications and infrastructure, organizations understand how changes or updates impact users, shedding insights into the root causes of problems or unexpected changes. Active monitoring becomes increasingly important as services must be available 24/7 and as application and infrastructure update frequency increases. Creating alerts or performing real-time analysis of this data also helps organizations more proactively monitor their services. Communication and CollaborationIncreased communication and collaboration in an organization is one of the key cultural aspects of DevOps. The use of DevOps tooling and automation of the software delivery process establishes collaboration by physically bringing together the workflows and responsibilities of development and operations.]]></content>
  </entry>
  <entry>
    <title><![CDATA[A Facial Recognition utility in a dozen of LOC]]></title>
    <url>%2F2017-04-28-Facial-Recognition_en%2F</url>
    <content type="text"><![CDATA[A Facial Recognition utility in a dozen of python LOC (Lines Of Code)CV (Computer Vision)I have been soak myself in open sourced libraries, such as OpenCV. I gradually came to discern concepts such as Machine Learning , Deep Learning are not academic standing water. As a matter of fact, those elusive topics and certain pragmatic use cases could coalesce in a amount of interesting products. For instance, in past couple of months, there were a hype of guess-ages-by-photo, below screenshot depicts such. What a seductive one! Initially been attracted by such funky features, after second thoughts, I found at the heart of it is two cohesive parts, the first one is how to locate human faces from background and whole picture, consequently to have a ballpark age guess for the recognized the faces. You may guess how difficult to codify a program to implement the 1st feature. Actually no need chunks of code, at here purely a dozen of lines of code are necessitated (actually only 10 lines of code, excluding space line and comments). I’d like to piggyback on such tiny utility to elaborate advanced topics of Computer Visions. Faces recognitionActually face recognition is not new to us, this feature prevailing in so-called auto focus in DC (Digital Camera) and many main stream smart phone built-in cameras. Just like below photo. You can get a sense of how commonplace of face recognition , which is becoming a widely used technology around us. Theoretically speaking, face recognition is also called face detection, it’s a type of technology/program to electronically identify human frontal faces in digital images, such as photos, camera or surveillance. Further more, face detection is kind of objects detection in computer vision area. Which will locate object (e.g. human face) and get the size. My ‘10 LOC program’First of all, let’s have some visual and concrete feeling of this program, below screenshot is the source code. The whole program source code can be found at this github repository https://github.com/CloudsDocker/pyFacialRecognition . Please feel free to fork , check it out and have a try. I’ll walk through this program one line by one line at this blog. “You serious? This is all the problem, just aforementioned 10 lines?” Let’s first take a look at the actual run output. Here is the origional image Below is the result of execution of this tiny utilityPlease be advised the red rectangle around faces. Souce CodePrerequiteFirst of first, as you know, this program is composed by python,therefore, make sure you work station or laptop equiped with python, vesrion is irrelavant for this program. In addition, this utility is built upon OpenCV (http://opencv.org/downloads.html), therefore please install this component as well. Just as its name suggested, it is an open source framework focus on computer vision related deep learning, surfaced decades ago. This is one Intel lab built by Rusian, which is a very active community. Particulary, if you are Mac users, it’s recommended to use brew to setup OpenCV. Below is sample commands(The 1st line of following command may raise some errors, in that case please contact me via the link at the rear of this blog): 12brew tap homebrew/sciencebrew install opencv Upon completion of preceding scripts, you can execute following scripts to verify whether it’s installed success or not, e.g. it means all fine if no exception/errors raised 1&gt;&gt;&gt; import cv2 Souce Code DissectionLet’s dissect file recognizeFace_loose_en.py as one example 1import cv2,sys To import library of OpenCV and python built-in system library, which is used to parse input arguments. 1inputImageFile=sys.argv[1] To read the 1st argument, which to be the file name of the image to be parsed, e.g. test.jpg 1faceClassifier=cv2.CascadeClassifier('haarcascade_frontalface_default.xml') To load HAAR Casscade Classifier, the human face recognition cascade categorizer which shipped with OpenCV. Which will do the actual computation, logic to recognize and size human faces from any given images. Expansion of computer vision knowledgeWe stop here not reading further code, avoiding perplex you, I’ll walk through certain CV topics pertaining to this blog. As for more deep concepts, please feel free to contact me or goole by yourself. ClassifierIn arena of computer vision and machine learning, a variaty of classifiers been and being built, to assemle special domain knowledge to recognize corresponding objects. For example, there are particular classifier to recognize cars, there are plane classifier, and classifiers to recognize smile, eyes, etc. For our case, we need a specific classifier help us to detect and locate human faces. Conceps of objects recognizeGenerally speaking，, to recognize one object (such as human faces) means finding and identifying objects in an image or video sequence. However, it’s neccessitate tons of sample/specimen to train machine to learn, for instance, it’s likely thousands of hundreds of digital images/video will be prepared as learning material, while all of specimen should be categorized to two mutax type, positive or negative. e.g. phots containss human face and ones without human face. When machine read one photo, it was told this is either a positive one or negative one, then machine could gradually analysys and induce some common facets and persist to files for future usages, e.g. when given a new photo, the machine can classify it whether it’s a positive or negative. That’s why it’s called classifier. CascadeYour feeling is right, just as it’s name suggrested, cascade implies propagating something. In this case, it’s specifically means Cascade classifier. Intuitively the next question is why cascade is required? Let me try to articulate the underlying logic, as you know, at the heart of digital images, which is the raw material of computer vision, are pixel。For one CV process, it need to scan each pixel per pixel, while in contemporary world, size of image tend to incresing more than we expected, e.g. normall one photo taken by smart phone tend to contains millions of pixels. At the meanwhile, to fine tune and get a more accuate result of one object recognition, it tend to lots of classifiers to work from different point of views of the underlying photo. Therefore these two factors interwhirled together, the final number would be astronomical. Therefore, one innovative solution is cascade, in a nutshell, all classifiers will be splited to multiple layers, one photo will be examined by classifiers on 1st layer at the very begining, if failed, the whole CV can retain negative immediately, with fewest efforts and time cost, while majority of other classifiers won’t be executed in actual. This should significantely accelerate the whole process of CV. This is similar to FF(Fail Fast) in other areas,severed for sake of running efficiency. 1objImage=cv2.imread(inputImageFile) To create one OpenCV image object by loading the input digital file via OpenCV 1cvtImage=cv2.cvtColor(objImage,cv2.COLOR_BGR2GRAY) Firstly, convert the digital colorful image to grayscale one, which easy the task to scan and analyse the image. Actually this is quite common in image analys area. e.g. this could eliminate those noisy pixel from the picture. 1foundFaces=faceClassifier.detectMultiScale(cvtImage,scaleFactor=1.3,minNeighbors=9,minSize=(50,50),flags = cv2.cv.CV_HAAR_SCALE_IMAGE) Call method detectMultiScale to recongnize object, i.e. human face in this case. The parameters overview as below: scaleFactor: For a photo, particualy from selpie, some faces are shows bigger than rest of others, due to the distance between each faces and lens. Therefore this parameter is used to config the factor, please be advised this double should greater than 1.0 minNeighbors: Because it need to gradually scan the photo by a certain window, i.e. a rectangle. So this parameter is telling how many other object in the vacinity to be detected, before making final decision that it’s positive or negative. minSize：For aforementioend window, this parameter is setting the size of this rectangle. 1print(" Found &#123;&#125; human faces in this image".format(len(foundFaces))) To print how many faces detected, be reminded returned value is a list, each item is the actual position of every faces. Therefore, using len to print total number of ojects found. 12for (x,y,w,h) in foundFaces: cv2.rectangle(objImage,(x,y),(x+w,y+h),(0,0,255),2) Traverese all faces detected, please be noted returning object is consist of 4 parts, i.e. the horizontal and vertial position, width and height. Consequently to draw a rectangle by an off-the-shelf method from OpenCV. Be advised (0,0,255) represents color of the rectangel. It use R/G/B mode, e.g. black is (0,0,0)，white is (255,255,255)，etc. Well versed web programmer should be familiar with it. 12cv2.imshow('Detected human faces highlighted. Press any key to exit. ', objImage)cv2.waitKey(0) To display this image via opencv provided method imshow, together with the rectangles we draw previously The last one is one user hint, remind you can quit the applicaiton by press any key on the image display window In summaryWe’ve skimmed source codes and pertaining knowledge. This is just scratched the surface of this framework, hope this can open the door to more advanced topics and insights, such as hack of CAPTCHA, newly open sourced project form Yahoo, NSFW, Not Suitable for Work (NSFW)，to detect images with pornagraphy, etc. Finally，please be reminded all related source are open sourced at github repository https://github.com/CloudsDocker/pyFacialRecognition ，please fork and sync to your local disk, check it out and paly it. 123git clone https://github.com/CloudsDocker/pyFacialRecognition.gitcd pyFacialRecognition./run.sh You can access my blog. Any comments/suggestions, feel free to contact me. Contact me： phray.zhang@gmail.com (email，whatsapp, linkedin) helloworld_2000 (wechat) weibo: cloudsdocker github jianshu wechat：vibex Reference Object recognition OpenCV HAAR features Face Detection using Haar Cascades NSFW]]></content>
      <tags>
        <tag>Python</tag>
        <tag>MyBlog</tag>
        <tag>DeepLearning</tag>
        <tag>FacialRecognition</tag>
        <tag>MachineLearning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GIT useful scripts or error solutions]]></title>
    <url>%2F2018-05-24-GIT-scripts-bible-errors%2F</url>
    <content type="text"><![CDATA[Script bibleto list merge conflicts files in command lineYou can use either one of below three commands 123git diff --name-only --diff-filter=Ugit status --short | grep "^UU "git ls-files -u One line command to add, commit and push one changed file1git status --short | awk '&#123;split($0, a);print a[2]&#125;' | xargs git add &amp;&amp; git commit -m 'commit changes' &amp;&amp; git push origin BRANCH_NAME to show files commited but not pushed1git diff --stat --cached origin/feature/BRANCH_NAME to view file content changed1git show PATH/abc.sql show file change logsgit log is the powerful command for this kind of tasks, as below sample commands 123git log --pretty=format:"%h [%an] %s" --graphgit log --pretty=format:"%h [%an] %s" --graph --since=7.days %h means short hash %s is subject 123git log --pretty=format:"%h [%an] %s" --graph --since=7.days -S bower.json git log --pretty=format:"%h [%an] %s" --graph --since=7.days --grep Npmgit log --pretty=format:"%h [%an] %s" --graph --since=7.days --committer todd -S keyword_of_filter_files Get correct branch nameSometimes, if you checkout new branch with incorrect case. It still can check it out to local but you’ll get errors when you try to push it to remote. To solve this issue, please use following command to get correct branch to checkout 12git fetch &amp;&amp; git for-each-ref | grep -i 'THE KEY WORD' | awk '&#123;split($0,a);print a[3]&#125;'git checkout -b BRANCH_NAME_FROM_ABOVE Errorsfailed to push changeErrors as below 12fatal: unable to access 'https://tzhang@stash.xxx.com/scm/abc.git/': SSL certificate problem: self signed certificate in certificate chain Solutions:1git config --global http.sslVerify false]]></content>
      <tags>
        <tag>DevOps</tag>
      </tags>
  </entry>
</search>
