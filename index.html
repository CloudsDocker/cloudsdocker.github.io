<!DOCTYPE html>
<html lang="en">

<!-- Head tag -->
<head><meta name="generator" content="Hexo 3.9.0">

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!--Description-->
    
        <meta name="description" content="Contact me via phray.zhang@gmail.com or wechat at helloworld_2000">
    

    <!--Author-->
    
        <meta name="author" content="Todd Zhang">
    

    <!--Open Graph Title-->
    
        <meta property="og:title" content="Clouds &amp; Docker">
    

    <!--Open Graph Description-->
    
        <meta property="og:description" content="Contact me via phray.zhang@gmail.com or wechat at helloworld_2000">
    

    <!--Open Graph Site Name-->
    <meta property="og:site_name" content="Clouds &amp; Docker">

    <!--Type page-->
    
        <meta property="og:type" content="website">
    

    <!--Page Cover-->
    

    <meta name="twitter:card" content="summary">
    

    <!-- Title -->
    
    <title>Clouds &amp; Docker</title>

    <!-- Bootstrap Core CSS -->
    <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.2/css/bootstrap.min.css" integrity="sha384-y3tfxAZXuh4HwSYylfB+J125MxIs6mR5FOHamPBG064zB+AFeWH94NdvaCBm8qnd" crossorigin="anonymous">

    <!-- Custom Fonts -->
    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="//oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="//oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- Gallery -->
    <link href="//cdnjs.cloudflare.com/ajax/libs/featherlight/1.3.5/featherlight.min.css" type="text/css" rel="stylesheet">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/style.css">

    <!-- Google Analytics -->
    


</head>


<body>

<div class="bg-gradient"></div>
<div class="bg-pattern"></div>

<!-- Menu -->
<!--Menu Links and Overlay-->
<div class="menu-bg">
    <div class="menu-container">
        <ul>
            
            <li class="menu-item">
                <a href="/">
                    Home
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/archives">
                    Archives
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/about.html">
                    About
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/tags">
                    Tags
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/categories">
                    Categories
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/contact.html">
                    Contact
                </a>
            </li>
            
        </ul>
    </div>
</div>

<!--Hamburger Icon-->
<nav>
    <a href="#menu"></a>
</nav>

<div class="container">

    <!-- Main Content -->
    <div class="row">
    <div class="col-sm-12">

        <!--Title and Logo-->
        <header>
    <div class="logo">
        <a href="/"><i class="logo-icon fa fa-cube" aria-hidden="true"></i></a>
        
            <h1 id="main-title" class="title">Clouds & Docker</h1>
        
    </div>
</header>

        <section class="main">
            
                
<div class="post">

    <div class="post-header index">
        <h1 class="title">
            <a href="/2020-03-03-Algorithm-Leecode-1/">
                Algorithm notes from Leecode -- 1
            </a>
        </h1>
        <div class="post-info">
            
                <span class="date">2020-06-04</span>
            
            
            
        </div>
    </div>

    
        <div class="content">
            <p>Algorithm Leetcode</p>
<p>Links</p>
<ul>
<li><p><a href="https://www.dailycodingproblem.com/?ref=csdojo" target="_blank" rel="noopener">[https://www.dailycodingproblem.com/?ref=csdojo]{.underline}</a></p>
</li>
<li><p><a href="https://www.csdojo.io/#" target="_blank" rel="noopener">[https://www.csdojo.io/#]{.underline}</a></p>
</li>
<li><p><a href="https://github.com/mission-peace/interview/tree/master/src/com/interview/dynamic" target="_blank" rel="noopener">https://github.com/mission-peace/interview/tree/master/src/com/interview/dynamic</a></p>
</li>
<li><p><a href="https://leetcode.com/discuss/general-discussion/458695/dynamic-programming-patterns" target="_blank" rel="noopener">[https://leetcode.com/discuss/general-discussion/458695/dynamic-programming-patterns]{.underline}</a></p>
</li>
<li><p>daily coding problem book pdf free download</p>
</li>
</ul>
<p><a href="https://github.com/CyC2018/CS-Notes/blob/master/notes/Leetcode%20%E9%A2%98%E8%A7%A3%20-%20%E7%9B%AE%E5%BD%95.md" target="_blank" rel="noopener">[https://github.com/CyC2018/CS-Notes/blob/master/notes/Leetcode%20题解%20-%20目录.md]{.underline}</a></p>
<p>leetcodeGithub project in intelliJ</p>
<p>tasks to hands on</p>
<ul>
<li><p><del>0/1 knapsack</del></p>
</li>
<li><p><del>fibnachi memoized and bottom up approaches</del></p>
</li>
<li><p>median of two sorted array</p>
</li>
<li><p>64 minimum path sum</p>
</li>
<li></li>
<li><p>Maximum sub array (kadane algorithm)</p>
</li>
<li></li>
</ul>
<p>[Slide Window]</p>
<p>（1）没有重复字符的子字符的最大长度：给一个字符串，获得没有重复字符的最长子字符的长度</p>
<p>例子：</p>
<p>输入：&quot;abcbabcbb&quot;</p>
<p>输出：3</p>
<p>解释：因为没有重复字符的子字符是&#39;abc&#39;，所以长度是3</p>
<p>public class Solution {//时间复杂度O(2n)</p>
<p>//滑动窗口算法</p>
<p>public int <strong>[lengthOfLongestSubstring]{.underline}</strong>(String s) {</p>
<p>int n = s.length();</p>
<p>Set&lt;Character&gt; set = new HashSet&lt;&gt;();</p>
<p>int ans = 0, i = 0, j = 0;</p>
<p>while (i &lt; n &amp;&amp; j &lt; n)<br>{//窗口的左边是i，右边是j，下列算法将窗口的左右移动，截取出其中一段</p>
<p>// try to extend the range [i, j]</p>
<p>if<br>(!set.contains(s.charAt(j))){//如果set中不存在该字母，就将j+1，相当于窗口右边向右移动一格，左边不动</p>
<p>set.add(s.charAt(j++));</p>
<p>ans = Math.max(ans, j - i);//记录目前存在过的最大的子字符长度</p>
<p>}</p>
<p>else<br>{//如果set中存在该字母，则将窗口左边向右移动一格，右边不动，直到该窗口中不存在重复的字符</p>
<p>set.remove(s.charAt(i++));</p>
<p>}</p>
<p>}</p>
<p>return ans;</p>
<p>}</p>
<p>}</p>
<p>作者：DrXu</p>
<p>链接：<a href="https://juejin.im/post/5c74a2e2f265da2dea053355" target="_blank" rel="noopener">https://juejin.im/post/5c74a2e2f265da2dea053355</a></p>
<p>来源：掘金</p>
<p>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p>
<p>====</p>
<h3 id="解法3：优化的滑动窗口算法"><a href="#解法3：优化的滑动窗口算法" class="headerlink" title="解法3：优化的滑动窗口算法"></a><strong>解法3：优化的滑动窗口算法</strong></h3><p>上面的滑动窗口算法最多需要2n的步骤，但这其实是能被优化为只需要n步。我们可以使用HashMap定义字符到索引之间的映射，然后，当我们发现子字符串中的重复字符时，可以直接跳过遍历过的字符了。</p>
<p>（2）public class Solution {//时间复杂度o(n)</p>
<p>public int lengthOfLongestSubstring(String s) {</p>
<p>int n = s.length(), ans = 0;</p>
<p>//使用hashmap记录遍历过的字符的索引，当发现重复的字符时，可以将窗口的左边直接跳到该重复字符的索引处</p>
<p>Map&lt;Character, Integer&gt; map = new HashMap&lt;&gt;(); // current index of<br>character</p>
<p>// try to extend the range [i, j]</p>
<p>for (int j = 0, i = 0; j &lt; n; j++)<br>{//j负责向右边遍历，i根据重复字符的情况进行调整</p>
<p>if (map.containsKey(s.charAt(j)))<br>{//当发现重复的字符时,将字符的索引与窗口的左边进行对比，将窗口的左边直接跳到该重复字符的索引处</p>
<p>i = Math.max(map.get(s.charAt(j)), i);</p>
<p>}</p>
<p>//记录子字符串的最大的长度</p>
<p>ans = Math.max(ans, j - i + 1);</p>
<p>//map记录第一次遍历到key时的索引位置，j+1,保证i跳到不包含重复字母的位置</p>
<p>map.put(s.charAt(j), j + 1);</p>
<p>}</p>
<p>return ans;</p>
<p>}</p>
<p>}</p>
<p>作者：DrXu</p>
<p>链接：<a href="https://juejin.im/post/5c74a2e2f265da2dea053355" target="_blank" rel="noopener">https://juejin.im/post/5c74a2e2f265da2dea053355</a></p>
<p>来源：掘金</p>
<p>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p>
<p>[Slide Window]</p>
<p>Min Windows</p>
<p>------------- best solution---</p>
<p>（3）public static String <strong>minWindowBetter</strong>(String s, String t){</p>
<p>if(s==null||t==null|s.length()==0||t.length()==0){</p>
<p>return &quot;&quot;;</p>
<p>}</p>
<p>int left=0,right=0,count=0,min=Integer.MAX_VALUE;</p>
<p>int pool[] = new int[256];</p>
<p>String rtn=&quot;&quot;;</p>
<p>for(int i =0;i&lt;t.length();i++){</p>
<p>pool[t.charAt(i)]++;</p>
<p>}</p>
<p>while(right&lt;s.length()){</p>
<p>if(pool[s.charAt(right++)]--&gt;0){//[!]</p>
<p>// (a) if(pool[s.charAt(right++)]--&gt;=0), rather than<br>if(pool[right++]--&gt;=0)</p>
<p>// (b) this is &quot;&gt;0&quot;, but not &quot;&gt;=0&quot;</p>
<p>count++;</p>
<p>}</p>
<p>while(count==t.length()){</p>
<p>if((right-left)&lt;min){</p>
<p>min=right-left;</p>
<p>rtn=s.substring(left,right);</p>
<p>}</p>
<p>//shrink window</p>
<p>if(++pool[s.charAt(left++)]&gt;0){</p>
<p>count--;</p>
<p>}</p>
<p>}</p>
<p>}</p>
<p>return rtn;</p>
<p>}</p>
<p>10 lines code to solve most “substring” problem</p>
<p>I will first give the solution then show you the magic template.</p>
<p>The code of solving this problem is below. It might be the shortest<br>among all solutions provided in Discuss.</p>
<p>string minWindow(string s, string t) {</p>
<p>vector&lt;int&gt; map(128,0);</p>
<p>for(auto c: t) map[c]++;</p>
<p>int counter=t.size(), begin=0, end=0, d=INT_MAX, head=0;</p>
<p>while(end&lt;s.size()){</p>
<p>if(map[s[end++]]--&gt;0) counter--; //in t</p>
<p>while(counter==0){ //valid</p>
<p>if(end-begin&lt;d) d=end-(head=begin);</p>
<p>if(map[s[begin++]]++==0) counter++; //make it invalid</p>
<p>}</p>
<p>}</p>
<p>return d==INT_MAX? &quot;&quot;:s.substr(head, d);</p>
<p>}</p>
<p>Here comes the template.</p>
<p>For most substring problem, we are given a string and need to find a<br>substring of it which satisfy some restrictions. A general way is to use<br>a hashmap assisted with two pointers. The template is given below.</p>
<p>int findSubstring(string s){</p>
<p>vector&lt;int&gt; map(128,0);</p>
<p>int counter; // check whether the substring is valid</p>
<p>int begin=0, end=0; //two pointers, one point to tail and one head</p>
<p>int d; //the length of substring</p>
<p>for() { /* initialize the hash map here */ }</p>
<p>while(end&lt;s.size()){</p>
<p>if(map[s[end++]]-- ?){ /* modify counter here */ }</p>
<p>while(/* counter condition */){</p>
<p>/* update d here if finding minimum*/</p>
<p>//increase begin to make it invalid/valid again</p>
<p>if(map[s[begin++]]++ ?){ /*modify counter here*/ }</p>
<p>}</p>
<p>/* update d here if finding maximum*/</p>
<p>}</p>
<p>return d;</p>
<p>}</p>
<p>One thing needs to be mentioned is that when asked to find maximum<br>substring, we should update maximum after the inner while loop to<br>guarantee that the substring is valid. On the other hand, when asked to<br>find minimum substring, we should update minimum inside the inner while<br>loop.</p>
<p>The code of solving <strong>[Longest Substring with At Most Two Distinct<br>Characters]{.underline}</strong> is below:</p>
<p>（4）int lengthOfLongestSubstringTwoDistinct(string s) {</p>
<p>vector&lt;int&gt; map(128, 0);</p>
<p>int counter=0, begin=0, end=0, d=0;</p>
<p>while(end&lt;s.size()){</p>
<p>if(map[s[end++]]++==0) counter++;</p>
<p>while(counter&gt;2) if(map[s[begin++]]--==1) counter--;</p>
<p>d=max(d, end-begin);</p>
<p>}</p>
<p>return d;</p>
<p>}</p>
<p>The code of solving <strong>[Longest Substring Without Repeating<br>Characters]{.underline}</strong> is below:</p>
<p>Update 01.04.2016, thanks @weiyi3 for advice.</p>
<p><strong>[（5）]{.underline}</strong>int lengthOfLongestSubstring(string s) {</p>
<p>vector&lt;int&gt; map(128,0);</p>
<p>int counter=0, begin=0, end=0, d=0;</p>
<p>while(end&lt;s.size()){</p>
<p>if(map[s[end++]]++&gt;0) counter++;</p>
<p>while(counter&gt;0) if(map[s[begin++]]--&gt;1) counter--;</p>
<p>d=max(d, end-begin); //while valid, update d</p>
<p>}</p>
<p>return d;</p>
<p>}</p>
<p>I think this post deserves some upvotes! : )</p>
<p><strong>[[sliding window ]]{.underline}</strong></p>
<p>string minWindow(string s, string t) {</p>
<p>unordered_map&lt;char, int&gt; m;</p>
<p>// Statistic for count of char in t</p>
<p>for (auto c : t) m[c]++;</p>
<p>// counter represents the number of chars of t to be found in s.</p>
<p>size_t start = 0, end = 0, counter = t.size(), minStart = 0, minLen =<br>INT_MAX;</p>
<p>size_t size = s.size();</p>
<p>// Move to find a valid window.</p>
<p>while (end &lt; size) {</p>
<p>// If char in s exists in t, decrease counter</p>
<p>if (m[s[end]] &gt; 0)</p>
<p>counter--;</p>
<p>// Decrease m[s[end]]. If char does not exist in t, m[s[end]]<br>will be negative.</p>
<p>m[s[end]]--;</p>
<p>end++;</p>
<p>// When we find a valid window, the move starts to find a smaller<br>window.</p>
<p>while (counter == 0) {</p>
<p>if (end - start &lt; minLen) {</p>
<p>minStart = start;</p>
<p>minLen = end - start;</p>
<p>}</p>
<p>m[s[start]]++;</p>
<p>// When char exists in t, increase the counter.</p>
<p>if (m[s[start]] &gt; 0)</p>
<p>counter++;</p>
<p>start++;</p>
<p>}</p>
<p>}</p>
<p>if (minLen != INT_MAX)</p>
<p>return s.substr(minStart, minLen);</p>
<p>return &quot;&quot;;</p>
<p>}</p>
<p>～～～～java version～～</p>
<p>public String minWindow(String s, String t) {</p>
<p>HashMap&lt;Character,Integer&gt; map = new HashMap();</p>
<p>for(char c : s.toCharArray())</p>
<p>map.put(c,0);</p>
<p>for(char c : t.toCharArray())</p>
<p>{</p>
<p>if(map.containsKey(c))</p>
<p>map.put(c,map.get(c)+1);</p>
<p>else</p>
<p>return &quot;&quot;;</p>
<p>}</p>
<p>int start =0, end=0, minStart=0,minLen = Integer.MAX_VALUE, counter =<br>t.length();</p>
<p>while(end &lt; s.length())</p>
<p>{</p>
<p>char c1 = s.charAt(end);</p>
<p>if(map.get(c1) &gt; 0)</p>
<p>counter--;</p>
<p>map.put(c1,map.get(c1)-1);</p>
<p>end++;</p>
<p>while(counter == 0)</p>
<p>{</p>
<p>if(minLen &gt; end-start)</p>
<p>{</p>
<p>minLen = end-start;</p>
<p>minStart = start;</p>
<p>}</p>
<p>char c2 = s.charAt(start);</p>
<p>map.put(c2, map.get(c2)+1);</p>
<p>if(map.get(c2) &gt; 0)</p>
<p>counter++;</p>
<p>start++;</p>
<p>}</p>
<p>}</p>
<p>return minLen == Integer.MAX_VALUE ? &quot;&quot; :<br>s.substring(minStart,minStart+minLen);</p>
<p>}</p>
<p>[DP]</p>
<p><strong>[（6]{.underline}</strong>）longestCommonSubsequence</p>
<p>public int longestCommonSubsequence(String text1, String text2) {</p>
<p>// xx-est is meant for dynamic programming</p>
<p>// x keys for DP</p>
<p>// 1st, declare a DP table for bottom up</p>
<p>// 2nd set global value</p>
<p>// ================</p>
<p>// for top down, use memo</p>
<p>int m = text1.length(); //[!!!!] it&#39;s &quot;length()&quot; method for String</p>
<p>int n = text2.length();</p>
<p>int[][] memo = new int[m+1][n+1];</p>
<p>for(int i=0;i&lt;m;i++){</p>
<p>for(int j=0;j&lt;n;j++){</p>
<p>// if(i==0||j==0){ //[!!!!!222] here is no need, because default<br>value in array is zero</p>
<p>// // 1st col or 1st row, set to 0</p>
<p>// memo[i][j]=0;</p>
<p>// }else{</p>
<p>if(text1.charAt(i)==text2.charAt(j)){</p>
<p>memo[i+1][j+1] = 1 + memo[i][j];</p>
<p>}else{</p>
<p>// current char is different, so go to carry previous biggest value from<br>either left or up</p>
<p>memo[i+1][j+1] = Math.max(memo[i+1][j],memo[i][j+1]);</p>
<p>}</p>
<p>// }</p>
<p>}</p>
<p>}</p>
<p>return memo[m][n];</p>
<p>}</p>
<p><strong>[DP]</strong></p>
<p>LengthOfLIS</p>
<p><strong>[（7]{.underline}</strong>）public class LengthOfLIS {</p>
<p>System.out.println(&quot;===test failed case (DP)<br>:&quot;+inst.lengthOfLIS_tail(new int[]{4,10,4,3,8,9}));</p>
<p>System.out.println(&quot;===test failed case (DP)<br>:&quot;+inst.lengthOfLIS_tail(new int[]{2,2})); //expect output &quot;1&quot;</p>
<p>public int lengthOfLIS_naive(int[] nums){</p>
<p>//edge case</p>
<p>if(nums.length&lt;0){</p>
<p>return 0;</p>
<p>}</p>
<p>int m=nums.length;</p>
<p>int max=0; // global max</p>
<p>int[] dp=new int[m];</p>
<p>//embedded loop to search max value brute forcely</p>
<p>for (int i = 0; i &lt;m ; i++) {</p>
<p>// loop each digits</p>
<p>int localMax=0; // holder for MAX length of increase sequence before i</p>
<p>for (int j = 0; j &lt; i; j++) {</p>
<p>// loop to find all increasing BEFORE this number</p>
<p>if(dp[j]&gt;localMax &amp;&amp; nums[j]&lt;nums[i]){</p>
<p>// previous number is SMALLER than i and greater than local max, that<br>means it&#39;s increasing</p>
<p>localMax=dp[j];</p>
<p>}</p>
<p>}</p>
<p>// after looped ALL previous numbers, add current one</p>
<p>dp[i]=localMax+1;</p>
<p>max = Math.max(max,dp[i]);</p>
<p>}</p>
<p>return max;</p>
<p>}</p>
<p>public int <strong>[lengthOfLIS_tail]{.underline}</strong>(int[] nums){</p>
<p>int m=nums.length;</p>
<p>if(m==0) return 0;</p>
<p>int[] dp=new int[m]; // dp[x]=y : value &quot;y&quot; of dp stores &quot;the<br>last number&quot; (tail) of increasing sequence whose length is &quot;x&quot;</p>
<p>int maxLen=0;</p>
<p>dp[0]=nums[0];</p>
<p>//for loop each number in array</p>
<p>for (int i = 1; i &lt; m; i++) { //[!!!!!!!!1111111] it should start<br>with &quot;1&quot;, as &quot;0&quot; is already setup</p>
<p>// there are 3 scenarios we need to update dp</p>
<p>if(nums[i]&lt;dp[0]){</p>
<p>// current number is even smaller than most smallest LIS, update it</p>
<p>dp[0]=nums[i];</p>
<p>}else if(nums[i]&gt;dp[maxLen]){</p>
<p>//current number is greater than &#39;tail&#39; of largest LIS, then update<br>the last LIS</p>
<p>dp[++maxLen]=nums[i];</p>
<p>}else{</p>
<p>// current number is in the middle, so we go to find the *correct*<br>position to locate the LIS in DP</p>
<p>dp[binarySearchLIS(dp,0,maxLen,nums[i])]=nums[i];</p>
<p>}</p>
<p>}</p>
<p>return maxLen+1; // because dp is zero based, so add one for result</p>
<p>}</p>
<p>public int <strong>binarySearchLIS</strong>(int[] dp, int min, int max, int<br>target){</p>
<p>while(min&lt;=max){</p>
<p>int middle =min + (max-min)/2; //[!!!!!!!] don&#39;t forget to add prefix<br>&quot;min +&quot; in front of (max-min)/2</p>
<p>if(dp[middle]==target){</p>
<p>return middle;</p>
<p>}else if(dp[middle]&gt;target){</p>
<p>max=middle-1;</p>
<p>} else if(dp[middle]&lt;target){</p>
<p>min=middle+1;</p>
<p>}</p>
<p>}</p>
<p>return min;</p>
<p>}</p>
<p>}</p>
<p>[Graph]</p>
<p>RottingOrange</p>
<p><strong>[（8]{.underline}</strong>）public class GraphRottingOrange {</p>
<p>public static void main(String[] args) {</p>
<p>/*</p>
<p>In a given grid, each cell can have one of three values:</p>
<p>the value 0 representing an empty cell;</p>
<p>the value 1 representing a fresh orange;</p>
<p>the value 2 representing a rotten orange.</p>
<p>Every minute, any fresh orange that is adjacent (4-directionally) to a<br>rotten orange becomes rotten.</p>
<p>Return the minimum number of minutes that must elapse until no cell has<br>a fresh orange. If this is impossible, return -1 instead.</p>
<p>Example 1:</p>
<p>Input: [[2,1,1],[1,1,0],[0,1,1]]</p>
<p>Output: 4</p>
<p>Example 2:</p>
<p>Input: [[2,1,1],[0,1,1],[1,0,1]]</p>
<p>Output: -1</p>
<p>Explanation: The orange in the bottom left corner (row 2, column 0) is<br>never rotten, because rotting only happens 4-directionally.</p>
<p>Example 3:</p>
<p>Input: [[0,2]]</p>
<p>Output: 0</p>
<p>Explanation: Since there are already no fresh oranges at minute 0, the<br>answer is just 0.</p>
<p>Note:</p>
<p>1 &lt;= grid.length &lt;= 10</p>
<p>1 &lt;= grid[0].length &lt;= 10</p>
<p>grid[i][j] is only 0, 1, or 2.</p>
<p>*/</p>
<p>GraphRottingOrange inst = new GraphRottingOrange();</p>
<h1 id="deleted-2-D-array-due-to-hexo-error"><a href="#deleted-2-D-array-due-to-hexo-error" class="headerlink" title="deleted 2-D array due to hexo error"></a>deleted 2-D array due to hexo error</h1><p>System.out.println(&quot;===output of findRottenMinutes:&quot; +<br>inst.orangesRotting(grid));</p>
<p>}</p>
<p>public int orangesRotting(int[][] grid) {</p>
<p>int m = grid.length;</p>
<p>int n = grid[0].length;</p>
<p>List&lt;String&gt; listRotten = new ArrayList&lt;&gt;();</p>
<p>List&lt;String&gt; listFresh = new ArrayList&lt;&gt;();</p>
<p>int nMinutes = 0;</p>
<p>//firstly, find and enlist rotten ones</p>
<p>for (int i = 0; i &lt; m; i++) {</p>
<p>for (int j = 0; j &lt; n; j++) {</p>
<p>if (grid[i][j] == 2) {</p>
<p>//current cell is a rotten tomato, so check adjacent and contract them</p>
<p>listRotten.add(i + &quot;&quot; + j);</p>
<p>} else if (grid[i][j] == 1) {</p>
<p>// fresh tomato, to record it , so check zero of this list to confirm<br>ALL tomato got infected</p>
<p>listFresh.add(i + &quot;&quot; + j);</p>
<p>}</p>
<p>}</p>
<p>}</p>
<p>// loop until empty of fresh ones</p>
<p>while (!listFresh.isEmpty()) {</p>
<p>List&lt;String&gt; infected = new ArrayList&lt;&gt;();</p>
<p>for (String strRotten : listRotten) {</p>
<p>int x = strRotten.charAt(0) - &#39;0&#39;;</p>
<p>int y = strRotten.charAt(1) - &#39;0&#39;;</p>
<p>//to search 4 directions both vertically and horizontally</p>
<h1 id="deleted-2-D-array-due-to-hexo-error-1"><a href="#deleted-2-D-array-due-to-hexo-error-1" class="headerlink" title="deleted 2-D array due to hexo error"></a>deleted 2-D array due to hexo error</h1><p>for (int[] direction : directions) {</p>
<p>int newX = x + direction[0];</p>
<p>int newY = y + direction[1];</p>
<p>String newLoc = newX + &quot;&quot; + newY;</p>
<p>if (listFresh.contains(newLoc)) {</p>
<p>// make new tomato as rotten</p>
<p>listFresh.remove(newLoc);</p>
<p>// listRotten.add(newLoc);</p>
<p>infected.add(newLoc); // add to infected, rather than Rotten to avoid<br>&quot;ConcurrentModificationException&quot; as it&#39;s our loop list</p>
<p>}</p>
<p>}</p>
<p>}</p>
<p>// return -1 in case no more been infected</p>
<p>if (infected.isEmpty()) {</p>
<p>return -1;</p>
<p>}</p>
<p>// assign infected to listRotten to further check</p>
<p>listRotten=infected;</p>
<p>++nMinutes;</p>
<p>}</p>
<p>return nMinutes;</p>
<p>}</p>
<p>}</p>
<p>public int orangesRotting_Iterative(int[][] grid) {</p>
<p>if(grid == null || grid.length == 0) return 0;</p>
<p>int rows = grid.length;</p>
<p>int cols = grid[0].length;</p>
<p>Queue&lt;int[]&gt; queue = new LinkedList&lt;&gt;();</p>
<p>int count_fresh = 0;</p>
<p>//Put the position of all rotten oranges in queue</p>
<p>//count the number of fresh oranges</p>
<p>for(int i = 0 ; i &lt; rows ; i++) {</p>
<p>for(int j = 0 ; j &lt; cols ; j++) {</p>
<p>if(grid[i][j] == 2) {</p>
<p>queue.offer(new int[]{i , j});</p>
<p>}</p>
<p>else if(grid[i][j] == 1) {</p>
<p>count_fresh++;</p>
<p>}</p>
<p>}</p>
<p>}</p>
<p>//if count of fresh oranges is zero --&gt; return 0</p>
<p>if(count_fresh == 0) return 0;</p>
<p>int count = 0;</p>
<h1 id="deleted-2-D-array-due-to-hexo-error-2"><a href="#deleted-2-D-array-due-to-hexo-error-2" class="headerlink" title="deleted 2-D array due to hexo error"></a>deleted 2-D array due to hexo error</h1><p>//bfs starting from initially rotten oranges</p>
<p>while(!queue.isEmpty()) {</p>
<p>++count;</p>
<p>int size = queue.size();</p>
<p>for(int i = 0 ; i &lt; size ; i++) {</p>
<p>int[] point = queue.poll();</p>
<p>for(int dir[] : dirs) {</p>
<p>int x = point[0] + dir[0];</p>
<p>int y = point[1] + dir[1];</p>
<p>//if x or y is out of bound</p>
<p>//or the orange at (x , y) is already rotten</p>
<p>//or the cell at (x , y) is empty</p>
<p>//we do nothing</p>
<p>if(x &lt; 0 || y &lt; 0 || x &gt;= rows || y &gt;= cols ||<br>grid[x][y] == 0 || grid[x][y] == 2) continue;</p>
<p>//mark the orange at (x , y) as rotten</p>
<p>grid[x][y] = 2;</p>
<p>//put the new rotten orange at (x , y) in queue</p>
<p>queue.offer(new int[]{x , y});</p>
<p>//decrease the count of fresh oranges by 1</p>
<p>count_fresh--;</p>
<p>}</p>
<p>}</p>
<p>}</p>
<p>return count_fresh == 0 ? count-1 : -1;</p>
<p>}</p>
<p>[DP]</p>
<p>DecodeWays</p>
<p>package algo;</p>
<p><strong>[（9]{.underline}</strong>）public class DecodeWays {</p>
<p>public static void main(String[] args) {</p>
<p>/*</p>
<p>Similar questions:</p>
<p>62. Unique Paths</p>
<p>70. Climbing Stairs</p>
<p>509. Fibonacci Number</p>
<p>91. Decode Ways</p>
<p>A message containing letters from A-Z is being encoded to numbers using<br>the following mapping:</p>
<p>&#39;A&#39; -&gt; 1</p>
<p>&#39;B&#39; -&gt; 2</p>
<p>...</p>
<p>&#39;Z&#39; -&gt; 26</p>
<p>Given a non-empty string containing only digits, determine the total<br>number of ways to decode it.</p>
<p>Example 1:</p>
<p>Input: &quot;12&quot;</p>
<p>Output: 2</p>
<p>Explanation: It could be decoded as &quot;AB&quot; (1 2) or &quot;L&quot; (12).</p>
<p>Example 2:</p>
<p>Input: &quot;226&quot;</p>
<p>Output: 3</p>
<p>Explanation: It could be decoded as &quot;BZ&quot; (2 26), &quot;VF&quot; (22 6), or<br>&quot;BBF&quot; (2 2 6).</p>
<p>*/</p>
<p>DecodeWays inst = new DecodeWays();</p>
<p>System.out.println(&quot; decode ways: &quot;+ inst.numDecodings(&quot;12&quot;));</p>
<p>}</p>
<p>/*</p>
<p>&quot;&quot;&quot;</p>
<p>s = 123</p>
<p>build up from right =&gt;</p>
<p>num_ways (&quot;&quot;) =&gt; 1 (empty string can be represented by empty string)<br>(i.e. num_ways[n] = 1) NOTE: only for build up with a valid string.<br>Empty string on it&#39;s own doesn&#39;t need to be decoded.</p>
<p>num_ways (&quot;3&quot;) =&gt; 1 (only one way), i.e. num_ways[n-1] = 1</p>
<p>num_ways (&quot;23&quot;) =&gt; &quot;23&quot; or &quot;2&quot;-&quot;3&quot;,</p>
<p>num_ways (&quot;33&quot;) =&gt; &quot;3&quot;&quot;3&quot;</p>
<p>num_ways (&quot;123&quot;) =&gt; &quot;12&quot;(num_ways(&quot;3&quot;)) +<br>&quot;1&quot;(&quot;num_ways(&quot;23&quot;)) (i.e. num_ways[i+2] + num_ways[i+1])</p>
<p>num_ways (&quot;323&quot;) =&gt; &quot;3&quot;(num_ways(&quot;23&quot;)) (i.e. num_ways[i+1])</p>
<p>so basically if s[i:i+1] (both included) &lt;= 26,</p>
<p>num_ways[i+2] + num_ways[i+1]</p>
<p>else:</p>
<p>num_ways[i+1]</p>
<p>case with 0:</p>
<p>num_ways (&quot;103&quot;)</p>
<p>num_ways (&quot;3&quot;) =&gt; 1 (only one way)</p>
<p>num_ways (&quot;03&quot;) =&gt; 0 (can&#39;t decode 0)</p>
<p>num_ways (&quot;003&quot;) =&gt; &quot;00&quot;(num_ways(&quot;3&quot;)) +<br>&quot;0&quot;(num_ways(&quot;03&quot;)) =&gt; no way to decode &quot;00&quot; = 0 + 0</p>
<p>num_ways (&quot;103&quot;) =&gt; &quot;10&quot;(num_ways(&quot;3&quot;)) +<br>&quot;1&quot;(num_ways(&quot;03&quot;)) =&gt; num_ways[i+2] + num_ways[i+1](= 0 in<br>this case)</p>
<p>num_ways (&quot;1003&quot;) =&gt; &quot;10&quot;(num_ways(&quot;03&quot;)) +<br>&quot;1&quot;(num_ways(&quot;003&quot;)) =&gt; same eq = 0(no way to decode &quot;03&quot;) +<br>0(no way to decode 003)</p>
<p>Therefore, if i = &#39;0&#39;, let memo[i] = 0, also implements for a string<br>where the ith character == &#39;0&#39;, string[i:end] can be decoded in 0<br>ways.</p>
<p>&quot;&quot;&quot;</p>
<p>*/</p>
<p>// public class Solution {</p>
<p>public int numDecodings(String s) {</p>
<p>int n = s.length();</p>
<p>if (n == 0) return 0;</p>
<p>int[] memo = new int[n+1];</p>
<p>memo[n] = 1;</p>
<p>memo[n-1] = s.charAt(n-1) != &#39;0&#39; ? 1 : 0;</p>
<p>for (int i = n - 2; i &gt;= 0; i--)</p>
<p>if (s.charAt(i) == &#39;0&#39;) continue;</p>
<p>else memo[i] = (Integer.parseInt(s.substring(i,i+2))&lt;=26) ?<br>memo[i+1]+memo[i+2] : memo[i+1];</p>
<p>return memo[0];</p>
<p>}</p>
<p>// }</p>
<p>/*</p>
<p>Thank you so much for this clean and intuitive solution!!</p>
<p>I wrote some notes for myself reference, hope it might help someone to<br>understand this solution.</p>
<p>dp[i]: represents possible decode ways to the ith char(include i),<br>whose index in string is i-1</p>
<p>Base case: dp[0] = 1 is just for creating base; dp[1], when there&#39;s
one character, if it is not zero, it can only be 1 decode way. If it is<br>0, there will be no decode ways.</p>
<p>Here only need to look at at most two digits before i, cuz biggest valid<br>code is 26, which has two digits.</p>
<p>For dp[i]: to avoid index out of boundry, extract substring of<br>(i-1,i)- which is the ith char(index in String is i-1) and<br>substring(i-2, i)</p>
<p>First check if substring (i-1,i) is 0 or not. If it is 0, skip it,<br>continue right to check substring (i-2,i), cuz 0 can only be decode by<br>being together with the char before 0.</p>
<p>Second, check if substring (i-2,i) falls in 10~26. If it does, means<br>there are dp[i-2] more new decode ways.</p>
<p>Time: should be O(n), where n is the length of String</p>
<p>Space: should be O(n), where n is the length of String</p>
<p>*/</p>
<p>public int <strong>numDecodings_v2</strong>(String s) {</p>
<p>// this is one DP question, so create DP matrxi first</p>
<p>int[] dp = new int[s.length()+1];</p>
<p>// base case</p>
<p>dp[0]=1;</p>
<p>// for only one char, if first char is 0, which is not in the mapping<br>list, so return 0, otherwise return 1</p>
<p>dp[1]=s.charAt(0)==&#39;0&#39;?0:1;</p>
<p>int m=s.length();</p>
<p>for (int i = 2; i &lt;=m ; i++) {</p>
<p>int digitOne=Integer.valueOf(s.substring(i-1,i));</p>
<p>int digitTwo=Integer.valueOf(s.substring(i-2,i));</p>
<p>if(digitOne&gt;=1){</p>
<p>dp[i] = dp[i] +dp[i-1]; // add one to DP as take this single digit<br>into account</p>
<p>}</p>
<p>if(digitTwo&gt;=10 &amp;&amp; digitTwo&lt;=26){</p>
<p>dp[i] = dp[i] + dp[i-2];</p>
<p>}</p>
<p>}</p>
<p>return dp[m];</p>
<p>}</p>
<p>}</p>
<p>[DP]</p>
<p>class Solution {</p>
<p>public int coinChange(int[] coins, int amount) {</p>
<p>// this is one DP problem, so create matrix for number of fewest numbers<br>of coins to form the</p>
<p>int[] dp = new int[amount+1]; // index of array is the amount to be<br>calculated</p>
<p>Arrays.fill(dp,amount+1); // fill DP with *invalid* value so we can<br>update it to valid one late</p>
<p>//base case</p>
<p>dp[0]=0;</p>
<p>for(int i=0;i&lt;=amount;i++){ //[!!!] should be &quot;&lt;=&quot;, rather than<br>&quot;&lt;&quot;</p>
<p>for(int j=0;j&lt;coins.length;j++){</p>
<p>// if current coin is not greater than i (current amount to calculate<br>fewest number)</p>
<p>if(coins[j]&lt;=i){</p>
<p>// two options, do not take current change OR take current change</p>
<p>dp[i] = Math.min(dp[i], 1+dp[i-coins[j]]);</p>
<p>}</p>
<p>}</p>
<p>}</p>
<p>// if dp[amount] &gt; amount, it means it&#39;s amount+1, which is invalid</p>
<p>return dp[amount] &gt; amount ? -1:dp[amount];</p>
<p>}</p>
<p>}</p>
<p><strong>[[Recursive]]{.underline}</strong></p>
<p><strong>[Combination sum II]{.underline}</strong></p>
<p>Each number in candidates may only be used <strong>once</strong> in the combination.</p>
<p>Example 1:</p>
<p>Input: candidates = [10,1,2,7,6,1,5], target = 8,</p>
<p>A solution set is:</p>
<p>[</p>
<p>[1, 7],</p>
<p>[1, 2, 5],</p>
<p>[2, 6],</p>
<p>[1, 1, 6]</p>
<p>]</p>
<p>class Solution {</p>
<p>public List&lt;List&lt;Integer&gt;&gt; combinationSum2(int[] candidates, int<br>target) {</p>
<p>List&lt;List&lt;Integer&gt;&gt; result = new ArrayList&lt;&gt;();</p>
<p>Arrays.sort(candidates); // here is key to make array increasing</p>
<p>findCombination(candidates,0,target,new ArrayList&lt;&gt;(),result);</p>
<p>return result;</p>
<p>}</p>
<p>public void findCombination(int[] candidates, int idx, int target,<br>List&lt;&gt; current, List&lt;List&lt;&gt;&gt; result){</p>
<p>//base case</p>
<p>if(target == 0){</p>
<p>// found correct combination</p>
<p>result.add(current);</p>
<p>return; // should return right away after add</p>
<p>}</p>
<p>// base case 2</p>
<p>if(target&lt;0){</p>
<p>// last element lead to combination&gt;target</p>
<p>return;</p>
<p>}</p>
<p>for(int i=idx;i&lt;candidates.length;i++){</p>
<p>// loop to try combination by DFS</p>
<p>if(i==idx || candidates[i]!=candidates[i-1]){</p>
<p>// here is key for &quot;non dup element&quot;</p>
<p>// as first loop is always unique, no dup</p>
<p>// for non first loop, check it with previous value</p>
<p>current.add(candidates[i]); // Not same as previous one</p>
<p>findCombination(candidates,i+1, target-candidates[i], current,<br>result); // here will DFS try to keep on adding new element to current</p>
<p>current.remove(candidates.length-1);// when above line returned, it<br>means last element is too big</p>
<p>}</p>
<p>}</p>
<p>}</p>
<p>}</p>
<h2 id="reverse-integer-underline"><a href="#reverse-integer-underline" class="headerlink" title="[reverse integer]{.underline}"></a><strong>[reverse integer]{.underline}</strong></h2><p>class Solution {</p>
<p>public int reverse(int x) {</p>
<p>long res = 0;</p>
<p>while (x != 0) {</p>
<p>res *= 10;</p>
<p>res += x % 10;</p>
<p>x /= 10;</p>
<p>}</p>
<p>return (int)res == res ? (int)res : 0;</p>
<p>}</p>
<p>}</p>
<p>public class Solution {</p>
<p>public int reverse(int x) {</p>
<p>long result =0;</p>
<p>while(x != 0)</p>
<p>{</p>
<p>result = (result*10) + (x%10);</p>
<p>if(result &gt; Integer.MAX_VALUE) return 0;</p>
<p>if(result &lt; Integer.MIN_VALUE) return 0;</p>
<p>x = x/10;</p>
<p>}</p>
<p>return (int)result;</p>
<p>}</p>
<p>}</p>
<p><strong>Find median of two sorted array</strong></p>
<p>&lt;1&gt; Set imin = 0, imax = m, then start searching in [imin, imax]</p>
<p>&lt;2&gt; Set i = (imin + imax)/2, j = (m + n + 1)/2 - i</p>
<p>&lt;3&gt; Now we have len(left_part)==len(right_part). And there are only<br>3 situations</p>
<p>that we may encounter:</p>
<p>&lt;a&gt; B[j-1] &lt;= A[i] and A[i-1] &lt;= B[j]</p>
<p>Means we have found the object `i`, so stop searching.</p>
<p>&lt;b&gt; B[j-1] &gt; A[i]</p>
<p>Means A[i] is too small. We must `ajust` i to get `B[j-1] &lt;=
A[i]`.</p>
<p>Can we `increase` i?</p>
<p>Yes. Because when i is increased, j will be decreased.</p>
<p>So B[j-1] is decreased and A[i] is increased, and `B[j-1] &lt;=
A[i]` may</p>
<p>be satisfied.</p>
<p>Can we `decrease` i?</p>
<p>`No!` Because when i is decreased, j will be increased.</p>
<p>So B[j-1] is increased and A[i] is decreased, and B[j-1] &lt;=
A[i] will</p>
<p>be never satisfied.</p>
<p>So we must `increase` i. That is, we must ajust the searching range to</p>
<p>[i+1, imax]. So, set imin = i+1, and goto &lt;2&gt;.</p>
<p>&lt;c&gt; A[i-1] &gt; B[j]</p>
<p>Means A[i-1] is too big. And we must `decrease` i to get<br>`A[i-1]&lt;=B[j]`.</p>
<p>That is, we must ajust the searching range to [imin, i-1].</p>
<p>So, set imax = i-1, and goto &lt;2&gt;.</p>
<p>When the object i is found, the median is:</p>
<p>max(A[i-1], B[j-1]) (when m + n is odd)</p>
<p>or (max(A[i-1], B[j-1]) + min(A[i], B[j]))/2 (when m + n is<br>even)</p>
<p>Number of distinct Islands</p>
<p>private static int rows, cols;</p>
<h1 id="deleted-2-D-array-due-to-hexo-error-3"><a href="#deleted-2-D-array-due-to-hexo-error-3" class="headerlink" title="deleted 2-D array due to hexo error"></a>deleted 2-D array due to hexo error</h1><p>public int numDistinctIslands(int[][] grid) {</p>
<p>cols = grid[0].length;</p>
<p>rows = grid.length;</p>
<p>Set&lt;String&gt; uniqueShapes = new HashSet&lt;&gt;(); // Unique shpes.</p>
<p>StringBuilder shape;</p>
<p>for (int i = 0; i &lt; rows; i++) {</p>
<p>for (int j = 0; j &lt; cols; j++) {</p>
<p>if (grid[i][j] == 1) {</p>
<p>grid[i][j] = 0; // mark it as &#39;visited&#39;</p>
<p>shape = new StringBuilder(&quot;s&quot;); //&#39;s&#39; indicate Start</p>
<p>dfsTraversal(i, j, grid, shape);</p>
<p>uniqueShapes.add(shape.toString());</p>
<p>}</p>
<p>}</p>
<p>}</p>
<p>return uniqueShapes.size();</p>
<p>}</p>
<p>private static void dfsTraversal(int x, int y, int[][] matrix,<br>StringBuilder shape) {</p>
<p>for (int i = 0; i &lt; directions.length; i++) {</p>
<p>int nx = x + directions[i][0];</p>
<p>int ny = y + directions[i][1];</p>
<p>if (nx &gt;= 0 &amp;&amp; ny &gt;= 0 &amp;&amp; nx &lt; rows &amp;&amp; ny &lt; cols) {</p>
<p>if (matrix[nx][ny] == 1) {</p>
<p>matrix[nx][ny] = 0; // mark as &#39;visited&#39;</p>
<p>shape.append(i);</p>
<p>dfsTraversal(nx, ny, matrix, shape);</p>
<p>}</p>
<p>}</p>
<p>}</p>
<p>shape.append(&quot;_&quot;);</p>
<p>}</p>
<p>//=======</p>
<p>class Solution {</p>
<h1 id="deleted-2-D-array-due-to-hexo-error-4"><a href="#deleted-2-D-array-due-to-hexo-error-4" class="headerlink" title="deleted 2-D array due to hexo error"></a>deleted 2-D array due to hexo error</h1><p>public int numDistinctIslands(int[][] grid) {</p>
<p>Set&lt;String&gt; set= new HashSet&lt;&gt;();</p>
<p>int res=0;</p>
<p>for(int i=0;i&lt;grid.length;i++){</p>
<p>for(int j=0;j&lt;grid[0].length;j++){</p>
<p>if(grid[i][j]==1) {</p>
<p>StringBuilder sb= new StringBuilder();</p>
<p>helper(grid,i,j,0,0, sb);</p>
<p>String s=sb.toString();</p>
<p>if(!set.contains(s)){</p>
<p>res++;</p>
<p>set.add(s);</p>
<p>}</p>
<p>}</p>
<p>}</p>
<p>}</p>
<p>return res;</p>
<p>}</p>
<p>public void helper(int[][] grid,int i,int j, int xpos, int<br>ypos,StringBuilder sb){</p>
<p>grid[i][j]=0;</p>
<p>sb.append(xpos+&quot;&quot;+ypos);</p>
<p>for(int[] dir : dirs){</p>
<p>int x=i+dir[0];</p>
<p>int y=j+dir[1];</p>
<p>if(x&lt;0 || y&lt;0 || x&gt;=grid.length || y&gt;=grid[0].length ||<br>grid[x][y]==0) continue;</p>
<p>helper(grid,x,y,xpos+dir[0],ypos+dir[1],sb);</p>
<p>}</p>
<p>}</p>
<p>}</p>
<p>UPDATE: We can use direction string instead of using number string in<br>set.</p>
<p>Below is @wavy code using direction string.</p>
<p>public int numDistinctIslands(int[][] grid) {</p>
<p>Set&lt;String&gt; set = new HashSet&lt;&gt;();</p>
<p>for(int i = 0; i &lt; grid.length; i++) {</p>
<p>for(int j = 0; j &lt; grid[i].length; j++) {</p>
<p>if(grid[i][j] != 0) {</p>
<p>StringBuilder sb = new StringBuilder();</p>
<p>dfs(grid, i, j, sb, &quot;o&quot;); // origin</p>
<p>grid[i][j] = 0;</p>
<p>set.add(sb.toString());</p>
<p>}</p>
<p>}</p>
<p>}</p>
<p>return set.size();</p>
<p>}</p>
<p>private void dfs(int[][] grid, int i, int j, StringBuilder sb,<br>String dir) {</p>
<p>if(i &lt; 0 || i == grid.length || j &lt; 0 || j == grid[i].length</p>
<p>|| grid[i][j] == 0) return;</p>
<p>sb.append(dir);</p>
<p>grid[i][j] = 0;</p>
<p>dfs(grid, i-1, j, sb, &quot;u&quot;);</p>
<p>dfs(grid, i+1, j, sb, &quot;d&quot;);</p>
<p>dfs(grid, i, j-1, sb, &quot;l&quot;);</p>
<p>dfs(grid, i, j+1, sb, &quot;r&quot;);</p>
<p>sb.append(&quot;b&quot;); // back</p>
<p>}</p>
<ul>
<li>In a <strong>complete</strong> binary tree every level, *except possibly the<blockquote>
<p>last<em>, is completely filled, and all nodes in the last level are<br>as far left as possible. It can have between 1 and 2</em>^h^* nodes at<br>the last level<br><em>h</em>.<a href="https://en.wikipedia.org/wiki/Binary_tree#cite_note-complete_binary_tree-18" target="_blank" rel="noopener">^[18]^</a><br>An alternative definition is a perfect tree whose rightmost leaves<br>(perhaps all) have been removed. Some authors use the term<br><strong>complete</strong> to refer instead to a perfect binary tree as defined<br>below, in which case they call this type of tree (with a possibly<br>not filled last level) an <strong>almost complete</strong> binary tree or<br><strong>nearly complete</strong> binary<br>tree.^<a href="https://en.wikipedia.org/wiki/Binary_tree#cite_note-almost_complete_binary_tree-19" target="_blank" rel="noopener">[19]</a><a href="https://en.wikipedia.org/wiki/Binary_tree#cite_note-nearly_complete_binary_tree-20" target="_blank" rel="noopener">[20]</a>^
A complete binary tree can be efficiently represented using an<br>array.<a href="https://en.wikipedia.org/wiki/Binary_tree#cite_note-complete_binary_tree-18" target="_blank" rel="noopener">^[18]^</a></p>
</blockquote>
</li>
</ul>
<blockquote>
<p><img src="media/image1.png" alt>{width=”2.2916666666666665in”<br>height=”1.1944444444444444in”}</p>
<p>A complete binary tree (that is not full)</p>
</blockquote>
<ul>
<li>A <strong>perfect</strong> binary tree is a binary tree in which all interior<blockquote>
<p>nodes have two children <em>and</em> all leaves have the same <em>depth</em> or<br>same<br><em>level</em>.<a href="https://en.wikipedia.org/wiki/Binary_tree#cite_note-21" target="_blank" rel="noopener">^[21]^</a><br>An example of a perfect binary tree is the (non-incestuous)<br><a href="https://en.wikipedia.org/wiki/Ancestry_chart" target="_blank" rel="noopener">ancestry chart</a> of<br>a person to a given depth, as each person has exactly two<br>biological parents (one mother and one father). Provided the<br>ancestry chart always displays the mother and the father on the<br>same side for a given node, their sex can be seen as an analogy of<br>left and right children, <em>children</em> being understood here as an<br>algorithmic term. A perfect tree is therefore always complete but<br>a complete tree is not necessarily perfect.</p>
</blockquote>
</li>
</ul>
<p>Heap Tree is a special balanced binary tree data structure where root<br>node is compared with its children and averaged accordingly. There are<br>two type of trees, min heap tree and map heap tree.</p>
<p>For Min heap tree, it’s parent is either smaller or equals its childers.</p>
<h3 id="Get-Tree-Height"><a href="#Get-Tree-Height" class="headerlink" title="Get Tree Height"></a>Get Tree Height</h3><p>static int getHeight_recursive(TreeNode root){</p>
<p>if(root==null){</p>
<p>return 0;</p>
<p>}</p>
<p>return<br>Math.max(getHeight_recursive(root.left),getHeight_recursive(root.right))+1;<br>//[!!!!!] Here is the key point, it should add &quot;1&quot; at last</p>
<p>}</p>
<p>/*</p>
<p>The basic idea:</p>
<p>1. traverse layer by layer</p>
<p>2. For each layer, firslty get number of element,</p>
<p>3. Then add its left &amp; right child for each element</p>
<p>4. Increase height once all element of current layer finished</p>
<p>*/</p>
<p>static int getHeight_Iteratively(TreeNode root) {</p>
<p>int height=0;</p>
<p>Stack&lt;TreeNode&gt; stack=new Stack&lt;&gt;();</p>
<p>stack.add(root);</p>
<p>while(!stack.isEmpty()){</p>
<p>int numberOfSibling=stack.size();</p>
<p>// loop in all element in this layer till none is left</p>
<p>while(numberOfSibling-- &gt;0){</p>
<p>root = stack.pop();</p>
<p>// add current element&#39;s children</p>
<p>if(root.left!=null) stack.push(root.left);</p>
<p>if(root.right!=null) stack.push(root.right);</p>
<p>}</p>
<p>height++;</p>
<p>}</p>
<p>return height;</p>
<p>}</p>
<h2 id="InvertTree"><a href="#InvertTree" class="headerlink" title="InvertTree"></a>InvertTree</h2><p>package algo;</p>
<p>public class TreeInvertBST {</p>
<p>public static void main(String[] args) {</p>
<p>System.out.printf(&quot;===start===&quot;);</p>
<p>TreeNode root = invertTree(TreeNode.buildBSTTree());</p>
<p>System.out.printf(&quot;invert tree: &quot;+ root);</p>
<p>}</p>
<p>static TreeNode invertTree(TreeNode root){</p>
<p>if(root==null) return null;</p>
<p><strong>TreeNode tmpLeft = root.left;</strong></p>
<p><strong>root.left=invertTree(root.right);</strong></p>
<p><strong>root.right=invertTree(tmpLeft);</strong></p>
<p>return root;</p>
<p>}</p>
<p>}</p>
<h3 id="Number-of-islands-underline"><a href="#Number-of-islands-underline" class="headerlink" title="[Number of islands]{.underline}"></a><strong>[Number of islands]{.underline}</strong></h3><p>static int numberOfIslands(char[][] grid){</p>
<p>int number = 0;</p>
<p>if(grid==null || grid.length &lt;0 || grid[0].length&lt;0 ) {</p>
<p>return 0;</p>
<p>}</p>
<p>for (int i = 0; i &lt; grid.length; i++) {</p>
<p>for (int j = 0; j &lt;grid[i].length ; j++) {</p>
<p>if(grid[i][j]==&#39;1&#39;) {</p>
<p>// DFS to clear adjacent &quot;1&quot; to avoid dup counting</p>
<p>DFS(grid, i, j);</p>
<p>++number;</p>
<p>}</p>
<p>}</p>
<p>}</p>
<p>return number;</p>
<p>}</p>
<p>// the main purpose of calling DFS is to set “0” for all adjacent “1”<br>cells. As they all together to form one island</p>
<p>static void DFS(char[][] grid, int x, int y){</p>
<p>//edge case</p>
<p>if(grid==null || x&lt;0 || x &gt;= grid.length || y&lt;0 ||<br>y&gt;=grid[0].length <strong>|| grid[x][y]==&#39;0&#39;) { //[!!!!] should<br>&gt;= length, not &quot;&gt;&quot;</strong></p>
<p>// if(grid==null || x&lt;0 || x &gt; grid.length || y&lt;0 ||<br>y&gt;grid[0].length || grid[x][y]==0) {</p>
<p>// return if cursor node is NOT 1</p>
<p>return;</p>
<p>}</p>
<p>// means current cursor node is &quot;1&quot;</p>
<p>grid[x][y]=&#39;0&#39;; // mark this cell as visited</p>
<p>// check all adjacent cells</p>
<p>DFS(grid, x-1, y);</p>
<p>DFS(grid, x+1, y);</p>
<p>DFS(grid, x, y-1);</p>
<p>DFS(grid, x, y+1);</p>
<p>}</p>
<p>---------</p>
<p><strong>Is a same tree:</strong></p>
<p>static boolean isSameTree(TreeNode tree1, TreeNode tree2) {</p>
<p>// check base case, null checking</p>
<p>if(tree1==null || tree2 ==null){</p>
<p>return tree1 == tree2; // true when both null, false when only one is<br>null</p>
<p>}</p>
<p>/* same tress should be :</p>
<p>1. node data is same</p>
<p>2. left sub tree is same</p>
<p>4. right sub tree is same</p>
<p>*/</p>
<p>return tree1.val==tree2.val &amp;&amp; isSameTree(tree1.left,tree2.left) &amp;&amp;<br>isSameTree(tree1.right,tree2.right);</p>
<p>}</p>
<p><strong>Search BST</strong></p>
<p>public static TreeNode searchBST(TreeNode root, int val) {</p>
<p>if(root==null){</p>
<p>return null;</p>
<p>}</p>
<p>if(root.val==val){</p>
<p>return root;</p>
<p>}else if(val &gt; root.val){</p>
<p>return searchBST(root.right, val);</p>
<p>} else {</p>
<p>return searchBST(root.left,val);</p>
<p>}</p>
<p>}</p>
<p>public static TreeNode searchBST_Iterative(TreeNode root, int val) {</p>
<p>// recursive approach means recursively assgin/update variables</p>
<p>while(root != null &amp;&amp; root.val != val){</p>
<p>root = val&lt;root.val? root.left:root.right;</p>
<p>}</p>
<p>return root;</p>
<p>}</p>
<p>}</p>
<p><strong>Tree Traverse:</strong></p>
<p>static public List&lt;Integer&gt; <strong>inorderTraversal_better</strong>(TreeNode<br>root) {</p>
<p>List&lt;Integer&gt; listRtn = new ArrayList&lt;&gt;();</p>
<p>// for inorder trave iteratively we&#39;ll push/pop stacks</p>
<p>Stack&lt;TreeNode&gt; stack = new Stack&lt;&gt;();</p>
<p>// to determine when to push stack</p>
<p>// for inorder traverse, to push left first, then pop root, then last<br>right</p>
<p>while(root!=null || !stack.empty()) {</p>
<p>while(root!=null){</p>
<p>stack.push(root);</p>
<p>// keep on assign left to root for in order traverse</p>
<p>root=root.left;</p>
<p>}</p>
<p>root = stack.pop(); // pop up value of root</p>
<p>listRtn.add(root.val);</p>
<p>root=root.right;</p>
<p>}</p>
<p>return listRtn;</p>
<p>}</p>
<p>/*</p>
<p>This one is more intuitive</p>
<p>*/</p>
<p>static public List&lt;Integer&gt; <strong>preorderTraversal_better</strong>(TreeNode<br>root) {</p>
<p>List&lt;Integer&gt; listRtn = new ArrayList&lt;&gt;();</p>
<p>Stack&lt;TreeNode&gt; stack = new Stack&lt;&gt;();</p>
<p>stack.push(root);</p>
<p>while (!stack.empty()) {</p>
<p>root = stack.pop(); // pop up value of root</p>
<p>if (root != null) {</p>
<p>listRtn.add(root.val);</p>
<p>stack.push(root.right);</p>
<p>stack.push(root.left);</p>
<p>}</p>
<p>}</p>
<p>return listRtn;</p>
<p>}</p>
<p>static List&lt;Integer&gt; <strong>postOrderTraversal_stack_better</strong>(TreeNode<br>node) {</p>
<p>List&lt;Integer&gt; list = new ArrayList&lt;&gt;();</p>
<p>if(node==null) {</p>
<p>return list;</p>
<p>}</p>
<p>Stack&lt;TreeNode&gt; stack = new Stack&lt;&gt;();</p>
<p>stack.push(node);</p>
<p>while(!stack.isEmpty()){</p>
<p>node= stack.pop();</p>
<p>list.add(0, node.val); //[!!!!!!!!!!] here is key logic, add item at<br>postion &quot;0&quot; means at the begining</p>
<p>if(node.left!=null) stack.push(node.left);</p>
<p>if(node.right!=null) stack.push(node.right);</p>
<p>}</p>
<p>return list;</p>
<p>}</p>
<p>binaryTreeIsBST</p>
<p>/*</p>
<p>The key logic are:</p>
<p>1. assign two boundaries (lower , upper) for each node,</p>
<p>2. update upper to current node for its left child and lower for its<br>right child</p>
<p>3. recursively check each node</p>
<p>*/</p>
<p>static boolean binaryTreeIsBST(TreeNode node, int lower, int upper){</p>
<p>// for recursive, base case</p>
<p>// Number 1: base case is null return true</p>
<p>if(node==null) return true;</p>
<p>// Number 2: check data</p>
<p>if(node.val &lt; lower || node.val&gt;upper) {</p>
<p>System.out.printf(&quot;%s failed in BST check [%d,%d]: &quot;, node,<br>lower,upper);</p>
<p>return false;</p>
<p>}</p>
<p>// for left child node, it&#39;s value should between current&#39;s node&#39;s
lower boundary and current node&#39;s value</p>
<p>// for right child node, it&#39;s value should between current node&#39;s
value and current&#39;s node&#39;s upper boundary</p>
<p>return binaryTreeIsBST(node.left,lower,node.val) &amp;&amp;<br>binaryTreeIsBST(node.right,node.val, upper);</p>
<p>}</p>
<p><strong>Iteratively check binary tree is BST</strong>: (use inOrder search , only<br>replace list.add with checking pre)</p>
<p>public boolean isValidBST(TreeNode root) {</p>
<p>if (root == null) return true;</p>
<p>Stack&lt;TreeNode&gt; stack = new Stack&lt;&gt;();</p>
<p>TreeNode pre = null;</p>
<p>while (root != null || !stack.isEmpty()) {</p>
<p>while (root != null) {</p>
<p>stack.push(root);</p>
<p>root = root.left;</p>
<p>}</p>
<p>root = stack.pop();</p>
<p><strong>if(pre != null &amp;&amp; root.val &lt;= pre.val) return false;</strong></p>
<p><strong>pre = root;</strong></p>
<p>root = root.right;</p>
<p>}</p>
<p>return true;</p>
<p>}</p>
<p>======</p>
<p><strong>Kth smallest element</strong></p>
<p>public int kthSmallest(TreeNode root, int k) {</p>
<p>Stack&lt;TreeNode&gt; stack = new Stack&lt;&gt;();</p>
<p>while(root != null || !stack.isEmpty()) {</p>
<p>while(root != null) {</p>
<p>stack.push(root);</p>
<p>root = root.left;</p>
<p>}</p>
<p>root = stack.pop();</p>
<p><strong>if(--k == 0) break;</strong></p>
<p>root = root.right;</p>
<p>}</p>
<p>return root.val;</p>
<p>}</p>
<p>contrapositive</p>
<p>contradiction</p>
<p>cases</p>
<p>induction</p>
<p>Interview tips:</p>
<ul>
<li><p>Do not silent, ask” can I think for a second “</p>
</li>
<li><p>Think out loud</p>
</li>
<li><p>Use examples</p>
</li>
<li><p>Ask “ does that sound a good strategy” rather than write code right</p>
<blockquote>
<p>away</p>
</blockquote>
</li>
<li><p>Better naming variable. For dynamic programing. If you use memoized</p>
<blockquote>
<p>solution, better to name array as “// memorized solitoon memo[]</p>
</blockquote>
</li>
</ul>
<p>编辑距离问题就是给我们两个字符串 s1 和 s2，只能用三种操作，让我们把 s1<br>变成 s2，求最少的操作数。需要明确的是，不管是把 s1 变成 s2<br>还是反过来，结果都是一样的，所以后文就以 s1 变成 s2 举例。</p>
<p>前文「」说过，解决两个字符串的动态规划问题，一般都是用两个指针 i,j<br>分别指向两个字符串的最后，然后一步步往前走，缩小问题的规模。</p>
<p>一、动态规划解法</p>
<p>动态规划的核心设计思想是数学归纳法</p>
<p>总结一下动态规划的设计流程：</p>
<p>首先明确 dp<br>数组所存数据的含义。这步很重要，如果不得当或者不够清晰，会阻碍之后的步骤。</p>
<p>然后根据 dp 数组的定义，运用数学归纳法的思想，假设 $dp[0...i-1]$<br>都已知，想办法求出 $dp[i]$，一旦这一步完成，整个题目基本就解决了。</p>
<p>但如果无法完成这一步，很可能就是 dp 数组的定义不够恰当，需要重新定义 dp<br>数组的含义；或者可能是 dp<br>数组存储的信息还不够，不足以推出下一步的答案，需要把 dp<br>数组扩大成二维数组甚至三维数组。</p>
<p>最后想一想问题的 base case 是什么，以此来初始化 dp<br>数组，以保证算法正确运行。</p>
<p>最长递增子序列（Longest Increasing Subsequence，简写<br>LIS）是比较经典的一个问题，比较容易想到的是动态规划解法，时间复杂度<br>O(N^2)，</p>
<p>我们的定义是这样的：dp[i] 表示以 nums[i]<br>这个数结尾的最长递增子序列的长度。</p>
<p>根据刚才我们对 dp 数组的定义，现在想求 dp[5] 的值，也就是想求以<br>nums[5] 为结尾的最长递增子序列。</p>
<p>nums[5] = 3，既然是递增子序列，我们只要找到前面那些结尾比 3<br>小的子序列，然后把 3<br>接到最后，就可以形成一个新的递增子序列，而且这个新的子序列长度加一。</p>
<p>当然，可能形成很多种新的子序列，但是我们只要最长的，把最长子序列的长度作为<br>dp[5] 的值即可。</p>
<p>还有一个细节问题，dp 数组应该全部初始化为<br>1，因为子序列最少也要包含自己，所以长度最小为<br>1。下面我们看一下完整代码：</p>
<p>public int <strong>[lengthOfLIS]{.underline}</strong>(int[] nums) {</p>
<p>int[] dp = new int[nums.length];</p>
<p>// dp 数组全都初始化为 1</p>
<p>Arrays.fill(dp, 1);</p>
<p>for (int i = 0; i &lt; nums.length; i++) {</p>
<p>for (int j = 0; j &lt; i; j++) {</p>
<p>if (nums[i] &gt; nums[j])</p>
<p>dp[i] = Math.max(dp[i], dp[j] + 1);</p>
<p>}</p>
<p>}</p>
<p>int res = 0;</p>
<p>for (int i = 0; i &lt; dp.length; i++) {</p>
<p>res = Math.max(res, dp[i]);</p>
<p>}</p>
<p>return res;</p>
<p>}</p>
<p>public int lengthOfLIS(int[] nums) {</p>
<p>int[] top = new int[nums.length];</p>
<p>// 牌堆数初始化为 0</p>
<p>int piles = 0;</p>
<p>for (int i = 0; i &lt; nums.length; i++) {</p>
<p>// 要处理的扑克牌</p>
<p>int poker = nums[i];</p>
<p>/***** 搜索左侧边界的二分查找 *****/</p>
<p>int left = 0, right = piles;</p>
<p>while (left &lt; right) {</p>
<p>int mid = (left + right) / 2;</p>
<p>if (top[mid] &gt; poker) {</p>
<p>right = mid;</p>
<p>} else if (top[mid] &lt; poker) {</p>
<p>left = mid + 1;</p>
<p>} else {</p>
<p>right = mid;</p>
<p>}</p>
<p>}</p>
<p>/*********************************/</p>
<p>// 没找到合适的牌堆，新建一堆</p>
<p>if (left == piles) piles++;</p>
<p>// 把这张牌放到牌堆顶</p>
<p>top[left] = poker;</p>
<p>}</p>
<p>// 牌堆数就是 LIS 长度</p>
<p>return piles;</p>
<p>}</p>
<p>至此，二分查找的解法也讲解完毕。</p>
<p>这个解法确实很难想到。首先涉及数学证明，谁能想到按照这些规则执行，就能得到最长递增子序列呢？其次还有二分查找的运用，要是对二分查找的细节不清楚，给了思路也很难写对</p>
<p>/* Dynamic Programming Java implementation of LIS problem */</p>
<p>class LIS</p>
<p>{</p>
<p>/* lis() returns the length of the longest increasing</p>
<p>subsequence in arr[] of size n */</p>
<p>static int lis(int arr[],int n)</p>
<p>{</p>
<p>int lis[] = new int[n];</p>
<p>int i,j,max = 0;</p>
<p>/* Initialize LIS values for all indexes */</p>
<p>for ( i = 0; i &lt; n; i++ )</p>
<p>lis[i] = 1;</p>
<p>/* Compute optimized LIS values in bottom up manner */</p>
<p>for ( i = 1; i &lt; n; i++ )</p>
<p>for ( j = 0; j &lt; i; j++ )</p>
<p>if ( arr[i] &gt; arr[j] &amp;&amp; lis[i] &lt; lis[j] + 1)</p>
<p>lis[i] = lis[j] + 1;</p>
<p>/* Pick maximum of all LIS values */</p>
<p>for ( i = 0; i &lt; n; i++ )</p>
<p>if ( max &lt; lis[i] )</p>
<p>max = lis[i];</p>
<p>return max;</p>
<p>}</p>
<p>public static void main(String args[])</p>
<p>{</p>
<p>int arr[] = { 10, 22, 9, 33, 21, 50, 41, 60 };</p>
<p>int n = arr.length;</p>
<p>System.out.println(&quot;Length of lis is &quot; + lis( arr, n ) + &quot;\n&quot; );</p>
<p>}</p>
<p>}</p>
<p>/*This code is contributed by Raja</p>
<p>-----</p>
<p>Find number of days between two given dates</p>
<p>Given two dates, find total number of days between them. The count of<br>days must be calculated in O(1) time and O(1) auxiliary space.</p>
<p>Examples:</p>
<p>Input: dt1 = {10, 2, 2014}</p>
<p>dt2 = {10, 3, 2015}</p>
<p>Output: 393</p>
<p>dt1 represents &quot;10-Feb-2014&quot; and dt2 represents &quot;10-Mar-2015&quot;</p>
<p>The difference is 365 + 28</p>
<p>Input: dt1 = {10, 2, 2000}</p>
<p>dt2 = {10, 3, 2000}</p>
<p>Output: 29</p>
<p>Note that 2000 is a leap year</p>
<p>Input: dt1 = {10, 2, 2000}</p>
<p>dt2 = {10, 2, 2000}</p>
<p>Output: 0</p>
<p>Both dates are same</p>
<p>Input: dt1 = {1, 2, 2000};</p>
<p>dt2 = {1, 2, 2004};</p>
<p>Output: 1461</p>
<p>Number of days is 365*4 + 1</p>
<p>One Naive Solution is to start from dt1 and keep counting days till dt2<br>is reached. This solution requires more than O(1) time.</p>
<p>A Better and Simple solution is to count total number of days before dt1<br>from i.e., total days from 00/00/0000 to dt1, then count total number of<br>days before dt2. Finally return the difference between two counts.</p>
<p>Let the given two dates be &quot;1-Feb-2000&quot; and &quot;1-Feb-2004&quot;</p>
<p>dt1 = {1, 2, 2000};</p>
<p>dt2 = {1, 2, 2004};</p>
<p>Count number of days before dt1. Let this count be n1.</p>
<p>Every leap year adds one extra day (29 Feb) to total days.</p>
<p>n1 = 2000*365 + 31 + 1 + Number of leap years</p>
<p>Count of leap years for a date &#39;d/m/y&#39; can be calculated</p>
<p>using following formula:</p>
<p>Number leap years</p>
<p>= y/4 - y/100 + y/400 if m &gt; 2</p>
<p>= (y-1)/4 - (y-1)/100 + (y-1)/400 if m &lt;= 2</p>
<p>All above divisions must be done using integer arithmetic</p>
<p>so that the remainder is ignored.</p>
<p>For 01/01/2000, leap year count is 1999/4 - 1999/100</p>
<p>+ 1999/400 which is 499 - 19 + 4 = 484</p>
<p>Therefore n1 is 2000*365 + 31 + 1 + 484</p>
<p>Similarly, count number of days before dt2. Let this</p>
<p>count be n2.</p>
<p>Finally return n2-n1</p>
<hr>
<p>// Java program two find number of</p>
<p>// days between two given dates</p>
<p>class GFG</p>
<p>{</p>
<p>// A date has day &#39;d&#39;, month &#39;m&#39; and year &#39;y&#39;</p>
<p>static class Date</p>
<p>{</p>
<p>int d, m, y;</p>
<p>public Date(int d, int m, int y)</p>
<p>{</p>
<p>this.d = d;</p>
<p>this.m = m;</p>
<p>this.y = y;</p>
<p>}</p>
<p>};</p>
<p>// To store number of days in</p>
<p>// all months from January to Dec.</p>
<p>static int monthDays[] = {31, 28, 31, 30, 31, 30,</p>
<p>31, 31, 30, 31, 30, 31};</p>
<p>// This function counts number of</p>
<p>// leap years before the given date</p>
<p>static int countLeapYears(Date d)</p>
<p>{</p>
<p>int years = d.y;</p>
<p>// Check if the current year needs to be considered</p>
<p>// for the count of leap years or not</p>
<p>if (d.m &lt;= 2)</p>
<p>{</p>
<p>years--;</p>
<p>}</p>
<p>// An year is a leap year if it is a multiple of 4,</p>
<p>// multiple of 400 and not a multiple of 100.</p>
<p>return years / 4 - years / 100 + years / 400;</p>
<p>}</p>
<p>// This function returns number</p>
<p>// of days between two given dates</p>
<p>static int getDifference(Date dt1, Date dt2)</p>
<p>{</p>
<p>// COUNT TOTAL NUMBER OF DAYS BEFORE FIRST DATE &#39;dt1&#39;</p>
<p>// initialize count using years and day</p>
<p>int n1 = dt1.y * 365 + dt1.d;</p>
<p>// Add days for months in given date</p>
<p>for (int i = 0; i &lt; dt1.m - 1; i++)</p>
<p>{</p>
<p>n1 += monthDays[i];</p>
<p>}</p>
<p>// Since every leap year is of 366 days,</p>
<p>// Add a day for every leap year</p>
<p>n1 += countLeapYears(dt1);</p>
<p>// SIMILARLY, COUNT TOTAL NUMBER OF DAYS BEFORE &#39;dt2&#39;</p>
<p>int n2 = dt2.y * 365 + dt2.d;</p>
<p>for (int i = 0; i &lt; dt2.m - 1; i++)</p>
<p>{</p>
<p>n2 += monthDays[i];</p>
<p>}</p>
<p>n2 += countLeapYears(dt2);</p>
<p>// return difference between two counts</p>
<p>return (n2 - n1);</p>
<p>}</p>
<p>// Driver code</p>
<p>public static void main(String[] args)</p>
<p>{</p>
<p>Date dt1 = new Date(1, 2, 2000);</p>
<p>Date dt2 = new Date(1, 2, 2004);</p>
<p>System.out.println(&quot;Difference between two dates is &quot; +</p>
<p>getDifference(dt1, dt2));</p>
<p>}</p>
<p>}</p>
<p>Last Edit: 6 hours ago</p>
<p>karansingh1559</p>
<p>karansingh1559</p>
<p>179</p>
<p>I am trying to compile a list of DP questions commonly asked in<br>interviews. This will help me and others trying to get better at DP. The<br>list will be sorted by difficulty. If you&#39;ve come across DP questions,<br>do mention them in the comments.</p>
<p>EASY:</p>
<p>121. Best time to buy and sell stock</p>
<p>198. House Robber</p>
<p>256. Paint House</p>
<p>MEDIUM:</p>
<p>63. Unique Paths II</p>
<p>64. Minimum Path Sum</p>
<p>91. Decode Ways</p>
<p>139. Word Break</p>
<p>221. Maximal Square</p>
<p>300. Longest Increasing Subsequence</p>
<p>322. Coin Change</p>
<p>464. Can I Win</p>
<p>474. Ones and Zeroes</p>
<p>516. Longest Palindromic Subsequence</p>
<p>698. Partition to K Equal Sum Subsets</p>
<p>787. Cheapest Flights Within K Stops</p>
<p>1027. Longest Arithmetic Sequence</p>
<p>1049. Last Stone Weight II</p>
<p>1105. Filling Bookcase Shelves</p>
<p>1143. Longest Common Subsequence</p>
<p>1155. Dice Roll Sum</p>
<p>HARD:</p>
<p>32. Longest Valid Parantheses</p>
<p>44. Wildcard Matching</p>
<p>72. Edit Distance</p>
<p>123. Best Time to Buy and Sell Stock III</p>
<p>312. Burst Balloons</p>
<p>1000. Minimum Cost to Merge Stones</p>
<p>1335. Minimum Difficulty of a Job Schedule</p>
<p>dynamic programming</p>
<p><strong>Minimum (Maximum) Path to Reach a Target</strong></p>
<p><strong>Generate problem statement for this pattern</strong></p>
<p>Statement</p>
<p>Given a target find minimum (maximum) cost / path / sum to reach the<br>target.</p>
<p>Approach</p>
<p>Choose minimum (maximum) path among all possible paths before the<br>current state, then add value for the current state.</p>
<p>routes[i] = min(routes[i-1], routes[i-2], ... , routes[i-k]) +<br>cost[i]</p>
<p>Generate optimal solutions for all values in the target and return the<br>value for the target.</p>
<p>for (int i = 1; i &lt;= target; ++i) {</p>
<p>for (int j = 0; j &lt; ways.size(); ++j) {</p>
<p>if (ways[j] &lt;= i) {</p>
<p>dp[i] = min(dp[i], dp[i - ways[j]]) + cost / path / sum;</p>
<p>}</p>
<p>}</p>
<p>}</p>
<p>return dp[target]</p>
<p>Similar Problems</p>
<p>746. <strong>[Min Cost Climbing Stairs Easy]{.underline}</strong></p>
<p>for (int i = 2; i &lt;= n; ++i) {</p>
<p>dp[i] = min(dp[i-1], dp[i-2]) + (i == n ? 0 : cost[i]);</p>
<p>}</p>
<p>return dp[n]</p>
<p>64. Minimum Path Sum Medium</p>
<p>for (int i = 1; i &lt; n; ++i) {</p>
<p>for (int j = 1; j &lt; m; ++j) {</p>
<p>grid[i][j] = min(grid[i-1][j], grid[i][j-1]) +<br>grid[i][j];</p>
<p>}</p>
<p>}</p>
<p>return grid[n-1][m-1]</p>
<p>322. Coin Change Medium</p>
<p>for (int j = 1; j &lt;= amount; ++j) {</p>
<p>for (int i = 0; i &lt; coins.size(); ++i) {</p>
<p>if (coins[i] &lt;= j) {</p>
<p>dp[j] = min(dp[j], dp[j - coins[i]] + 1);</p>
<p>}</p>
<p>}</p>
<p>}</p>
<h1 id="Tree"><a href="#Tree" class="headerlink" title="Tree"></a>Tree</h1><h2 id="Find-minimum-path"><a href="#Find-minimum-path" class="headerlink" title="Find minimum path"></a>Find minimum path</h2><p>class Solution {</p>
<p>public int minDepth(TreeNode root) {</p>
<p>if(root == null) return 0; // base case</p>
<p>int left = minDepth(root.left); // get depth of left</p>
<p>int right = minDepth(root.right); // get depth of right</p>
<p>if(root.left == null) return right + 1; // leaf nodes are in right<br>subtree</p>
<p>if(root.right == null) return left + 1; // leaf nodes are in left<br>subtree</p>
<p>// if left/right subtrees both contains leaf nodes</p>
<p>return Math.min(left, right) + 1;</p>
<p>}</p>
<p>}</p>
<p>Get min depth in Iterative approach</p>
<p>public int minDepth(TreeNode root) {</p>
<p>if(root == null)</p>
<p>return 0;</p>
<p>Queue&lt;TreeNode&gt; que = new LinkedList();</p>
<p>int level =1;</p>
<p>que.add(root);</p>
<p>while(!que.isEmpty()){</p>
<p>int size = que.size();</p>
<p>while(size&gt;0){</p>
<p>TreeNode node =que.poll();</p>
<p>if(node.left == null &amp;&amp; node.right ==null)</p>
<p>return level;</p>
<p>if(node.left != null)</p>
<p>que.add(node.left);</p>
<p>if(node.right != null)</p>
<p>que.add(node.right);</p>
<p>size--;</p>
<p>}</p>
<p>level++;</p>
<p>}</p>
<p>return level;</p>
<p>}</p>
<p>=================</p>
<p>two solutions with explanation: DFS &amp; BFS:</p>
<p>/** Solution 1: DFS</p>
<p>* Key point:</p>
<p>* if a node only has one child -&gt; MUST return the depth of the side<br>with child, i.e. MAX(left, right) + 1</p>
<p>* if a node has two children on both side -&gt; return min depth of two<br>sides, i.e. MIN(left, right) + 1</p>
<p>* */</p>
<p>public int minDepth(TreeNode root) {</p>
<p>if (root == null) {</p>
<p>return 0;</p>
<p>}</p>
<p>int left = minDepth(root.left);</p>
<p>int right = minDepth(root.right);</p>
<p>if (left == 0 || right == 0) {</p>
<p>return Math.max(left, right) + 1;</p>
<p>}</p>
<p>else {</p>
<p>return Math.min(left, right) + 1;</p>
<p>}</p>
<p>}</p>
<p>/** Solution 2: BFS level order traversal */</p>
<p>public int minDepth2(TreeNode root) {</p>
<p>if (root == null) {</p>
<p>return 0;</p>
<p>}</p>
<p>Queue&lt;TreeNode&gt; queue = new LinkedList&lt;&gt;();</p>
<p>queue.offer(root);</p>
<p>int level = 1;</p>
<p>while (!queue.isEmpty()) {</p>
<p>int size = queue.size();</p>
<p>for (int i = 0; i &lt; size; i++) {</p>
<p>TreeNode curNode = queue.poll();</p>
<p>if (curNode.left == null &amp;&amp; curNode.right == null) {</p>
<p>return level;</p>
<p>}</p>
<p>if (curNode.left != null) {</p>
<p>queue.offer(curNode.left);</p>
<p>}</p>
<p>if (curNode.right != null) {</p>
<p>queue.offer(curNode.right);</p>
<p>}</p>
<p>}</p>
<p>level++;</p>
<p>}</p>
<p>return level;</p>
<p>}</p>
<p>Preorder Traversal</p>
<p>In preorder traversal, we traverse the root first, then the left and<br>right subtrees.</p>
<p>We can simply implement preorder traversal using recursion:</p>
<p>public void traversePreOrder(Node node) {</p>
<p>if (node != null) {</p>
<p>visit(node.value);</p>
<p>traversePreOrder(node.left);</p>
<p>traversePreOrder(node.right);</p>
<p>}</p>
<p>}</p>
<p>We can also implement preorder traversal without recursion.</p>
<p>To implement an iterative preorder traversal, we&#39;ll need a Stack, and<br>we&#39;ll go through these steps:</p>
<p>Push root in our stack</p>
<p>While stack is not empty</p>
<p>Pop current node</p>
<p>Visit current node</p>
<p>Push right child, then left child to stack</p>
<p>public void traversePreOrderWithoutRecursion() {</p>
<p>Stack&lt;Node&gt; stack = new Stack&lt;Node&gt;();</p>
<p>Node current = root;</p>
<p>stack.push(root);</p>
<p>while(!stack.isEmpty()) {</p>
<p>current = stack.pop();</p>
<p>visit(current.value);</p>
<p>if(current.right != null) {</p>
<p>stack.push(current.right);</p>
<p>}</p>
<p>if(current.left != null) {</p>
<p>stack.push(current.left);</p>
<p>}</p>
<p>}</p>
<p>}</p>
<p>Convert Sorted List to Binary Search Tree</p>
<p>Medium</p>
<p>1503</p>
<p>79</p>
<p>Add to List</p>
<p>Share</p>
<p>Given a singly linked list where elements are sorted in ascending order,<br>convert it to a height balanced BST.</p>
<p>For this problem, a height-balanced binary tree is defined as a binary<br>tree in which the depth of the two subtrees of <em>every</em> node never differ<br>by more than 1.</p>
<p>Example:</p>
<p>Given the sorted linked list: [-10,-3,0,5,9],</p>
<p>One possible answer is: [0,-3,9,-10,null,5], which represents the<br>following height balanced BST:</p>
<p>0</p>
<p>/ \</p>
<p>-3 9</p>
<p>/ /</p>
<p>-10 5</p>
<p><strong>breadth first search</strong></p>
<p>First of all, let&#39;s reuse the algorithm from above, adapted to the new<br>structure:</p>
<p>public static &lt;T&gt; Optional&lt;Node&lt;T&gt;&gt; search(T value, Node&lt;T&gt;<br>start) {</p>
<p>Queue&lt;Node&lt;T&gt;&gt; queue = new ArrayDeque&lt;&gt;();</p>
<p>queue.add(start);</p>
<p>Node&lt;T&gt; currentNode;</p>
<p>while (!queue.isEmpty()) {</p>
<p>currentNode = queue.remove();</p>
<p>if (currentNode.getValue().equals(value)) {</p>
<p>return Optional.of(currentNode);</p>
<p>} else {</p>
<p>queue.addAll(currentNode.getNeighbors());</p>
<p>}</p>
<p>}</p>
<p>return Optional.empty();</p>
<p>}</p>
<h1 id="Binary-search-underline"><a href="#Binary-search-underline" class="headerlink" title="[Binary search]{.underline} "></a>[Binary search]{.underline} </h1><p><strong>分析二分查找的一个技巧是：不要出现 else，而是把所有情况用 else if<br>写清楚，这样可以清楚地展现所有细节</strong>。</p>
<p>------</p>
<p>Sliding window</p>
<p>In any sliding window based problem we have two pointers. One <em>right</em><br>pointer whose job is to expand the current window and then we have the<br><em>left</em> pointer whose job is to contract a given window. At any point in<br>time only one of these pointers move and the other one remains fixed.</p>
<p>Smallest window contains sub string</p>
<p>public class Solution {</p>
<p>public String minWindow(String s, String t) {</p>
<p>if(s == null || s.length() &lt; t.length() || s.length() == 0){</p>
<p>return &quot;&quot;;</p>
<p>}</p>
<p>HashMap&lt;Character,Integer&gt; map = new HashMap&lt;Character,Integer&gt;();</p>
<p>for(char c : t.toCharArray()){</p>
<p>if(map.containsKey(c)){</p>
<p>map.put(c,map.get(c)+1);</p>
<p>}else{</p>
<p>map.put(c,1);</p>
<p>}</p>
<p>}</p>
<p>int left = 0;</p>
<p>int minLeft = 0;</p>
<p>int minLen = s.length()+1;</p>
<p>int count = 0;</p>
<p>for(int right = 0; right &lt; s.length(); right++){</p>
<p>if(map.containsKey(s.charAt(right))){</p>
<p>map.put(s.charAt(right),map.get(s.charAt(right))-1);</p>
<p>if(map.get(s.charAt(right)) &gt;= 0){</p>
<p>count ++;</p>
<p>}</p>
<p>while(count == t.length()){</p>
<p>if(right-left+1 &lt; minLen){</p>
<p>minLeft = left;</p>
<p>minLen = right-left+1;</p>
<p>}</p>
<p>if(map.containsKey(s.charAt(left))){</p>
<p>map.put(s.charAt(left),map.get(s.charAt(left))+1);</p>
<p>if(map.get(s.charAt(left)) &gt; 0){</p>
<p>count --;</p>
<p>}</p>
<p>}</p>
<p>left ++ ;</p>
<p>}</p>
<p>}</p>
<p>}</p>
<p>if(minLen&gt;s.length())</p>
<p>{</p>
<p>return &quot;&quot;;</p>
<p>}</p>
<p>return s.substring(minLeft,minLeft+minLen);</p>
<p>}</p>
<p>}</p>
<p>--</p>
<p>public static String minWindowOp(String s, String t) {</p>
<p>int [] map = new int[128];//map to track number of occurrence of<br>each character of sub string</p>
<p>for (char c : t.toCharArray()) {</p>
<p>map[c]++;</p>
<p>}</p>
<p>int start = 0, end = 0, minStart = 0, minLen = Integer.MAX_VALUE,<br>counter = t.length();</p>
<p>// counter is number of distinct chars in sub string</p>
<p>while (end &lt; s.length()) {</p>
<p>final char c1 = s.charAt(end);// walk through each char in source string</p>
<p>if (map[c1] &gt; 0) {</p>
<p>counter--; // if cached char number greater than 0, decrease counter</p>
<p>}</p>
<p>map[c1]--;//decrease cached char number, for chars not in substring,<br>it will be negative</p>
<p>end++; //move right pointer</p>
<p>while (counter == 0) { //counter is zero means all chars found</p>
<p>if (minLen &gt; end - start) { //to find and cache minimum sliding window<br>length and minimum start</p>
<p>minLen = end - start;</p>
<p>minStart = start;</p>
<p>}</p>
<p>final char c2 = s.charAt(start);</p>
<p>map[c2]++;// A is -2， B is 1</p>
<p>if (map[c2] &gt; 0) {</p>
<p>counter++; //if current char exist in cache, increase counter, otherwise<br>keep counter zero</p>
<p>}</p>
<p>start++;</p>
<p>}</p>
<p>}</p>
<p>return minLen == Integer.MAX_VALUE ? &quot;&quot; : s.substring(minStart,<br>minStart + minLen);</p>
<p>}</p>
<p>------</p>
<p>I agree with your code, but I prefer this code when count == t.length(),</p>
<p>class Solution {</p>
<p>public String minWindow(String s, String t) {</p>
<p>// corner case</p>
<p>if(s == null || t == null || s.length() == 0 || t.length() == 0<br>|| s.length() &lt; t.length()) return &quot;&quot;;</p>
<p>// construct model</p>
<p>int minLeft = 0;</p>
<p>int minRight = 0;</p>
<p>int min = s.length();</p>
<p>boolean flag = false;</p>
<p>Map&lt;Character, Integer&gt; map = new HashMap&lt;&gt;();</p>
<p>int count = t.length(); // the number of characters that I need to match</p>
<p>for(char c : t.toCharArray()) map.put(c, map.getOrDefault(c, 0) + 1);</p>
<p>// unfixed sliding window, 2 pointers</p>
<p>int i = 0;</p>
<p>int j = 0;</p>
<p>while(j &lt; s.length()){</p>
<p>char c = s.charAt(j);</p>
<p>if(map.containsKey(c)){</p>
<p>map.put(c, map.get(c) - 1);</p>
<p>if(map.get(c) &gt;= 0) count--; // if still unmatched characters, then<br>count--</p>
<p>}</p>
<p>// if found a susbtring</p>
<p>while(count == 0 &amp;&amp; i &lt;= j){</p>
<p>// update global min</p>
<p>flag = true;</p>
<p>int curLen = j + 1 - i;</p>
<p>if(curLen &lt;= min){</p>
<p>minLeft = i;</p>
<p>minRight = j;</p>
<p>min = curLen;</p>
<p>}</p>
<p>// shrink left pointer</p>
<p>char leftC = s.charAt(i);</p>
<p>if(map.containsKey(leftC)){</p>
<p>map.put(leftC, map.get(leftC) + 1);</p>
<p>if(map.get(leftC) &gt;= 1) count++;</p>
<p>}</p>
<p>i++;</p>
<p>}</p>
<p>j++;</p>
<p>}</p>
<p>return flag == true ? s.substring(minLeft, minRight + 1): &quot;&quot;;</p>
<p>}</p>
<p>}</p>
<p>First part: when the right pointer is getting incremented we are<br>decrementing the map count of char if it&#39;s part of &#39;t&#39; string. When<br>we see that the map count of that char after decrementing is<br>positive/zero means that the right ptr has found a useful char and hence<br>we increment the &#39;count&#39; variable (which is keeping track of the<br>number of useful chars)</p>
<p>Second part: when the left pointer is getting incremented we are<br>essentially making the window smaller and giving back the chars to the<br>map (i.e. incrementing the map count). If we find that for the<br>particular char the map count has now become positive means that we<br>actually gave back a useful char and hence the &#39;count&#39; is to be<br>decremented.</p>
<p>At this point then we again start increasing our window and see each<br>time if the count has become equal to the number of chars in &#39;t&#39;<br>string.</p>
<p>----</p>
<p>Generally, there are following steps:</p>
<ol>
<li><p>create a hashmap for each character in t and count their frequency</p>
<blockquote>
<p>in t as the value of hashmap.</p>
</blockquote>
</li>
<li><p>Find the first window in S that contains T. But how? there the</p>
<blockquote>
<p>author uses the count.</p>
</blockquote>
</li>
<li><p>Checking from the leftmost index of the window and to see if it</p>
<blockquote>
<p>belongs to t. The reason we do so is that we want to shrink the<br>size of the window.<br>3-1) If the character at leftmost index does not belong to t, we<br>can directly remove this leftmost value and update our window(its<br>minLeft and minLen value)<br>3-2) If the character indeed exists in t, we still remove it, but<br>in the next step, we will increase the right pointer and expect<br>the removed character. If find so, repeat step 3.</p>
</blockquote>
</li>
</ol>
<p>public String minWindow(String s, String t) {</p>
<p>HashMap&lt;Character, Integer&gt; map = new HashMap();</p>
<p>for(char c : t.toCharArray()){</p>
<p>if(map.containsKey(c)){</p>
<p>map.put(c, map.get(c)+1);</p>
<p>}</p>
<p>else{</p>
<p>map.put(c, 1);</p>
<p>}</p>
<p>}</p>
<p>int left = 0, minLeft=0, minLen =s.length()+1, count = 0;</p>
<p>for(int right = 0; right&lt;s.length(); right++){</p>
<p>char r = s.charAt(right);</p>
<p>if(map.containsKey(r)){//the goal of this part is to get the first<br>window that contains whole t</p>
<p>map.put(r, map.get(r)-1);</p>
<p>if(map.get(r)&gt;=0) count++;//identify if the first window is found by<br>counting frequency of the characters of t showing up in S</p>
<p>}</p>
<p>while(count == t.length()){//if the count is equal to the length of t,<br>then we find such window</p>
<p>if(right-left+1 &lt; minLen){//jsut update the minleft and minlen value</p>
<p>minLeft = left;</p>
<p>minLen = right-left+1;</p>
<p>}</p>
<p>char l = s.charAt(left);</p>
<p>if(map.containsKey(l)){//starting from the leftmost index of the window,<br>we want to check if s[left] is in t. If so, we will remove it from the<br>window, and increase 1 time on its counter in hashmap which means we<br>will expect the same character later by shifting right index. At the<br>same time, we need to reduce the size of the window due to the removal.</p>
<p>map.put(l, map.get(l)+1);</p>
<p>if(map.get(l)&gt;0) count--;</p>
<p>}</p>
<p>left++;//if it doesn&#39;t exist in t, it is not supposed to be in the<br>window, left++. If it does exist in t, the reason is stated as above.<br>left++.</p>
<p>}</p>
<p>}</p>
<p>return minLen==s.length()+1?&quot;&quot;:s.substring(minLeft, minLeft+minLen);</p>
<p>------------- best solution---</p>
<p>public static String minWindowBetter(String s, String t){</p>
<p>if(s==null||t==null|s.length()==0||t.length()==0){</p>
<p>return &quot;&quot;;</p>
<p>}</p>
<p>int left=0,right=0,count=0,min=Integer.MAX_VALUE;</p>
<p>int pool[] = new int[256];</p>
<p>String rtn=&quot;&quot;;</p>
<p>for(int i =0;i&lt;t.length();i++){</p>
<p>pool[t.charAt(i)]++;</p>
<p>}</p>
<p>while(right&lt;s.length()){</p>
<p>if(pool[s.charAt(right++)]--&gt;0){//[!]</p>
<p>// (a) if(pool[s.charAt(right++)]--&gt;=0), rather than<br>if(pool[right++]--&gt;=0)</p>
<p>// (b) this is &quot;&gt;0&quot;, but not &quot;&gt;=0&quot;</p>
<p>count++;</p>
<p>}</p>
<p>while(count==t.length()){</p>
<p>if((right-left)&lt;min){</p>
<p>min=right-left;</p>
<p>rtn=s.substring(left,right);</p>
<p>}</p>
<p>//shrink window</p>
<p>if(++pool[s.charAt(left++)]&gt;0){</p>
<p>count--;</p>
<p>}</p>
<p>}</p>
<p>}</p>
<p>return rtn;</p>
<p>}</p>
<p>while(right &lt; length of s){</p>
<blockquote>
<p>Deincrement characters frequence at right pointer in String s from<br>bank</p>
<p>Right -- (expand window)</p>
<p>If that character was inside of t, increase count</p>
<p>while(count equal to length of t - condition){</p>
<p>Check if right-left less than min, if so, update min and curr string</p>
<p>Increate characters frequences at left pointer in string, s from bank</p>
<p>Left++ (shift window)</p>
<p>If bank[character at left pointer]&gt;=0, then decrease count.</p>
</blockquote>
<p>}</p>
<p><strong>[knapsack 0/1 背包]{.underline}</strong></p>
<p>用子问题定义状态：即f[i][v]表示前i件物品恰放入一个容量为v的背包可以获得的最大价值。则其状态转移方程便是：</p>
<p>f[i][v]=max{f[i-1][v],f[i-1][v-c[i]]+w[i]}</p>
<p>这个方程非常重要，基本上所有跟背包相关的问题的方程都是由它衍生出来的。所以有必要将它详细解释一下：”将前i件物品放入容量为v的背包中”这个子问题，若只考虑第i件物品的策略（放或不放），那么就可以转化为一个只牵扯前i-1件物品的问题。如果不放第i件物品，那么问题就转化为”前i-1件物品放入容量为v的背包中”，价值为f[i-1][v]；如果放第i件物品，那么问题就转化为”前i-1件物品放入剩下的容量为v-c[i]的背包中”，此时能获得的最大价值就是f[i-1][v-c[i]]再加上通过放入第i件物品获得的价值w[i]。</p>
<p>private static int knapsack01(int[] weights, int[] value, int quota)<br>{</p>
<p>// we are using dynamic programming bottom up</p>
<p>// one tab to keep track of value, size is quota + 1</p>
<p>int[][] dp = new int[value.length+1][quota+1];</p>
<p>// as size is actual size + 1, so here is &quot;&lt;=&quot; , rather than &quot;&lt;&quot;</p>
<p>for(int i=0;i&lt;=value.length;i++){</p>
<p>for (int j =0;j&lt;=quota;j++){</p>
<p>//base value</p>
<p>if(i==0 || j==0){</p>
<p>// initilize first line and first column to &#39;0&#39;</p>
<p>dp[i][j] = 0;</p>
<p>continue;</p>
<p>}</p>
<p>// non zero</p>
<p>if(j&gt;=weights[i-1]){</p>
<p>// current weight not bigger than current quota</p>
<p>// so add it to our backtrack</p>
<p>// get the max one of (1) Not include , (2) include this node</p>
<p>dp[i][j]= Math.<em>max</em>(dp[i-1][j],
dp[i-1][j-weights[i-1]]+value[i-1]);</p>
<p>}else{</p>
<p>// required weight is less than provided, so skip this</p>
<p>dp[i][j] = dp[i-1][j]; //use value (j) of previous (i-1</p>
<p>}</p>
<p>}</p>
<p>}</p>
<p>return dp[value.length][quota];</p>
<p>}</p>
<p><strong>[KMP 算法]{.underline}</strong></p>
<p>KMP<br>算法永不回退txt的指针i，不走回头路（不会重复扫描txt），而是借助dp数组中储存的信息把pat移到正确的位置继续匹配，时间复杂度只需<br>O(N)，用空间换时间，所以我认为它是一种动态规划算法。</p>
<p>// 暴力匹配（伪码）</p>
<p>int search(String pat, String txt) {</p>
<p>int M = pat.length;</p>
<p>int N = txt.length;</p>
<p>for (int i = 0; i &lt; N - M; i++) {</p>
<p>int j;</p>
<p>for (j = 0; j &lt; M; j++) {</p>
<p>if (pat[j] != txt[i+j])</p>
<p>break;</p>
<p>}</p>
<p>// pat 全都匹配了</p>
<p>if (j == M) return i;</p>
<p>}</p>
<p>// txt 中不存在 pat 子串</p>
<p>return -1;</p>
<p>---</p>
<p>dynamic programming</p>
<p>public class KMP {</p>
<p>private int[][] dp;</p>
<p>private String pat;</p>
<p>public KMP(String pat) {</p>
<p>this.pat = pat;</p>
<p>// 通过 pat 构建 dp 数组</p>
<p>// 需要 O(M) 时间</p>
<p>}</p>
<p>public int search(String txt) {</p>
<p>// 借助 dp 数组去匹配 txt</p>
<p>// 需要 O(N) 时间</p>
<p>}</p>
<p>}</p>
<p>为了描述状态转移图，我们定义一个二维 dp 数组，它的含义如下：</p>
<p>dp[j][c] = next</p>
<p>0 &lt;= j &lt; M，代表当前的状态</p>
<p>0 &lt;= c &lt; 256，代表遇到的字符（ASCII 码）</p>
<p>0 &lt;= next &lt;= M，代表下一个状态</p>
<p>dp[4][&#39;A&#39;] = 3 表示：</p>
<p>当前是状态 4，如果遇到字符 A，</p>
<p>pat 应该转移到状态 3</p>
<p>dp[1][&#39;B&#39;] = 2 表示：</p>
<p>当前是状态 1，如果遇到字符 B，</p>
<p>pat 应该转移到状态 2</p>
<p>根据我们这个 dp 数组的定义和刚才状态转移的过程，我们可以先写出 KMP<br>算法的 search 函数代码：</p>
<p>public int search(String txt) {</p>
<p>int M = pat.length();</p>
<p>int N = txt.length();</p>
<p>// pat 的初始态为 0</p>
<p>int j = 0;</p>
<p>for (int i = 0; i &lt; N; i++) {</p>
<p>// 当前是状态 j，遇到字符 txt[i]，</p>
<p>// pat 应该转移到哪个状态？</p>
<p>j = dp[j][txt.charAt(i)];</p>
<p>// 如果达到终止态，返回匹配开头的索引</p>
<p>if (j == M) return i - M + 1;</p>
<p>}</p>
<p>// 没到达终止态，匹配失败</p>
<p>return -1;</p>
<p>}</p>
<p>for 0 &lt;= j &lt; M: # 状态</p>
<p>for 0 &lt;= c &lt; 256: # 字符</p>
<p>dp[j][c] = next</p>
<p>这个 next<br>状态应该怎么求呢？显然，如果遇到的字符c和pat[j]匹配的话，状态就应该向前推进一个，也就是说next<br>= j + 1，我们不妨称这种情况为状态推进：</p>
<p>如果遇到的字符c和pat[j]不匹配的话，状态就要回退（或者原地不动），我们不妨称这种情况为状态重启：</p>
<p>那么，如何得知在哪个状态重启呢？解答这个问题之前，我们再定义一个名字：影子状态（我编的名字），用变量X表示。所谓影子状态，就是和当前状态具有相同的前缀。比如下面这种情况：</p>
<p>当前状态j = 4，其影子状态为X = 2，它们都有相同的前缀<br>&quot;AB&quot;。因为状态X和状态j存在相同的前缀，所以当状态j准备进行状态重启的时候（遇到的字符c和pat[j]不匹配），可以通过X的状态转移图来获得最近的重启位置。</p>
<p>比如说刚才的情况，如果状态j遇到一个字符<br>&quot;A&quot;，应该转移到哪里呢？首先状态 4 只有遇到 &quot;C&quot; 才能推进状态，遇到<br>&quot;A&quot;<br>显然只能进行状态重启。状态j会把这个字符委托给状态X处理，也就是dp[j][&#39;A&#39;]<br>= dp[X][&#39;A&#39;]：</p>
<p>int X # 影子状态</p>
<p>for 0 &lt;= j &lt; M:</p>
<p>for 0 &lt;= c &lt; 256:</p>
<p>if c == pat[j]:</p>
<p># 状态推进</p>
<p>dp[j][c] = j + 1</p>
<p>else:</p>
<p># 状态重启</p>
<p># 委托 X 计算重启位置</p>
<p>dp[j][c] = dp[X][c]</p>
<p>---</p>
<p>影子状态X是如何得到的呢？下面先直接看完整代码吧。</p>
<p>public class KMP {</p>
<p>private int[][] dp;</p>
<p>private String pat;</p>
<p>public KMP(String pat) {</p>
<p>this.pat = pat;</p>
<p>int M = pat.length();</p>
<p>// dp[状态][字符] = 下个状态</p>
<p>dp = new int[M][256];</p>
<p>// base case</p>
<p>dp[0][pat.charAt(0)] = 1;</p>
<p>// 影子状态 X 初始为 0</p>
<p>int X = 0;</p>
<p>// 当前状态 j 从 1 开始</p>
<p>for (int j = 1; j &lt; M; j++) {</p>
<p>for (int c = 0; c &lt; 256; c++) {</p>
<p>if (pat.charAt(j) == c)</p>
<p>dp[j][c] = j + 1;</p>
<p>else</p>
<p>dp[j][c] = dp[X][c];</p>
<p>}</p>
<p>// 更新影子状态</p>
<p>X = dp[X][pat.charAt(j)];</p>
<p>}</p>
<p>}</p>
<p>public int search(String txt) {...}</p>
<p>}</p>
<p>先解释一下这一行代码：</p>
<p>// base case</p>
<p>dp[0][pat.charAt(0)] = 1;</p>
<p>这行代码是 base case，只有遇到 pat[0] 这个字符才能使状态从 0 转移到<br>1，遇到其它字符的话还是停留在状态 0（Java 默认初始化数组全为 0）。</p>
<p>影子状态X是先初始化为<br>0，然后随着j的前进而不断更新的。下面看看到底应该如何更新影子状态X：</p>
<p>int X = 0;</p>
<p>for (int j = 1; j &lt; M; j++) {</p>
<p>...</p>
<p>// 更新影子状态</p>
<p>// 当前是状态 X，遇到字符 pat[j]，</p>
<p>// pat 应该转移到哪个状态？</p>
<p>X = dp[X][pat.charAt(j)];</p>
<p>}</p>
<p>更新X其实和search函数中更新状态j的过程是非常相似的：</p>
<p>int j = 0;</p>
<p>for (int i = 0; i &lt; N; i++) {</p>
<p>// 当前是状态 j，遇到字符 txt[i]，</p>
<p>// pat 应该转移到哪个状态？</p>
<p>j = dp[j][txt.charAt(i)];</p>
<p>...</p>
<p>}</p>
<p>其中的原理非常微妙，注意代码中 for<br>循环的变量初始值，可以这样理解：后者是在txt中匹配pat，前者是在pat中匹配pat[1:]，状态X总是落后状态j一个状态，与j具有最长的相同前缀。所以我把X比喻为影子状态，似乎也有一点贴切。</p>
<p>另外，构建 dp 数组是根据 base casedp[0][..]向后推演。这就是我认为<br>KMP 算法就是一种动态规划算法的原因。</p>
<p>至此，KMP 算法就已经再无奥妙可言了！看下 KMP 算法的完整代码吧：</p>
<p>public class KMP {</p>
<p>private int[][] dp;</p>
<p>private String pat;</p>
<p>public KMP(String pat) {</p>
<p>this.pat = pat;</p>
<p>int M = pat.length();</p>
<p>// dp[状态][字符] = 下个状态</p>
<p>dp = new int[M][256];</p>
<p>// base case</p>
<p>dp[0][pat.charAt(0)] = 1;</p>
<p>// 影子状态 X 初始为 0</p>
<p>int X = 0;</p>
<p>// 构建状态转移图（稍改的更紧凑了）</p>
<p>for (int j = 1; j &lt; M; j++) {</p>
<p>for (int c = 0; c &lt; 256; c++) {</p>
<p>dp[j][c] = dp[X][c];</p>
<p>dp[j][pat.charAt(j)] = j + 1;</p>
<p>// 更新影子状态</p>
<p>X = dp[X][pat.charAt(j)];</p>
<p>}</p>
<p>}</p>
<p>public int search(String txt) {</p>
<p>int M = pat.length();</p>
<p>int N = txt.length();</p>
<p>// pat 的初始态为 0</p>
<p>int j = 0;</p>
<p>for (int i = 0; i &lt; N; i++) {</p>
<p>// 计算 pat 的下一个状态</p>
<p>j = dp[j][txt.charAt(i)];</p>
<p>// 到达终止态，返回结果</p>
<p>if (j == M) return i - M + 1;</p>
<p>}</p>
<p>// 没到达终止态，匹配失败</p>
<p>return -1;</p>
<p>}</p>
<p>}</p>
<p>经过之前的详细举例讲解，你应该可以理解这段代码的含义了，当然你也可以把<br>KMP 算法写成一个函数。核心代码也就是两个函数中 for<br>循环的部分，数一下有超过十行吗？</p>
<h3 id="labuladong-underline"><a href="#labuladong-underline" class="headerlink" title="[labuladong]{.underline}"></a><strong>[labuladong]{.underline}</strong></h3><p>你只要把住两点就行了：</p>
<p>1、遍历的过程中，所需的状态必须是已经计算出来的。</p>
<p>2、遍历的终点必须是存储结果的那个位置。</p>
<p>PS：但凡遇到需要递归的问题，最好都画出递归树，这对你分析算法的复杂度，寻找算法低效的原因都有巨大帮助</p>
<p>int fib(int n) {</p>
<p>if (n == 2 || n == 1)</p>
<p>return 1;</p>
<p>int prev = 1, curr = 1;</p>
<p>for (int i = 3; i &lt;= n; i++) {</p>
<p>int sum = prev + curr;</p>
<p>prev = curr;</p>
<p>curr = sum;</p>
<p>}</p>
<p>return curr;</p>
<p>}</p>
<p>首先，这个问题是动态规划问题，因为它具有「最优子结构」的。要符合「最优子结构」，子问题间必须互相独立。啥叫相互独立？你肯定不想看数学证明，我用一个直观的例子来讲解。</p>
<p>比如说，你的原问题是考出最高的总成绩，那么你的子问题就是要把语文考到最高，数学考到最高……<br>为了每门课考到最高，你要把每门课相应的选择题分数拿到最高，填空题分数拿到最高……<br>当然，最终就是你每门课都是满分，这就是最高的总成绩。</p>
<p>得到了正确的结果：最高的总成绩就是总分。因为这个过程符合最优子结构，”每门科目考到最高”这些子问题是互相独立，互不干扰的。</p>
<p>但是，如果加一个条件：你的语文成绩和数学成绩会互相制约，此消彼长。这样的话，显然你能考到的最高总成绩就达不到总分了，按刚才那个思路就会得到错误的结果。因为子问题并不独立，语文数学成绩无法同时最优，所以最优子结构被破坏。</p>
<p>PS：为啥 dp 数组初始化为 amount + 1 呢，因为凑成 amount<br>金额的硬币数最多只可能等于 amount（全用 1 元面值的硬币），所以初始化为<br>amount + 1 就相当于初始化为正无穷，便于后续取最小值</p>
<p>最优子结构并不是动态规划独有的一种性质，能求最值的问题大部分都具有这个性质；但反过来，最优子结构性质作为动态规划问题的必要条件，一定是让你求最值的，以后碰到那种恶心人的最值题，思路往动态规划想就对了，这就是套路。</p>
<p>动态规划不就是从最简单的 base case<br>往后推导吗，可以想象成一个链式反应，以小博大。但只有符合最优子结构的问题，才有发生这种链式反应的性质</p>
<p>----------</p>
<p>「状态」很明显，就是当前拥有的鸡蛋数K和需要测试的楼层数N。随着测试的进行，鸡蛋个数可能减少，楼层的搜索范围会减小，这就是状态的变化。</p>
<p>「选择」其实就是去选择哪层楼扔鸡蛋。回顾刚才的线性扫描和二分思路，二分查找每次选择到楼层区间的中间去扔鸡蛋，而线性扫描选择一层层向上测试。不同的选择会造成状态的转移。</p>
<p>现在明确了「状态」和「选择」，动态规划的基本思路就形成了：肯定是个二维的dp数组或者带有两个状态参数的dp函数来表示状态转移；外加一个<br>for 循环来遍历所有选择，择最优的选择更新结果 ：</p>
<p>SuperEggDrop</p>
<p>Drop eggs is a very classical problem.</p>
<p>Some people may come up with idea O(KN^2)</p>
<p>where dp[K][N] = 1 + max(dp[K - 1][i - 1],dp[K][N - i]) for<br>i in 1...N.</p>
<p>However this idea is very brute force, for the reason that you check all<br>possiblity.</p>
<p>So I consider this problem in a different way:</p>
<p>dp[M][K]means that, given K eggs and M moves,</p>
<p>what is the maximum number of floor that we can check.</p>
<p>The dp equation is:</p>
<p>dp[m][k] = dp[m - 1][k - 1] + dp[m - 1][k] + 1,</p>
<p>which means we take 1 move to a floor,</p>
<p>if egg breaks, then we can check dp[m - 1][k - 1] floors.</p>
<p>if egg doesn&#39;t breaks, then we can check dp[m - 1][k] floors.</p>
<p>dp[m][k] is similar to the number of combinations and it increase<br>exponentially to N</p>
<p>public int superEggDrop(int K, int N) {</p>
<p>int[][] floors = new int[N + 1][K + 1];</p>
<p>int move = 0;</p>
<p>while (floors[move][K] &lt; N) {</p>
<p>++m;</p>
<p>for (int egg = 1; egg &lt;= K; ++egg)</p>
<p>floors[move][egg] = floors[move - 1][egg] + 1 + floors[move -<br>1][egg - 1];</p>
<p>}</p>
<p>return m;</p>
<p>}</p>
<p>The dp equation is:</p>
<p>dp[m][k] = dp[m - 1][k - 1] + dp[m - 1][k] + 1,</p>
<p>assume, dp[m-1][k-1] = n0, dp[m-1][k] = n1</p>
<p>the first floor to check is n0+1.</p>
<p>if egg breaks, F must be in [1,n0] floors, we can use m-1 moves and<br>k-1 eggs to find out F is which one.</p>
<p>if egg doesn&#39;t breaks and F is in [n0+2, n0+n1+1] floors, we can use<br>m-1 moves and k eggs to find out F is which one.</p>
<p>So, with m moves and k eggs, we can find out F in n0+n1+1 floors,<br>whichever F is.</p>
<p>---</p>
<p>Great, I understand this solution too.</p>
<p>The key concept of original O(KN^2) solution is to try all the floor to<br>get the min cost min(max(broke, not broke)) as the answer.</p>
<p>This solution is somehow a reverse thinking:</p>
<ol>
<li><p>No matter which floor you try, egg will only break or not break, if</p>
<blockquote>
<p>break, go to downstairs, if not break, go to upstairs.</p>
</blockquote>
</li>
<li><p>No matter you go up or go down, the num of all the floors is always</p>
<blockquote>
<p>upstairs + downstairs + the floor you try, which is dp[m][k] =<br>dp[m - 1][k - 1] + dp[m - 1][k] + 1.</p>
</blockquote>
</li>
</ol>
<p>====</p>
<p>the logic of &quot;dp[m][k] = dp[m - 1][k - 1] + dp[m - 1][k] +<br>1&quot;, my confusion is, if dp[m - 1][k - 1] and dp[m - 1][k] are<br>two different case break or not break; why we combine them together,<br>instead of use the smaller one? would you please explain more?</p>
<p>→</p>
<p>This one move will separate the floors into two non-overlapping groups,<br>below or above (the current level we choose to drop the egg); so no<br>matter what happened to the egg, we only need to check one of those two<br>group. If we need to check the level below the current level, then it<br>means the egg is break, so the maximum level we are able to check is<br>dp[m - 1][k - 1]. Otherwise if we need to check the level above or<br>equal o the current level, it means the egg is not break, so the maximum<br>level we can check is dp[m - 1][k], we should only return dp[m -<br>1][k - 1] + dp[m - 1][k]; however, we count the level from 0,<br>instead of 1, so we need to add the extra one level (i.e; if dp[m -<br>1][k - 1] = 1 and dp[m - 1][k] = 2, means we can check (2 + 3 ==<br>5) levels, so we need to return 4; which is dp[m - 1][k - 1] + dp[m</p>
<ul>
<li>1][k] + 1)</li>
</ul>
<p>Notes:</p>
<ul>
<li>Find max “1” matrix in a square</li>
</ul>
<p>Instead to create a cache and initialise all element by copying when I,j<br>=0.</p>
<p>Better solution is to clone input “matrix” this will lead to faster and<br>cleaner code</p>
<p>If(I=0 || j=0) {// do nothing because those element remain unchanged<br>in matrix copy}</p>
<ul>
<li>For Two sum, should raise Exception e.g. no findings rather than<blockquote>
<p>return “null”</p>
</blockquote>
</li>
</ul>
<p>public int[] <strong>[twoSum]{.underline}</strong>(int[] nums, int target) {</p>
<p>for (int i = 0; i &lt; nums.length; i++) {</p>
<p>for (int j = i + 1; j &lt; nums.length; j++) { // I used int j=i, which is<br>wrong as it may cause one item to be used twice</p>
<p>if (nums[j] == target - nums[i]) {</p>
<p>return new int[] { i, j };</p>
<p>}</p>
<p>}</p>
<p>}</p>
<p>throw new IllegalArgumentException(&quot;No two sum solution&quot;);</p>
<p>}</p>
<ul>
<li>Return specific Exception, e.g.</li>
</ul>
<p>throw new IllegalArgumentException(&quot;No two sum solution&quot;);</p>
<ul>
<li>Error of two sums</li>
</ul>
<blockquote>
<p>if(map1.containsKey(diff)) {</p>
<p>return new int[]{i, map1.get(diff)};</p>
<p>} else{</p>
<p>// be careful put number itself (rather than supplement) to map</p>
<p>map1.put(nums[i], i);</p>
<p>}</p>
</blockquote>
<ul>
<li><p>Summary:</p>
<ul>
<li><p>If possible, make use of HashMap to increase search performance</p>
</li>
<li><p>If there are two loops, try to reduce to use one in-flight</p>
<blockquote>
<p>hashmap lookup</p>
</blockquote>
</li>
</ul>
</li>
<li><p>Multiply string</p>
<ul>
<li>Naiive solution</li>
</ul>
</li>
</ul>
<blockquote>
<p><strong>ublic</strong> String <strong>[multiply]{.underline}</strong>(String num1, String num2)<br>{</p>
<p>String n1 = <strong>new</strong> StringBuilder(num1).reverse().toString();</p>
<p>String n2 = <strong>new</strong> StringBuilder(num2).reverse().toString();</p>
<p><strong>int</strong>[] d = <strong>new</strong> <strong>int</strong>[num1.length()+num2.length()];</p>
<p><em>//multiply each digit and sum at the corresponding positions</em></p>
<p><strong>for</strong>(<strong>int</strong> i=0; i&lt;n1.length(); i++){</p>
<p><strong>for</strong>(<strong>int</strong> j=0; j&lt;n2.length(); j++){</p>
<p><strong>d[i+j] += (n1.charAt(i)-&#39;0&#39;) * (n2.charAt(j)-&#39;0&#39;);</strong></p>
<p>}</p>
<p>}</p>
<p>StringBuilder sb = <strong>new</strong> StringBuilder();</p>
<p><em>//calculate each digit</em></p>
<p><strong>for</strong>(<strong>int</strong> i=0; i&lt;d.length; i++){</p>
<p><strong>int</strong> mod = d[i]%10;</p>
<p><strong>int</strong> carry = d[i]/10;</p>
<p><strong>if</strong>(i+1&lt;d.length){</p>
<p>d[i+1] += carry;</p>
<p>}</p>
<p>sb.insert(0, mod);</p>
<p>}</p>
<p><em>//remove front 0&#39;s</em></p>
<p><strong>while</strong>(sb.charAt(0) == &#39;0&#39; &amp;&amp; sb.length()&gt; 1){</p>
<p>sb.deleteCharAt(0);</p>
<p>}</p>
<p><strong>return</strong> sb.toString();</p>
<p>}</p>
<p>-----Best Multiply String---</p>
<p>private static String multiply(String num1, String num2) {</p>
<p>int nLen1 = num1.length(), nLen2=num2.length();</p>
<p>int[] result = new int[nLen1+nLen2];</p>
<p>for(int c1=nLen1-1;c1&gt;=0;c1--) {</p>
<p>for(int c2=nLen2-1;c2&gt;=0;c2--){</p>
<p>int nMulti= (num1.charAt(c1) - &#39;0&#39;) * (num2.charAt(c2) - &#39;0&#39;);</p>
<p>int sum = nMulti + result[c1+c2+1];</p>
<p>result[c1+c2] += sum / 10; //This is for “carry”, so must to be “+”<br>in front of “=”</p>
<p>result[c1+c2+1] = sum % 10; // This is a reminder, so this only be<br>assigned without “+”</p>
<p>}</p>
<p>}</p>
<p>StringBuffer buff = new StringBuffer();</p>
<p>for(int p:result) {</p>
<p>if(!(result.length==0 &amp;&amp; p==0)) {</p>
<p>//remove prefix 0</p>
<p>buff.append(p);</p>
<p>}</p>
<p>}</p>
<p>return buff.toString();</p>
<p>}</p>
</blockquote>
<ul>
<li>greedy algorithm</li>
</ul>
<blockquote>
<p>[Greedy]</p>
<p>const int N = 5;</p>
<p>int Count[N] = {5,2,2,3,5};//每一张纸币的数量</p>
<p>int Value[N] = {1,5,10,50,100};</p>
<p>int <strong>[solve]{.underline}</strong>(int money) {</p>
<p>int num = 0;</p>
<p>for(int i = N-1;i&gt;=0;i--) {</p>
<p>int c = min(money/Value[i],Count[i]);//每一个所需要的张数</p>
<p>money = money-c*Value[i];</p>
<p>num += c;//总张数</p>
<p>}</p>
<p>if(money&gt;0) num=-1;</p>
<p>return num;</p>
<p>}</p>
</blockquote>
<ul>
<li>Add One:</li>
</ul>
<blockquote>
<p>private static int[] plusOneBest(int[] ary) {</p>
<p>for (int i = ary.length - 1; i &gt;= 0; i--) {</p>
<p>if (ary[i] != 9) {</p>
<p>ary[i]++; //[!] Here is key step, for two cases: (1) last digit,<br>add one then exit (2) Next digit with carry, add on and exit</p>
<p>break;</p>
<p>} else {</p>
<p>ary[i] = 0;</p>
<p>}</p>
<p>}</p>
<p>if (ary[0] == 0) {</p>
<p>int[] aryRtn = new int[ary.length + 1];</p>
<p>System.arraycopy(aryRtn, 1, ary, 0, ary.length);</p>
<p>aryRtn[0] = 1;</p>
<p>return aryRtn;</p>
<p>} else {</p>
<p>return ary;</p>
<p>}</p>
<p>}</p>
</blockquote>
<ul>
<li>Add two Single nodes</li>
</ul>
<blockquote>
<p>/**</p>
<p>* Definition for singly-linked list.</p>
<p>* public class ListNode {</p>
<p>* int val;</p>
<p>* ListNode next;</p>
<p>* ListNode(int x) { val = x; }</p>
<p>* }</p>
<p>*/</p>
<p>class Solution {</p>
<p>public ListNode addTwoNumbers(ListNode l1, ListNode l2) {</p>
<p>ListNode lResult = new ListNode(0);</p>
<p>int carry=0,sum =0;</p>
<p>ListNode itNode = lResult; // [!] to setup an iterator</p>
<p>while(l1!=null || l2!=null){</p>
<p>// sum = l1.val + l2.val;</p>
<p>sum = (l1!=null?l1.val:0) + (l2!=null?l2.val:0); // to void null in<br>get reference</p>
<p>sum += carry; // to accumulate &#39;carry&#39;</p>
<p>carry = sum /10;</p>
<p>// lResult.val = sum % 10;</p>
<p>itNode.next = new ListNode(sum % 10);</p>
<p>itNode = itNode.next; // [!] this is the key step</p>
<p>if(l1!=null) {</p>
<p>l1 = l1.next;</p>
<p>}</p>
<p>if(l2!=null){</p>
<p>l2 = l2.next;</p>
<p>}</p>
<p>if(carry&gt;0) itNode.next = new ListNode(carry);</p>
<p>}</p>
<p>return lResult.next;</p>
<p>}</p>
<p>}</p>
</blockquote>
<ul>
<li><p>Longest unique characters</p>
<ul>
<li>Naive approach</li>
</ul>
</li>
</ul>
<blockquote>
<p>class Solution {</p>
<p>public int lengthOfLongestSubstring(String s) {</p>
<p>// analysis:</p>
<p>// embeded two loops to foreach every element and inner loop step from<br>current element of 1st loop</p>
<p>// check whether each sub-string is all unique</p>
<p>// if so, get length and compare with global temp max length</p>
<p>int maxLen=0;</p>
<p>for(int i=0;i&lt;s.length();i++) {</p>
<p>//for inner loop, it start from i+1 (rather than i)</p>
<p>for(int j=i+1;i&lt;=s.length();j++){</p>
<p>if(noDup(s, i , j)){</p>
<p>maxLen = Math.max(maxLen, (j-i));</p>
<p>}</p>
<p>}</p>
<p>}</p>
<p>return maxLen;</p>
<p>}</p>
<p>private boolean noDup(String sub, int start, int end){</p>
<p>//check whether thsi sub string is unique</p>
<p>// char[] aryOccurance=new char[127];</p>
<p>// int[] aryOccurance=new int[127];</p>
<p>Set&lt;Character&gt; set=new HashSet&lt;&gt;();</p>
<p>for(int k=start;k&lt;end;k++){</p>
<p>Character c = sub.charAt(k);</p>
<p>if(set.contains(c)){</p>
<p>return false;</p>
<p>}else{</p>
<p>set.add(c);</p>
<p>}</p>
<p>}</p>
<p>return true;</p>
<p>}</p>
<p>}</p>
</blockquote>
<ul>
<li>Sliding window</li>
</ul>
<blockquote>
<p><a href="https://developpaper.com/share-several-algorithmic-interview-questions-related-to-sliding-window/" target="_blank" rel="noopener">[https://developpaper.com/share-several-algorithmic-interview-questions-related-to-sliding-window/]{.underline}</a></p>
<p><strong>*[Longest Substring Without Repeating Characters]{.underline}*</strong></p>
<p>public int lengthOfLongestSubstring(String s) {</p>
<p>Map&lt;Character, Integer&gt; map= new HashMap&lt;&gt;();// map to cache<br>position of each occruance</p>
<p>int start=0, len=0;</p>
<p>// abba</p>
<p>for(int i=0; i&lt;s.length(); i++) {</p>
<p>char c = s.charAt(i);</p>
<p>if (map.containsKey(c)) {// here is the key step for “without<br>repeating chars” in questions.</p>
<p>if (map.get(c) &gt;= start)</p>
<p>start = map.get(c) + 1;// found duplicate, so get started a new round,<br>assign start from 1st occurance of ‘duplicate char’ plus one.</p>
<p>}</p>
<p>len = Math.max(len, i-start+1);</p>
<p>map.put(c, i);</p>
<p>}</p>
<p>return len;</p>
<p>}</p>
<p>My solution vs leetcode one, latter one is much more consice</p>
<p>// better solution leveraging slide window</p>
<p>// Runtime: 12 ms, faster than 26.95% of Java online submissions for<br>Longest Substring Without Repeating Characters.</p>
<p>public int lengthOfLongestSubstring(String s){</p>
<p>Set&lt;Character&gt; set =new HashSet&lt;&gt;();</p>
<p>int maxLen = 0, left=0,right =-1, n=s.length();</p>
<p>while(left&lt;n) {</p>
<p>if((right+1)&lt;n &amp;&amp; !set.contains(s.charAt(right+1))){</p>
<p>// not in slide window</p>
<p>right++;//expand slide window</p>
<p>set.add(s.charAt(right));</p>
<p>}else{</p>
<p>// dup with existing slide window</p>
<p>// shrink window</p>
<p>set.remove(s.charAt(left));</p>
<p>left++;</p>
<p>}</p>
<p>maxLen = Math.max(maxLen, right - left +1);// [!] be carefulf there<br>is &quot;+1&quot; as this is for getting count</p>
<p>}</p>
<p>return maxLen;</p>
<p>}</p>
<p>-----leetcode solution-----</p>
<p>public int lengthOfLongestSubstring(String s){</p>
<p>int i=0,j=0,n=s.length(),rtn=0;</p>
<p>Set&lt;Character&gt; set = new HashSet&lt;&gt;();</p>
<p>while(i&lt;n &amp;&amp; j&lt;n) {</p>
<p>if(!set.contains(s.charAt(j))) {</p>
<p>set.add(s.charAt(j++));</p>
<p>rtn = Math.max(rtn, j-i) ;</p>
<p>}else{</p>
<p>set.remove(s.charAt(i++));</p>
<p>}</p>
<p>}</p>
<p>return rtn;</p>
<p>}</p>
<p>-----------</p>
</blockquote>
<ul>
<li>Palindrome integer (not string)</li>
</ul>
<blockquote>
<p>class Solution {</p>
<p>public boolean isPalindrome(int x) {</p>
<p>//first of all, boundary (or edge case)</p>
<p>if(x&lt;0 || (x!=0 &amp;&amp; x%10==0)) {</p>
<p>return false;</p>
<p>}</p>
<p>int reverse =0;</p>
<p>while(x&gt; reverse) {</p>
<p>reverse = reverse * 10 + x%10;</p>
<p>x /= 10;</p>
<p>}</p>
<p>// When the length is an odd number, we can get rid of the middle<br>digit by revertedNumber/10</p>
<p>// For example when the input is 12321, at the end of the while loop<br>we get x = 12, revertedNumber = 123,</p>
<p>// since the middle digit doesn&#39;t matter in palindrome(it will always<br>equal to itself), we can simply get rid of it.</p>
<p>return x == reverse || x == reverse/10;</p>
<p>}</p>
<p>}</p>
</blockquote>
<ul>
<li>Find longest common sub array among two arrays</li>
</ul>
<p>[DP]</p>
<blockquote>
<p>public static int findLength_dp(int[] A, int[] B) {</p>
<p>// for dynamic programing, normally it compare itself with its<br>sibling, using max/min</p>
<p>// try to construct a matrix to keep track of path</p>
<p>int m=A.length,n=B.length,max=0;</p>
<p>int[][] memo = new int[m+1][n+1]; // &quot;+1&quot; to keep extra<br>space</p>
<p>for(int i = 0;i &lt;= m;i++) {</p>
<p>for (int j = 0; j &lt;= n; j++) {</p>
<p>//for DP, firstly to setup begin point</p>
<p>if(i==0 || j==0) {</p>
<p>memo[i][j]=0;</p>
<p>}else{</p>
<p>if(A[i-1]==B[j-1]){ // it they are same</p>
<p>memo[i][j] = 1+ memo[i-1][j-1]; // increase one to cache</p>
<p>max = Math.max(max,memo[i][j]); // get global max</p>
<p>}</p>
<p>}</p>
<p>}</p>
<p>}</p>
<p>return max;</p>
<p>}</p>
</blockquote>
<ul>
<li>Dynamic programing:</li>
</ul>
<p>重叠子问题、最优子结构、状态转移方程就是动态规划三要素。</p>
<p>What is dynamic programming?</p>
<p>Simply put, dynamic programming is an optimization technique that we can<br>use to solve problems where the same work is being repeated over and<br>over. You know how a web server may use caching? Dynamic programming is<br>basically that.</p>
<p>However, dynamic programming doesn’t work for every problem. There are a<br>lot of cases in which dynamic programming simply won’t help us improve<br>the runtime of a problem at all. If we aren’t doing repeated work, then<br>no amount of caching will make any difference.</p>
<p><strong>A problem can be optimized using dynamic programming if it:</strong></p>
<ol>
<li><p>has an optimal substructure.</p>
</li>
<li><p>has overlapping subproblems</p>
</li>
</ol>
<p><strong>[Optimal substructure]{.underline}</strong> simply means that you can find<br>the optimal solution to a problem by considering the optimal solution to<br>its subproblems.</p>
<h3 id="Overlapping-Subproblems"><a href="#Overlapping-Subproblems" class="headerlink" title="Overlapping Subproblems"></a>Overlapping Subproblems</h3><p><a href="https://en.wikipedia.org/wiki/Overlapping_subproblems" target="_blank" rel="noopener">Overlapping<br>subproblems</a> is<br>the second key property that our problem must have to allow us to<br>optimize using dynamic programming. Simply put, having overlapping<br>subproblems means we are computing the same problem more than once.</p>
<p>Imagine you have a server that caches images. If the same image gets<br>requested over and over again, you’ll save a ton of time. However, if no<br>one ever requests the same image more than once, what was the benefit of<br>caching them?</p>
<h2 id="Dynamic-Programming-Methods-underline"><a href="#Dynamic-Programming-Methods-underline" class="headerlink" title="[Dynamic Programming Methods]{.underline}"></a><strong>[Dynamic Programming Methods]{.underline}</strong></h2><p>DP offers two methods to solve a problem:</p>
<p><strong>1. Top-down with Memoization</strong></p>
<p>In this approach, we try to solve the bigger problem by recursively<br>finding the solution to smaller sub-problems. Whenever we solve a<br>sub-problem, we cache its result so that we don’t end up solving it<br>repeatedly if it’s called multiple times. Instead, we can just return<br>the saved result. This technique of storing the results of already<br>solved subproblems is called <strong>Memoization</strong>.</p>
<p><strong>2. Bottom-up with Tabulation</strong></p>
<p>Tabulation is the opposite of the top-down approach and avoids<br>recursion. In this approach, we solve the problem “bottom-up” (i.e. by<br>solving all the related sub-problems first). This is typically done by<br>filling up an n-dimensional table. Based on the results in the table,<br>the solution to the top/original problem is then computed.</p>
<p>Tabulation is the opposite of Memoization, as in Memoization we solve<br>the problem and maintain a map of already solved sub-problems. In other<br>words, in memoization, we do it top-down in the sense that we solve the<br>top problem first (which typically recurses down to solve the<br>sub-problems).</p>
<p>Let’s apply Tabulation to our example of Fibonacci numbers. Since we<br>know that every Fibonacci number is the sum of the two preceding<br>numbers, we can use this fact to populate our table.</p>
<p>Here is the code for our bottom-up dynamic programming approach:</p>
<p><strong>class</strong> Fibonacci {</p>
<p><strong>public</strong> int CalculateFibonacci(int n) {</p>
<p>int dp[] = <strong>new</strong> int[n+1];</p>
<p>//base cases</p>
<p>dp[0] = 0;</p>
<p>dp[1] = 1;</p>
<p><strong>for</strong>(int i=2; i&lt;=n; i++)</p>
<p>dp[i] = dp[i-1] + dp[i-2];</p>
<p><strong>return</strong> dp[n];</p>
<p>}</p>
<p><strong>public</strong> <strong>static</strong> void main(<strong>String</strong>[] args) {</p>
<p>Fibonacci fib = <strong>new</strong> Fibonacci();</p>
<p><strong>System</strong>.out.println(&quot;5th Fibonacci is ---&gt; &quot; +<br>fib.CalculateFibonacci(5));</p>
<p><strong>System</strong>.out.println(&quot;6th Fibonacci is ---&gt; &quot; +<br>fib.CalculateFibonacci(6));</p>
<p><strong>System</strong>.out.println(&quot;7th Fibonacci is ---&gt; &quot; +<br>fib.CalculateFibonacci(7));</p>
<p>}</p>
<p>}</p>
<p>Generally speaking, dynamic programming is the technique of storing<br>repeated computations in memory, rather than recomputing them every time<br>you need them. The ultimate goal of this process is to improve runtime.<br>Dynamic programming allows you to use more space to take less time.</p>
<p>Dynamic programming relies on overlapping subproblems, because it uses<br>memory to save the values that have already been computed to avoid<br>computing them again. The more overlap there is, the more computational<br>time is saved.</p>
<p><strong>Top-down and bottom-up</strong></p>
<p>Top-down and bottom-up refer to two general approaches to dynamic<br>programming. A top-down solution starts with the final result and<br>recursively breaks it down into subproblems. The bottom-up method does<br>the opposite. It takes an iterative approach to solve the subproblems<br>first and then works up to the desired solution.</p>
<p>both solutions are equally valid and that one solution can be determined<br>from the other. In an interview situation, although bottom-up solutions<br>often result in more concise code, either approach is appropriate. I<br>recommend that you use whatever solution makes the most sense to you.</p>
<p>The important point is that top-down = recursive and bottom-up =<br>iterative.</p>
<blockquote>
<p>There are four steps in the FAST method:</p>
</blockquote>
<ol>
<li><p><strong>F</strong>irst solution</p>
</li>
<li><p><strong>A</strong>nalyze the first solution</p>
</li>
<li><p>Identify the <strong>S</strong>ubproblems</p>
</li>
<li><p><strong>T</strong>urn the solution around</p>
</li>
</ol>
<p><strong>First solution</strong></p>
<p>This is an important step for any interview question but is particularly<br>important for dynamic programming. This step finds the first possible<br>solution. This solution will be brute force and recursive. The goal is<br>to solve the problem without concern for efficiency. It means that if<br>you need to find the biggest/ smallest/longest/shortest something, you<br>should write code that goes through every possibility and then compares<br>them all to find the best one.</p>
<p>Your solution must also meet these restrictions:</p>
<ul>
<li><p>The recursive calls must be self-contained. That means no global</p>
<blockquote>
<p>variables.</p>
</blockquote>
</li>
<li><p>You cannot do tail recursion. Your solution must compute the results</p>
<blockquote>
<p>to each subproblem and then combine them afterwards.</p>
</blockquote>
</li>
<li><p>Do not pass in unnecessary variables. Eg. If you can count the depth</p>
<blockquote>
<p>of your recursion as you return, don’t pass a count variable into<br>your recursive function.</p>
</blockquote>
</li>
</ul>
<blockquote>
<p><strong>Analyze the first solution</strong></p>
</blockquote>
<p>In this step, we will analyze the first solution that you came up with.<br>This involves determining the time and space complexity of your first<br>solution and asking whether there is obvious room for improvement.</p>
<p><em>// Compute the nth Fibonacci number recursively. // Optimized by<br>caching subproblem results</em> public int fib(int n) {</p>
<blockquote>
<p>if (n &lt; 2) return n;</p>
<p><em>// Create cache and initialize to -1</em></p>
<p>int[] cache = new int[n+1];</p>
<p>for (int i = 0; i &lt; cache.length; i++) {</p>
<p>cache[i] = -1;</p>
<p>}</p>
<p><em>// Fill initial values in cache</em></p>
<p>cache[0] = 0;</p>
<p>cache[1] = 1;</p>
<p>return fib(n, cache);</p>
</blockquote>
<p>}</p>
<p><em>// Overloaded private method</em></p>
<p>private int fib(int n, int[] cache) {</p>
<p><em>// If value is set in cache, return</em></p>
<blockquote>
<p>if (cache[n] &gt;= 0) return cache[n];</p>
<p><em>// Compute and add to cache before returning</em></p>
<p>cache[n] = fib(n-1, cache) + fib(n-2, cache);</p>
<p>return cache[n];</p>
<p>}</p>
</blockquote>
<p><em>Fig 3. Top-down dynamic Fibonacci solution</em></p>
<blockquote>
<p><strong>Turn the solution around</strong></p>
</blockquote>
<p>Since we now have a <strong>top-down solution</strong>, it is possible to reverse the<br>process and solve it from the bottom up. This <strong>*[can be done by<br>starting with the base cases and building up the solution from there by<br>computing the results of each subsequent subproblem, until we reach our<br>result.]{.underline}*</strong></p>
<p>In this problem, *[our base cases are fib(0) = 0 and fib(1) = 1. From<br>these two values, we can compute the next largest Fibonacci number,<br>fib(2) = fib(0) + fib(1). Once we have the value of fib(2), we can<br>calculate fib(3) etc. As we successively compute each Fibonacci number,<br>the previous values are saved and referred to as necessary, eventually<br>reaching fib(n).]{.underline}*</p>
<p>Our code for this process is fairly straightforward (<em>fig 5</em>).</p>
<p>This process yields a bottom-up solution. Since we iterate through all<br>of the numbers from 0 to n once, our time complexity will be O(n) and<br>our space will also be O(n), since we create a 1D array from 0 to n.<br>This makes our current solution comparable to the top-down solution,<br>although without recursion. This code is likely easier to understand.</p>
<p><em>// Compute the nth Fibonacci number iteratively</em></p>
<blockquote>
<p>public int fib(int n) {</p>
<p>if (n == 0) return 0;</p>
<p><em>// Initialize cache</em></p>
<p>int[] cache = new int[n+1];</p>
<p>cache[1] = 1;</p>
<p><em>// Fill cache iteratively</em></p>
<p>for (int i = 2; i &lt;= n; i++) {</p>
<p>cache[i] = cache[i-1] + cache[i-2];</p>
<p>}</p>
<p>return cache[n];</p>
<p>}</p>
<p><em>Fig 5. Bottom-up dynamic Fibonacci solution</em></p>
<p>it is possible to improve our solution further. During the computation<br>process, we only refer to the most recent two subproblems<br>(cache[i-1] and cache[i-2]) to compute the value of the current<br>subproblem. Therefore, cache[0] through cache[i-3] are unnecessary<br>and do not need to be kept in memory.</p>
</blockquote>
<p>We can, therefore, improve the space complexity of our solution to O(1)<br>by only caching the most recent two values.</p>
<blockquote>
<p><em>// Compute the nth Fibonacci number iteratively // with constant<br>space. We only need to save // the two most recently computed values</em></p>
<p>public int fib(int n) {</p>
<p>if (n &lt; 2) return n;</p>
<p>int n1 = 1, n2 = 0;</p>
<p>for (int i = 2; i &lt; n; i++) {</p>
<p>int n0 = n1 + n2;</p>
<p>n2 = n1;</p>
<p>n1 = n0;</p>
<p>}</p>
<p>return n1 + n2;</p>
<p>}</p>
</blockquote>
<p>For any problem where you are asked <strong>[to find the most/least/<br>largest/smallest]{.underline}</strong> etc, an excellent technique <strong>[is to<br>compare every possible combination]{.underline}</strong>. Although it will be<br>inefficient, efficiency is not the most important current consideration<br>and a solution of that nature is easy to make dynamic.</p>
<blockquote>
<p>Make change</p>
<p>// Brute force solution. Go through every</p>
<p>// combination of coins that sum up to c to // find the minimum number</p>
<p>public static int makeChange(int c) {</p>
<p>int[] coins = new int[]{10, 6, 1};</p>
<p>if (c == 0) return 0;</p>
<p>int minCoins = Integer.<em>MAX_VALUE</em>;</p>
<p>// Try removing each coin from the total and // see how many more<br>coins are required</p>
<p>for (int coin : coins) {</p>
<p>// Skip a coin if it’s value is greater</p>
<p>// than the amount remaining</p>
<p>if (c - coin &gt;= 0) {</p>
<p>int currMinCoins = <em>makeChange</em>(c - coin);</p>
<p>if (currMinCoins &lt; minCoins)</p>
<p>minCoins = currMinCoins;</p>
<p>} }</p>
<p>// Add back the coin removed recursively</p>
<p>return minCoins + 1;</p>
<p>}</p>
</blockquote>
<ul>
<li>How to convert one naive loop solution to dynamic programming<blockquote>
<p>top-down approach</p>
</blockquote>
</li>
</ul>
<p>Based on this understanding, we can turn our solution into a top-down<br>dynamic solution. We can cache the results as they are computed. That<br>means that we will cache the minimum number of coins needed to make<br>various smaller amounts of change.</p>
<p>Like the Fibonacci problem, our code doesn’t actually have to change<br>very much. It’s only necessary to overload our function with another<br>that can initialize the cache. Then we update the original function in<br>order to check the cache before doing the computation and saving the<br>result to the cache afterwards</p>
<p>// Top down dynamic solution. Cache the values as we compute them</p>
<p>// transform naive approach to top-down need:</p>
<p>// overload existing method with new one accept cache</p>
<p>// while existing one do two tasks: (1) initialize cache (2) call new<br>method passing in cache</p>
<p>public int makeChange_top_down(int c) {</p>
<p>// Initialize cache with values as -1</p>
<p>int[] cache = new int[c + 1];</p>
<p>for (int i = 1; i &lt; c + 1; i++)</p>
<p>cache[i] = -1;</p>
<p>return makeChange_top_down(c, cache);</p>
<p>}</p>
<p>// Overloaded recursive function</p>
<p>private int makeChange_top_down(int c, int[] cache) {</p>
<p>int[] coins = new int[]{10, 6, 1};</p>
<p>// Return the value if it’s in the cache</p>
<p>if (cache[c] &gt;= 0) return cache[c];</p>
<p>int minCoins = Integer.<em>MAX_VALUE</em>; //declare result oppositely, e.g.<br>question is &quot;min&quot;, so init return value would be Integer.MAX_VALUE</p>
<p>// Find the best coin</p>
<p>for (int coin : coins) {</p>
<p>if (c - coin &gt;= 0) {</p>
<p>int currMinCoins =</p>
<p>makeChange_top_down(c - coin, cache);</p>
<p>if (currMinCoins &lt; minCoins)</p>
<p>minCoins = currMinCoins;</p>
<p>} }</p>
<p>// Save the value into the cache</p>
<p>cache[c] = minCoins + 1; // add one to the return value</p>
<p>return cache[c];</p>
<p>}</p>
<p><strong>Turn the solution around</strong></p>
<p>Once the top-down solution is completed, it’s possible to flip it<br>around. We do this by solving the same subproblems in reverse order.<br>Rather than starting with our result in mind, we start with no change<br>and work our way up until we reach the solution.</p>
<p>The next step is to determine the subproblems that must be solved, in<br>order to solve successive subproblems. If we want to compute<br>makeChange(c), then we will have n different subproblems. If our coins<br>are {10, 6, 1}, we need to have the solutions for makeChange(c - 10),<br>makeChange(c - 6), and makeChange(c - 1).</p>
<p>Once makeChange() is solved for 0 through c - 1, it will be easy to<br>compute the value of makeChange(c). This is done by using the first<br>value, 0 as our base case. We can then compute the remaining values from<br>the previously computed values.</p>
<p><em>// Bottom up dynamic programming solution. // Iteratively compute<br>number of coins for // larger and larger amounts of change</em></p>
<p>public int makeChange(int c) {</p>
<p>int[] cache = new int[c + 1];</p>
<p>for (int i = 1; i &lt;= c; i++) {</p>
<p>int minCoins = Integer.MAX_VALUE;</p>
<p><em>// Try removing each coin from the total</em></p>
<p><em>// and see which requires the fewest</em></p>
<p><em>// extra coins</em></p>
<p>for (int coin : coins) {</p>
<p>if (i - coin &gt;= 0) {</p>
<p>int currCoins = cache[i-coin] + 1;</p>
<p>if (currCoins &lt; minCoins) {</p>
<p>minCoins = currCoins;</p>
<p>}</p>
<p>} }</p>
<p>cache[i] = minCoins;</p>
<p>}</p>
<p>return cache[c];</p>
<p>}</p>

        </div>
    

</div>
            
                
<div class="post">

    <div class="post-header index">
        <h1 class="title">
            <a href="/2020-05-10-Java-Deep-Notes/">
                Java Deep Notes
            </a>
        </h1>
        <div class="post-info">
            
                <span class="date">2020-05-10</span>
            
            
            
        </div>
    </div>

    
        <div class="content">
            <p>Java Deep Notes</p>
<h1 id="Is-string-concatenation-a-devil"><a href="#Is-string-concatenation-a-devil" class="headerlink" title="Is string concatenation a devil?"></a>Is string concatenation a devil?</h1><p>In fact, a string concatenation is going to be just fine, as the javac compiler will optimize the string concatenation as a series of append operations on a StringBuilder anyway. Here’s a part of the disassembly of the bytecode from the for loop from the above program:</p>

        </div>
    

</div>
            
                
<div class="post">

    <div class="post-header index">
        <h1 class="title">
            <a href="/2019-08-25-AWS-Certificate/">
                AWS certification
            </a>
        </h1>
        <div class="post-info">
            
                <span class="date">2020-01-19</span>
            
            
            
        </div>
    </div>

    
        <div class="content">
            <h1 id="Concepting"><a href="#Concepting" class="headerlink" title="Concepting"></a>Concepting</h1><p>Cloud computing is the on-demand delivery of IT resources and applications via the Internet with pay-as-you-go pricing. Whether you run applications that share photos to millions of mobile users or deliver services that support the critical operations of your business, the cloud provides rapid access to flexible and low-cost IT resources.</p>
<p>In its simplest form, cloud computing <code>provides an easy way to access servers, storage</code>, databases, and a broad set of application services <code>over the Internet</code>.</p>
<h1 id="Benefits-of-AWS"><a href="#Benefits-of-AWS" class="headerlink" title="Benefits of AWS"></a>Benefits of AWS</h1><p>There are six advantages for AWS clouding</p>
<ol>
<li>Global in minutes</li>
<li>Variable vs capital expense</li>
<li>Economies of scale</li>
<li>Stop guessing capacity</li>
<li>Focus on business differentaiors</li>
<li>Increate speed and agility </li>
</ol>
<h2 id="Cost-saving"><a href="#Cost-saving" class="headerlink" title="Cost saving"></a>Cost saving</h2><p>One of the key benefits of cloud computing is the opportunity to <code>replace up-front capital infrastructure expenses with low variable costs</code> that scale with your business. With the cloud, businesses no longer need to plan for and procure servers and other IT infrastructure weeks or months in advance. Instead, they can instantly spin up hundres or thousands of servers in minutes and deliver results faster.</p>
<p>With pay-per-use billing, AWS clouding services <code>become an operational expense instead of a capital expense</code>.</p>
<h1 id="Metadata"><a href="#Metadata" class="headerlink" title="Metadata"></a>Metadata</h1><p>Metadata, known as tags, that you can create and assign to your Amazon EC2 resources</p>
<h1 id="AZ-Available-Zones"><a href="#AZ-Available-Zones" class="headerlink" title="AZ (Available Zones)"></a>AZ (Available Zones)</h1><ul>
<li>Each availability zone is a physical data center in the region, but separate from the other ones (so that they’re isolated from disasters)</li>
<li>AWS Consoles are region scoped (except IAM, S3 &amp; Route53)</li>
</ul>
<h1 id="EC2"><a href="#EC2" class="headerlink" title="EC2"></a>EC2</h1><p>Here you need to create an AMI, but because AMI are bounded in the regions they are created, they need to be copied across regions for disaster recovery purposes</p>
<h2 id="EC2-instance-stopping"><a href="#EC2-instance-stopping" class="headerlink" title="EC2 instance stopping"></a>EC2 instance stopping</h2><p>If you stopped an EBS-backed EC2 instance, the volume is preserved but the data in any attached Instance store volumes will be erased. Keep in mind that an EC2 instance has an underlying physical host computer. If the instance is stopped, AWS usually moves the instance to a new host computer. Your instance may stay on the same host computer if there are no problems with the host computer. In addition, its Elastic IP address is disassociated from the instance if it is an EC2-Classic instance. Otherwise, if it is an EC2-VPC instance, the Elastic IP address remains associated.</p>
<h2 id="Placement-group"><a href="#Placement-group" class="headerlink" title="Placement group"></a>Placement group</h2><p> Placements groups are the answer here, where “cluster” guarantees high network performance (correct answer), whereas “spread” would guarantee independent failures between instances.</p>
<p> When you launch a new EC2 instance, the EC2 service attempts to place the instance in such a way that all of your instances are spread out across underlying hardware to minimize correlated failures. You can use placement groups to influence the placement of a group of interdependent instances to meet the needs of your workload. Depending on the type of workload, you can create a placement group using one of the following placement strategies:</p>
<p> Cluster – packs instances close together inside an Availability Zone. This strategy enables workloads to achieve the low-latency network performance necessary for tightly-coupled node-to-node communication that is typical of HPC applications.</p>
<p> Partition – spreads your instances across logical partitions such that groups of instances in one partition do not share the underlying hardware with groups of instances in different partitions. This strategy is typically used by large distributed and replicated workloads, such as Hadoop, Cassandra, and Kafka.</p>
<p> Spread – strictly places a small group of instances across distinct underlying hardware to reduce correlated failures.</p>
<p> There is no charge for creating a placement group.</p>
<p> Placement Groups is primarily used to determine how your instances are placed on the underlying hardware while Enhanced Networking, on the other hand, is for providing high-performance networking capabilities using single root I/O virtualization (SR-IOV) on supported EC2 instance types.</p>
<h2 id="Security-Group"><a href="#Security-Group" class="headerlink" title="Security Group"></a>Security Group</h2><p>When you create a security group, it has no inbound rules. Therefore, no inbound traffic originating from another host to your instance is allowed until you add inbound rules to the security group. By default, a security group includes an outbound rule that allows all outbound traffic. You can remove the rule and add outbound rules that allow specific outbound traffic only. If your security group has no outbound rules, no outbound traffic originating from your instance is allowed.</p>
<p>Options 1 and 4 are both incorrect because any changes to the Security Groups or Network Access Control Lists are applied immediately and not after 60 minutes or after the instance reboot.</p>
<p>Option 2 is incorrect because the scenario says that VPC is using a default configuration. Since by default, Network ACL allows all inbound and outbound IPv4 traffic, then there is no point of explicitly allowing the port in the Network ACL. Security Groups, on the other hand, does not allow incoming traffic by default, unlike Network ACL.</p>
<h3 id="Custom-port"><a href="#Custom-port" class="headerlink" title="Custom port"></a>Custom port</h3><p>To allow the custom port, you have to change the Inbound Rules in your Security Group to allow traffic coming from the mobile devices. Security Groups usually control the list of ports that are allowed to be used by your EC2 instances and the NACLs control which network or list of IP addresses can connect to your whole VPC.</p>
<h3 id="Cluster"><a href="#Cluster" class="headerlink" title="Cluster"></a>Cluster</h3><p> Cluster Placement Groups</p>
<p>A cluster placement group is a logical grouping of instances within a single Availability Zone. A placement group can span peered VPCs in the same Region. The chief benefit of a cluster placement group, in addition to a 10 Gbps flow limit, is the non-blocking, non-oversubscribed, fully bi-sectional nature of the connectivity. In other words, all nodes within the placement group can talk to all other nodes within the placement group at the full line rate of 10 Gbps flows and 100 Gbps aggregate without any slowing due to over-subscription.</p>
<h2 id="ASG"><a href="#ASG" class="headerlink" title="ASG"></a>ASG</h2><h3 id="ASG-Lauch-configuration"><a href="#ASG-Lauch-configuration" class="headerlink" title="ASG Lauch configuration"></a>ASG Lauch configuration</h3><p>Launch configurations are immutable meaning they cannot be updated. You have to create a new launch configuration, attach it to the ASG and then terminate old instances / launch new instances</p>
<h3 id="ASG-termination"><a href="#ASG-termination" class="headerlink" title="ASG termination"></a>ASG termination</h3><p>AZs will be balanced first, then the instance with the oldest launch configuration within that AZ will be terminated. For a reference to the default termination policy logic, have a look at this link: <a href="https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-termination.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-termination.html</a></p>
<h2 id="AMI"><a href="#AMI" class="headerlink" title="AMI"></a>AMI</h2><p>the EC2 instances you are currently using depends on a pre-built AMI. This AMI is not accessible to another region hence,  you have to copy it to the us-west-2 region to properly establish your disaster recovery instance.</p>
<p>You can copy an Amazon Machine Image (AMI) within or across an AWS region using the AWS Management Console, the AWS command line tools or SDKs, or the Amazon EC2 API, all of which support the CopyImage action. You can copy both Amazon EBS-backed AMIs and instance store-backed AMIs. You can copy encrypted AMIs and AMIs with encrypted snapshots</p>
<h1 id="AWS-Device-Farm"><a href="#AWS-Device-Farm" class="headerlink" title="AWS Device Farm"></a>AWS Device Farm</h1><p>AWS Device Farm is an app testing service that lets you test and interact with your Android, iOS, and web apps on many devices at once, or reproduce issues on a device in real time.</p>
<h1 id="IAM"><a href="#IAM" class="headerlink" title="IAM"></a>IAM</h1><p>Your whole AWS security is there:<br>• Users<br>• Groups<br>• Roles</p>
<p>Policies are written in JSON (JavaScript Object Notation)</p>
<p>IAM has a <code>global</code> view</p>
<p>Permissions let you specify access to AWS resources. Permissions are granted to IAM entities (users, groups, and roles) and by default these entities start with no permissions. In other words, IAM entities can do nothing in AWS until you grant them your desired permissions. To give entities permissions, you can attach a policy that specifies the type of access, the actions that can be performed, and the resources on which the actions can be performed. In addition, you can specify any conditions that must be set for access to be allowed or denied.</p>
<h2 id="To-enforce-IAM"><a href="#To-enforce-IAM" class="headerlink" title="To enforce IAM"></a>To enforce IAM</h2><ul>
<li>Enable Multi-Factor Authentication</li>
<li>Assign an IAM role to the Amazon EC2 instance</li>
</ul>
<p>Always remember that you should associate IAM roles to EC2 instances and not an IAM user, for the purpose of accessing other AWS services. IAM roles are designed so that your applications can securely make API requests from your instances, without requiring you to manage the security credentials that the applications use. Instead of creating and distributing your AWS credentials, you can delegate permission to make API requests using IAM roles.</p>
<h2 id="IAM-policies"><a href="#IAM-policies" class="headerlink" title="IAM policies"></a>IAM policies</h2><p>A permissions policy describes who has access to what. Policies attached to an IAM identity are identity-based policies (IAM policies) and policies attached to a resource are resource-based policies. Amazon RDS supports only identity-based policies (IAM policies).</p>
<h2 id="DB-authenticatioin-via-IAM"><a href="#DB-authenticatioin-via-IAM" class="headerlink" title="DB authenticatioin via IAM"></a>DB authenticatioin via IAM</h2><p>MySQL and PostgreSQL both support IAM database authentication.</p>
<p>To protect the confidential data of your customers, you have to ensure that your RDS database can only be accessed using the profile credentials specific to your EC2 instances via an authentication token.   </p>
<p>You can authenticate to your DB instance using AWS Identity and Access Management (IAM) database authentication. IAM database authentication works with MySQL and PostgreSQL. With this authentication method, you don’t need to use a password when you connect to a DB instance. Instead, you use an authentication token.</p>
<p>An authentication token is a unique string of characters that Amazon RDS generates on request. Authentication tokens are generated using AWS Signature Version 4. Each token has a lifetime of 15 minutes. You don’t need to store user credentials in the database, because authentication is managed externally using IAM. You can also still use standard database authentication.</p>
<h2 id="IAM-Federation"><a href="#IAM-Federation" class="headerlink" title="IAM Federation"></a>IAM Federation</h2><p>• Big enterprises usually integrate their own repository of users with IAM<br>• This way, one can login into AWS using their company credentials<br>• Identity Federation uses the SAML standard (Active Directory)</p>
<p>• One IAM User per PHYSICAL PERSON<br>• One IAM Role per Application</p>
<h2 id="STS"><a href="#STS" class="headerlink" title="STS"></a>STS</h2><p>Temporary Security Credentials<br>You can use the AWS Security Token Service (AWS STS) to create and provide trusted users with temporary security credentials that can control access to your AWS resources. Temporary security credentials work almost identically to the long-term access key credentials that your IAM users can use, with the following differences:</p>
<p>Temporary security credentials are short-term, as the name implies. They can be configured to last for anywhere from a few minutes to several hours. After the credentials expire, AWS no longer recognizes them or allows any kind of access from API requests made with them.</p>
<p>Temporary security credentials are not stored with the user but are generated dynamically and provided to the user when requested. When (or even before) the temporary security credentials expire, the user can request new credentials, as long as the user requesting them still has permissions to do so.</p>
<h3 id="Amazon-Cognito"><a href="#Amazon-Cognito" class="headerlink" title="Amazon Cognito"></a>Amazon Cognito</h3><p>This service is primarily used for user authentication and not for providing access to your AWS resources. A JSON Web Token (JWT) is meant to be used for user authentication and session management.</p>
<h3 id="AWS-SSO"><a href="#AWS-SSO" class="headerlink" title="AWS SSO"></a>AWS SSO</h3><p>This service uses STS, it does not issue short-lived credentials by itself. AWS Single Sign-On (SSO) is a cloud SSO service that makes it easy to centrally manage SSO access to multiple AWS accounts and business applications.</p>
<h1 id="Storage"><a href="#Storage" class="headerlink" title="Storage"></a>Storage</h1><h3 id="Performance"><a href="#Performance" class="headerlink" title="Performance"></a>Performance</h3><p>Instance Store will have the highest disk performance but comes with the storage being wiped if the instance is terminated, which is acceptable in this case. EBS volumes would provide good performance as far as disk goes, but not as good as Instance Store. EBS data survives instance termination or reboots. EFS is a network drive, and finally S3 cannot be mounted as a local disk (natively).</p>
<p>Need to define two terms:<br>• RPO: Recovery Point Objective<br>• RTO: Recovery Time Objective</p>
<h2 id="S3"><a href="#S3" class="headerlink" title="S3"></a>S3</h2><p>Generating S3 pre-signed URLs would bypass CloudFront, therefore we should use CloudFront signed URL. To generate that URL we must code, and Lambda is the perfect tool for running that code on the fly. </p>
<p>As the file is greater than 5GB in size, you must use Multi Part upload to upload that file to S3.</p>
<h3 id="S3-Glacier"><a href="#S3-Glacier" class="headerlink" title="S3 Glacier"></a>S3 Glacier</h3><p>Expedited retrievals allow you to quickly access your data when occasional urgent requests for a subset of archives are required. For all but the largest archives (250 MB+), data accessed using Expedited retrievals are typically made available within 1–5 minutes. Provisioned Capacity ensures that retrieval capacity for Expedited retrievals is available when you need it.</p>
<p>To make an Expedited, Standard, or Bulk retrieval, set the Tier parameter in the Initiate Job (POST jobs) REST API request to the option you want, or the equivalent in the AWS CLI or AWS SDKs. If you have purchased provisioned capacity, then all expedited retrievals are automatically served through your provisioned capacity.</p>
<p>Provisioned capacity ensures that your retrieval capacity for expedited retrievals is available when you need it. Each unit of capacity provides that at least three expedited retrievals can be performed every five minutes and provides up to 150 MB/s of retrieval throughput. You should purchase provisioned retrieval capacity if your workload requires highly reliable and predictable access to a subset of your data in minutes. Without provisioned capacity Expedited retrievals are accepted, except for rare situations of unusually high demand. However, if you require access to Expedited retrievals under all circumstances, you must purchase provisioned retrieval capacity.</p>
<h4 id="Amazon-Glacier-Select"><a href="#Amazon-Glacier-Select" class="headerlink" title="Amazon Glacier Select"></a>Amazon Glacier Select</h4><p>It is not an archive retrieval option and is primarily used to perform filtering operations using simple Structured Query Language (SQL) statements directly on your data archive in Glacier.</p>
<h4 id="Bulk-retrievals"><a href="#Bulk-retrievals" class="headerlink" title="Bulk retrievals"></a>Bulk retrievals</h4><p>It typically complete within 5–12 hours hence, this does not satisfy the requirement of retrieving the data within 15 minutes. The provisioned capacity option is also not compatible with Bulk retrievals.</p>
<h4 id="ranged-archive-retrievals"><a href="#ranged-archive-retrievals" class="headerlink" title="ranged archive retrievals"></a>ranged archive retrievals</h4><p>using ranged archive retrievals is not enough to meet the requirement of retrieving the whole archive in the given timeframe. In addition, it does not provide additional retrieval capacity which is what the provisioned capacity option can offer.</p>
<h3 id="S3-Select"><a href="#S3-Select" class="headerlink" title="S3 Select"></a>S3 Select</h3><p>It is an Amazon S3 feature that makes it easy to retrieve specific data from the contents of an object using simple SQL expressions without having to retrieve the entire object.<br>Similiarly, Amazon Redshift Spectrum is a feature of Amazon Redshift that enables you to run queries against exabytes of unstructured data in Amazon S3 with no loading or ETL required.</p>
<h3 id="OAI"><a href="#OAI" class="headerlink" title="OAI"></a>OAI</h3><p>Don’t make the S3 bucket public. You cannot attach IAM roles to the CloudFront distribution. S3 buckets don’t have security groups. Here you need to use an OAI. Read more here: <a href="https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html</a></p>
<p>Restricting Access to Amazon S3 Content by Using an Origin Access Identity<br>To restrict access to content that you serve from Amazon S3 buckets, you create CloudFront signed URLs or signed cookies to limit access to files in your Amazon S3 bucket, and then you create a special CloudFront user called an origin access identity (OAI) and associate it with your distribution. Then you configure permissions so that CloudFront can use the OAI to access and serve files to your users, but users can’t use a direct URL to the S3 bucket to access a file there. Taking these steps help you maintain secure access to the files that you serve through CloudFront.</p>
<p>In general, if you’re using an Amazon S3 bucket as the origin for a CloudFront distribution, you can either allow everyone to have access to the files there, or you can restrict access. If you limit access by using, for example, CloudFront signed URLs or signed cookies, you also won’t want people to be able to view files by simply using the direct URL for the file. Instead, you want them to only access the files by using the CloudFront URL, so your protections work. For more information about using signed URLs and signed cookies, see Serving Private Content with Signed URLs and Signed Cookies</p>
<h2 id="S3-Q-amp-A"><a href="#S3-Q-amp-A" class="headerlink" title="S3 Q&amp;A"></a>S3 Q&amp;A</h2><p>Amazon S3 now provides increased performance to support at least 3,500 requests per second to add data and 5,500 requests per second to retrieve data, which can save significant processing time for no additional charge. Each S3 prefix can support these request rates, making it simple to increase performance significantly.</p>
<p>Applications running on Amazon S3 today will enjoy this performance improvement with no changes, and customers building new applications on S3 do not have to make any application customizations to achieve this performance. Amazon S3’s support for parallel requests means you can scale your S3 performance by the factor of your compute cluster, without making any customizations to your application. Performance scales per prefix, so you can use as many prefixes as you need in parallel to achieve the required throughput. There are no limits to the number of prefixes.</p>
<p>This S3 request rate performance increase removes any previous guidance to randomize object prefixes to achieve faster performance. That means you can now use logical or sequential naming patterns in S3 object naming without any performance implications. This improvement is now available in all AWS Regions. </p>
<p>Option 1 is incorrect because it is an archival/long term storage solution, which is not optimal if you are serving objects frequently and fast retrieval is a must.</p>
<p>Option 2 is incorrect. Adding a random prefix is not required in this scenario because S3 can now scale automatically to adjust perfomance. You do not need to add a random prefix anymore for this purpose since S3 has increased performance to support at least 3,500 requests per second to add data and 5,500 requests per second to retrieve data, which covers the workload in the scenario.</p>
<p>Option 4 is incorrect because Amazon S3 already maintains an index of object key names in each AWS region. S3 stores key names in alphabetical order. The key name dictates which partition the key is stored in. Using a sequential prefix increases the likelihood that Amazon S3 will target a specific partition for a large number of your keys, overwhelming the I/O capacity of the partition.</p>
<h3 id="Encryption"><a href="#Encryption" class="headerlink" title="Encryption"></a>Encryption</h3><p>With SSE-C, your company can still provide the encryption key but let AWS do the encryption</p>
<h2 id="EBS-Elastic-Block-Storage"><a href="#EBS-Elastic-Block-Storage" class="headerlink" title="EBS (Elastic Block Storage)"></a>EBS (Elastic Block Storage)</h2><p>EBS is already redundant storage (replicated within an AZ)<br>But what if you want to increase IOPS to say 100 000 IOPS?</p>
<h3 id="RAID"><a href="#RAID" class="headerlink" title="RAID"></a>RAID</h3><h4 id="RAID-0-increase-performance"><a href="#RAID-0-increase-performance" class="headerlink" title="RAID 0 (increase performance)"></a>RAID 0 (increase performance)</h4><p> EC2 instance<br>One logical volume<br>either<br>EBS Volume 1<br>• Combining 2 or more volumes and getting the total disk space and I/O<br>• But one disk fails, all the data is failed</p>
<h4 id="RAID-1-increase-fault-tolerance"><a href="#RAID-1-increase-fault-tolerance" class="headerlink" title="RAID 1 (increase fault tolerance)"></a>RAID 1 (increase fault tolerance)</h4><p> EC2 instance<br>One logical volume<br>both<br>• RAID 1 = Mirroring a volume to another<br>• If one disk fails, our logical volume is still working<br>• We have to send the data to two EBS volume at the same time (2x network)</p>
<h3 id="EBS-types"><a href="#EBS-types" class="headerlink" title="EBS types"></a>EBS types</h3><p> keeping as io1 but reducing the iops may interfere with the burst of performance we need. The EC2 instance type changes won’t affect the 90% of the costs that are incurred to us. CloudFormation is a free service to use. Therefore, gp2 is the right choice, allowing us to save on cost while keeping a burst in performance when needed</p>
<p>You can now choose between three Amazon EBS volume types to best meet the needs of your workloads: General Purpose (SSD), Provisioned IOPS (SSD), and Magnetic volumes. </p>
<h4 id="General-Purpose-SSD"><a href="#General-Purpose-SSD" class="headerlink" title="General Purpose (SSD)"></a>General Purpose (SSD)</h4><p>GP2 volumes are suitable for a broad range of workloads, including small to medium-sized databases, development and test environments, and boot volumes. </p>
<h4 id="Provisioned-IOPS-SSD"><a href="#Provisioned-IOPS-SSD" class="headerlink" title="Provisioned IOPS (SSD)"></a>Provisioned IOPS (SSD)</h4><p>Such volumes offer storage with consistent and low-latency performance, are designed for I/O-intensive applications such as large relational or NoSQL databases, and allow you to choose the level of performance you need. </p>
<h4 id="Magnetic-volumes"><a href="#Magnetic-volumes" class="headerlink" title="Magnetic volumes"></a>Magnetic volumes</h4><p>formerly known as Standard volumes, provide the lowest cost per gigabyte of all Amazon EBS volume types and are ideal for workloads where data is accessed infrequently and applications where the lowest storage cost is important.</p>
<p>Backed by Solid-State Drives (SSDs), General Purpose (SSD) volumes provide the ability to burst to 3,000 IOPS per volume, independent of volume size, to meet the performance needs of most applications and also deliver a consistent baseline of 3 IOPS/GB. General Purpose (SSD) volumes offer the same five nines of availability and durable snapshot capabilities as other volume types. Pricing and performance for General Purpose (SSD) volumes are simple and predictable. You pay for each GB of storage you provision, and there are no additional charges for I/O performed on a volume. Prices start as low as $0.10/GB.</p>
<h4 id="EBS-snapshot"><a href="#EBS-snapshot" class="headerlink" title="EBS snapshot"></a>EBS snapshot</h4><p>While it is completing, an in-progress snapshot is not affected by ongoing reads and writes to the volume.</p>
<p>You can take a snapshot of an attached volume that is in use. However, snapshots only capture data that has been written to your Amazon EBS volume at the time the snapshot command is issued. This might exclude any data that has been cached by any applications or the operating system. If you can pause any file writes to the volume long enough to take a snapshot, your snapshot should be complete. However, if you can’t pause all file writes to the volume, you should unmount the volume from within the instance, issue the snapshot command, and then remount the volume to ensure a consistent and complete snapshot. You can remount and use your volume while the snapshot status is pending.</p>
<h4 id="Save-network-cost"><a href="#Save-network-cost" class="headerlink" title="Save network cost"></a>Save network cost</h4><p> S3 would imply changing the application code, Glacier is not applicable as the files are frequently requested, Storage Gateway isn’t for distributing files to end users. CloudFront is the right answer, because we can put it in front of our ASG and leverage a Global Caching feature that will help us distribute the content reliably with dramatically reduced costs (the ASG won’t need to scale as much)</p>
<h2 id="EFS"><a href="#EFS" class="headerlink" title="EFS"></a>EFS</h2><p>Instance Stores or EBS volumes are local disks and cannot be shared across instances. Here, we need a network file system (NFS), which is exactly what EFS is designed for.</p>
<h1 id="Redshift"><a href="#Redshift" class="headerlink" title="Redshift"></a>Redshift</h1><p>Creating a smaller cluster with the cold data would not decrease the storage cost of Redshift, which will increase as we keep on creating data. Moving the data to S3 glacier will prevent us from being able to query it. Redshift’s internal storage does not have “tiers”. Therefore, we should migrate the data to S3 IA and use Athena (serverless SQL query engine on top of S3) to analyze the cold data.</p>
<p>Amazon Redshift is a fast, scalable data warehouse that makes it simple and cost-effective to analyze all your data across your data warehouse and data lake. Redshift delivers ten times faster performance than other data warehouses by using machine learning, massively parallel query execution, and columnar storage on high-performance disk.</p>
<p>In this scenario, there is a requirement to have a storage service which will be used by a business intelligence application and where the data must be stored in a columnar fashion. Business Intelligence reporting systems is a type of Online Analytical Processing (OLAP) which Redshift is known to support. In addition, Redshift also provides columnar storage unlike the other options. Hence, the correct answer in this scenario is Option 1: Amazon Redshift.</p>
<h2 id="RedShift-Spectrum"><a href="#RedShift-Spectrum" class="headerlink" title="RedShift Spectrum"></a>RedShift Spectrum</h2><p>Enables you to run queries against exabytes of data in S3 without having to load or transform any data.<br>Redshift Spectrum doesn’t use Enhanced VPC Routing.<br>If you store data in a columnar format, Redshift Spectrum scans only the columns needed by your query, rather than processing entire rows.<br>If you compress your data using one of Redshift Spectrum’s supported compression algorithms, less data is scanned.</p>
<h1 id="CloundFront"><a href="#CloundFront" class="headerlink" title="CloundFront"></a>CloundFront</h1><h2 id="Origin"><a href="#Origin" class="headerlink" title="Origin"></a>Origin</h2><p>Until now, CloudFront could serve up content from Amazon S3. In content-distribution lingo, S3 was the only supported origin server. You would store your web objects (web pages, style sheets, images, JavaScript, and so forth) in S3, and then create a CloudFront distribution. Here is the basic flow:</p>
<p>Effective today we are opening up CloudFront and giving you the ability to use the origin server of your choice.</p>
<p>You can now create a CloudFront distribution using a custom origin. Each distribution will can point to an S3 or to a custom origin. This could be another storage service, or it could be something more interesting and more dynamic, such as an EC2 instance or even an Elastic Load Balancer:</p>
<h1 id="CloudFormation"><a href="#CloudFormation" class="headerlink" title="CloudFormation"></a>CloudFormation</h1><h2 id="CloudFormation-vs-Elastic-Beanstalk"><a href="#CloudFormation-vs-Elastic-Beanstalk" class="headerlink" title="CloudFormation vs Elastic Beanstalk"></a>CloudFormation vs Elastic Beanstalk</h2><p>Elastic Beanstalk provides an environment to easily deploy and run applications in the cloud.<br>CloudFormation is a convenient provisioning mechanism for a broad range of AWS resources.</p>
<h1 id="VPC"><a href="#VPC" class="headerlink" title="VPC"></a>VPC</h1><p>you can optionally connect to your own network, known as virtual private clouds (VPCs)</p>
<p>Amazon Web Services. Amazon Elastic Compute Cloud.</p>
<p>Amazon VPC lets you provision a logically isolated section of the Amazon Web Services (AWS) cloud where you can launch AWS resources in a virtual network that you define. You have complete control over your virtual networking environment, including selection of your own IP address ranges, creation of subnets, and configuration of route tables and network gateways. You can also create a hardware Virtual Private Network (VPN) connection between your corporate datacenter and your VPC and leverage the AWS cloud as an extension of your corporate datacenter.</p>
<h2 id="Subnet"><a href="#Subnet" class="headerlink" title="Subnet"></a>Subnet</h2><p>A subnet is a range of IP addresses in your VPC. You can launch AWS resources into a specified subnet. Use a public subnet for resources that must be connected to the internet, and a private subnet for resources that won’t be connected to the internet.<br>To protect the AWS resources in each subnet, use security groups and network access control lists (ACL).</p>
<p>Remember that one subnet is mapped into one specific Availability Zone.</p>
<h3 id="Subnet-and-Avaialability-Zone-and-VPC"><a href="#Subnet-and-Avaialability-Zone-and-VPC" class="headerlink" title="Subnet and Avaialability Zone and VPC"></a>Subnet and Avaialability Zone and VPC</h3><p>A VPC spans all the Availability Zones in the region. After creating a VPC, you can add one or more subnets in each Availability Zone. When you create a subnet, you specify the CIDR block for the subnet, which is a subset of the VPC CIDR block. Each subnet must reside entirely within one Availability Zone and cannot span zones. Availability Zones are distinct locations that are engineered to be isolated from failures in other Availability Zones. By launching instances in separate Availability Zones, you can protect your applications from the failure of a single location. AWS assigns a unique ID to each subnet.</p>
<h3 id="Default-subnet"><a href="#Default-subnet" class="headerlink" title="Default subnet"></a>Default subnet</h3><p>By default, a “default subnet” of your VPC is actually a public subnet, because the main route table sends the subnet’s traffic that is destined for the internet to the internet gateway. You can make a default subnet into a private subnet by removing the route from the destination 0.0.0.0/0 to the internet gateway. However, if you do this, any EC2 instance running in that subnet can’t access the internet.</p>
<p>Instances that you launch into a default subnet receive both a public IPv4 address and a private IPv4 address, and both public and private DNS hostnames. Instances that you launch into a nondefault subnet in a default VPC don’t receive a public IPv4 address or a DNS hostname. You can change your subnet’s default public IP addressing behavior</p>
<p>By default, nondefault subnets have the IPv4 public addressing attribute set to false, and default subnets have this attribute set to true. An exception is a nondefault subnet created by the Amazon EC2 launch instance wizard — the wizard sets the attribute to true. </p>
<p>Newly created instance does not have a public IP address since it was deployed on a nondefault subnet. The other 4 instances are accessible over the Internet because they each have an Elastic IP address attached, unlike the last instance which only has a private IP address. An Elastic IP address is a public IPv4 address, which is reachable from the Internet. If your instance does not have a public IPv4 address, you can associate an Elastic IP address with your instance to enable communication with the Internet.</p>
<h2 id="VPC-Endpoint"><a href="#VPC-Endpoint" class="headerlink" title="VPC Endpoint"></a>VPC Endpoint</h2><p>You must remember that the two services that use a VPC Endpoint Gateway are Amazon S3 and DynamoDB. The rest are VPC Endpoint Interface</p>
<h3 id="NACL-Network-ACL"><a href="#NACL-Network-ACL" class="headerlink" title="NACL (Network ACL)"></a>NACL (Network ACL)</h3><p>NACL is stateless.</p>
<p>• NACL are like a firewall which control traffic from and to subnet<br>• Default NACL allows everything outbound and everything inbound<br>• One NACL per Subnet, new Subnets are assigned the Default NACL<br>• Define NACL rules:<br>• Rules have a number (1-32766) and higher precedence with a lower number<br>• E.g. If you define #100 ALLOW <ip> and #200 DENY <ip> , IP will be allowed • Last rule is an asterisk (*) and denies a request in case of no rule match<br>• AWS recommends adding rules by increment of 100<br>• Newly created NACL will deny everything<br>• NACL are a great way of blocking a specific IP at the subnet level</ip></ip></p>
<h3 id="NACL-noteworhty-points"><a href="#NACL-noteworhty-points" class="headerlink" title="NACL noteworhty points"></a>NACL noteworhty points</h3><p>Network ACL Basics</p>
<ul>
<li>Your VPC automatically comes with a modifiable default network ACL. By default, it allows all inbound and outbound IPv4 traffic and, if applicable, IPv6 traffic.</li>
</ul>
<h3 id="IGW-Internet-GateWay"><a href="#IGW-Internet-GateWay" class="headerlink" title="IGW (Internet GateWay)"></a>IGW (Internet GateWay)</h3><p>After creating an IGW, make sure the route tables are updated. Additionally, ensure the security group allow the ICMP protocol for ping requests</p>
<h3 id="NAT"><a href="#NAT" class="headerlink" title="NAT"></a>NAT</h3><p>NAT Instances would work but won’t scale and you would have to manage them (as they’re EC2 instances). Egress-Only Internet Gateways are for IPv6, not IPv4. Internet Gateways must be deployed in a public subnet. Therefore you must use a NAT Gateway in your public subnet in order to provide internet access to your instances in your private subnets.</p>
<p>You do not need a NAT Gateway nor a NAT instance when the instances are already in public subnet. Remember that a NAT Gateway or a NAT instance is primarily used to enable instances in a private subnet to connect to the Internet or other AWS services, but prevent the Internet from initiating a connection with those instances.</p>
<h2 id="How-to-prevent-DDoS"><a href="#How-to-prevent-DDoS" class="headerlink" title="How to prevent DDoS"></a>How to prevent DDoS</h2><p>AWS provides flexible infrastructure and services that help customers implement strong DDoS mitigations and create highly available application architectures that follow AWS Best Practices for DDoS Resiliency. These include services such as Amazon Route 53, Amazon CloudFront, Elastic Load Balancing, and AWS WAF to control and absorb traffic, and deflect unwanted requests. These services integrate with AWS Shield, a managed DDoS protection service that provides always-on detection and automatic inline mitigations to safeguard web applications running on AWS.</p>
<h3 id="AWS-Shield"><a href="#AWS-Shield" class="headerlink" title="AWS Shield"></a>AWS Shield</h3><p>AWS Shield is a managed DDoS protection service that is available in two tiers: Standard and Advanced. AWS Shield Standard applies always-on detection and inline mitigation techniques, such as deterministic packet filtering and priority-based traffic shaping, to minimize application downtime and latency. </p>
<h4 id="AWS-Shield-Standard"><a href="#AWS-Shield-Standard" class="headerlink" title="AWS Shield Standard"></a>AWS Shield Standard</h4><p>It is included automatically and transparently to your Elastic Load Balancing load balancers, Amazon CloudFront distributions, and Amazon Route 53 resources at no additional cost. When you use these services that include AWS Shield Standard, you receive comprehensive availability protection against all known infrastructure layer attacks. Customers who have the technical expertise to manage their own monitoring and mitigation of application layer attacks can use AWS Shield together with AWS WAF rules to create a comprehensive DDoS attack mitigation strategy.</p>
<h4 id="AWS-Shield-Advanced"><a href="#AWS-Shield-Advanced" class="headerlink" title="AWS Shield Advanced"></a>AWS Shield Advanced</h4><p>It provides enhanced DDoS attack detection and monitoring for application-layer traffic to your Elastic Load Balancing load balancers, CloudFront distributions, Amazon Route 53 hosted zones and resources attached to an Elastic IP address, such Amazon EC2 instances. AWS Shield Advanced uses additional techniques to provide granular detection of DDoS attacks, such as resource-specific traffic monitoring to detect HTTP floods or DNS query floods. AWS Shield Advanced includes 24x7 access to the AWS DDoS Response Team (DRT), support experts who apply manual mitigations for more complex and sophisticated DDoS attacks, directly create or update AWS WAF rules, and can recommend improvements to your AWS architectures. AWS WAF is included at no additional cost for resources that you protect with AWS Shield Advanced.</p>
<p>AWS Shield Advanced includes access to near real-time metrics and reports, for extensive visibility into infrastructure layer and application layer DDoS attacks. You can combine AWS Shield Advanced metrics with additional, fine-tuned AWS WAF metrics for a more comprehensive CloudWatch monitoring and alarming strategy. Customers subscribed to AWS Shield Advanced can also apply for a credit for charges that result from scaling during a DDoS attack on protected Amazon EC2, Amazon CloudFront, Elastic Load Balancing, or Amazon Route 53 resources. See the AWS Shield Developer Guide for a detailed comparison of the two AWS Shield offerings.</p>
<h2 id="CIDR"><a href="#CIDR" class="headerlink" title="CIDR"></a>CIDR</h2><p>To add a CIDR block to your VPC, the following rules apply:</p>
<p>-The allowed block size is between a /28 netmask and /16 netmask.<br>-The CIDR block must not overlap with any existing CIDR block that’s associated with the VPC.<br>-You cannot increase or decrease the size of an existing CIDR block.<br>-You have a limit on the number of CIDR blocks you can associate with a VPC and the number of routes you can add to a route table. You cannot associate a CIDR block if this results in you exceeding your limits.<br>-The CIDR block must not be the same or larger than the CIDR range of a route in any of the VPC route tables. For example, if you have a route with a destination of 10.0.0.0/24 to a virtual private gateway, you cannot associate a CIDR block of the same range or larger. However, you can associate a CIDR block of 10.0.0.0/25 or smaller.<br>-The first four IP addresses and the last IP address in each subnet CIDR block are not available for you to use, and cannot be assigned to an instance.</p>
<p>IPv4 CIDR block size should be between a /16 netmask (65,536 IP addresses) and /28 netmask (16 IP addresses).<br>The first four IP addresses and the last IP address in each subnet CIDR block are NOT available for you to use, and cannot be assigned to an instance.</p>
<p>If you’re using AWS Direct Connect to connect to multiple VPCs through a direct connect gateway, the VPCs that are associated with the direct connect gateway must not have overlapping CIDR blocks.</p>
<h2 id="Subnet-Routing"><a href="#Subnet-Routing" class="headerlink" title="Subnet Routing"></a>Subnet Routing</h2><p>Each subnet must be associated with a route table, which specifies the allowed routes for outbound traffic leaving the subnet.<br>Every subnet that you create is automatically associated with the main route table for the VPC.<br>You can change the association, and you can change the contents of the main route table.<br>You can allow an instance in your VPC to initiate outbound connections to the internet over IPv4 but prevent unsolicited inbound connections from the internet using a NAT gateway or NAT instance.<br>To initiate outbound-only communication to the internet over IPv6, you can use an egress-only internet gateway</p>
<h2 id="Subnet-Security"><a href="#Subnet-Security" class="headerlink" title="Subnet Security"></a>Subnet Security</h2><h3 id="Security-Groupss-—-control-inbound-and-outbound-traffic-for-your-instances"><a href="#Security-Groupss-—-control-inbound-and-outbound-traffic-for-your-instances" class="headerlink" title="Security Groupss — control inbound and outbound traffic for your instances"></a><code>Security Groups</code>s — control inbound and outbound traffic for your <code>instances</code></h3><p>You can associate one or more (up to five) security groups to an instance in your VPC.<br>If you don’t specify a security group, the instance automatically belongs to the default security group.<br>When you create a security group, it has no inbound rules. By default, it includes an outbound rule that allows all outbound traffic.</p>
<h3 id="Security-groups-are-associated-with-network-interfaces"><a href="#Security-groups-are-associated-with-network-interfaces" class="headerlink" title="Security groups are associated with network interfaces."></a>Security groups are associated with network interfaces.</h3><p><code>Network Access Control Lists</code> — control inbound and outbound traffic for your <code>subnets</code><br>Each subnet in your VPC must be associated with a network ACL. If none is associated, automatically associated with the default network ACL.<br>You can associate a network ACL with multiple subnets; however, a subnet can be associated with only one network ACL at a time.<br>A network ACL contains a numbered list of rules that is evaluated in order, starting with the lowest numbered rule, to determine whether traffic is allowed in or out of any subnet associated with the network ACL.<br>The default network ACL is configured to allow all traffic to flow in and out of the subnets to which it is associated.</p>
<p>Amazon security groups and network ACLs don’t filter traffic to or from link-local addresses or AWS-reserved IPv4 addresses. Flow logs do not capture IP traffic to or from these addresses.</p>
<h3 id="Security-Group-vs-NetACL"><a href="#Security-Group-vs-NetACL" class="headerlink" title="Security Group vs NetACL"></a>Security Group vs NetACL</h3><ul>
<li>Security group operates at the instance level while NetACL at the subnet level</li>
<li>Security group support allow rules only while ACL allows rules and deny rules</li>
<li>Security group is stateful, return traffic is automatcially allowed, regardless of any rules.<br>While ACL is stateless, return traffic must be explicitely allowed by rules</li>
<li>Security group evalaute all rules before deciding whether to allow traffic, while ACL process rules in number order when deciding whether to allow traffic.</li>
<li>Security group appiles to an insatnce only while ACL appies to all instances in teh subect it’s associated with.</li>
</ul>
<h2 id="VPC-NetACL"><a href="#VPC-NetACL" class="headerlink" title="VPC NetACL"></a>VPC NetACL</h2><p>A network access control list (ACL) is an optional layer of security for your VPC that acts as a firewall for controlling traffic in and out of one or more subnets. You might set up network ACLs with rules similar to your security groups in order to add an additional layer of security to your VPC.</p>
<p>Network ACL Rules are evaluated by rule number, <code>from lowest to highest</code>, and <code>executed immediately when a matching allow/deny rule is found</code>s.</p>
<h3 id="Internet-access"><a href="#Internet-access" class="headerlink" title="Internet access"></a>Internet access</h3><p>To enable access to or from the Internet for instances in a VPC subnet, you must do the following:</p>
<ul>
<li>Attach an Internet Gateway to your VPC</li>
<li>Ensure that your subnet’s route table points to the Internet Gateway.</li>
<li>Ensure that instances in your subnet have a globally unique IP address (public IPv4 address, Elastic IP address, or IPv6 address).</li>
<li>Ensure that your network access control and security group rules allow the relevant traffic to flow to and from your instance</li>
</ul>
<h2 id="NAT-1"><a href="#NAT-1" class="headerlink" title="NAT"></a>NAT</h2><p>Enable instances in a private subnet to connect to the internet or other AWS services, but prevent the internet from initiating connections with the instances.<br>NAT Gateways<br>You must specify the public subnet in which the NAT gateway should reside.<br>You must specify an Elastic IP address to associate with the NAT gateway when you create it.</p>
<h2 id="Accessing-a-Corporate-or-Home-Network"><a href="#Accessing-a-Corporate-or-Home-Network" class="headerlink" title="Accessing a Corporate or Home Network"></a>Accessing a Corporate or Home Network</h2><p>You can optionally connect your VPC to your own corporate data center using an IPsec AWS managed VPN connection, making the AWS Cloud an extension of your data center.<br>A VPN connection consists of:</p>
<ul>
<li>a virtual private gateway (which is the VPN concentrator on the Amazon side of the VPN connection) attached to your VPC.</li>
<li>a customer gateway (which is a physical device or software appliance on your side of the VPN connection) located in your data center.</li>
</ul>
<p>By default, instances that you launch into a virtual private cloud (VPC) can’t communicate with your own network. You can enable access to your network from your VPC by attaching a virtual private gateway to the VPC, creating a custom route table, updating your security group rules, and creating an AWS managed VPN connection.</p>
<p>Although the term VPN connection is a general term, in the Amazon VPC documentation, a VPN connection refers to the connection between your VPC and your own network. AWS supports Internet Protocol security (IPsec) VPN connections.</p>
<p>A customer gateway is a physical device or software application on your side of the VPN connection.</p>
<p>To create a VPN connection, you must create a customer gateway resource in AWS, which provides information to AWS about your customer gateway device. Next, you have to set up an Internet-routable IP address (static) of the customer gateway’s external interface.</p>
<p>The following diagram illustrates single VPN connections. The VPC has an attached virtual private gateway, and your remote network includes a customer gateway, which you must configure to enable the VPN connection. You set up the routing so that any traffic from the VPC bound for your network is routed to the virtual private gateway.</p>
<h2 id="Site-to-Site-VPN"><a href="#Site-to-Site-VPN" class="headerlink" title="Site-to-Site VPN"></a>Site-to-Site VPN</h2><p>With AWS Site-to-Site VPN, you can connect to an Amazon VPC in the cloud the same way you connect to your branches. AWS Site-to-Site VPN establishes secure and private sessions with IP Security (IPSec) and Transport Layer Security (TLS) tunnels. a VPN connection is that you will be able to connect your Amazon VPC to other remote networks securely.  Although it is true that a VPN provides a cost-effective, hybrid connection from your VPC to your on-premises data centers, it certainly does not bypasses the public Internet. A VPN connection actually goes through the public Internet, unlike the AWS Direct Connect connection which has a direct and dedicated connection to your on-premises network.</p>
<h2 id="AWS-Direct-Connect"><a href="#AWS-Direct-Connect" class="headerlink" title="AWS Direct Connect"></a>AWS Direct Connect</h2><p>AWS Direct Connect connection which has a direct and dedicated connection to your on-premises network.</p>
<h2 id="AWS-PrivateLink"><a href="#AWS-PrivateLink" class="headerlink" title="AWS PrivateLink"></a>AWS PrivateLink</h2><p>It enables you to privately connect your VPC to supported AWS services, services hosted by other AWS accounts (VPC endpoint services), and supported AWS Marketplace partner services. You do not require an internet gateway, NAT device, public IP address, AWS Direct Connect connection, or VPN connection to communicate with the service. Traffic between your VPC and the service does not leave the Amazon network.</p>
<p>You can create a VPC peering connection between your VPCs, or with a VPC in another AWS account, and enable routing of traffic between the VPCs using private IP addresses. You cannot create a VPC peering connection between VPCs that have overlapping CIDR blocks.</p>
<p>Applications in an Amazon VPC can securely access AWS PrivateLink endpoints across VPC peering connections. The support of VPC peering by AWS PrivateLink makes it possible for customers to privately connect to a service even if that service’s endpoint resides in a different Amazon VPC that is connected using VPC peering.<br>AWS PrivateLink endpoints can now be accessed across both intra- and inter-region VPC peering connections.</p>
<h2 id="HA-for-VPN"><a href="#HA-for-VPN" class="headerlink" title="HA for VPN"></a>HA for VPN</h2><p> You can do the following to provide a highly available, fault-tolerant network connection:</p>
<ul>
<li>Establish a hardware VPN over the Internet between the VPC and the on-premises network.</li>
<li>Establish another AWS Direct Connect connection and private virtual interface in the same AWS region. </li>
</ul>
<h2 id="Q-amp-A"><a href="#Q-amp-A" class="headerlink" title="Q&amp;A"></a>Q&amp;A</h2><p>First, the Network ACL should be properly set to allow communication between the two subnets. The security group should also be properly configured so that your web server can communicate with the database server. Hence, options 1 and 4 are the correct answers:</p>
<p>Check if all security groups are set to allow the application host to communicate to the database on the right port and protocol.<br>Check the Network ACL if it allows communication between the two subnets.</p>
<p>Option 2 is incorrect because the EC2 instances do not need to be of the same class in order to communicate with each other.</p>
<p>Option 3 is incorrect because an Internet gateway is primarily used to communicate to the Internet.</p>
<p>Option 5 is incorrect because Placement Group is mainly used to provide low-latency network performance necessary for tightly-coupled node-to-node communication.</p>
<h1 id="AWS-WAF"><a href="#AWS-WAF" class="headerlink" title="AWS WAF"></a>AWS WAF</h1><p>AWS WAF is a web application firewall that helps protect web applications from common web exploits that could affect application availability, compromise security, or consume excessive resources. You can use AWS WAF to define customizable web security rules that control which traffic accesses your web applications. If you use AWS Shield Advanced, you can use AWS WAF at no extra cost for those protected resources and can engage the DRT to create WAF rules.</p>
<p>AWS WAF rules use conditions to target specific requests and trigger an action, allowing you to identify and block common DDoS request patterns and effectively mitigate a DDoS attack. These include size constraint conditions to block a web request based on the length of its query string or request body, and geographic match conditions to implement geo restriction (also known as geoblocking) on requests that originate from specific countries. </p>
<h1 id="AWS-SSM-Simple-System-Manager"><a href="#AWS-SSM-Simple-System-Manager" class="headerlink" title="AWS SSM (Simple System Manager)"></a>AWS SSM (Simple System Manager)</h1><p>AWS SSM is parameter store.</p>
<h1 id="ELB-Elastic-Load-Balancing"><a href="#ELB-Elastic-Load-Balancing" class="headerlink" title="ELB: Elastic Load Balancing"></a>ELB: Elastic Load Balancing</h1><p>To automatically distribute incoming application traffic across multiple instances, use Elastic Load Balancing. </p>
<p>For HA, even though our ASG is deployed across 3 AZ, the minimum capacity to be highly available is 2. Finally, we can save costs by reserving these two instances as we know they’ll be up and running at any time</p>
<h2 id="Application-Load-Balancer-vs-Network-load-balancer"><a href="#Application-Load-Balancer-vs-Network-load-balancer" class="headerlink" title="Application Load Balancer vs Network load balancer"></a>Application Load Balancer vs Network load balancer</h2><p>Path based routing and host based routing are only available for the Application Load Balancer (ALB). Deploying an NGINX load balancer on EC2 would work but would suffer management and scaling issues. Read more here: <a href="https://aws.amazon.com/blogs/aws/new-host-based-routing-support-for-aws-application-load-balancers/" target="_blank" rel="noopener">https://aws.amazon.com/blogs/aws/new-host-based-routing-support-for-aws-application-load-balancers/</a></p>
<h3 id="ALB-amp-ASG"><a href="#ALB-amp-ASG" class="headerlink" title="ALB &amp; ASG"></a>ALB &amp; ASG</h3><p>Adding the entire CIDR of the ALB would work, but wouldn’t guarantee that only the ALB can access the EC2 instances that are part of the ASG. Here, the right solution is to add a rule on the ASG security group to allow incoming traffic from the security group configured for the ALB.</p>
<h3 id="SNI"><a href="#SNI" class="headerlink" title="SNI"></a>SNI</h3><p>support for multiple TLS/SSL certificates on Application Load Balancers (ALB) using Server Name Indication (SNI). You can now host multiple TLS secured applications, each with its own TLS certificate, behind a single load balancer. In order to use SNI, all you need to do is bind multiple certificates to the same secure listener on your load balancer. ALB will automatically choose the optimal TLS certificate for each client. These new features are provided at no additional charge.</p>
<p>One of our most frequent requests on forums, reddit, and in my e-mail inbox has been to use the Server Name Indication (SNI) extension of TLS to choose a certificate for a client. Since TLS operates at the transport layer, below HTTP, it doesn’t see the hostname requested by a client. SNI works by having the client tell the server “This is the domain I expect to get a certificate for” when it first connects. The server can then choose the correct certificate to respond to the client. All modern web browsers and a large majority of other clients support SNI. In fact, today we see SNI supported by over 99.5% of clients connecting to CloudFront.</p>
<h1 id="RDS"><a href="#RDS" class="headerlink" title="RDS"></a>RDS</h1><p>RDS stands for Relational Database Service</p>
<h2 id="Read-Replics"><a href="#Read-Replics" class="headerlink" title="Read Replics"></a>Read Replics</h2><p>RDS Read Replicas for read scalability</p>
<ul>
<li>Up to 5 Read Replicas</li>
<li>Within AZ, Cross AZ or Cross Region</li>
<li>Replication is ASYNC, so reads are eventually consistent</li>
<li>Replicas can be promoted to their own DB</li>
<li>Applications must update the connection string to leverage read replicas</li>
</ul>
<h2 id="Amazon-RDS-Multi-AZ-Deployments"><a href="#Amazon-RDS-Multi-AZ-Deployments" class="headerlink" title="Amazon RDS Multi-AZ Deployments"></a>Amazon RDS Multi-AZ Deployments</h2><p>Amazon RDS Multi-AZ deployments provide enhanced availability and durability for Database (DB) Instances, making them a natural fit for production database workloads. When you provision a Multi-AZ DB Instance, Amazon RDS automatically creates a primary DB Instance and synchronously replicates the data to a standby instance in a different Availability Zone (AZ). Each AZ runs on its own physically distinct, independent infrastructure, and is engineered to be highly reliable. In case of an infrastructure failure, Amazon RDS performs an automatic failover to the standby (or to a read replica in the case of Amazon Aurora), so that you can resume database operations as soon as the failover is complete. Since the endpoint for your DB Instance remains the same after a failover, your application can resume database operation without the need for manual administrative intervention.</p>
<h2 id="Security"><a href="#Security" class="headerlink" title="Security"></a>Security</h2><p>Use security groups to control what IP addresses or Amazon EC2 instances can connect to your databases on a DB instance.<br>Run your DB instance in an Amazon Virtual Private Cloud (VPC) for the greatest possible network access control.</p>
<h2 id="Working-with-a-DB-Instance-in-a-VPC"><a href="#Working-with-a-DB-Instance-in-a-VPC" class="headerlink" title="Working with a DB Instance in a VPC"></a>Working with a DB Instance in a VPC</h2><p>Your VPC must have at least two subnets. These subnets must be in two different Availability Zones in the region where you want to deploy your DB instance.<br>If you want your DB instance in the VPC to be publicly accessible, you must enable the VPC attributes DNS hostnames and DNS resolution.</p>
<h3 id="performs-a-failover"><a href="#performs-a-failover" class="headerlink" title="performs a failover"></a>performs a failover</h3><p>Amazon RDS automatically performs a failover in the event of any of the following:</p>
<p>Loss of availability in primary Availability Zone<br>Loss of network connectivity to primary<br>Compute unit failure on primary<br>Storage failure on primary</p>
<h1 id="Elastic-Cache"><a href="#Elastic-Cache" class="headerlink" title="Elastic Cache"></a>Elastic Cache</h1><p>IAM Auth is not supported by ElastiCache</p>
<h1 id="Amazon-CloudWatch"><a href="#Amazon-CloudWatch" class="headerlink" title="Amazon CloudWatch"></a>Amazon CloudWatch</h1><p>To monitor basic statistics for your instances and Amazon EBS volumes, use Amazon CloudWatch. </p>
<p>Amazon Web Services. Amazon Elastic Compute Cloud .</p>
<p>Disabling the Termination from the ASG would prevent our ASG to be Elastic and impact our costs. Making a snapshot of the EC2 instance before it gets terminated <em>could</em> work but it’s tedious, not elastic and very expensive, as all we’re interested about are log files. Using AWS Lambda would be extremely hard to use for this task. Here, the natural and by far easiest solution would be to use the CloudWatch Logs agents on the EC2 instances to automatically send log files into CloudWatch, so we can analyze them in the future easily should any problems arise.</p>
<h2 id="Auto-terminate"><a href="#Auto-terminate" class="headerlink" title="Auto terminate"></a>Auto terminate</h2><p>Using Amazon CloudWatch alarm actions, you can create alarms that automatically stop, terminate, reboot, or recover your EC2 instances. You can use the stop or terminate actions to help you save money when you no longer need an instance to be running. You can use the reboot and recover actions to automatically reboot those instances or recover them onto new hardware if a system impairment occurs.</p>
<h3 id="Q-amp-A-1"><a href="#Q-amp-A-1" class="headerlink" title="Q&amp;A"></a>Q&amp;A</h3><p>you can use Amazon CloudWatch to monitor the database and then Amazon SNS to send the emails to the Operations team. Take note that you should use SNS instead of SES (Simple Email Service) when you want to monitor your EC2 instances.</p>
<p>CloudWatch collects monitoring and operational data in the form of logs, metrics, and events, providing you with a unified view of AWS resources, applications, and services that run on AWS, and on-premises servers.</p>
<p>SNS is a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications.</p>
<p>Option 1 is incorrect. SES is a cloud-based email sending service designed to send notification and transactional emails.</p>
<p>Option 3 is incorrect. SQS is a fully-managed message queuing service. It does not monitor applications nor send email notifications unlike SES.</p>
<p>Option 4 is incorrect. Route 53 is a highly available and scalable cloud Domain Name System (DNS) web service. It does not monitor applications nor send email notifications.</p>
<h1 id="API-Gateway"><a href="#API-Gateway" class="headerlink" title="API Gateway"></a>API Gateway</h1><p>Q: What API types are supported by Amazon API Gateway?</p>
<p>Amazon API Gateway offers two options to create RESTful APIs, HTTP APIs (Preview) and REST APIs, as well as an option to create WebSocket APIs.</p>
<p>HTTP API: HTTP APIs, currently available in Preview, are optimized for building APIs that proxy to AWS Lambda functions or HTTP backends, making them ideal for serverless workloads. They do not currently offer API management functionality.</p>
<p>REST API: REST APIs offer API proxy functionality and API management features in a single solution. REST APIs offer API management features such as usage plans, API keys, publishing, and monetizing APIs.</p>
<p>WebSocket API: WebSocket APIs maintain a persistent connection between connected clients to enable real-time message communication. With WebSocket APIs in API Gateway, you can define backend integrations with AWS Lambda functions, Amazon Kinesis, or any HTTP endpoint to be invoked when messages are received from the connected clients.</p>
<h1 id="CloudTrail"><a href="#CloudTrail" class="headerlink" title="CloudTrail"></a>CloudTrail</h1><p>To monitor the calls made to the Amazon EC2 API for your account, including calls made by the AWS Management Console, command line tools, and other services, use AWS CloudTrail.</p>
<p>In general, to analyze any API calls made within your AWS account, you should use CloudTrail</p>
<p>​
Set up a new CloudTrail trail in a new S3 bucket using the AWS CLI and also pass both the –is-multi-region-trail and –include-global-service-events parameters then encrypt log files using KMS encryption. Apply Multi Factor Authentication (MFA) Delete on the S3 bucket and ensure that only authorized users can access the logs by configuring the bucket policies.</p>
<h2 id="Charge"><a href="#Charge" class="headerlink" title="Charge"></a>Charge</h2><p> the first copy of management events is free.</p>
<h2 id="Cloud-Trail-retention-days"><a href="#Cloud-Trail-retention-days" class="headerlink" title="Cloud Trail retention days"></a>Cloud Trail retention days</h2><p>Management event activity is recorded by AWS CloudTrail for the last 90 days, and can be viewed and searched free of charge from the AWS CloudTrail console, or by using the AWS CLI.</p>
<h1 id="AWS-ECS"><a href="#AWS-ECS" class="headerlink" title="AWS ECS"></a>AWS ECS</h1><h2 id="Summary-AWS-ECS-–-Elastic-Container-Service"><a href="#Summary-AWS-ECS-–-Elastic-Container-Service" class="headerlink" title="Summary: AWS ECS – Elastic Container Service"></a>Summary: AWS ECS – Elastic Container Service</h2><p>• ECS is a container orchestration service<br>• ECS helps you run Docker containers on EC2 machines<br>• ECS is complicated, and made of:<br>• “ECS Core”: Running ECS on user-provisioned EC2 instances<br>• Fargate: Running ECS tasks on AWS-provisioned compute (serverless)<br>• EKS: Running ECS on AWS-powered Kubernetes (running on EC2)<br>• ECR: Docker Container Registry hosted by AWS<br>• ECS &amp; Docker are very popular for microservices<br>• For now, for the exam, only “ECS Core” &amp; ECR is in scope<br>• IAM security and roles at the ECS task level</p>
<h2 id="AWS-ECS-–-Concepts"><a href="#AWS-ECS-–-Concepts" class="headerlink" title="AWS ECS – Concepts"></a>AWS ECS – Concepts</h2><p>• ECS cluster: set of EC2 instances<br>• ECS service: applications definitions running on ECS cluster<br>• ECS tasks + definition: containers running to create the application<br>• ECS IAM roles: roles assigned to tasks to interact with AWS</p>
<h2 id="AWS-ECS-–-ALB-integration"><a href="#AWS-ECS-–-ALB-integration" class="headerlink" title="AWS ECS – ALB integration"></a>AWS ECS – ALB integration</h2><p>• Application Load Balancer (ALB) has a direct integration feature with ECS called “port mapping”, This allows you to run multiple instances of the same application on the same EC2 machine</p>
<h3 id="Dynamic-mapping"><a href="#Dynamic-mapping" class="headerlink" title="Dynamic mapping"></a>Dynamic mapping</h3><p>Dynamic Port Mapping is available for the Application Load Balancer. A reverse proxy solution would work but would be too much work to manage. Here the ALB has a feature that provides a direct dynamic port mapping feature and integration with the ECS service so we will leverage that. Read more here: <a href="https://aws.amazon.com/premiumsupport/knowledge-center/dynamic-port-mapping-ecs/" target="_blank" rel="noopener">https://aws.amazon.com/premiumsupport/knowledge-center/dynamic-port-mapping-ecs/</a></p>
<h2 id="AWS-ECS-–-ECS-Setup-amp-Config-file"><a href="#AWS-ECS-–-ECS-Setup-amp-Config-file" class="headerlink" title="AWS ECS – ECS Setup &amp; Config file"></a>AWS ECS – ECS Setup &amp; Config file</h2><p>• Run an EC2 instance, install the ECS agent with ECS config file<br>• Or use an ECS-ready Linux AMI (still need to modify config file) • ECS Config file is at /etc/ecs/ecs.config</p>
<h2 id="AWS-ECR-–-Elastic-Container-Registry"><a href="#AWS-ECR-–-Elastic-Container-Registry" class="headerlink" title="AWS ECR – Elastic Container Registry"></a>AWS ECR – Elastic Container Registry</h2><p>• Store, managed and deploy your containers on AWS<br>• Fully integrated with IAM &amp; ECS<br>• Sent over HTTPS (encryption in flight) and encrypted at rest</p>
<h2 id="Specifying-Sensitive-Data"><a href="#Specifying-Sensitive-Data" class="headerlink" title="Specifying Sensitive Data"></a>Specifying Sensitive Data</h2><p>Amazon ECS enables you to inject sensitive data into your containers by storing your sensitive data in either AWS Secrets Manager secrets or AWS Systems Manager Parameter Store parameters and then referencing them in your container definition. This feature is supported by tasks using both the EC2 and Fargate launch types.</p>
<h3 id="Required-IAM-Permissions-for-Amazon-ECS-Secrets"><a href="#Required-IAM-Permissions-for-Amazon-ECS-Secrets" class="headerlink" title="Required IAM Permissions for Amazon ECS Secrets"></a>Required IAM Permissions for Amazon ECS Secrets</h3><p>To use this feature, you must have the Amazon ECS task execution role and reference it in your task definition. This allows the container agent to pull the necessary AWS Systems Manager or Secrets Manager resources. For more information, see Amazon ECS Task Execution IAM Role.</p>
<p>To provide access to the AWS Systems Manager Parameter Store parameters that you create, manually add the following permissions as an inline policy to the task execution role. For more information, see Adding and Removing IAM Policies.</p>
<p>ssm:GetParameters—Required if you are referencing a Systems Manager Parameter Store parameter in a task definition.</p>
<p>secretsmanager:GetSecretValue—Required if you are referencing a Secrets Manager secret either directly or if your Systems Manager Parameter Store parameter is referencing a Secrets Manager secret in a task definition.</p>
<p>kms:Decrypt—Required only if your secret uses a custom KMS key and not the default key. The ARN for your custom key should be added as a resource.</p>
<h1 id="Lambda"><a href="#Lambda" class="headerlink" title="Lambda"></a>Lambda</h1><p>AWS Lambda functions time out after 15 minutes, and are not usually meant for long running jobs.</p>
<h2 id="Lambda-parameters-Encryption"><a href="#Lambda-parameters-Encryption" class="headerlink" title="Lambda parameters Encryption"></a>Lambda parameters Encryption</h2><p>Although Lambda encrypts the environment variables in your function by default, the sensitive information would still be visible to other users who have access to the Lambda console. This is because Lambda uses a default KMS key to encrypt the variables, which is usually accessible by other users. The best option in this scenario is to use encryption helpers to secure your environment variables.</p>
<p>Enabling SSL would encrypt data only when in-transit. Your other teams would still be able to view the plaintext at-rest. Use AWS KMS instead.</p>
<p>Option 3 is incorrect since, as mentioned, Lambda does provide encryption functionality of environment variables.    </p>
<h2 id="Lambda-functions"><a href="#Lambda-functions" class="headerlink" title="Lambda functions"></a>Lambda functions</h2><p>You upload your application code in the form of one or more Lambda functions. Lambda stores code in Amazon S3 and encrypts it at rest.</p>
<h3 id="layer"><a href="#layer" class="headerlink" title="layer"></a>layer</h3><p>A layer is a ZIP archive that contains libraries, a custom runtime, or other dependencies. Use layers to manage your function’s dependencies independently and keep your deployment package small.</p>
<h3 id="Invoking-Functions"><a href="#Invoking-Functions" class="headerlink" title="Invoking Functions"></a>Invoking Functions</h3><p>Lambda supports synchronous and asynchronous invocation of a Lambda function. You can control the invocation type only when you invoke a Lambda function (referred to as on-demand invocation).<br>An event source is the entity that publishes events, and a Lambda function is the custom code that processes the events.<br>Event source mapping maps an event source to a Lambda function. It enables automatic invocation of your Lambda function when events occur.</p>
<h2 id="Lambda-deployment"><a href="#Lambda-deployment" class="headerlink" title="Lambda deployment"></a>Lambda deployment</h2><p>If you’re using the AWS Lambda compute platform, you must choose one of the following deployment configuration types to specify how traffic is shifted from the original AWS Lambda function version to the new AWS Lambda function version:</p>
<p>-Canary: Traffic is shifted in two increments. You can choose from predefined canary options that specify the percentage of traffic shifted to your updated Lambda function version in the first increment and the interval, in minutes, before the remaining traffic is shifted in the second increment.<br>-Linear: Traffic is shifted in equal increments with an equal number of minutes between each increment. You can choose from predefined linear options that specify the percentage of traffic shifted in each increment and the number of minutes between each increment.<br>-All-at-once: All traffic is shifted from the original Lambda function to the updated Lambda function version at once.</p>
<h2 id="Lambda-Edge"><a href="#Lambda-Edge" class="headerlink" title="Lambda@Edge"></a>Lambda@Edge</h2><p>Lets you run Lambda functions to customize content that CloudFront delivers, executing the functions in AWS locations closer to the viewer. The functions run in response to CloudFront events, without provisioning or managing servers.</p>
<h1 id="Disaster-Recovery"><a href="#Disaster-Recovery" class="headerlink" title="Disaster Recovery"></a>Disaster Recovery</h1><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>Need to define two terms:<br>• RPO: Recovery Point Objective<br>• RTO: Recover y Time Objective</p>
<p>Disaster Recovery Strategies<br>• Backup and Restore<br>• Pilot Light<br>• Warm Standby<br>• Hot Site / Multi Site Approach</p>
<h1 id="Route53"><a href="#Route53" class="headerlink" title="Route53"></a>Route53</h1><p>Simple Records do not have health checks, here the most likely issue is that the TTL is still in effect so you have to wait until it expires for the new users to perform another DNS query and get the value for your new Load Balancer.</p>
<h1 id="Database"><a href="#Database" class="headerlink" title="Database"></a>Database</h1><p>ElastiCache / RDS / Neptune are not serverless databases. DynamoDB is serverless, single digit latency and horizontally scales.</p>
<h2 id="DynamoDB"><a href="#DynamoDB" class="headerlink" title="DynamoDB"></a>DynamoDB</h2><p>DynamoDB Streams will contain a stream of all the changes that happen to a DynamoDB table. It can be chained with a Lambda function that will be triggered to react to these changes, one of which being a developer’s milestone. DAX is a caching layer </p>
<p>DynamoDB Auto Scaling is primarily used to automate capacity management for your tables and global secondary indexes.</p>
<h3 id="DAX"><a href="#DAX" class="headerlink" title="DAX"></a>DAX</h3><p>DAX will be transparent and won’t require an application refactoring, and will cache the “hot keys”. ElastiCache could also be a solution, but it will require a lot of refactoring work on the AWS Lambda side.</p>
<p>DynamoDB is horizontally scalable, has a DynamoDB streams capability and is multi AZ by default. On top of it, we can adjust the RCU and WCU automatically using Auto Scaling.</p>
<h2 id="DynamoDB’s-partition-key"><a href="#DynamoDB’s-partition-key" class="headerlink" title="DynamoDB’s partition key"></a>DynamoDB’s partition key</h2><p>DynamoDB supports two types of primary keys:</p>
<p>Partition key: A simple primary key, composed of one attribute known as the partition key. Attributes in DynamoDB are similar in many ways to fields or columns in other database systems.<br>Partition key and sort key: Referred to as a composite primary key, this type of key is composed of two attributes. The first attribute is the partition key, and the second attribute is the sort key. Following is an example.</p>
<h3 id="Recommendations-for-partition-keys"><a href="#Recommendations-for-partition-keys" class="headerlink" title="Recommendations for partition keys"></a>Recommendations for partition keys</h3><p>Use high-cardinality attributes. These are attributes that have distinct values for each item, like e-mailid, employee_no, customerid, sessionid, orderid, and so on.</p>
<p>Use composite attributes. Try to combine more than one attribute to form a unique key, if that meets your access pattern. For example, consider an orders table with customerid+productid+countrycode as the partition key and order_date as the sort key.</p>
<p>Cache the popular items when there is a high volume of read traffic using Amazon DynamoDB Accelerator (DAX). The cache acts as a low-pass filter, preventing reads of unusually popular items from swamping partitions. For example, consider a table that has deals information for products. Some deals are expected to be more popular than others during major sale events like Black Friday or Cyber Monday. DAX is a fully managed, in-memory cache for DynamoDB that doesn’t require developers to manage cache invalidation, data population, or cluster management. DAX also is compatible with DynamoDB API calls, so developers can incorporate it more easily into existing applications.</p>
<p>Add random numbers or digits from a predetermined range for write-heavy use cases. Suppose that you expect a large volume of writes for a partition key (for example, greater than 1000 1 K writes per second). In this case, use an additional prefix or suffix (a fixed number from predetermined range, say 1–10) and add it to the partition key.</p>
<h2 id="Durability"><a href="#Durability" class="headerlink" title="Durability"></a>Durability</h2><p>When the word durability pops out, the first service that should come to your mind is Amazon S3. Since this service is not available in the answer options, we can look at the other data store available which is Amazon DynamoDB.</p>
<p>DynamoDB is durable, scalable, and highly available data store which can be used for real-time tabulation. You can also use AppSync with DynamoDB to make it easy for you to build collaborative apps that keep shared data updated in real time. You just specify the data for your app with simple code statements and AWS AppSync manages everything needed to keep the app data updated in real time. This will allow your app to access data in Amazon DynamoDB, trigger AWS Lambda functions, or run Amazon Elasticsearch queries and combine data from these services to provide the exact data you need for your app.</p>
<p>Option 2 is incorrect as Amazon Redshift is mainly used as a data warehouse and for online analytic processing (OLAP). Although this service can be used for this scenario, DynamoDB is still the top choice given its better durability and scalability. </p>
<p>Options 3 and 4 are possible answers in this scenario, however, DynamoDB is much more suitable for simple mobile apps which do not have complicated data relationships compared with enterprise web applications. The scenario says that the mobile app will be used from around the world, which is why you need a data storage service which can be supported globally. It would be a management overhead to implement multi-region deployment for your RDS and Aurora database instances compared to using the Global table feature of DynamoDB.</p>
<h2 id="Aurora"><a href="#Aurora" class="headerlink" title="Aurora"></a>Aurora</h2><p>Aurora Read Replicas can be deployed globally</p>
<h3 id="Aurora-endpoints"><a href="#Aurora-endpoints" class="headerlink" title="Aurora endpoints"></a>Aurora endpoints</h3><p>Amazon Aurora typically involves a cluster of DB instances instead of a single instance. Each connection is handled by a specific DB instance. When you connect to an Aurora cluster, the host name and port that you specify point to an intermediate handler called an endpoint. Aurora uses the endpoint mechanism to abstract these connections. Thus, you don’t have to hardcode all the hostnames or write your own logic for load-balancing and rerouting connections when some DB instances aren’t available.</p>
<p>For certain Aurora tasks, different instances or groups of instances perform different roles. For example, the primary instance handles all data definition language (DDL) and data manipulation language (DML) statements. Up to 15 Aurora Replicas handle read-only query traffic.</p>
<p>Using endpoints, you can map each connection to the appropriate instance or group of instances based on your use case. For example, to perform DDL statements you can connect to whichever instance is the primary instance. To perform queries, you can connect to the reader endpoint, with Aurora automatically performing load-balancing among all the Aurora Replicas. For clusters with DB instances of different capacities or configurations, you can connect to custom endpoints associated with different subsets of DB instances. For diagnosis or tuning, you can connect to a specific instance endpoint to examine details about a specific DB instance.</p>
<h4 id="Types-of-Aurora-Endpoints"><a href="#Types-of-Aurora-Endpoints" class="headerlink" title="Types of Aurora Endpoints"></a>Types of Aurora Endpoints</h4><p>An endpoint is represented as an Aurora-specific URL that contains a host address and a port. The following types of endpoints are available from an Aurora DB cluster.</p>
<h5 id="Cluster-endpoint"><a href="#Cluster-endpoint" class="headerlink" title="Cluster endpoint"></a>Cluster endpoint</h5><p>A cluster endpoint for an Aurora DB cluster that connects to the current primary DB instance for that DB cluster. This endpoint is the only one that can perform write operations such as DDL statements. Because of this, the cluster endpoint is the one that you connect to when you first set up a cluster or when your cluster only contains a single DB instance.<br>Each Aurora DB cluster has one cluster endpoint and one primary DB instance.</p>
<p>You use the cluster endpoint for all write operations on the DB cluster, including inserts, updates, deletes, and DDL changes. You can also use the cluster endpoint for read operations, such as queries.</p>
<p>The cluster endpoint provides failover support for read/write connections to the DB cluster. If the current primary DB instance of a DB cluster fails, Aurora automatically fails over to a new primary DB instance. During a failover, the DB cluster continues to serve connection requests to the cluster endpoint from the new primary DB instance, with minimal interruption of service.</p>
<h5 id="Reader-endpoint"><a href="#Reader-endpoint" class="headerlink" title="Reader endpoint"></a>Reader endpoint</h5><p>A reader endpoint for an Aurora DB cluster connects to one of the available Aurora Replicas for that DB cluster. Each Aurora DB cluster has one reader endpoint. If there is more than one Aurora Replica, the reader endpoint directs each connection request to one of the Aurora Replicas.</p>
<p>The reader endpoint provides load-balancing support for read-only connections to the DB cluster. Use the reader endpoint for read operations, such as queries. You can’t use the reader endpoint for write operations.</p>
<h5 id="Custom-endpoint"><a href="#Custom-endpoint" class="headerlink" title="Custom endpoint"></a>Custom endpoint</h5><p>A custom endpoint for an Aurora cluster represents a set of DB instances that you choose. When you connect to the endpoint, Aurora performs load balancing and chooses one of the instances in the group to handle the connection. You define which instances this endpoint refers to, and you decide what purpose the endpoint serves.</p>
<p>An Aurora DB cluster has no custom endpoints until you create one. You can create up to five custom endpoints for each provisioned Aurora cluster. You can’t use custom endpoints for Aurora Serverless clusters.</p>
<p>The custom endpoint provides load-balanced database connections based on criteria other than the read-only or read-write capability of the DB instances. For example, you might define a custom endpoint to connect to instances that use a particular AWS instance class or a particular DB parameter group. Then you might tell particular groups of users about this custom endpoint. For example, you might direct internal users to low-capacity instances for report generation or ad hoc (one-time) querying, and direct production traffic to high-capacity instances.</p>
<p>Instead of using one DB instance for each specialized purpose and connecting to its instance endpoint, you can have multiple groups of specialized DB instances. In this case, each group has its own custom endpoint. This way, Aurora can perform load balancing among all the instances dedicated to tasks such as reporting or handling production or internal queries. The custom endpoints provide load balancing and high availability for each group of DB instances within your cluster. If one of the DB instances within a group becomes unavailable, Aurora directs subsequent custom endpoint connections to one of the other DB instances associated with the same endpoint.</p>
<h5 id="Instance-endpoint"><a href="#Instance-endpoint" class="headerlink" title="Instance endpoint"></a>Instance endpoint</h5><p>An instance endpoint connects to a specific DB instance within an Aurora cluster. Each DB instance in a DB cluster has its own unique instance endpoint. So there is one instance endpoint for the current primary DB instance of the DB cluster, and there is one instance endpoint for each of the Aurora Replicas in the DB cluster.</p>
<p>The instance endpoint provides direct control over connections to the DB cluster, for scenarios where using the cluster endpoint or reader endpoint might not be appropriate. For example, your client application might require more fine-grained load balancing based on workload type. In this case, you can configure multiple clients to connect to different Aurora Replicas in a DB cluster to distribute read workloads. </p>
<h1 id="Kinesis"><a href="#Kinesis" class="headerlink" title="Kinesis"></a>Kinesis</h1><p>Amazon Kinesis is the streaming data platform of AWS and has four distinct services under it: Kinesis Data Firehose, Kinesis Data Streams, Kinesis Video Streams, and Amazon Kinesis Data Analytics. For a specific use case of the requirement by the question, use Kinesis Data Firehose.</p>
<p>SQS FIFO will not work here as they cannot sustain thousands of messages per second. SNS cannot be used for data streaming. Lambda isn’t meant to retain data. Kinesis is the right answer here, with providing a partition key in our message we can guarantee ordering for a specific sensor, even if our stream is sharded</p>
<h1 id="Amazon-Macie"><a href="#Amazon-Macie" class="headerlink" title="Amazon Macie"></a>Amazon Macie</h1><p>Amazon Macie is mainly used as a security service that uses machine learning to automatically discover, classify, and protect sensitive data in AWS. As a security feature of AWS, it does not meet the requirements of being able to load and stream data into data stores for analytics. You have to use Kinesis Data Firehose instead.</p>
<h2 id="Concepts"><a href="#Concepts" class="headerlink" title="Concepts"></a>Concepts</h2><h3 id="Data-Producer-–-An-application-that-typically-emits-data-records-as-they-are-generated-to-a-Kinesis-data-stream-Data-producers-assign-partition-keys-to-records-Partition-keys-ultimately-determine-which-shard-ingests-the-data-record-for-a-data-stream"><a href="#Data-Producer-–-An-application-that-typically-emits-data-records-as-they-are-generated-to-a-Kinesis-data-stream-Data-producers-assign-partition-keys-to-records-Partition-keys-ultimately-determine-which-shard-ingests-the-data-record-for-a-data-stream" class="headerlink" title="Data Producer – An application that typically emits data records as they are generated to a Kinesis data stream. Data producers assign partition keys to records. Partition keys ultimately determine which shard ingests the data record for a data stream."></a>Data Producer – An application that typically emits data records as they are generated to a Kinesis data stream. Data producers assign partition keys to records. Partition keys ultimately determine which shard ingests the data record for a data stream.</h3><h3 id="Data-Consumer-–-A-distributed-Kinesis-application-or-AWS-service-retrieving-data-from-all-shards-in-a-stream-as-it-is-generated-Most-data-consumers-are-retrieving-the-most-recent-data-in-a-shard-enabling-real-time-analytics-or-handling-of-data"><a href="#Data-Consumer-–-A-distributed-Kinesis-application-or-AWS-service-retrieving-data-from-all-shards-in-a-stream-as-it-is-generated-Most-data-consumers-are-retrieving-the-most-recent-data-in-a-shard-enabling-real-time-analytics-or-handling-of-data" class="headerlink" title="Data Consumer – A distributed Kinesis application or AWS service retrieving data from all shards in a stream as it is generated. Most data consumers are retrieving the most recent data in a shard, enabling real-time analytics or handling of data."></a>Data Consumer – A distributed Kinesis application or AWS service retrieving data from all shards in a stream as it is generated. Most data consumers are retrieving the most recent data in a shard, enabling real-time analytics or handling of data.</h3><h3 id="Data-Stream-–-A-logical-grouping-of-shards-There-are-no-bounds-on-the-number-of-shards-within-a-data-stream-A-data-stream-will-retain-data-for-24-hours-or-up-to-7-days-when-extended-retention-is-enabled"><a href="#Data-Stream-–-A-logical-grouping-of-shards-There-are-no-bounds-on-the-number-of-shards-within-a-data-stream-A-data-stream-will-retain-data-for-24-hours-or-up-to-7-days-when-extended-retention-is-enabled" class="headerlink" title="Data Stream – A logical grouping of shards. There are no bounds on the number of shards within a data stream. A data stream will retain data for 24 hours, or up to 7 days when extended retention is enabled."></a>Data Stream – A logical grouping of shards. There are no bounds on the number of shards within a data stream. A data stream will retain data for 24 hours, or up to 7 days when extended retention is enabled.</h3><h3 id="Shard-–-The-base-throughput-unit-of-a-Kinesis-data-stream"><a href="#Shard-–-The-base-throughput-unit-of-a-Kinesis-data-stream" class="headerlink" title="Shard – The base throughput unit of a Kinesis data stream."></a>Shard – The base throughput unit of a Kinesis data stream.</h3><p>A shard is an append-only log and a unit of streaming capability. A shard contains an ordered sequence of records ordered by arrival time.<br>Add or remove shards from your stream dynamically as your data throughput changes.<br>One shard can ingest up to 1000 data records per second, or 1MB/sec. Add more shards to increase your ingestion capability.<br>When consumers use enhanced fan-out, one shard provides 1MB/sec data input and 2MB/sec data output for each data consumer registered to use enhanced fan-out.<br>When consumers do not use enhanced fan-out, a shard provides 1MB/sec of input and 2MB/sec of data output, and this output is shared with any consumer not using enhanced fan-out.<br>You will specify the number of shards needed when you create a stream and can change the quantity at any time.</p>
<h3 id="Data-Record"><a href="#Data-Record" class="headerlink" title="Data Record"></a>Data Record</h3><p>A record is the unit of data stored in a Kinesis stream. A record is composed of a sequence number, partition key, and data blob.<br>A data blob is the data of interest your data producer adds to a stream. The maximum size of a data blob is 1 MB.</p>
<h3 id="Partition-Key"><a href="#Partition-Key" class="headerlink" title="Partition Key"></a>Partition Key</h3><p>A partition key is typically a meaningful identifier, such as a user ID or timestamp. It is specified by your data producer while putting data into a Kinesis data stream, and useful for consumers as they can use the partition key to replay or build a history associated with the partition key.<br>The partition key is also used to segregate and route data records to different shards of a stream.</p>
<h3 id="Sequence-Number"><a href="#Sequence-Number" class="headerlink" title="Sequence Number"></a>Sequence Number</h3><p>A sequence number is a unique identifier for each data record. Sequence number is assigned by Kinesis Data Streams when a data producer calls PutRecord or PutRecords API to add data to a Kinesis data stream.</p>
<h2 id="Monitoring"><a href="#Monitoring" class="headerlink" title="Monitoring"></a>Monitoring</h2><p>You can monitor shard-level metrics in Kinesis Data Streams.<br>You can monitor your data streams in Amazon Kinesis Data Streams using CloudWatch, Kinesis Agent, Kinesis libraries.<br>Log API calls with CloudTrail.</p>
<h2 id="Kinesis-firehose"><a href="#Kinesis-firehose" class="headerlink" title="Kinesis firehose"></a>Kinesis firehose</h2><p>You can specify a batch size or batch interval to control how quickly data is uploaded to destinations. Additionally, you can specify if data should be compressed.</p>
<p>You can configure Kinesis Data Firehose to prepare your streaming data before it is loaded to data stores. Kinesis Data Firehose provides pre-built Lambda blueprints for converting common data sources such as Apache logs and system logs to JSON and CSV formats. You can use these pre-built blueprints without any change, or customize them further, or write your own custom functions.</p>
<h1 id="BeanStalk"><a href="#BeanStalk" class="headerlink" title="BeanStalk"></a>BeanStalk</h1><p>When you create an AWS Elastic Beanstalk environment, you can specify an Amazon Machine Image (AMI) to use instead of the standard Elastic Beanstalk AMI included in your platform version. A custom AMI can improve provisioning times when instances are launched in your environment if you need to install a lot of software that isn’t included in the standard AMIs. Read more here: <a href="https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.customenv.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.customenv.html</a></p>
<h1 id="MQ"><a href="#MQ" class="headerlink" title="MQ"></a>MQ</h1><p>SNS, SQS and Kinesis are AWS’ proprietary technologies and do not come with MQTT compatibility. Here the only possible answer is Amazon MQ</p>
<h1 id="X-Ray"><a href="#X-Ray" class="headerlink" title="X Ray"></a>X Ray</h1><p> AWS X-Ray<br>Analyze and debug production, distributed applications</p>
<p>AWS X-Ray helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture. With X-Ray, you can understand how your application and its underlying services are performing to identify and troubleshoot the root cause of performance issues and errors. X-Ray provides an end-to-end view of requests as they travel through your application, and shows a map of your application’s underlying components. You can use X-Ray to analyze both applications in development and in production, from simple three-tier applications to complex microservices applications consisting of thousands of services.</p>
<h1 id="ElasticCache"><a href="#ElasticCache" class="headerlink" title="ElasticCache"></a>ElasticCache</h1><h2 id="Redis"><a href="#Redis" class="headerlink" title="Redis"></a>Redis</h2><h3 id="What-are-Amazon-ElastiCache-for-Redis-nodes-clusters-and-replications-groups"><a href="#What-are-Amazon-ElastiCache-for-Redis-nodes-clusters-and-replications-groups" class="headerlink" title="What are Amazon ElastiCache for Redis nodes, clusters, and replications groups?"></a>What are Amazon ElastiCache for Redis nodes, clusters, and replications groups?</h3><p>An ElastiCache for Redis node is the smallest building block of an Amazon ElastiCache for Redis deployment. Each ElastiCache for Redis node supports the Redis protocol and has its own DNS name and port. Multiple types of ElastiCache for Redis nodes are supported, each with varying amount of CPU capability, and associated memory. An ElastiCache for Redis node may take on a primary or a read replica role. A primary node can be replicated to multiple read replica nodes. An ElastiCache for Redis cluster is a collection of one or more ElastiCache for Redis nodes of the same role; the primary node will be in the primary cluster and the read replica node will be in a read replica cluster. At this time a cluster can only have one node. In the future, we will increase this limit. A cluster manages a logical key space, where each node is responsible for a part of the key space. Most of your management operations will be performed at the cluster level. An ElastiCache for Redis replication group encapsulates the primary and read replica clusters for a Redis installation. A replication group will have only one primary cluster and zero or many read replica clusters. All nodes within a replication group (and consequently cluster) will be of the same node type, and have the same parameter and security group settings.</p>
<h3 id="Does-Amazon-ElastiCache-for-Redis-support-Multi-AZ-operation"><a href="#Does-Amazon-ElastiCache-for-Redis-support-Multi-AZ-operation" class="headerlink" title="Does Amazon ElastiCache for Redis support Multi-AZ operation?"></a>Does Amazon ElastiCache for Redis support Multi-AZ operation?</h3><p>Yes, with Amazon ElastiCache for Redis you can create a read replica in another AWS Availability Zone. Upon a failure of the primary node, we will provision a new primary node. In scenarios where the primary node cannot be provisioned, you can decide which read replica to promote to be the new primary. </p>
<h3 id="What-is-Multi-AZ-for-an-ElastiCache-for-Redis-replication-group"><a href="#What-is-Multi-AZ-for-an-ElastiCache-for-Redis-replication-group" class="headerlink" title="What is Multi-AZ for an ElastiCache for Redis replication group?"></a>What is Multi-AZ for an ElastiCache for Redis replication group?</h3><p>An ElastiCache for Redis replication group consists of a primary and up to five read replicas. Redis asynchronously replicates the data from the primary to the read replicas. During certain types of planned maintenance, or in the unlikely event of ElastiCache node failure or Availability Zone failure, Amazon ElastiCache will automatically detect the failure of a primary, select a read replica, and promote it to become the new primary. ElastiCache also propagates the DNS changes of the promoted read replica, so if your application is writing to the primary node endpoint, no endpoint change will be needed.</p>
<h3 id="How-can-I-use-encryption-in-transit-at-rest-and-Redis-AUTH"><a href="#How-can-I-use-encryption-in-transit-at-rest-and-Redis-AUTH" class="headerlink" title="How can I use encryption in-transit, at-rest, and Redis AUTH?"></a>How can I use encryption in-transit, at-rest, and Redis AUTH?</h3><p>Encryption in-transit, encryption at-rest, and Redis AUTH are all opt-in features. At the time of Redis cluster creation via the console or command line interface, you can specify if you want to enable encryption and Redis AUTH and can proceed to provide an authentication token for communication with the Redis cluster. Once the cluster is setup with encryption enabled, ElastiCache seamlessly manages certificate expiration and renewal without requiring any additional action from the application. Additionally, the Redis clients need to support TLS to avail of the encrypted in-transit traffic.</p>
<h3 id="Is-Redis-password-functionality-supported-in-Amazon-ElastiCache-for-Redis"><a href="#Is-Redis-password-functionality-supported-in-Amazon-ElastiCache-for-Redis" class="headerlink" title="Is Redis password functionality supported in Amazon ElastiCache for Redis?"></a>Is Redis password functionality supported in Amazon ElastiCache for Redis?</h3><p>Yes, Amazon ElastiCache for Redis supports Redis passwords via Redis AUTH feature. It is an opt-in feature available in ElastiCache for Redis version 3.2.6 onwards. You must enable encryption in-transit to use Redis AUTH on your ElastiCache for Redis cluster.</p>
<h3 id="encryption"><a href="#encryption" class="headerlink" title="encryption"></a>encryption</h3><p>ElastiCache for Redis at-rest encryption is an optional feature to increase data security by encrypting on-disk data. When enabled on a replication group, it encrypts the following aspects:</p>
<p>Disk during sync, backup and swap operations</p>
<p>Backups stored in Amazon S3</p>
<p>ElastiCache for Redis offers default (service managed) encryption at rest, as well as ability to use your own symetric customer managed customer master keys in AWS Key Management Service (KMS).</p>
<h4 id="KMS"><a href="#KMS" class="headerlink" title="KMS"></a>KMS</h4><p>Manage keys used for encrypted DB instances using the AWS KMS. KMS encryption keys are specific to the region that they are created in.</p>
<h1 id="BYOIP"><a href="#BYOIP" class="headerlink" title="BYOIP"></a>BYOIP</h1><p>You can bring part or all of your public IPv4 address range from your on-premises network to your AWS account. You continue to own the address range, but AWS advertises it on the Internet. After you bring the address range to AWS, it appears in your account as an address pool. You can create an Elastic IP address from your address pool and use it with your AWS resources, such as EC2 instances, NAT gateways, and Network Load Balancers. This is also called “Bring Your Own IP Addresses (BYOIP)”.</p>
<p>To ensure that only you can bring your address range to your AWS account, you must authorize Amazon to advertise the address range and provide proof that you own the address range.</p>
<p>A Route Origin Authorization (ROA) is a document that you can create through your Regional internet registry (RIR), such as the American Registry for Internet Numbers (ARIN) or Réseaux IP Européens Network Coordination Centre (RIPE). It contains the address range, the ASNs that are allowed to advertise the address range, and an expiration date. Hence, Option 3 is the correct answer.</p>
<p>The ROA authorizes Amazon to advertise an address range under a specific AS number. However, it does not authorize your AWS account to bring the address range to AWS. To authorize your AWS account to bring an address range to AWS, you must publish a self-signed X509 certificate in the RDAP remarks for the address range. The certificate contains a public key, which AWS uses to verify the authorization-context signature that you provide. You should keep your private key secure and use it to sign the authorization-context message.</p>
<p>Option 1 is incorrect because you cannot map the IP address of your on-premises network, which you are migrating to AWS, to an EIP address of your VPC. To satisfy the requirement, you must authorize Amazon to advertise the address range that you own.</p>
<p>Option 2 is incorrect because the IP match condition in CloudFront is primarily used in allowing or blocking the incoming web requests based on the IP addresses that the requests originate from. This is the opposite of what is being asked in the scenario, where you have to migrate your suite of applications from your on-premises network and advertise the address range that you own in your VPC.</p>
<p>Option 4 is incorrect because you don’t need to submit an AWS request in order to do this. You can simply create a Route Origin Authorization (ROA) then once done, provision and advertise your whitelisted IP address range to your AWS account.</p>
<h1 id="AWS-IoT-Core"><a href="#AWS-IoT-Core" class="headerlink" title="AWS IoT Core"></a>AWS IoT Core</h1><p>It is a managed cloud service that lets connected devices easily and securely interact with cloud applications and other devices. AWS IoT Core provides secure communication and data processing across different kinds of connected devices and locations so you can easily build IoT applications.</p>
<h1 id="SQS"><a href="#SQS" class="headerlink" title="SQS"></a>SQS</h1><p>You cannot set a priority to individual items in the SQS queue.<br>In this scenario, it is stated that the SQS queue is configured with the maximum message retention period. The maximum message retention in SQS is 14 days, there will be no missing messages.<br>The queue can contain an unlimited number of messages, not just 10,000 messages.</p>
<p>In Amazon SQS, you can configure the message retention period to a value from 1 minute to 14 days. The default is 4 days. Once the message retention limit is reached, your messages are automatically deleted.</p>
<p>A single Amazon SQS message queue can contain an unlimited number of messages. However, there is a 120,000 limit for the number of inflight messages for a standard queue and 20,000 for a FIFO queue. Messages are inflight after they have been received from the queue by a consuming component, but have not yet been deleted from the queue.</p>
<h1 id="SWF"><a href="#SWF" class="headerlink" title="SWF"></a>SWF</h1><p>SWF workflow defines all the activities in the workflow.</p>
<p>The purpose of a decision task tells the decider the state of the workflow execution. The decider can be viewed as a special type of worker. Like workers, it can be written in any language and asks Amazon SWF for tasks. However, it handles special tasks called decision tasks.</p>
<p>Amazon SWF issues decision tasks whenever a workflow execution has transitions such as an activity task completing or timing out. A decision task contains information on the inputs, outputs, and current state of previously initiated activity tasks. Your decider uses this data to decide the next steps, including any new activity tasks, and returns those to Amazon SWF. Amazon SWF in turn enacts these decisions, initiating new activity tasks where appropriate and monitoring them.</p>
<p>By responding to decision tasks in an ongoing manner, the decider controls the order, timing, and concurrency of activity tasks and consequently the execution of processing steps in the application. SWF issues the first decision task when an execution starts. From there on, Amazon SWF enacts the decisions made by your decider to drive your execution. The execution continues until your decider makes a decision to complete it.</p>
<p>An activity task tells the worker to perform a function</p>
<p>A single task in the workflow represents a single task in the workflow.</p>
<h2 id="Distributed-systems"><a href="#Distributed-systems" class="headerlink" title="Distributed systems"></a>Distributed systems</h2><p>Amazon Simple Queue Service (SQS) and Amazon Simple Workflow Service (SWF) are the services that you can use for creating a decoupled architecture in AWS. Decoupled architecture is a type of computing architecture that enables computing components or layers to execute independently while still interfacing with each other.</p>
<p>Amazon SQS offers reliable, highly-scalable hosted queues for storing messages while they travel between applications or microservices. Amazon SQS lets you move data between distributed application components and helps you decouple these components. Amazon SWF is a web service that makes it easy to coordinate work across distributed application components.</p>
<h1 id="AppSync"><a href="#AppSync" class="headerlink" title="AppSync"></a>AppSync</h1><p>Power your applications with the right data, from one or more data sources, at global scale<br>AWS AppSync simplifies application development by letting you create a flexible API to securely access, manipulate, and combine data from one or more data sources. AppSync is a managed service that uses GraphQL to make it easy for applications to get exactly the data they need.</p>
<p>With AppSync, you can build scalable applications, including those requiring real-time updates, on a range of data sources such as NoSQL data stores, relational databases, HTTP APIs, and your custom data sources with AWS Lambda. For mobile and web apps, AppSync additionally provides local data access when devices go offline, and data synchronization with customizable conflict resolution, when they are back online.</p>
<p>Benefits<br>Start effortlessly; scale with your business<br>Get started in minutes directly from your IDE of choice (such as Xcode, Android Studio, VS Code), leverage the intuitive AWS AppSync management console, or use AWS Amplify CLI to automatically generate your API and client-side code. AWS AppSync integrates with Amazon DynamoDB, Amazon Aurora, Amazon Elasticsearch, AWS Lambda, and other AWS services, enabling you to create sophisticated applications, with virtually unlimited throughput and storage, that scale according to your business needs. </p>
<p>Real-time subscriptions and offline access<br>AWS AppSync enables real-time subscriptions across millions of devices, as well as offline access to app data. When an offline device reconnects, AWS AppSync automatically syncs only the updates that occurred when the device was disconnected, and not the entire data set. AWS AppSync offers user-customizable server-side conflict detection and resolution that does the heavy lifting of managing data conflicts so you don’t have to. </p>
<p>Unify and secure access to your distributed data<br>Perform complex queries and aggregation across multiple data sources with a single network call using GraphQL. AWS AppSync makes it easy to secure your app data using multiple concurrent authentication modes as well as allowing to define security and fine grained access control at the data definition level directly from your GraphQL schema. </p>
<p>Make Mobile Development Easier with Open Source Libraries for iOS and Android<br>Build real-time, global mobile apps that can handle millions of requests per second using Amplify libraries.<br>Read the blog<br>How it works<br>product-page-diagram_AppSync@2x<br>AWS AppSync is generally available. If you would like try building data driven mobile and web applications, watch the re:Invent session video to learn more and open the AWS AppSync console to get started. For pricing details, please see the pricing page. AWS AppSync is available in multiple regions. For details on region availability, please see the regions detail page.</p>
<p>AWS AppSync re:Invent session<br>Customers using AWS AppSync<br>600x400_AmericanCommBargeLine<br>600x400_Puresec<br>600x400_IDT<br>600x400_ASU<br>600x400_PublicGood<br>600x400_cookpad<br>600x400_ALDO<br>ticketmaster-clear-migration<br>Use cases<br>Real Time Collaboration<br>Data Broadcasting<br>You can use AWS AppSync to enable scalable, real-time collaboration use cases by broadcasting data from the backend to all connected clients (one-to-many) or broadcasting data between clients themselves (many-to-many). For example, you can build a second screen scenario where you broadcast the same data to all clients, and users then respond in real time by voting and commenting about what’s on the screen.<br>Reference Architecture: Sample Code</p>
<p>product-page-diagram_AppSync_Data-Broadcasting@2x<br>Chat Applications<br>You can use AWS AppSync to power collaborative and conversational applications. For example, you can build a mobile and web application that supports multiple private chat rooms, offers access to conversation history, and enqueues outbound messages, even when the device is offline.</p>
<p>Reference Architecture: Sample Code</p>
<p>Product-Page-Diagram_AppSync_Chat-Applications_2@2x<br>Internet of Things<br>You can use AWS AppSync to access IoT device data sent to AWS IoT. For instance, you can build a real-time dashboard in a mobile or web application to visualize telemetry from a connected car.<br>Product-Page-Diagram_AppSync_IoT@2x<br>Data Layer<br>Polyglot Backend Data Access<br>You can retrieve or modify data from multiple data sources (SQL databases in Amazon Aurora Serverless, NoSQL tables in Amazon DynamoDB, search data in Amazon Elasticsearch Service, REST endpoints in Amazon API Gateway, or serverless backends in AWS Lambda) with a single call. Query and create relations between data sources using GraphQL connections. Provide real-time and offline capabilities to web and mobile clients. </p>
<p>Product-Page-Diagram_AppSync_Polyglot-Back-end-Data-Access@2x<br>Microservices Access Layer<br>You can use AWS AppSync as a single interface to access and combine data from multiple microservices in your application, even if they’re running in different environments such as containers in a VPC, behind a REST API on Amazon API Gateway, or behind a GraphQL API on another AWS AppSync endpoint.<br>Product-Page-Diagram_AppSync_Microservices-Aggregation@2x<br>Offline<br>Offline Delta Sync<br>You can use AppSync with Amplify DataStore, an on-device persistent storage engine that automatically synchronizes data between mobile/web apps and the cloud using GraphQL with a local-first and familiar programming model, leveraging AWS AppSync built-in support for data versioning with advanced conflict detection and resolution strategies such as auto-merge, optimistic concurrency or custom resolution with your own Lambda functions.</p>
<h1 id="Q-amp-A-2"><a href="#Q-amp-A-2" class="headerlink" title="Q&amp;A"></a>Q&amp;A</h1><ul>
<li><p>Lambda would time out after 15 minutes (2000*3=6000 seconds = 100 minutes). Glue is for performing ETL, but cannot run custom Python scripts. Kinesis Streams is for real time data (here we are in a batch setup), RDS could be used to run SQL queries on the data, but no Python script. The correct answer is EC2</p>
</li>
<li><p>Elastic Beanstalk cannot manage AWS Lambda functions, OpsWorks is for Chef / Puppet, and Trusted Advisor is to get recommendations regarding the 5 pillars of the well architected framework.</p>
</li>
<li><p>Kinesis cannot scale infinitely and we may have the same throughput issues. SNS won’t keep our data if it cannot be delivered, and DAX is used for caching reads, not to help with writes. Here, using SQS as a middleware will help us sustain the write throughput during write peaks</p>
</li>
<li><p>CloudFront is not a good solution here as the content is highly dynamic, and CloudFront will cache things. Dynamic applications cannot be deployed to S3, and Route53 does not help in scaling your application. ASG is the correct answer here</p>
</li>
<li><p>Network Load Balancers expose a fixed IP to the public web, therefore allowing your application to be predictably reached using these IPs, while allowing you to scale your application behind the Network Load Balancer using an ASG. Application and Classic Load Balancers expose a fixed DNS (=URL). Finally, the ASG does not have a dynamic Elastic IPs attachment feature</p>
</li>
<li><p>Hosting the master pack into S3 will require an application code refactor. Upgrading the EC2 instances won’t help save network cost and ELB does not have any caching capability. Here you need to create a CloudFront distribution to add a caching layer in front of your ELB. That caching layer will be very effective as the image pack is a static file, and tremendously save in network cost.</p>
</li>
<li><p>Adding Aurora Read Replicas would greatly increase the cost, switching to a Load Balancer would not improve the problems, and AWS Lambda has no native in memory caching capability. Here, using API Gateway Caching feature is the answer, as we can accept to serve stale data to our users</p>
</li>
<li><p>SQS will allow you to buffer the image compression requests and process them asynchronously. It also has a direct built-in mechanism for retries and scales seamlessly. To process these jobs, due to the unpredictable nature of their volume, and the desire to save on costs, Spot Instances are recommended.</p>
</li>
<li><p>Distributing the static content through S3 allows to offload most of the network usage to S3 and free up our applications running on ECS. EFS will not change anything as static content on EFS would still have to be distributed by our ECS instances</p>
</li>
<li><p>S3 does not work as overwrites are eventually consistent so the latest data will not always be read. Neptune is a graph database so it’s not a good fit, ElastiCache could work but it’s a better fit as a caching technology to enhance reads. Here, the best fit is RDS.</p>
</li>
<li><p>RDS Multi AZ helps with disaster recovery in case of an AZ failure. ElastiCache would definitely help with the read load, but would require a refactor of the application’s core logic. DynamoDB with DAX would also probably help with the read load, but once again it would require a refactor of the application’s core logic. Here, our only option to scale reads is to use RDS Read Replicas</p>
</li>
<li><p>Which of the following AWS services encrypts data at rest by default? (Choose 2)<br>All data transferred between any type of gateway appliance and AWS storage is encrypted using SSL. By default, all data stored by AWS Storage Gateway in S3 is encrypted server-side with Amazon S3-Managed Encryption Keys (SSE-S3). Also, when using the file gateway, you can optionally configure each file share to have your objects encrypted with AWS KMS-Managed Keys using SSE-KMS. This is the reason why Option 1 is correct.</p>
</li>
</ul>
<p>Data stored in Amazon Glacier is protected by default; only vault owners have access to the Amazon Glacier resources they create. Amazon Glacier encrypts your data at rest by default and supports secure data transit with SSL. This is the reason why Option 4 is correct.</p>
<p>Although Amazon RDS, ECS and Lambda all support encryption, you still have to enable and configure them first with tools like AWS KMS to encrypt the data at rest.</p>
<h1 id="Resources"><a href="#Resources" class="headerlink" title="Resources"></a>Resources</h1><ul>
<li>RDS cheatsheet: <a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-relational-database-service-amazon-rds/" target="_blank" rel="noopener">https://tutorialsdojo.com/aws-cheat-sheet-amazon-relational-database-service-amazon-rds/</a></li>
<li>VPC cheatsheet: <a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-vpc/" target="_blank" rel="noopener">https://tutorialsdojo.com/aws-cheat-sheet-amazon-vpc/</a></li>
<li>Amazon Kinesis cheatsheet: <a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-kinesis/" target="_blank" rel="noopener">https://tutorialsdojo.com/aws-cheat-sheet-amazon-kinesis/</a></li>
</ul>

        </div>
    

</div>
            
                
<div class="post">

    <div class="post-header index">
        <h1 class="title">
            <a href="/2017-01-21-Algorithm/">
                Algorithm
            </a>
        </h1>
        <div class="post-info">
            
                <span class="date">2019-12-28</span>
            
            
            
        </div>
    </div>

    
        <div class="content">
            <h1 id="This-page-is-about-key-points-about-Algorithm"><a href="#This-page-is-about-key-points-about-Algorithm" class="headerlink" title="This page is about key points about Algorithm"></a>This page is about key points about Algorithm</h1><h2 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h2><ul>
<li>The easiest way to improve search efficiency on a set of data is to put it in a data structure that allows more efficient searching.<br>What data structures can be searched more efficiency than O(n)? Binary tree can be searched in O(log(n)). Arrays and hash tables both have constant time element look up (has tables have worse-case lookup of O(n) but the average case is O(1)).</li>
<li>Then need to determine which data structure to be used. If the underlying characters are just ASCII, then a array[128] would be enough. But characters are UNICODe, then it need 100,000 (100K) array, which is a concern of memory, so hash table would be a better option, which only keep exist characters.<br>In general, arrays are a better choice for long strings with a limited set of possible characters values, hash tables are more efficient for shorter strings or when there are many possible character values.  </li>
<li>For some problems, obvious iterative alternatives like the one just shown don’t exist, but it’s always possible to implement a recursive algorithm without using recursive calls.</li>
<li>For a simple recursive function like factorial, many computer architectures spend more time on call overhead than on the actual calculation. Iterative functions, which use looping constructs instead of recursive function calls, do not suffer from this overhead and are frequently more efficient.</li>
<li>NOTE Iterative solutions are usually more efficient than recursive solutions.</li>
<li>NOTE Every recursive case must eventually lead to a base case.</li>
<li>NOTE Recursive algorithms have two cases: recursive cases and base cases</li>
</ul>
<h1 id="Sort"><a href="#Sort" class="headerlink" title="Sort"></a>Sort</h1><h2 id="I-collections-sort"><a href="#I-collections-sort" class="headerlink" title="I collections.sort()"></a>I collections.sort()</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Object[] a = list.toArray();</span><br><span class="line">        Arrays.sort(a);</span><br><span class="line">        ListIterator&lt;T&gt; i = list.listIterator();</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j=<span class="number">0</span>; j&lt;a.length; j++) &#123;</span><br><span class="line">            i.next();</span><br><span class="line">            i.set((T)a[j]);</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure>

<h2 id="Arrays-sort"><a href="#Arrays-sort" class="headerlink" title="Arrays.sort"></a>Arrays.sort</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">sort</span><span class="params">(Object[] a)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (LegacyMergeSort.userRequested)</span><br><span class="line">            legacyMergeSort(a);</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            ComparableTimSort.sort(a);</span><br><span class="line">    &#125;</span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">mergeSort</span><span class="params">(Object[] src,</span></span></span><br><span class="line"><span class="function"><span class="params">                                  Object[] dest,</span></span></span><br><span class="line"><span class="function"><span class="params">                                  <span class="keyword">int</span> low,</span></span></span><br><span class="line"><span class="function"><span class="params">                                  <span class="keyword">int</span> high,</span></span></span><br><span class="line"><span class="function"><span class="params">                                  <span class="keyword">int</span> off)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> length = high - low;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Insertion sort on smallest arrays</span></span><br><span class="line">        <span class="keyword">if</span> (length &lt; INSERTIONSORT_THRESHOLD) &#123; <span class="comment">// threshold is 7</span></span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> i=low; i&lt;high; i++)</span><br><span class="line">                <span class="keyword">for</span> (<span class="keyword">int</span> j=i; j&gt;low &amp;&amp;</span><br><span class="line">                         ((Comparable) dest[j-<span class="number">1</span>]).compareTo(dest[j])&gt;<span class="number">0</span>; j--)</span><br><span class="line">                    swap(dest, j, j-<span class="number">1</span>);</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// else use mergeSort</span></span><br></pre></td></tr></table></figure>

<h1 id="Self-review"><a href="#Self-review" class="headerlink" title="Self review"></a>Self review</h1><h2 id="CeasarCipher"><a href="#CeasarCipher" class="headerlink" title="CeasarCipher:"></a>CeasarCipher:</h2><p>Generally: it’s a rotation of English alphabic. E.g. if rotation is 2, the encode is start from A+2, i.e. A is at -2 of the encode array.<br>And decode is start with 26-2, and “A” start at positon 2, then the increase by 1 character to constitute the array<br>That’s why need to “%26”, to make it loop across 26 characters</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">If rotation is 2:</span><br><span class="line">--- encrytpion code is:[C, D, E, F, G, H, I, J, K, L, M, N, O, P, Q, R, S, T, U, V, W, X, Y, Z, A, B]</span><br><span class="line">--- decrytpion code is:[Y, Z, A, B, C, D, E, F, G, H, I, J, K, L, M, N, O, P, Q, R, S, T, U, V, W, X]</span><br><span class="line">If rotation is 4:</span><br><span class="line">--- encrytpion code is:[E, F, G, H, I, J, K, L, M, N, O, P, Q, R, S, T, U, V, W, X, Y, Z, A, B, C, D]</span><br><span class="line">--- decrytpion code is:[W, X, Y, Z, A, B, C, D, E, F, G, H, I, J, K, L, M, N, O, P, Q, R, S, T, U, V]</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">ABC:</span><br><span class="line">Msg =&#123;‘A’,’B’,’C’&#125;;</span><br><span class="line">Char[] encode=’A’+(k+rotation)%<span class="number">26</span>);</span><br><span class="line">Char[] decode=’A’+(k-rotation+<span class="number">26</span>)%<span class="number">26</span>); <span class="comment">// +26 to avoid negative</span></span><br><span class="line"></span><br><span class="line">Encode=&#123;‘C’,’D’,’E’&#125;; <span class="comment">// rotation=3, so A+3, A+4,A+5,xxx, A+26=&gt;3,4,5,6,xxx,0</span></span><br><span class="line">Decode=&#123;‘M’,’N’,’O’&#125;;<span class="comment">//as k-rotation+26 % 26, so it’s 26-3+0,26-3+1 ,xx: =&gt; 23,24,25,0,1,2,3,4,is: A+23,A+24,A+25=&gt;‘M’,’N’,’O’. that’s rotation, rotain-1, rotaion -2 xxxx</span></span><br><span class="line">For(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;msg.length;i++)&#123;</span><br><span class="line">  Int j=msg[i]-‘A’; <span class="comment">// to remove the base ‘A”, so sync with the “k” in encode, 3,4,5,xxx 3+26</span></span><br><span class="line">  Msg[i]=codes[j];</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// encode</span></span><br><span class="line">Int j=’A’-‘A’; <span class="comment">//0</span></span><br><span class="line">Msg[<span class="number">0</span>]=’C’;</span><br><span class="line">Msg[<span class="number">1</span>]=’D’;</span><br><span class="line">Msg[<span class="number">2</span>]=’E’;</span><br><span class="line"></span><br><span class="line"><span class="comment">//decode</span></span><br><span class="line">Int j=msg[i]-‘A’; <span class="comment">//’C’-‘A’=3</span></span><br><span class="line">Msg[i]=decode[j]; <span class="comment">// correspoindg to the postion 0,-xxx, 26 in decode, </span></span><br><span class="line">Msg[<span class="number">0</span>]=</span><br></pre></td></tr></table></figure>

<ul>
<li>If you says “tree,” it’s a good idea to clarify whether she is referring to a generic tree or a binary tree.</li>
</ul>
<h1 id="To-print-content-of-Array"><a href="#To-print-content-of-Array" class="headerlink" title="To print content of Array"></a>To print content of Array</h1><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Import java.util.Arrays;</span><br><span class="line">Arrays.toString(ary);</span><br><span class="line">Arrays.deepToString(ary);</span><br></pre></td></tr></table></figure>

<h1 id="search-without-recursive"><a href="#search-without-recursive" class="headerlink" title="search without recursive"></a>search without recursive</h1><p>Node findNode( Node root, int value ){<br>    while( root != null ){<br>        int currval = root.getValue();<br>        if( currval == value ) break;<br>        if( currval &lt; value ){<br>            root = root.getRight();<br>        } else { // currval &gt; value<br>            root = root.getLeft();<br>        }<br>    }</p>
<pre><code>return root;</code></pre><p>}</p>
<ul>
<li><p>preceding lookup operation can be reimplemented recursively as follows:<br>Node findNode( Node root, int value ){<br>  if( root == null ) return null;<br>  int currval = root.getValue();<br>  if( currval == value ) return root;<br>  if( currval &lt; value ){</p>
<pre><code>return findNode( root.getRight(), value );</code></pre><p>  } else { // currval &gt; value</p>
<pre><code>return findNode( root.getLeft(), value );</code></pre><p>  }<br>}</p>
</li>
<li><p>This subtree property is conducive to recursion because recursion generally involves solving a problem in terms of similar subproblems and a base case. </p>
</li>
</ul>
<h1 id="Big-O-sequencey"><a href="#Big-O-sequencey" class="headerlink" title="Big O sequencey"></a>Big O sequencey</h1><p>1, logn, n, n log n, n2, n3, 2n(2 power n).</p>
<h1 id="Big-O"><a href="#Big-O" class="headerlink" title="Big O"></a>Big O</h1><ul>
<li>It is also considered poor taste to include constant factors and lower-order terms in the big-Oh notation. For example, it is not fashionable to say that the function 2n2 is O(4n2 + 6n log n), although this is completely correct. We should strive instead to describe the function in the big-Oh <strong>in simplest terms</strong>.</li>
<li>So, for example, we would say that an algorithm that runs in worst-case time 4n2 + n log n is a quadratic-time algorithm, since it runs in O(n2) time. Likewise, an algorithm running in time at most 5n + 20logn + 4 would be called a linear-time algorithm.</li>
</ul>
<h1 id="Big-Omega"><a href="#Big-Omega" class="headerlink" title="Big Omega"></a>Big Omega</h1><ul>
<li>Just as the big-Oh notation provides an asymptotic way of saying that a function is “less than or equal to” another function, the following notations provide <strong>an asymptotic way</strong> of saying that a function grows at a rate that is “<strong>greater than or equal to</strong>” that of another.</li>
<li>Example 4.14: 3n log n ? 2n is Ω(n log n).</li>
</ul>
<h1 id="Big-Theta"><a href="#Big-Theta" class="headerlink" title="Big-Theta"></a>Big-Theta</h1><ul>
<li>In addition, there is a notation that allows us to say that two functions grow at the same rate, up to constant factors. We say that f(n) is Θ(g(n)), pronounced “f(n) is big-Theta of g(n),”</li>
</ul>
<h1 id="Comparative-Analysis"><a href="#Comparative-Analysis" class="headerlink" title="Comparative Analysis"></a>Comparative Analysis</h1><ul>
<li>asymptotically[,?simp’t?tik,-k?l] better</li>
<li>Suppose two algorithms solving the same problem are available: an algorithm A, which has a running time of O(n), and an algorithm B, which has a running time of O(n2). Which algorithm is better? We know that n is O(n2), which implies that algorithm A <strong>is asymptotically better</strong> than algorithm B, although for a small value of n, B may have a lower running time than A.</li>
</ul>
<h1 id="Some-Words-of-Caution"><a href="#Some-Words-of-Caution" class="headerlink" title="Some Words of Caution"></a>Some Words of Caution</h1><ul>
<li>First, note that the use of the big-Oh and related notations can be somewhat misleading should the constant factors they “hide” be very large. For example, while it is true that the function 10100n is O(n), if this is the running time of an algorithm being compared to one whose running time is 10n log n, we should prefer the O(nlog n)-time algorithm, even though the linear-time algorithm is asymptotically faster. This preference is because the constant factor, 10100, which is called “one googol,” is believed by many astronomers to be an upper bound on the number of atoms in the observable universe. So we are unlikely to ever have a real-world problem that has this number as its input size.</li>
</ul>
<h1 id="Exponential-eksp-’nen-l-Running-Times"><a href="#Exponential-eksp-’nen-l-Running-Times" class="headerlink" title="Exponential [,eksp?’nen?(?)l] Running Times"></a>Exponential [,eksp?’nen?(?)l] Running Times</h1><ul>
<li>To see how fast the function 2n grows, consider the famous story about the inventor of the game of chess. He asked only that his king pay him 1 grain of rice for the first square on the board, 2 grains for the second, 4 grains for the third, 8 for the fourth, and so on. The number of grains in the 64th square would be<br>263 = 9, 223, 372, 036, 854, 775, 808,<br>which is about nine billion billions!</li>
<li>If we must draw a line between efficient and inefficient algorithms, therefore, it is natural to make this distinction be that between those algorithms running in <strong>polynomial [,p?l?’n??m??l] time</strong> and those running in <strong>exponential time</strong>. That is, make the distinction between algorithms with a running time that is <strong>O(nc)</strong> (power c based on n), for some constant c &gt; 1, and those with a running time that is <strong>O(bn)</strong> (power n based on b), for some constant b &gt; 1. Like so many notions we have discussed in this section, this too should be taken with a “grain of salt,” for an algorithm running in O(n100) time should probably not be considered “efficient.” Even so, the distinction between polynomial-time and exponential-time algorithms is considered a robust measure of tractability.</li>
</ul>
<h1 id="Examples-of-Algorithm-Analysis"><a href="#Examples-of-Algorithm-Analysis" class="headerlink" title="Examples of Algorithm Analysis"></a>Examples of Algorithm Analysis</h1><h2 id="constant-time-operation"><a href="#constant-time-operation" class="headerlink" title="constant time operation"></a>constant time operation</h2><ul>
<li>All of the primitive operations, originally described on page 154, are assumed to run in constant time; Assume that variable A is an array of n elements. The expression A.length in Java is evaluated in constant time, because arrays are represented internally with an explicit variable that records the length of the array. Another central behavior of arrays is that for any valid index j, the individual element, A[j], can be accessed in constant time. This is because an array uses a consecutive block of memory. The jth element can be found, not by iterating through the array one element at a time, but by validating the index, and using it as an offset from the beginning of the array in determining the appropriate memory address. Therefore, we say that the expression A[j] is evaluated in O(1) time for an array.</li>
</ul>
<h2 id="Finding-the-Maximum-of-an-Array"><a href="#Finding-the-Maximum-of-an-Array" class="headerlink" title="Finding the Maximum of an Array"></a>Finding the Maximum of an Array</h2><p>Proposition 4.16: The algorithm, arrayMax, for computing the maximum element of an array of n numbers, runs in O(n) time.</p>
<p>Justification: The initialization at lines 3 and 4 and the return statement at line 8 require only a constant number of primitive operations. Each iteration of the loop also requires only a constant number of primitive operations, and the loop executes n ? 1 times.</p>
<h2 id="Composing-Long-Strings"><a href="#Composing-Long-Strings" class="headerlink" title="Composing Long Strings"></a>Composing Long Strings</h2><ul>
<li><p>Therefore, the overall time taken by this algorithm is proportional to<br>1 + 2 + ··· + n,<br>which we recognize as the familiar O(n<sup>2</sup>) summation from Proposition 4.3. Therefore, the total time complexity of the repeat1 algorithm is O(n<sup>2</sup>).</p>
</li>
<li><p>x = logbn if and only if bx = n.<br>The value b is known as the base of the logarithm. Note that by the above definition, for any base b &gt; 0, we have that logb 1 = 0.</p>
</li>
</ul>
<h1 id="Three-Way-Set-Disjointness"><a href="#Three-Way-Set-Disjointness" class="headerlink" title="Three-Way Set Disjointness"></a>Three-Way Set Disjointness</h1><h2 id="Origional-solution"><a href="#Origional-solution" class="headerlink" title="Origional solution"></a>Origional solution</h2><p>Suppose we are given three sets, A, B, and C, stored in three different integer arrays. We will assume that no individual set contains duplicate values, but that there may be some numbers that are in two or three of the sets. The three-way set disjointness problem is to determine if the intersection of the three sets is empty, namely, that there is no element x such that x ∈ A, x ∈ B, and x ∈ C.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">boolean</span> <span class="title">disjoint1</span><span class="params">(<span class="keyword">int</span>[]  groupA, <span class="keyword">int</span>[] groupB, <span class="keyword">int</span>[] groupC)</span></span>&#123;</span><br><span class="line">    <span class="keyword">for</span> ( <span class="keyword">int</span> i : groupA) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j : groupB) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> k : groupC) &#123;</span><br><span class="line">                <span class="keyword">if</span>(i==j &amp;&amp; j==k)&#123;</span><br><span class="line">                    <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>This simple algorithm loops through each possible triple of values from the three sets to see if those values are equivalent. If each of the original sets has size n, then the worst-case running time of this method is O(n<sup>3</sup>) .<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">boolean</span> <span class="title">disjoint2</span><span class="params">(<span class="keyword">int</span>[]  groupA, <span class="keyword">int</span>[] groupB, <span class="keyword">int</span>[] groupC)</span></span>&#123;</span><br><span class="line">        <span class="keyword">for</span> ( <span class="keyword">int</span> i : groupA) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> j : groupB) &#123;</span><br><span class="line">                <span class="keyword">if</span>(i==j)&#123;</span><br><span class="line">                    <span class="comment">// add this checking to reduce complexitiy</span></span><br><span class="line">                    <span class="keyword">for</span> (<span class="keyword">int</span> k : groupC) &#123;</span><br><span class="line">                        <span class="keyword">if</span>(j==k)&#123;</span><br><span class="line">                            <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

</li>
</ul>
<p>In the improved version, it is not simply that we save time if we get lucky. We claim that the worst-case running time for disjoint2 is O(n<sup>2</sup>).</p>
<h1 id="by-sorting"><a href="#by-sorting" class="headerlink" title="by sorting"></a>by sorting</h1><p>Sorting algorithms will be the focus of Chapter 12. The best sorting algorithms (including those used by Array.sort in Java) guarantee a worst-case running time of O(nlog n). Once the data is sorted, the subsequent loop runs in O(n) time, and so the entire unique2 algorithm runs in O(n log n) time. Exercise C-4.35 explores the use of sorting to solve the three-way set disjointness problem in O(n log n) time.</p>
<h1 id="prefixAverage"><a href="#prefixAverage" class="headerlink" title="prefixAverage"></a>prefixAverage</h1><p>Check the source code at PrefixAverage.java, the inital implementation is two for loop, which is O(n<sup>2</sup>), while the better approach is reuse existing total sum.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// naiive approach,</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++) &#123;</span><br><span class="line">			<span class="keyword">double</span> total=<span class="number">0</span>;</span><br><span class="line">			<span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt;=i; j++) &#123; <span class="comment">// be awre it's &lt;=, instead of "&lt;"</span></span><br><span class="line">				total+=x[j];				</span><br><span class="line">			&#125;</span><br><span class="line">			a[i]=total/(i+<span class="number">1</span>);			</span><br><span class="line">		&#125;</span><br><span class="line"><span class="comment">// better approach</span></span><br><span class="line"><span class="keyword">double</span> total=<span class="number">0</span>;</span><br><span class="line">		<span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++) &#123;</span><br><span class="line">			total += x[i];</span><br><span class="line">			a[i]=total/(i+<span class="number">1</span>);			</span><br><span class="line">		&#125;</span><br></pre></td></tr></table></figure>

<h1 id="Recursive"><a href="#Recursive" class="headerlink" title="Recursive"></a>Recursive</h1><h2 id="Definitions"><a href="#Definitions" class="headerlink" title="Definitions"></a>Definitions</h2><ol>
<li>We have one or more <strong>base cases</strong>, which refer to fixed values of the function. e.g. for n!=1 as  n=1 is base.</li>
<li>Then we have one or more <strong>recursive cases</strong>, which define the function in terms of itself. for n!, it’s =n*(n-1)! for n&gt;=1</li>
</ol>
<ul>
<li>Repetition is achieved through repeated recursive invocations of the method. The process i finite because each time the method is invoked, its argument is smaller by one, and when a base case is reached, no further recursive calls are made.</li>
<li>In the case of computing the factorial function, there is no compelling reason for prefereing recursion over a direct iteration with a loop.</li>
</ul>
<h1 id="Tree"><a href="#Tree" class="headerlink" title="Tree"></a>Tree</h1><h2 id="ADT-Abstract-Data-Type"><a href="#ADT-Abstract-Data-Type" class="headerlink" title="ADT (Abstract Data Type)"></a>ADT (Abstract Data Type)</h2><ul>
<li>we define a tree ADT using the concept of a position as an abstraction for a node of a tree. An element is stored at each position, and positions satisfy parent-child relationships that define the tree structure. </li>
</ul>
<h2 id="Depth-and-Height"><a href="#Depth-and-Height" class="headerlink" title="Depth and Height"></a>Depth and Height</h2><h3 id="Depth"><a href="#Depth" class="headerlink" title="Depth"></a>Depth</h3><ul>
<li>The depth of p is the number of ancestors of p, other than p itself.</li>
<li>The running time of depth(p) for position p is O(dp + 1), where dp denotes the depth of p in the tree, because the algorithm performs a constant-time recursive step for each ancestor of p. Thus, algorithm depth(p) runs in O(n) worst-case time, where n is the total number of positions of T, because a position of T may have depth n - 1 <strong>if all nodes form a single branch</strong>. </li>
<li>Method depth, as implemented within the AbstractTree class.<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">depth</span><span class="params">(Position&lt;E&gt; p)</span></span>&#123;</span><br><span class="line">	<span class="keyword">if</span>(isRoot(p))</span><br><span class="line">		<span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">	<span class="keyword">else</span></span><br><span class="line">		<span class="keyword">return</span> <span class="number">1</span>+depth(parent(p));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h3 id="Height"><a href="#Height" class="headerlink" title="Height"></a>Height</h3><ul>
<li>We next define the height of a tree to be equal to the maximum of the depths of its positions (or zero, if the tree is empty).</li>
<li>Folloing worst time cost is O(n), it progresses in a <strong>top-down</strong> fashion.</li>
<li>If the method is initially called on the root of T, it will eventually be called once for each position of T. This is because the root eventually invokes the recursion on each of its children, which in turn invokes the recursion on each of their children, and so on.<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">height</span><span class="params">(Position&lt;E&gt; p)</span></span>&#123;</span><br><span class="line">	<span class="keyword">int</span> h=<span class="number">0</span>;</span><br><span class="line">	<span class="keyword">for</span>(Position&lt;E&gt; c: children(p))</span><br><span class="line">		h=Math.max(h,<span class="number">1</span>+height(c));</span><br><span class="line">	<span class="keyword">return</span> h;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h2 id="Binary-Tree"><a href="#Binary-Tree" class="headerlink" title="Binary Tree"></a>Binary Tree</h2><ul>
<li><p>A binary tree is an ordered tree with the following properties:</p>
<ul>
<li>Every node has at most two children.</li>
<li>Each child node is labeled as being either a left child or a right child.</li>
<li>A left child precedes a right child in the order of children of a node.</li>
</ul>
</li>
<li><p>A binary <strong>tree is proper if each node has either zero or two children</strong>. Some people also refer to <strong>such trees as being full binary trees</strong>. Thus, <strong>in a proper binary tree, every internal node has exactly two children</strong>. A binary tree that is not proper is <strong>improper</strong>.</p>
<h3 id="Some-binary-trees"><a href="#Some-binary-trees" class="headerlink" title="Some binary trees"></a>Some binary trees</h3><h4 id="decision-tree"><a href="#decision-tree" class="headerlink" title="decision tree"></a>decision tree</h4></li>
<li><p>An important class of binary trees arises in contexts where we wish to represent a number of different outcomes that can result from answering a series of yes-or-no questions. Each internal node is associated with a question. Starting at the root, we go to the left or right child of the current node, depending on whether the answer to the question is “Yes” or “No.” With each decision, we follow an edge from a parent to a child, eventually tracing a path in the tree from the root to a leaf. Such binary trees are known as decision trees, because a leaf position p in such a tree represents a decision of what to do if the questions associated with p’s ancestors are answered in a way that leads to p. <strong>A decision tree is a proper binary tree</strong>. </p>
<h4 id="Arithmetic-expression"><a href="#Arithmetic-expression" class="headerlink" title="Arithmetic expression"></a>Arithmetic expression</h4></li>
<li><p>An arithmetic expression can be represented by a binary tree whose leaves are associated with variables or constants, and whose internal nodes are associated with one of the operators +, ?, *, and /, as demonstrated in Figure 8.6. Each node in such a tree has a value associated with it.</p>
<ul>
<li>If a node is leaf, then its value is that of its variable or constant.</li>
<li>If a node is internal, then its value is defined by applying its operation to the values of its children.<h3 id="Properties-of-Binary-trees"><a href="#Properties-of-Binary-trees" class="headerlink" title="Properties of Binary trees"></a>Properties of Binary trees</h3></li>
</ul>
</li>
<li><p>level d has at most 2<sup>d</sup> nodes</p>
</li>
<li><p>Let T be a nonempty binary tree, and let n, nE, nI, and h denote the number of nodes, number of external nodes, number of internal nodes, and height of T, respectively. Then T has the following properties:</p>
<ul>
<li>h + 1 ≤ n ≤ 2<sup>h+1</sup> - 1</li>
<li>1 ≤ nE ≤ 2<sup>h</sup></li>
<li>h ≤ nI ≤ 2<sup>h</sup> - 1</li>
<li>log(n + 1) - 1 ≤ h ≤ n - 1</li>
</ul>
</li>
<li><p>Also, if T is proper, then T has the following properties:</p>
<ul>
<li>2h + 1 ≤ n ≤ 2<sup>h+1</sup> - 1</li>
<li>h + 1 ≤ nE ≤ 2<sup>h</sup></li>
<li>h ≤ nI ≤ 2<sup>h</sup> - 1</li>
<li>log(n + 1) - 1 ≤ h ≤ (n - 1)/2</li>
</ul>
</li>
<li><p>In a nonempty proper binary tree T, with nE external nodes and nI internal nodes, we have nE = nI + 1.</p>
</li>
</ul>
<h1 id="Why-use-tree"><a href="#Why-use-tree" class="headerlink" title="Why use tree"></a>Why use tree</h1><ul>
<li>You can search, insert/delete items quickly in a tree</li>
<li>Ordered Arrays are bad at Insertions/Deletions</li>
<li>Finding items in a Linkedlist is slow</li>
<li>Time needed to perform an operation on a tree is O(log N)</li>
<li>On average a tree is more efficient if you need to perform many different types of operations.</li>
</ul>
<h1 id="Code-practice"><a href="#Code-practice" class="headerlink" title="Code practice"></a>Code practice</h1><p><a href="http://www.practice.geeksforgeeks.org/problem-page.php?pid=700159" target="_blank" rel="noopener">http://www.practice.geeksforgeeks.org/problem-page.php?pid=700159</a></p>
<h1 id="Geek-IDE"><a href="#Geek-IDE" class="headerlink" title="Geek IDE"></a>Geek IDE</h1><p><a href="http://code.geeksforgeeks.org/index.php" target="_blank" rel="noopener">http://code.geeksforgeeks.org/index.php</a></p>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul>
<li><a href="http://www.geeksforgeeks.org/maximum-width-of-a-binary-tree/" target="_blank" rel="noopener">http://www.geeksforgeeks.org/maximum-width-of-a-binary-tree/</a></li>
</ul>

        </div>
    

</div>
            
                
<div class="post">

    <div class="post-header index">
        <h1 class="title">
            <a href="/2019-09-12-Conversations-with-God/">
                Conversations with God
            </a>
        </h1>
        <div class="post-info">
            
                <span class="date">2019-09-12</span>
            
            
            
        </div>
    </div>

    
        <div class="content">
            <p>Feelings is the language of the soul.<br>If you want to know what’s true for you about something, look to how your’re feeling about.</p>

        </div>
    

</div>
            
                
<div class="post">

    <div class="post-header index">
        <h1 class="title">
            <a href="/2019-07-07-Kafka/">
                Kafka
            </a>
        </h1>
        <div class="post-info">
            
                <span class="date">2019-09-11</span>
            
            
            
        </div>
    </div>

    
        <div class="content">
            <h1 id="Kafka"><a href="#Kafka" class="headerlink" title="Kafka"></a>Kafka</h1><p>Kafka is fast. A single node can handle hundreds of read/writes from thousands of clients in real time. Kafka is also distributed and scalable. It creates and takes down nodes in an elastic manner, without incurring any downtime. Data streams are split into partitions and spread over different brokers for capability and redundancy.</p>
<h1 id="History-of-Kafka"><a href="#History-of-Kafka" class="headerlink" title="History of Kafka"></a>History of Kafka</h1><p>The result was a publish/subscribe messaging system that had an interface typical of messaging systems but a storage layer more like a log-aggregation system. Combined with the adoption of Apache Avro for message serialization, Kafka was effective for handling both metrics and user-activity tracking at a scale of billions of messages per day.</p>
<h2 id="Kafka-features"><a href="#Kafka-features" class="headerlink" title="Kafka features"></a>Kafka features</h2><ul>
<li><p>Language Agnostic<br>Producers and consumers use binary protocol to talk to a Kafka cluster.</p>
</li>
<li><p>Durability<br>Kafka does not track which messages were read by each consumer. Kafka keeps all messages for a finite amount of time, and it is consumers’ responsibility to track their location per topic, i.e. offsets.</p>
</li>
</ul>
<h2 id="Terminology"><a href="#Terminology" class="headerlink" title="Terminology:"></a>Terminology:</h2><p>Topic: a feed of messages or packages<br>Partition: group of topics split for scalability and redundancy<br>Producer: process that introduces messages into the queue<br>Consumer: process that subscribes to various topics and processes from a feed of published messages<br>Broker: a node that is part of the Kafka cluster</p>
<h2 id="Topics-and-Partitions"><a href="#Topics-and-Partitions" class="headerlink" title="Topics and Partitions"></a>Topics and Partitions</h2><p>Messages in Kafka are categorized into topics. The closest analogies for a topic are a database table or a folder in a filesystem. Topics are additionally broken down into a number of partitions. Going back to the “commit log” description, a partition is a sin‐ gle log. Messages are written to it in an append-only fashion, and are read in order from beginning to end. Note that as a topic typically has multiple partitions, there is no guarantee of message time-ordering across the entire topic, just within a single partition. </p>
<p>Partitions are also the way that Kafka provides redundancy and scalability. Each partition can be hosted on a different server, which means that a single topic can be scaled horizontally across multiple servers to provide performance far beyond the ability of a single server.</p>
<h1 id="Producers-and-consumers"><a href="#Producers-and-consumers" class="headerlink" title="Producers and consumers"></a>Producers and consumers</h1><p>Kafka clients are users of the system, and there are two basic types: producers and consumers. There are also advanced client APIs—Kafka Connect API for data inte‐ gration and Kafka Streams for stream processing. The advanced clients use producers and consumers as building blocks and provide higher-level functionality on top.</p>
<h2 id="producers"><a href="#producers" class="headerlink" title="producers"></a>producers</h2><p>Producers create new messages. In other publish/subscribe systems, these may be called publishers or writers. In general, a message will be produced to a specific topic. By default, the producer does not care what partition a specific message is written to and will balance messages over all partitions of a topic evenly. In some cases, the pro‐ ducer will direct messages to specific partitions. This is typically done using the mes‐ sage key and a partitioner that will generate a hash of the key and map it to a specific partition. This assures that all messages produced with a given key will get written to the same partition. The producer could also use a custom partitioner that follows other business rules for mapping messages to partitions.</p>
<h2 id="Consumers"><a href="#Consumers" class="headerlink" title="Consumers"></a>Consumers</h2><p>Consumers read messages. In other publish/subscribe systems, these clients may be called subscribers or readers. The consumer subscribes to one or more topics and reads the messages in the order in which they were produced. The consumer keeps track of which messages it has already consumed by keeping track of the offset of messages. The offset is another bit of metadata—an integer value that continually increases—that Kafka adds to each message as it is produced. Each message in a given partition has a unique offset. By storing the offset of the last consumed message for each partition, either in Zookeeper or in Kafka itself, a consumer can stop and restart without losing its place.</p>
<p>Consumers work as part of a consumer group, which is one or more consumers that work together to consume a topic. The group assures that each partition is only con‐ sumed by one member. there are three consumers in a single group consuming a topic. Two of the consumers are working from one partition each, while the third consumer is working from two partitions. The mapping of a consumer to a partition is often called ownership of the partition by the consumer.</p>
<h3 id="Consumer-group"><a href="#Consumer-group" class="headerlink" title="Consumer group"></a>Consumer group</h3><p>Consumers may be grouped in a consumer group with multiple consumers. Each consumer in a consumer group will read messages from a unique subset of partitions in each topic they subscribe to. Each message is delivered to one consumer in the group, and all messages with the same key arrive at the same consumer.</p>
<h2 id="Brokers-and-Clusters"><a href="#Brokers-and-Clusters" class="headerlink" title="Brokers and Clusters"></a>Brokers and Clusters</h2><p>A single Kafka server is called a broker. The broker receives messages from producers, assigns offsets to them, and commits the messages to storage on disk. It also services consumers, responding to fetch requests for partitions and responding with the mes‐ sages that have been committed to disk. Depending on the specific hardware and its performance characteristics, a single broker can easily handle thousands of partitions and millions of messages per second.<br>Kafka brokers are designed to operate as part of a cluster. Within a cluster of brokers, one broker will also function as the cluster controller (elected automatically from the live members of the cluster). The controller is responsible for administrative operations, including assigning partitions to brokers and monitoring for broker failures. A partition is owned by a single broker in the cluster, and that broker is called the leader of the partition. A partition may be assigned to multiple brokers, which will result in the partition being replicated This provides redundancy of messages in the partition, such that another broker can take over leadership if there is a broker failure. However, all consumers and producers operating on that partition must connect to the leader. </p>
<h2 id="retentions"><a href="#retentions" class="headerlink" title="retentions"></a>retentions</h2><p>A key feature of Apache Kafka is that of retention, which is the durable storage of messages for some period of time. Kafka brokers are configured with a default reten‐ tion setting for topics, either retaining messages for some period of time (e.g., 7 days) or until the topic reaches a certain size in bytes (e.g., 1 GB). Once these limits are reached, messages are expired and deleted so that the retention configuration is a minimum amount of data available at any time. Individual topics can also be config‐ ured with their own retention settings so that messages are stored for only as long as they are useful. For example, a tracking topic might be retained for several days, whereas application metrics might be retained for only a few hours. Topics can also be configured as log compacted, which means that Kafka will retain only the last mes‐ sage produced with a specific key. This can be useful for changelog-type data, where only the last update is interesting.</p>
<h2 id="mirror-maker"><a href="#mirror-maker" class="headerlink" title="mirror maker"></a>mirror maker</h2><p>The Kafka project includes a tool called MirrorMaker, used for this purpose. At its core, MirrorMaker is simply a Kafka consumer and producer, linked together with a queue. Messages are consumed from one Kafka cluster and produced for another.</p>
<h1 id="Why-Kafka"><a href="#Why-Kafka" class="headerlink" title="Why Kafka?"></a>Why Kafka?</h1><h2 id="Multiple-Producers"><a href="#Multiple-Producers" class="headerlink" title="Multiple Producers"></a>Multiple Producers</h2><p>Kafka is able to seamlessly handle multiple producers, whether those clients are using many topics or the same topic. </p>
<h2 id="Multiple-Consumers"><a href="#Multiple-Consumers" class="headerlink" title="Multiple Consumers"></a>Multiple Consumers</h2><p>In addition to multiple producers, Kafka is designed for multiple consumers to read any single stream of messages without interfering with each other. This is in contrast to many queuing systems where once a message is consumed by one client, it is not available to any other. Multiple Kafka consumers can choose to operate as part of a group and share a stream, assuring that the entire group processes a given message only once.</p>
<p>##Disk-Based Retention<br>Not only can Kafka handle multiple consumers, but durable message retention means that consumers do not always need to work in real time. Messages are committed to disk, and will be stored with configurable retention rules. </p>
<h2 id="Scalable"><a href="#Scalable" class="headerlink" title="Scalable"></a>Scalable</h2><p>Kafka’s flexible scalability makes it easy to handle any amount of data. Users can start with a single broker as a proof of concept, expand to a small development cluster of three brokers, and move into production with a larger cluster of tens or even hun‐ dreds of brokers that grows over time as the data scales up.</p>
<h2 id="High-Performance"><a href="#High-Performance" class="headerlink" title="High Performance"></a>High Performance</h2><p>All of these features come together to make Apache Kafka a publish/subscribe mes‐ saging system with excellent performance under high load. Producers, consumers, and brokers can all be scaled out to handle very large message streams with ease. This can be done while still providing subsecond message latency from producing a mes‐ sage to availability to consumers.</p>
<h1 id="Process-of-producing-message"><a href="#Process-of-producing-message" class="headerlink" title="Process of producing message"></a>Process of producing message</h1><p>We start producing messages to Kafka by creating a ProducerRecord, which must include the topic we want to send the record to and a value. Optionally, we can also specify a key and/or a partition. Once we send the ProducerRecord, the first thing the producer will do is serialize the key and value objects to ByteArrays so they can be sent over the network.<br>Next, the data is sent to a partitioner. If we specified a partition in the ProducerRecord, the partitioner doesn’t do anything and simply returns the partition we specified. If we didn’t, the partitioner will choose a partition for us, usually based on the ProducerRecord key. Once a partition is selected, the producer knows which topic and partition the record will go to. It then adds the record to a batch of records that will also be sent to the same topic and partition. A separate thread is responsible for sending those batches of records to the appropriate Kafka brokers.<br>When the broker receives the messages, it sends back a response. If the messages were successfully written to Kafka, it will return a RecordMetadata object with the topic, partition, and the offset of the record within the partition. If the broker failed to write the messages, it will return an error. When the producer receives an error, it may retry sending the message a few more times before giving up and returning an error.</p>
<h1 id="Constructing-a-Kafka-Producer"><a href="#Constructing-a-Kafka-Producer" class="headerlink" title="Constructing a Kafka Producer"></a>Constructing a Kafka Producer</h1><p>The first step in writing messages to Kafka is to create a producer object with the properties you want to pass to the producer. A Kafka producer has three mandatory properties:</p>
<h2 id="bootstrap-servers"><a href="#bootstrap-servers" class="headerlink" title="bootstrap.servers"></a>bootstrap.servers</h2><p>List of host:port pairs of brokers that the producer will use to establish initial connection to the Kafka cluster. This list doesn’t need to include all brokers, since the producer will get more information after the initial connection. But it is rec‐ ommended to include at least two, so in case one broker goes down, the producer will still be able to connect to the cluster.</p>
<h2 id="key-serializer"><a href="#key-serializer" class="headerlink" title="key.serializer"></a>key.serializer</h2><p>Name of a class that will be used to serialize the keys of the records we will pro‐ duce to Kafka. Kafka brokers expect byte arrays as keys and values of messages. However, the producer interface allows, using parameterized types, any Java object to be sent as a key and value. This makes for very readable code, but it also means that the producer has to know how to convert these objects to byte arrays. key.serializer should be set to a name of a class that implements the org.apache.kafka.common.serialization.Serializer interface. The producer will use this class to serialize the key object to a byte array. The Kafka client pack‐ age includes ByteArraySerializer (which doesn’t do much), StringSerializer, and IntegerSerializer, so if you use common types, there is no need to implement your own serializers. Setting key.serializer is required even if you intend to send only values.</p>
<h2 id="value-serializer"><a href="#value-serializer" class="headerlink" title="value.serializer"></a>value.serializer</h2><p>Name of a class that will be used to serialize the values of the records we will pro‐ duce to Kafka. The same way you set key.serializer to a name of a class that will serialize the message key object to a byte array, you set value.serializer to a class that will serialize the message value object.</p>
<h2 id="Sample-code-to-generate-producer-record"><a href="#Sample-code-to-generate-producer-record" class="headerlink" title="Sample code to generate producer record"></a>Sample code to generate producer record</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> Properties kafkaProps = <span class="keyword">new</span> Properties();</span><br><span class="line">    kafkaProps.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"broker1:9092,broker2:9092"</span>);</span><br><span class="line">    kafkaProps.put(<span class="string">"key.serializer"</span>,       <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">    kafkaProps.put(<span class="string">"value.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">    producer = <span class="keyword">new</span> KafkaProducer&lt;String, String&gt;(kafkaProps);</span><br></pre></td></tr></table></figure>

<h2 id="Deliver-message"><a href="#Deliver-message" class="headerlink" title="Deliver message"></a>Deliver message</h2><p>Once we instantiate a producer, it is time to start sending messages. There are three primary methods of sending messages:</p>
<h3 id="Fire-and-forget"><a href="#Fire-and-forget" class="headerlink" title="Fire-and-forget"></a>Fire-and-forget</h3><p>We send a message to the server and don’t really care if it arrives succesfully or not. Most of the time, it will arrive successfully, since Kafka is highly available and the producer will retry sending messages automatically. However, some mes‐ sages will get lost using this method.</p>
<h3 id="Synchronous-send"><a href="#Synchronous-send" class="headerlink" title="Synchronous send"></a>Synchronous send</h3><p>We send a message, the send() method returns a Future object, and we use get() to wait on the future and see if the send() was successful or not.</p>
<h3 id="Asynchronous-send"><a href="#Asynchronous-send" class="headerlink" title="Asynchronous send"></a>Asynchronous send</h3><p>We call the send() method with a callback function, which gets triggered when it receives a response from the Kafka broker.</p>
<p>Sample code</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">ProducerRecord&lt;String, String&gt; record =</span><br><span class="line">            <span class="keyword">new</span> ProducerRecord&lt;&gt;(<span class="string">"CustomerCountry"</span>, <span class="string">"Precision Products"</span>,</span><br><span class="line"><span class="string">"France"</span>);</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      producer.send(record);  <span class="comment">//fire and forget</span></span><br><span class="line">      producer.send(record).get(); <span class="comment">// synchronously, calling Future.get()</span></span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>We use the producer object send() method to send the ProducerRecord. As we’ve seen in the producer architecture diagram in Figure 3-1, the message will be placed in a buffer and will be sent to the broker in a separate thread. The send() method returns a Java Future object with RecordMetadata</p>
<h3 id="Samle-code-of-asynchronous"><a href="#Samle-code-of-asynchronous" class="headerlink" title="Samle code of asynchronous"></a>Samle code of asynchronous</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="class"><span class="keyword">class</span> <span class="title">DemoProducerCallback</span> <span class="keyword">implements</span> <span class="title">Callback</span> </span>&#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onCompletion</span><span class="params">(RecordMetadata recordMetadata, Exception e)</span> </span>&#123;</span><br><span class="line">         <span class="keyword">if</span> (e != <span class="keyword">null</span>) &#123;</span><br><span class="line">             e.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">&#125; &#125;</span><br><span class="line">    ProducerRecord&lt;String, String&gt; record =</span><br><span class="line">            <span class="keyword">new</span> ProducerRecord&lt;&gt;(<span class="string">"CustomerCountry"</span>, <span class="string">"Biomedical Materials"</span>, <span class="string">"USA"</span>);</span><br><span class="line">    producer.send(record, <span class="keyword">new</span> DemoProducerCallback());</span><br></pre></td></tr></table></figure>

<h2 id="Rebalancing"><a href="#Rebalancing" class="headerlink" title="Rebalancing"></a>Rebalancing</h2><p>Moving partition ownership from one consumer to another is called a rebalance. Rebalances are important because they provide the consumer group with high availa‐ bility and scalability (allowing us to easily and safely add and remove consumers), but in the normal course of events they are fairly undesirable. During a rebalance, con‐ sumers can’t consume messages, so a rebalance is basically a short window of unavail‐ ability of the entire consumer group. In addition, when partitions are moved from one consumer to another, the consumer loses its current state; if it was caching any data, it will need to refresh its caches—slowing down the application until the con‐ sumer sets up its state again. Throughout this chapter we will discuss how to safely handle rebalances and how to avoid unnecessary ones.</p>
<h2 id="cosumber"><a href="#cosumber" class="headerlink" title="cosumber"></a>cosumber</h2><h3 id="Subscribing-to-Topics"><a href="#Subscribing-to-Topics" class="headerlink" title="Subscribing to Topics"></a>Subscribing to Topics</h3><p>Once we create a consumer, the next step is to subscribe to one or more topics. The subcribe() method takes a list of topics as a parameter, so it’s pretty simple to use:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">consumer.subscribe(Collections.singletonList(<span class="string">"customerCountries"</span>));</span><br></pre></td></tr></table></figure>

<p>Here we simply create a list with a single element: the topic name customerCountries.</p>
<h3 id="Sample-consumer-code"><a href="#Sample-consumer-code" class="headerlink" title="Sample consumer code"></a>Sample consumer code</h3><p>The Poll Loop<br>At the heart of the consumer API is a simple loop for polling the server for more data. Once the consumer subscribes to topics, the poll loop handles all details of coordina‐ tion, partition rebalances, heartbeats, and data fetching, leaving the developer with a clean API that simply returns available data from the assigned partitions. The main body of a consumer will look as follows:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">          ConsumerRecords&lt;String, String&gt; records = consumer.poll(<span class="number">100</span>);</span><br><span class="line">          <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records)</span><br><span class="line">          &#123;</span><br><span class="line">              log.debug(<span class="string">"topic = %s, partition = %s, offset = %d,</span></span><br><span class="line"><span class="string">                 customer = %s, country = %s\n"</span>,</span><br><span class="line">                 record.topic(), record.partition(), record.offset(),</span><br><span class="line">                 record.key(), record.value());</span><br><span class="line">              <span class="keyword">int</span> updatedCount = <span class="number">1</span>;</span><br><span class="line">              <span class="keyword">if</span> (custCountryMap.countainsValue(record.value())) &#123;</span><br><span class="line">                  updatedCount = custCountryMap.get(record.value()) + <span class="number">1</span>;</span><br><span class="line">              &#125;</span><br><span class="line">              custCountryMap.put(record.value(), updatedCount)</span><br><span class="line">              JSONObject json = <span class="keyword">new</span> JSONObject(custCountryMap);</span><br><span class="line">              System.out.println(json.toString(<span class="number">4</span>))</span><br><span class="line">          &#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      consumer.close();</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<h3 id="Thread-safety"><a href="#Thread-safety" class="headerlink" title="Thread safety"></a>Thread safety</h3><p>Thread Safety<br>You can’t have multiple consumers that belong to the same group in one thread and you can’t have multiple threads safely use the same consumer. One consumer per thread is the rule. To run mul‐ tiple consumers in the same group in one application, you will need to run each in its own thread. It is useful to wrap the con‐ sumer logic in its own object and then use Java’s ExecutorService to start multiple threads each with its own consumer. </p>
<h2 id="Commit"><a href="#Commit" class="headerlink" title="Commit"></a>Commit</h2><p>As discussed before, one of Kafka’s unique characteristics is that it does not track acknowledgments from consumers the way many JMS queues do. Instead, it allows consumers to use Kafka to track their posi‐ tion (offset) in each partition.<br>We call the action of updating the current position in the partition a commit.</p>
<p>How does a consumer commit an offset? It produces a message to Kafka, to a special <code>__consumer_offsets</code> topic, with the committed offset for each partition. As long as all your consumers are up, running, and churning away, this will have no impact. However, if a consumer crashes or a new consumer joins the consumer group, this will trigger a rebalance. After a rebalance, each consumer may be assigned a new set of partitions than the one it processed before. In order to know where to pick up the work, the consumer will read the latest committed offset of each partition and con‐ tinue from there.</p>
<h3 id="Automatic-Commit"><a href="#Automatic-Commit" class="headerlink" title="Automatic Commit"></a>Automatic Commit</h3><p>With autocommit enabled, a call to poll will always commit the last offset returned by the previous poll. It doesn’t know which events were actually processed, so it is critical to always process all the events returned by poll() before calling poll() again. (Just like poll(), close() also commits offsets automatically.) This is usually not an issue, but pay attention when you handle exceptions or exit the poll loop prematurely.</p>
<h3 id="Manual-commit"><a href="#Manual-commit" class="headerlink" title="Manual commit"></a>Manual commit</h3><p>It is important to remember that commitSync() will commit the latest offset returned by poll(), so make sure you call commitSync() after you are done processing all the records in the collection, or you risk missing messages as described previously. When rebalance is triggered, all the messages from the beginning of the most recent batch until the time of the rebalance will be processed twice.<br>Here is how we would use commitSync to commit offsets after we finished processing the latest batch of messages:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">        ConsumerRecords&lt;String, String&gt; records = consumer.poll(<span class="number">100</span>);</span><br><span class="line">        <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records)</span><br><span class="line">        &#123;</span><br><span class="line">            System.out.printf(<span class="string">"topic = %s, partition = %s, offset =</span></span><br><span class="line"><span class="string">              %d, customer = %s, country = %s\n"</span>,</span><br><span class="line">                 record.topic(), record.partition(),</span><br><span class="line">                 record.offset(), record.key(), record.value());</span><br><span class="line">&#125; <span class="keyword">try</span> &#123;</span><br><span class="line">          consumer.commitSync();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (CommitFailedException e) &#123;</span><br><span class="line">            log.error(<span class="string">"commit failed"</span>, e)</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="Combining-Synchronous-and-Asynchronous-Commits"><a href="#Combining-Synchronous-and-Asynchronous-Commits" class="headerlink" title="Combining Synchronous and Asynchronous Commits"></a>Combining Synchronous and Asynchronous Commits</h2><p>Normally, occasional failures to commit without retrying are not a huge problem because if the problem is temporary, the following commit will be successful. But if we know that this is the last commit before we close the consumer, or before a reba‐ lance, we want to make extra sure that the commit succeeds.<br>Therefore, a common pattern is to combine commitAsync() with commitSync() just before shutdown. Here is how it works (we will discuss how to commit just before rebalance when we get to the section about rebalance listeners):</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">            ConsumerRecords&lt;String, String&gt; records = consumer.poll(<span class="number">100</span>);</span><br><span class="line">            <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line">                System.out.printf(<span class="string">"topic = %s, partition = %s, offset = %d,</span></span><br><span class="line"><span class="string">                customer = %s, country = %s\n"</span>,</span><br><span class="line">                record.topic(), record.partition(),</span><br><span class="line">                record.offset(), record.key(), record.value());</span><br><span class="line">&#125;</span><br><span class="line">            consumer.commitAsync();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">        log.error(<span class="string">"Unexpected error"</span>, e);</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            consumer.commitSync();</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">            consumer.close();</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="Exit"><a href="#Exit" class="headerlink" title="Exit"></a>Exit</h2><p>When you decide to exit the poll loop, you will need another thread to call con sumer.wakeup(). If you are running the consumer loop in the main thread, this can be done from ShutdownHook. <code>Note that consumer.wakeup() is the only consumer method that is safe to call from a different thread.</code> Calling wakeup will cause poll() to exit with WakeupException, or if consumer.wakeup() was called while the thread was not waiting on poll, the exception will be thrown on the next iteration when poll() is called. </p>
<h1 id="The-Controller"><a href="#The-Controller" class="headerlink" title="The Controller"></a>The Controller</h1><p>The controller is one of the Kafka brokers that, in addition to the usual broker func‐ tionality, is responsible for electing partition leaders (we’ll discuss partition leaders and what they do in the next section). The first broker that starts in the cluster becomes the controller by creating an ephemeral node in ZooKeeper called /control ler. When other brokers start, they also try to create this node, but receive a “node already exists” exception, which causes them to “realize” that the controller node already exists and that the cluster already has a controller. </p>
<h1 id="Replication"><a href="#Replication" class="headerlink" title="Replication"></a>Replication</h1><p>Replication is at the heart of Kafka’s architecture. The very first sentence in Kafka’s documentation describes it as “a distributed, partitioned, replicated commit log ser‐ vice.” Replication is critical because it is the way Kafka guarantees availability and durability when individual nodes inevitably fail.</p>
<h2 id="Memeroy"><a href="#Memeroy" class="headerlink" title="Memeroy"></a>Memeroy</h2><p>Kafka should run entirely on RAM. JVM heap size shouldn’t be bigger than your available RAM. That is to avoid swapping.</p>
<h3 id="Swap-usage"><a href="#Swap-usage" class="headerlink" title="Swap usage"></a>Swap usage</h3><p>Watch for swap usage, as it will degrade performance on Kafka and lead to operations timing out (set vm.swappiness = 0).    When used swap is &gt; 128MB.</p>
<h1 id="Kafka-Monitoring-Tools"><a href="#Kafka-Monitoring-Tools" class="headerlink" title="Kafka Monitoring Tools"></a>Kafka Monitoring Tools</h1><p>Any monitoring tools with JMX support should be able to monitor a Kafka cluster. Here are 3 monitoring tools we liked:</p>
<p>First one is check_kafka.pl from Hari Sekhon. It performs a complete end to end test, i.e. it inserts a message in Kafka as a producer and then extracts it as a consumer. This makes our life easier when measuring service times.</p>
<p>Another useful tool is KafkaOffsetMonitor for monitoring Kafka consumers and their position (offset) in the queue. It aids our understanding of how our queue grows and which consumers groups are lagging behind.</p>
<p>Last but not least, the LinkedIn folks have developed what we think is the smartest tool out there: Burrow. It analyzes consumer offsets and lags over a window of time and determines the consumer status. You can retrieve this status over an HTTP endpoint and then plug it into your favourite monitoring tool (Server Density for example).</p>
<p>Oh, and we would be amiss if we didn’t mention Yahoo’s Kafka-Manager. While it does include some basic monitoring, it is more of a management tool. If you are just looking for a Kafka management tool, check out AirBnb’s kafkat.</p>
<h1 id="commands"><a href="#commands" class="headerlink" title="commands"></a>commands</h1><h2 id="Start-zookeeper"><a href="#Start-zookeeper" class="headerlink" title="Start zookeeper"></a>Start zookeeper</h2><p>bin/zookeeper-server-start.sh config/zookeeper.properties</p>
<p>bin/kafka-server-start.sh config/server.properties</p>
<p>~/dev/git/kafka-demo/kafka_2.11-2.0.0/bin/kafka-topics.sh –create –zookeeper localhost:2181 –replication-factor 1 –partitions 1 –topic todtest<br> bin/kafka-topics.sh –list –zookeeper localhost:2181<br>bin/kafka-console-producer.sh –broker-list localhost:9092 –topic todtest<br>bin/kafka-console-consumer.sh –bootstrap-server localhost:9092 –topic todtest –from-beginning</p>
<p>bin/kafka-topics.sh –describe –zookeeper localhost:2181 –topic test</p>
<h2 id="list-topics"><a href="#list-topics" class="headerlink" title="list topics"></a>list topics</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./kafka-topics.sh --list --zookeeper localhost:2181</span><br></pre></td></tr></table></figure>

<h2 id="describe-topics"><a href="#describe-topics" class="headerlink" title="describe topics"></a>describe topics</h2><p>./kafka-topics.sh –describe –zookeeper localhost:2181</p>
<h3 id="using-connector"><a href="#using-connector" class="headerlink" title="using connector"></a>using connector</h3><p>bin/connect-standalone.sh config/connect-standalone.properties config/connect-file-source.properties config/connect-file-sink.properties</p>
<p>mvn archetype:generate <br>    -DarchetypeGroupId=org.apache.kafka <br>    -DarchetypeArtifactId=streams-quickstart-java <br>    -DarchetypeVersion=2.0.0 <br>    -DgroupId=io <br>    -DartifactId=todzhang <br>    -Dversion=0.1 <br>    -Dpackage=todzhangapp</p>
<h1 id="Security"><a href="#Security" class="headerlink" title="Security"></a>Security</h1><p>The keystore stores each machine’s own identity. The truststore stores all the certificates that the machine should trust. Importing a certificate into one’s truststore also means trusting all certificates that are signed by that certificate. As the analogy above, trusting the government (CA) also means trusting all passports (certificates) that it has issued. This attribute is called the chain of trust, and it is particularly useful when deploying SSL on a large Kafka cluster. You can sign all certificates in the cluster with a single CA, and have all machines share the same truststore that trusts the CA. That way all machines can authenticate all other machines.</p>
<p>To deploy SSL, the general steps are:</p>
<ul>
<li>Generate the keys and certificates</li>
<li>Create your own Certificate Authority (CA)</li>
<li>Sign the certificate</li>
</ul>
<p>Generate the key and the certificate for each Kafka broker in the cluster. Generate the key into a keystore called kafka.server.keystore so that you can export and sign it later with CA. The keystore file contains the private key of the certificate; therefore, it needs to be kept safely.</p>
<h2 id="With-user-prompts"><a href="#With-user-prompts" class="headerlink" title="With user prompts"></a>With user prompts</h2><p>keytool -keystore kafka.server.keystore.jks -alias localhost -genkey</p>
<h2 id="Without-user-prompts-pass-command-line-arguments"><a href="#Without-user-prompts-pass-command-line-arguments" class="headerlink" title="Without user prompts, pass command line arguments"></a>Without user prompts, pass command line arguments</h2><p>keytool -keystore kafka.server.keystore.jks -alias localhost -validity {validity} -genkey -storepass {keystore-pass} -keypass {key-pass} -dname {distinguished-name} -ext SAN=DNS:{hostname}<br>Ensure that the common name (CN) exactly matches the fully qualified domain name (FQDN) of the server. The client compares the CN with the DNS domain name to ensure that it is indeed connecting to the desired server, not a malicious one. The hostname of the server can also be specified in the Subject Alternative Name (SAN). Since the distinguished name is used as the server principal when SSL is used as the inter-broker security protocol, it is useful to have hostname as a SAN rather than the CN.</p>
<h2 id="Create-your-own-Certificate-Authority-CA"><a href="#Create-your-own-Certificate-Authority-CA" class="headerlink" title="Create your own Certificate Authority (CA)"></a>Create your own Certificate Authority (CA)</h2><p>Generate a CA that is simply a public-private key pair and certificate, and it is intended to sign other certificates.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openssl req -new -x509 -keyout ca-key -out ca-cert -days &#123;validity&#125;</span><br></pre></td></tr></table></figure>

<p>Add the generated CA to the clients’ truststore so that the clients can trust this CA:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keytool -keystore kafka.client.truststore.jks -<span class="built_in">alias</span> CARoot -import -file ca-cert</span><br></pre></td></tr></table></figure>

<p>Add the generated CA to the brokers’ truststore so that the brokers can trust this CA.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keytool -keystore kafka.server.truststore.jks -<span class="built_in">alias</span> CARoot -import -file ca-cert</span><br></pre></td></tr></table></figure>

<h2 id="Sign-the-certificate"><a href="#Sign-the-certificate" class="headerlink" title="Sign the certificate"></a>Sign the certificate</h2><p>To sign all certificates in the keystore with the CA that you generated:</p>
<p>Export the certificate from the keystore:</p>
<p>keytool -keystore kafka.server.keystore.jks -alias localhost -certreq -file cert-file<br>Sign it with the CA:</p>
<p>openssl x509 -req -CA ca-cert -CAkey ca-key -in cert-file -out cert-signed -days {validity} -CAcreateserial -passin pass:{ca-password}<br>Import both the certificate of the CA and the signed certificate into the broker keystore:</p>
<p>keytool -keystore kafka.server.keystore.jks -alias CARoot -import -file ca-cert<br>keytool -keystore kafka.server.keystore.jks -alias localhost -import -file cert-signed</p>
<h2 id="SASL"><a href="#SASL" class="headerlink" title="SASL"></a>SASL</h2><p>Simple Authentication and Security Layer (SASL) is a framework for authentication and data security in Internet protocols. It decouples authentication mechanisms from application protocols, in theory allowing any authentication mechanism supported by SASL to be used in any application protocol that uses SASL. Authentication mechanisms can also support proxy authorization, a facility allowing one user to assume the identity of another. They can also provide a data security layer offering data integrity and data confidentiality services. DIGEST-MD5 provides an example of mechanisms which can provide a data-security layer. Application protocols that support SASL typically also support Transport Layer Security (TLS) to complement the services offered by SASL.</p>
<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul>
<li><a href="https://blog.serverdensity.com/how-to-monitor-kafka/" target="_blank" rel="noopener">https://blog.serverdensity.com/how-to-monitor-kafka/</a></li>
</ul>

        </div>
    

</div>
            
                
<div class="post">

    <div class="post-header index">
        <h1 class="title">
            <a href="/2019-09-02-Kafka-In-Spring/">
                Kafka In Spring
            </a>
        </h1>
        <div class="post-info">
            
                <span class="date">2019-09-03</span>
            
            
            
        </div>
    </div>

    
        <div class="content">
            <p>Enable Kafka listener annotated endpoints that are created under the covers by a AbstractListenerContainerFactory. To be used on Configuration classes as follows:<br>   @Configuration<br>   @EnableKafka<br>   public class AppConfig {<br>       @Bean<br>       public ConcurrentKafkaListenerContainerFactory myKafkaListenerContainerFactory() {<br>           ConcurrentKafkaListenerContainerFactory factory = new ConcurrentKafkaListenerContainerFactory();<br>           factory.setConsumerFactory(consumerFactory());<br>           factory.setConcurrency(4);<br>           return factory;<br>       }<br>       // other @Bean definitions<br>   }</p>
<p>The KafkaListenerContainerFactory is responsible to create the listener container for a particular endpoint. Typical implementations, as the ConcurrentKafkaListenerContainerFactory used in the sample above, provides the necessary configuration options that are supported by the underlying MessageListenerContainer.<br>@EnableKafka enables detection of KafkaListener annotations on any Spring-managed bean in the container. For example, given a class MyService:<br>   package com.acme.foo;</p>
<p>   public class MyService {<br>       @KafkaListener(containerFactory = “myKafkaListenerContainerFactory”, topics = “myTopic”)<br>       public void process(String msg) {<br>           // process incoming message<br>       }<br>   }</p>
<p>The container factory to use is identified by the containerFactory attribute defining the name of the KafkaListenerContainerFactory bean to use. When none is set a KafkaListenerContainerFactory bean with name kafkaListenerContainerFactory is assumed to be present.<br>the following configuration would ensure that every time a message is received from topic “myQueue”, MyService.process() is called with the content of the message:<br>   @Configuration<br>   @EnableKafka<br>   public class AppConfig {<br>       @Bean<br>       public MyService myService() {<br>           return new MyService();<br>       }</p>
<pre><code>// Kafka infrastructure setup</code></pre><p>   }</p>
<p>Alternatively, if MyService were annotated with @Component, the following configuration would ensure that its @KafkaListener annotated method is invoked with a matching incoming message:<br>   @Configuration<br>   @EnableKafka<br>   @ComponentScan(basePackages = “com.acme.foo”)<br>   public class AppConfig {<br>   }</p>
<p>Note that the created containers are not registered with the application context but can be easily located for management purposes using the KafkaListenerEndpointRegistry.<br>Annotated methods can use a flexible signature; in particular, it is possible to use the Message abstraction and related annotations, see KafkaListener Javadoc for more details. For instance, the following would inject the content of the message and the kafka partition header:<br>   @KafkaListener(containerFactory = “myKafkaListenerContainerFactory”, topics = “myTopic”)<br>   public void process(String msg, @Header(“kafka_partition”) int partition) {<br>       // process incoming message<br>   }</p>
<p>These features are abstracted by the MessageHandlerMethodFactory that is responsible to build the necessary invoker to process the annotated method. By default, DefaultMessageHandlerMethodFactory is used.<br>When more control is desired, a @Configuration class may implement KafkaListenerConfigurer. This allows access to the underlying KafkaListenerEndpointRegistrar instance. The following example demonstrates how to specify an explicit default KafkaListenerContainerFactory<br>   {<br>       @code<br>       @Configuration<br>       @EnableKafka<br>       public class AppConfig implements KafkaListenerConfigurer {<br>           @Override<br>           public void configureKafkaListeners(KafkaListenerEndpointRegistrar registrar) {<br>               registrar.setContainerFactory(myKafkaListenerContainerFactory());<br>           }</p>
<pre><code>    @Bean
    public KafkaListenerContainerFactory&lt;?, ?&gt; myKafkaListenerContainerFactory() {
        // factory settings
    }

    @Bean
    public MyService myService() {
        return new MyService();
    }
}</code></pre><p>   }</p>
<p>It is also possible to specify a custom KafkaListenerEndpointRegistry in case you need more control on the way the containers are created and managed. The example below also demonstrates how to customize the org.springframework.messaging.handler.annotation.support.DefaultMessageHandlerMethodFactory as well as how to supply a custom Validator so that payloads annotated with Validated are first validated against a custom Validator.<br>   {<br>       @code<br>       @Configuration<br>       @EnableKafka<br>       public class AppConfig implements KafkaListenerConfigurer {<br>           @Override<br>           public void configureKafkaListeners(KafkaListenerEndpointRegistrar registrar) {<br>               registrar.setEndpointRegistry(myKafkaListenerEndpointRegistry());<br>               registrar.setMessageHandlerMethodFactory(myMessageHandlerMethodFactory);<br>               registrar.setValidator(new MyValidator());<br>           }</p>
<pre><code>    @Bean
    public KafkaListenerEndpointRegistry myKafkaListenerEndpointRegistry() {
        // registry configuration
    }

    @Bean
    public MessageHandlerMethodFactory myMessageHandlerMethodFactory() {
        DefaultMessageHandlerMethodFactory factory = new DefaultMessageHandlerMethodFactory();
        // factory configuration
        return factory;
    }

    @Bean
    public MyService myService() {
        return new MyService();
    }
}</code></pre><p>   }</p>
<p>Implementing KafkaListenerConfigurer also allows for fine-grained control over endpoints registration via the KafkaListenerEndpointRegistrar. For example, the following configures an extra endpoint:<br>   {<br>       @code<br>       @Configuration<br>       @EnableKafka<br>       public class AppConfig implements KafkaListenerConfigurer {<br>           @Override<br>           public void configureKafkaListeners(KafkaListenerEndpointRegistrar registrar) {<br>               SimpleKafkaListenerEndpoint myEndpoint = new SimpleKafkaListenerEndpoint();<br>               // … configure the endpoint<br>               registrar.registerEndpoint(endpoint, anotherKafkaListenerContainerFactory());<br>           }</p>
<pre><code>    @Bean
    public MyService myService() {
        return new MyService();
    }

    @Bean
    public KafkaListenerContainerFactory&lt;?, ?&gt; anotherKafkaListenerContainerFactory() {
        // ...
    }

    // Kafka infrastructure setup
}</code></pre><p>   }</p>
<p>Note that all beans implementing KafkaListenerConfigurer will be detected and invoked in a similar fashion. The example above can be translated in a regular bean definition registered in the context in case you use the XML configuration.<br>See Also:<br>KafkaListener, KafkaListenerAnnotationBeanPostProcessor, org.springframework.kafka.config.KafkaListenerEndpointRegistrar, org.springframework.kafka.config.KafkaListenerEndpointRegistry<br>  spring-kafka-dist.spring-kafka.main</p>
<h1 id="flush"><a href="#flush" class="headerlink" title="flush"></a>flush</h1><p>If you wish to block the sending thread, to await the result, you can invoke the future’s get() method. You may wish to invoke flush() before waiting or, for convenience, the template has a constructor with an autoFlush parameter which will cause the template to flush() on each send. Note, however that flushing will likely significantly reduce performance.</p>
<h2 id="Non-Blocking-Async"><a href="#Non-Blocking-Async" class="headerlink" title="Non Blocking (Async)."></a>Non Blocking (Async).</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">sendToKafka</span><span class="params">(<span class="keyword">final</span> MyOutputData data)</span> </span>&#123;</span><br><span class="line"><span class="keyword">final</span> ProducerRecord&lt;String, String&gt; record = createRecord(data);</span><br><span class="line">ListenableFuture&lt;SendResult&lt;Integer, String&gt;&gt; future = template.send(record); future.addCallback(<span class="keyword">new</span> ListenableFutureCallback&lt;SendResult&lt;Integer, String&gt;&gt;() &#123;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onSuccess</span><span class="params">(SendResult&lt;Integer, String&gt; result)</span> </span>&#123; handleSuccess(data);</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onFailure</span><span class="params">(Throwable ex)</span> </span>&#123; handleFailure(data, record, ex);</span><br><span class="line">&#125;</span><br><span class="line">&#125;); &#125;</span><br></pre></td></tr></table></figure>

<h2 id="Blocking-Sync"><a href="#Blocking-Sync" class="headerlink" title="Blocking (Sync)."></a>Blocking (Sync).</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">sendToKafka</span><span class="params">(<span class="keyword">final</span> MyOutputData data)</span> </span>&#123;</span><br><span class="line"><span class="keyword">final</span> ProducerRecord&lt;String, String&gt; record = createRecord(data);</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">template.send(record).get(<span class="number">10</span>, TimeUnit.SECONDS); handleSuccess(data);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">catch</span> (ExecutionException e) &#123;</span><br><span class="line">        handleFailure(data, record, e.getCause());</span><br><span class="line">    &#125;</span><br><span class="line"><span class="keyword">catch</span> (TimeoutException | InterruptedException e) &#123; handleFailure(data, record, e);</span><br><span class="line">&#125; &#125;</span><br></pre></td></tr></table></figure>

<h2 id="KafkaTransactionManager"><a href="#KafkaTransactionManager" class="headerlink" title="KafkaTransactionManager"></a>KafkaTransactionManager</h2><p>The <code>KafkaTransactionManager</code> is an implementation of Spring Framework’s <code>PlatformTransactionManager</code>;</p>

        </div>
    

</div>
            
                
<div class="post">

    <div class="post-header index">
        <h1 class="title">
            <a href="/2019-02-26-TLS-SSL-HTTPS/">
                SSL certificates
            </a>
        </h1>
        <div class="post-info">
            
                <span class="date">2019-08-15</span>
            
            
            
        </div>
    </div>

    
        <div class="content">
            <h1 id="What’s-TLS"><a href="#What’s-TLS" class="headerlink" title="What’s TLS"></a>What’s TLS</h1><p>TLS (Transport Layer Security) and its predecessor, SSL (Secure Sockets Layer), are security protocols designed to secure the communication between a server and a client, for example, a web server and a browser. Both protocols are frequently referred to as SSL.</p>
<p>A TLS/SSL certificate (simply called SSL certificate) is required to enable SSL/TLS on your site and serve your website using the secure HTTPS protocol.</p>
<p>We offer different types of domain-validated SSL certificates signed by globally recognized certificate authorities.</p>
<h1 id="CA"><a href="#CA" class="headerlink" title="CA"></a>CA</h1><p>A Certificate Authority (CA) (or Certification Authority) is an entity that issues digital certificates.</p>
<p>The digital certificate <em>certifies the ownership</em> of a public key by the named subject of the certificate. This allows others (relying parties) to rely upon signatures or assertions made by the private key that corresponds to the public key that is certified.</p>
<h1 id="Root-certificate"><a href="#Root-certificate" class="headerlink" title="Root certificate"></a>Root certificate</h1><p>In the SSL ecosystem, anyone can generate a signing key and sign a new certificate with that signature. However, that certificate is not considered valid unless it has been directly or indirectly signed by a trusted CA.</p>
<p>A trusted certificate authority is an entity that has been entitled to verify that someone is effectively who it declares to be. In order for this model to work, all the participants on the game must agree on a set of CA which they trust. All operating systems and most of web browsers ship with a set of trusted CAs.</p>
<p>The SSL ecosystem is based on a * model of trust relationship<em>, also called *</em> “chain of trust” **. When a device validates a certificate, it compares the certificate issuer with the list of trusted CAs. If a match is not found, the client will then check to see if the certificate of the issuing CA was issued by a trusted CA, and so on until the end of the certificate chain. The top of the chain, the root certificate, must be issued by a trusted Certificate Authority.</p>
<h2 id="Tips"><a href="#Tips" class="headerlink" title="Tips"></a>Tips</h2><p>The root certificate is generally embedded in your connected device. In the case of web browsers, root certificates are packaged with the browser software.</p>
<h3 id="To-install-the-Intermediate-SSL-certificates"><a href="#To-install-the-Intermediate-SSL-certificates" class="headerlink" title="To  install the Intermediate SSL certificates?"></a>To  install the Intermediate SSL certificates?</h3><p>The procedure to install the Intermediate SSL certificates depends on the web server and the environment where you install the certificate.</p>
<p>For instance, Apache requires you to bundle the intermediate SSL certificates and assign the location of the bundle to the SSLCertificateChainFile configuration. Conversely, NGINX requires you to package the intermediate SSL certificates in a single bundle with the end-user certificate.</p>
<h1 id="SSL-certificate-chain"><a href="#SSL-certificate-chain" class="headerlink" title="SSL certificate chain"></a>SSL certificate chain</h1><p>There are two types of certificate authorities (CAs): root CAs and intermediate CAs. In order for an SSL certificate to be trusted, that certificate must have been issued by a CA that is included in the trusted store of the device that is connecting.</p>
<p>In this model of trust relationships, a CA is a trusted third party that is trusted by both the subject (owner) of the certificate and the party relying upon the certificate.</p>
<p>In the context of a website, when we use the term digital certificate we often refer to SSL certificates. The CA is the authority responsible for issuing SSL certificates publicly trusted by web browsers.</p>
<p>Anyone can issue SSL certificates, but those certificates would not be trusted automatically by web browsers. Certificates such as these are called self-signed. The CA has the responsibility to validate the entity behind an SSL certificate request and, upon successful validation, the ability to issue publicly trusted SSL certificates that will be accepted by web browsers. Essentially, the browser vendors rely on CAs to validate the entity behind a web site.</p>
<h1 id="How-SSL-work-in-browser"><a href="#How-SSL-work-in-browser" class="headerlink" title="How SSL work in browser"></a>How SSL work in browser</h1><p>There are 3 essential elements at work in the process described above: a protocol for communications (SSL), credentials for establishing identity (the SSL certificate), and a third party that vouches for the credentials (the certificate authority). </p>
<pre><code>Computers use protocols to allow different systems to work together. Web servers and web browsers rely on the Secure Sockets Layer (SSL) protocol to enable encrypted communications. The browser’s request that the server identify itself is a function of the SSL protocol.
Credentials for establishing identity are common to our everyday lives: a driver’s license, a passport, a company badge. An SSL certificate is a type of digital certificate that serves as a credential in the online world. Each SSL certificate uniquely identifies a specific domain (such as thawte.com) and a web server.
Our trust of a credential depends on our confidence in the organization that issued it. Certificate authorities have a variety of methods to verify information provided by individuals or organizations. Established certificate authorities, such as Thawte, are well known and trusted by browser vendors. Browsers extend that trust to digital certificates that are verified by the certificate authority.</code></pre><h2 id="PKI"><a href="#PKI" class="headerlink" title="PKI"></a>PKI</h2><p>You are correct that SSL uses an asymmetric key pair. One public and one private key is generated which also known as public key infrastructure (PKI). The public key is what is distributed to the world, and is used to encrypt the data. Only the private key can actually decrypt the data though.</p>
<blockquote>
<p>Say we both go to walmart.com and buy stuff. Each of us get a copy of Walmart’s public key to sign our transaction with. Once the transaction is signed by Walmart’s public key, only Walmart’s private key can decrypt the transaction. If I use my copy of Walmart’s public key, it will not decrypt your transaction. Walmart must keep their private key very private and secure, else anyone who gets it can decrypt transactions to Walmart. This is why the DigiNotar breach was such a big deal</p>
</blockquote>
<h1 id="A-sample-of-how-browser-get-SSL-certificate"><a href="#A-sample-of-how-browser-get-SSL-certificate" class="headerlink" title="A sample of how browser get SSL certificate"></a>A sample of how browser get SSL certificate</h1><p>If I get an SSL certificate from a well-known provider, what does that prove about my site and how?</p>
<p>Here’s what I know:</p>
<pre><code>Assume Alice and Bob both have public and private keys
If Alice encrypts something with Bob&apos;s public key, she ensures that only Bob can decrypt it (using his private key)
If Alice encrypts something with her own private key, anyone can decrypt it (using her public key), but they will know that it was encrypted by her
Therefore, if Alice encrypts a message first with her own private key, then with Bob&apos;s public key, she will ensure that only Bob can decrypt it and that Bob will know the message is from her.</code></pre><p>Regarding certificates, here’s what I think happens (updated):</p>
<pre><code>I generate a request for a certificate. In that request, I put my public key and a bunch of information about myself.
The certificate issuer (in theory) checks me out to make sure it knows who I am: talks to me in person, sees my driver&apos;s license, retina scan, or whatever.
If they&apos;re satisfied, the certificate issuer then encrypts my request with their private key. Anyone who decrypts it with their public key knows that they vouch for the information it contains: they agree that the public key is mine and that the information stated is true about me. This encrypted endorsement is the certificate that they issue to me.
When you connect to my site via https, I send you the certificate.
Your browser already knows the issuer&apos;s public key because your browser came installed with that information.
Your browser uses the issuer&apos;s public key to decrypt what I sent you. The fact that the issuer&apos;s public key works to decrypt it proves that the issuer&apos;s private key was used to encrypt it, and therefore, that the issuer really did create this certificate.
Inside the decrypted information is my public key, which you now know has been vouched for. You use that to encrypt some data to send to me.</code></pre><p>Your key theory: basically right, but authentication is usually done by encrypting a cryptographically secure hash of the data rather than the data itself.</p>
<p>A CA’s signature on an SSL certificate should indicate that the CA has done a certain amount of diligence to ensure that the credentials on the certificate match the owner. That diligence varies, but the ultimate point is that they’re saying that the certificate they signed belongs to the entity named on it.</p>
<p>See <a href="http://en.wikipedia.org/wiki/Digital_signature#Definition" target="_blank" rel="noopener">http://en.wikipedia.org/wiki/Digital_signature#Definition</a></p>
<p>A public key certificate is the signed combination between a public key, identifiers, and possibly other attributes. Those who sign this document effectively assert the authenticity of the binding between the public key and the identifier and these attributes, in the same way as a passport issuing authority asserts the binding between the picture and the name in a passport, as various other pieces of information (nationality, date of birth, …).</p>
<pre><code>The private key is used for signing and deciphering/decrypting.
The public key is used for verifying signatures and enciphering/encrypting.</code></pre><p>public key cryptography: A class of cryptographic techniques employing two-key ciphers. Messages encrypted with the public key can only be decrypted with the associated private key. Conversely, messages signed with the private key can be verified with the public key.</p>
<p>It should be pointed out, along with all the other answers, that your private key is not always just one key that is used for both decrypting and signing messages. These should be two separate keys. This would create 4 keys for each person:</p>
<p>Public Encryption Key - Used to encrypt data to send to me.</p>
<p>Private Decryption Key - Used to decrypt messages that were encrypted using my Public Encryption Key.</p>
<p>Private Signing Key - Used to sign messages that I send to other people.</p>
<p>Public Verify Key - Used to verify that a message was, in fact, signed by me.</p>
<p><a href="https://en.wikipedia.org/wiki/Savvis" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Savvis</a></p>
<p>Savvis - Wikipedia</p>
<p>Savvis, formerly SVVS on Nasdaq and formerly known as Savvis Communications Corporation, and, later, Savvis Inc., is a subsidiary of CenturyLink, a company headquartered in Monroe, Louisiana.[1] The company sells managed hosting and colocation services with more than 50 data centers[2] (over 2 million square feet) in North America, Europe, and Asia, automated management and provisioning systems, and information technology consulting. Savvis has approximately 2,500 unique business and government customers.[3][4] </p>
<p>The file extensions .CRT and .CER are interchangeable.  If your server requires that you use the .CER file extension, you can change the extension by following the steps below:</p>
<pre><code>Double-click on the yourwebsite.crt file to open it into the certificate display.
Select the Details tab, then select the Copy to file button.
Hit Next on the Certificate Wizard.
Select Base-64 encoded X.509 (.CER), then Next.
Select Browse (to locate a destination) and type in the filename yourwebsite.
Hit Save. You now have the file yourwebsite.cer


File extensions for cryptographic certificates aren&apos;t really as standardized as you&apos;d expect. Windows by default treats double-clicking a .crt file as a request to import the certificate into the Windows Root Certificate store, but treats a .cer file as a request just to view the certificate. So, they&apos;re different in that sense, at least, that Windows has some inherent different meaning for what happens when you double click each type of file.</code></pre><p>But the way that Windows handles them when you double-click them is about the only difference between the two. Both extensions just represent that it contains a public certificate. You can rename a file or use one in place of the other in any system or configuration file that I’ve seen. And on non-Windows platforms (and even on Windows), people aren’t particularly careful about which extension they use, and treat them both interchangeably, as there’s no difference between them as long as the contents of the file are correct.</p>
<p>*.pem, *.crt, *.ca-bundle, *.cer, *.p7b, *.p7s files contain one or more X.509 digital certificate files that use base64 (ASCII) encoding. </p>
<p>.DER = The DER extension is used for binary DER encoded certificates. These files may also bear the CER or the CRT extension.   Proper English usage would be “I have a DER encoded certificate” not “I have a DER certificate”.</p>
<p>.PEM = The PEM extension is used for different types of X.509v3 files which contain ASCII (Base64) armored data prefixed with a “—– BEGIN …” line.</p>
<p>.CRT = The CRT extension is used for certificates. The certificates may be encoded as binary DER or as ASCII PEM. The CER and CRT extensions are nearly synonymous.  Most common among *nix systems</p>
<p>CER = alternate form of .crt (Microsoft Convention) You can use MS to convert .crt to .cer (.both DER encoded .cer, or base64[PEM] encoded .cer)  The .cer file extension is also recognized by IE as a command to run a MS cryptoAPI command (specifically rundll32.exe cryptext.dll,CryptExtOpenCER) which displays a dialogue for importing and/or viewing certificate contents.</p>
<p>.KEY = The KEY extension is used both for public and private PKCS#8 keys. The keys may be encoded as binary DER or as ASCII PEM.</p>
<p>The only time CRT and CER can safely be interchanged is when the encoding type can be identical.  (ie  PEM encoded CRT = PEM encoded CER)</p>
<p>What is the SSL Certificate Chain?</p>
<p>There are two types of certificate authorities (CAs): root CAs and intermediate CAs. In order for an SSL certificate to be trusted, that certificate must have been issued by a CA that is included in the trusted store of the device that is connecting.</p>
<p>Good. I see you want to access this particular page.  I need to send the page to you in a secure way. If I<br>encrypt it using my public key, you won’t be able to decrypt it because you don’t have my private key. And since you don’t have any public key of your own that I can use to encrypt the page for you here’s what I propose<br>Since you can send me encrypted messages that only me can read (you have my public key), send me an encrypted message with an encryption key in it. Just make up a random encryption key that we’ll both use to encrypt and decrypt the messages between us during this session . </p>
<p>A simple symmetric key is enought. We’ll use the same key to encrypt and decrypt the messages.</p>
<ul>
<li>So there’s no way that anybody with your public<br>key can trick others to believe that he is you ? </li>
<li>Nope. That’s the beauty of the assymetric encryption. </li>
</ul>
<p>When you send the public key to the victim’s contain your public key + a certificate that this public key belongs to you. If you are a website, then the certificate will contain the domain name of the website. Basically, a certificate says something like:  the following public key “XYZ123” belongs to example.com. </p>
<p>that’s why we have “Certificate Authorities” like Verisign, Digicert or even Symantec. It is believed that these companies have the necessary trustworthiness to deliver certificates to different •entities.<br>Think of a CA like a registrar for public keys. Just like registrars assert that a domain name belongs to a certain person or company, CAS assert that a public key belongs to a certain domain name (or IP address) . </p>
<p>The certificate will contain the CA that delivered it, but you don’t even have to check with them because the certificate is signed by them. That signature alone is enough proof that the certificate comes from them. </p>
<p>A signature is simply a small message that is encrypted with their private key. Since private keys are asymetric, that means that only the associated public key can decrypt it.</p>
<p>Asymmetric encryption works in both way. public -&gt; private and private -&gt; public.<br>What the public key encrypts only the private key can decrypt, and what the private key can encrypt only the public key can decrypt.</p>
<p>for PKI, we’re not looking for secrecy here, we only want to prove that we’ re the real authors of the message. Suppose I send you the message “HELLO WORLD”, encrypted with my private key. The encrypted message would be, for example, “XYZ1234”. So you receive “XYZ1234” . If I give you my public key, you would be able to decrypt “XYZ1234” into “HELLO WORLD” . And by doing so, you would have proof that that message was sent by me, because the public key you used decrypts messages that were encrypted by my private key only. And since I am the only person in the universe who has that private key, that proves that I am the author of that message. </p>
<p>Really nice. So I don’t have to contact the CA to check the validity of the certificate, all I have to do is use their public key to decrypt the signature that’s in it. If it’s the same as err, wait, what should I compare the decrypted signature to again ? </p>
<p>You have to find the same hash as the one you have calculated. They are sending a small hash of the whole certificate. So what you have to do is to calculate the hash of the certificate yourself, then compare it to the hash you get when you decrypt the signature. If the two are the same that means two things </p>
<ol>
<li>The CA’s public key worked, so the signature was encrypted by the associated private key, which means the certificate was really issued by the CA. </li>
<li>Since the hash is the same, it also means that you are seeing the exact same certificate that the CA delivered to the website you are visiting. The information contained inside hasn’t been tampered with. </li>
</ol>
<p>That’s really good. So, let me recap one more time . </p>
<ol>
<li>I contact you for an HTTPS page. </li>
<li>You send me an SSL certificate that contains your public key and a signature from the CA that delivered </li>
<li>I make sure the certificate is valid by using the CA’s public key to decrypt the signature. In parallel, I also calculate the hash of the certificate. </li>
</ol>
<p>If my hash and the one I got from decrypting the signature are equal, that means that the certificate was really issued by the CA and that I can be sure that the public key you sent me is really yours. </p>
<p>Because you implicitly trust the CA.</p>
<p>Let’s continue: </p>
<ol start="4">
<li>I generate a random key that we’ll both use as a symmetric key to encrypt and decrypt the messages we’ll be sending each other. </li>
<li>I encrypt this symmetric key with your public key and send it to you. </li>
<li>You decrypt my message with your private key and find my secret key. </li>
<li>Every request or response between us will be encrypted with this shared secret symmetric key. </li>
</ol>
<h2 id="CN"><a href="#CN" class="headerlink" title="CN"></a>CN</h2><p>The Common Name (AKA CN) represents the server name protected by the SSL certificate.</p>
<p>The certificate is valid only if the request hostname matches the certificate common name.</p>
<p>To check the status, such as </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo openssl x509 -noout -<span class="keyword">in</span> xxx.com.cer -text</span><br></pre></td></tr></table></figure>

<p> Subject: C=UK, ST=London, L=London, O=AAA Bank, OU=Product and Markets, CN=*.xxxtest.com<br>        Subject Public Key Info:</p>
<h3 id="commonName-format"><a href="#commonName-format" class="headerlink" title="commonName format"></a>commonName format</h3><p>The common name is not a URL. It doesn’t include any protocol (e.g. http:// or https://), port number, or pathname. For instance, <a href="https://example.com" target="_blank" rel="noopener">https://example.com</a> or example.com/path are incorrect. In both cases, the common name should be example.com</p>
<h4 id="Common-Name-vs-Subject-Alternative-Name"><a href="#Common-Name-vs-Subject-Alternative-Name" class="headerlink" title="Common Name vs Subject Alternative Name"></a>Common Name vs Subject Alternative Name</h4><p>The common name can only contain up to one entry: either a wildcard or non-wildcard name. It’s not possible to specify a list of names covered by an SSL certificate in the common name field.</p>
<p>The Subject Alternative Name extension (also called Subject Alternate Name or SAN) was introduced to solve this limitation. The SAN allows issuance of multi-name SSL certificates.</p>
<h1 id="SHA-2-SSL-Certificates"><a href="#SHA-2-SSL-Certificates" class="headerlink" title="SHA-2 SSL Certificates"></a>SHA-2 SSL Certificates</h1><p>Almost all certificates are currently signed with the SHA-2 hash algorithm.</p>
<p>This article provides a simple overview of the SHA-1 to SHA-2 transition plans, as well additional informations on the SHA-2 hash algorithm and SSL certificates purchased with DNSimple previous than September 2014.</p>
<p>The SHA family of hashing algorithms were developed by the National Institute of Standards and Technology (NIST) and are used by certificate authorities (CAs) when digitally signing issued certificates.</p>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul>
<li><a href="https://support.dnsimple.com/articles/what-is-ssl-certificate-chain/" target="_blank" rel="noopener">https://support.dnsimple.com/articles/what-is-ssl-certificate-chain/</a></li>
<li><a href="https://www.thawte.com/resources/getting-started/how-ssl-works/" target="_blank" rel="noopener">https://www.thawte.com/resources/getting-started/how-ssl-works/</a></li>
</ul>

        </div>
    

</div>
            
                
<div class="post">

    <div class="post-header index">
        <h1 class="title">
            <a href="/2019-07-26-Terraform/">
                Terraform
            </a>
        </h1>
        <div class="post-info">
            
                <span class="date">2019-07-26</span>
            
            
            
        </div>
    </div>

    
        <div class="content">
            <h1 id="Why-Terraform"><a href="#Why-Terraform" class="headerlink" title="Why Terraform"></a>Why Terraform</h1><p>Software isn’t done when the code is working on your computer. It’s not done when the tests pass. And it’s not done when someone gives you a “ship it” on a code review. Software isn’t done until you deliver it to the user.</p>

        </div>
    

</div>
            
                
<div class="post">

    <div class="post-header index">
        <h1 class="title">
            <a href="/2016-06-10-Linux-Tips/">
                Linux Tips
            </a>
        </h1>
        <div class="post-info">
            
                <span class="date">2019-07-07</span>
            
            
            
        </div>
    </div>

    
        <div class="content">
            <h1 id="Get-permission-denied-error-when-sudo-su-or-hyphen-in-sudo-command"><a href="#Get-permission-denied-error-when-sudo-su-or-hyphen-in-sudo-command" class="headerlink" title="Get permission denied error when sudo su (or hyphen in sudo command)"></a>Get permission denied error when sudo su (or hyphen in sudo command)</h1><p>bash: /home/YOURNAME/.bashrc: Permission denied<br>That’s because you didn’t add “-“ hyphen in your sudo command.</p>
<p>The difference between “-“ and “no hyphen” is that the latter keeps your existing environment (variables, etc); the former creates a new environment (with the settings of the actual user, not your own).</p>
<p>The hyphen has two effects:</p>
<p>1) switches from the current directory to the home directory of the new user (e.g., to /root in the case of the root user) by logging in as that user</p>
<p>2) changes the environmental variables to those of the new user as dictated by their ~/.bashrc. That is, if the first argument to su is a hyphen, the current directory and environment will be changed to what would be expected if the new user had actually logged on to a new session (rather than just taking over an existing session).</p>
<h1 id="To-delete-lines-in-files-contain-pattern"><a href="#To-delete-lines-in-files-contain-pattern" class="headerlink" title="To delete lines in files contain pattern"></a>To delete lines in files contain pattern</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed -i <span class="string">'/.*167\=OPT.*/d'</span> testdata.txt</span><br></pre></td></tr></table></figure>

<h1 id="to-select-only-only-one-element-value-of-XML-file"><a href="#to-select-only-only-one-element-value-of-XML-file" class="headerlink" title="to select only only one element value of XML file :"></a>to select only only one element value of XML file :</h1><p>grep -oPm1 “(?&lt;=<theuniqid>)[^&lt;]+” </theuniqid></p>
<h1 id="To-check-Linux-release-name"><a href="#To-check-Linux-release-name" class="headerlink" title="To check Linux release name"></a>To check Linux release name</h1><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /etc/os-release</span><br></pre></td></tr></table></figure>

<h1 id="How-to-check-whether-your-linux-is-32bit-or-64-bit"><a href="#How-to-check-whether-your-linux-is-32bit-or-64-bit" class="headerlink" title="How to check whether your linux is 32bit or 64 bit"></a>How to check whether your linux is 32bit or 64 bit</h1><p>To run “arch” command,  this is similar to “uname -m” , it prints to the screen whether your system is running 32-bit (“i686”) or 64-bit (“x86_64”).</p>
<h1 id="convert-line-ending-to-unix-sometimes-git-submit-is-dos-format"><a href="#convert-line-ending-to-unix-sometimes-git-submit-is-dos-format" class="headerlink" title="convert line ending to unix (sometimes git submit is dos format)"></a>convert line ending to unix (sometimes git submit is dos format)</h1><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dos2unix the_script_file_name</span><br></pre></td></tr></table></figure>

<h1 id="To-check-redhat-Linux-version"><a href="#To-check-redhat-Linux-version" class="headerlink" title="To check redhat Linux version"></a>To check redhat Linux version</h1><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat  /etc/redhat-release</span><br></pre></td></tr></table></figure>

<h1 id="To-list-all-users-in-linux"><a href="#To-list-all-users-in-linux" class="headerlink" title="To list all users in linux"></a>To list all users in linux</h1><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /etc/passwd</span><br></pre></td></tr></table></figure>

<h1 id="Show-IP-address-in-Linux"><a href="#Show-IP-address-in-Linux" class="headerlink" title="Show IP address in Linux"></a>Show IP address in Linux</h1><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$ifconfig</span></span><br><span class="line">eth0      Link encap:Ethernet  HWaddr 00:50:56:9B:19:81</span><br><span class="line">          inet addr:133.14.16.5  Bcast:133.14.16.255  Mask:255.255.255.0</span><br></pre></td></tr></table></figure>

<h1 id="Check-system-resource"><a href="#Check-system-resource" class="headerlink" title="Check system resource"></a>Check system resource</h1><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">execute `cat /proc/cpuinfo` and `free -m` to gain information about the server’s CPU and memory.</span><br></pre></td></tr></table></figure>

<h1 id="chmod-command"><a href="#chmod-command" class="headerlink" title="chmod command"></a>chmod command</h1><p>From one to four octal digits<br>Any omitted digits are assumed to be leading zeros. </p>
<p>The first digit = selects attributes for the set user ID (4) and set group ID (2) and save text image (1)S<br>The second digit = permissions for the user who owns the file: read (4), write (2), and execute (1)<br>The third digit = permissions for other users in the file’s group: read (4), write (2), and execute (1)<br>The fourth digit = permissions for other users NOT in the file’s group: read (4), write (2), and execute (1)</p>
<p>The octal (0-7) value is calculated by adding up the values for each digit<br>User (rwx) = 4+2+1 = 7<br>Group(rx) = 4+1 = 5<br>World (rx) = 4+1 = 5<br>chmode mode = 0755</p>
<p>Examples</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">chmod 400 file - Read by owner</span><br><span class="line">chmod 040 file - Read by group</span><br><span class="line">chmod 004 file - Read by world </span><br><span class="line"></span><br><span class="line">chmod 200 file - Write by owner</span><br><span class="line">chmod 020 file - Write by group</span><br><span class="line">chmod 002 file - Write by world</span><br></pre></td></tr></table></figure>

<h1 id="top"><a href="#top" class="headerlink" title="top"></a>top</h1><ul>
<li>enter u, then user id to show only user process</li>
<li>Z: global color scheme, e.g. red or green</li>
<li>B: global bold for all</li>
<li>z: show color, then b to hightlight, and x highlight sort fidl, y highlight running tasks</li>
<li>#3: show only 3 threads</li>
<li>c: show command line</li>
<li>F: sort, e.g. Fk sort by CPU%</li>
<li>R: reverse order</li>
</ul>
<h1 id="Sample-config-files"><a href="#Sample-config-files" class="headerlink" title="Sample config files"></a>Sample config files</h1><h2 id="vimrc"><a href="#vimrc" class="headerlink" title=".vimrc"></a>.vimrc</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="built_in">set</span> number</span><br><span class="line"><span class="built_in">set</span> incsearch</span><br><span class="line"><span class="built_in">set</span> hlsearch</span><br><span class="line">syntax on</span><br><span class="line">colorscheme desert</span><br></pre></td></tr></table></figure>

<h2 id="screenrc"><a href="#screenrc" class="headerlink" title="==== screenrc  ="></a>==== screenrc  =</h2><p><a href="https://gist.githubusercontent.com/ChrisWills/1337178/raw/8275b66c3ea86a562cdaa16f1cc6d9931d521e1b/.screenrc-main-example" target="_blank" rel="noopener">https://gist.githubusercontent.com/ChrisWills/1337178/raw/8275b66c3ea86a562cdaa16f1cc6d9931d521e1b/.screenrc-main-example</a></p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GNU Screen - main configuration file </span></span><br><span class="line"><span class="comment"># All other .screenrc files will source this file to inherit settings.</span></span><br><span class="line"><span class="comment"># Author: Christian Wills - cwills.sys@gmail.com</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Allow bold colors - necessary for some reason</span></span><br><span class="line">attrcolor b <span class="string">".I"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Tell screen how to set colors. AB = background, AF=foreground</span></span><br><span class="line">termcapinfo xterm <span class="string">'Co#256:AB=\E[48;5;%dm:AF=\E[38;5;%dm'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Enables use of shift-PgUp and shift-PgDn</span></span><br><span class="line">termcapinfo xterm|xterms|xs|rxvt ti@:te@</span><br><span class="line"></span><br><span class="line"><span class="comment"># Erase background with current bg color</span></span><br><span class="line">defbce <span class="string">"on"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Enable 256 color term</span></span><br><span class="line">term xterm-256color</span><br><span class="line"></span><br><span class="line"><span class="comment"># Cache 30000 lines for scroll back</span></span><br><span class="line">defscrollback 30000</span><br><span class="line"></span><br><span class="line"><span class="comment"># New mail notification</span></span><br><span class="line"><span class="comment"># backtick 101 30 15 $HOME/bin/mailstatus.sh</span></span><br><span class="line"></span><br><span class="line">hardstatus alwayslastline </span><br><span class="line"><span class="comment"># Very nice tabbed colored hardstatus line</span></span><br><span class="line">hardstatus string <span class="string">'%&#123;= Kd&#125; %&#123;= Kd&#125;%-w%&#123;= Kr&#125;[%&#123;= KW&#125;%n %t%&#123;= Kr&#125;]%&#123;= Kd&#125;%+w %-= %&#123;KG&#125; %H%&#123;KW&#125;|%&#123;KY&#125;%101`%&#123;KW&#125;|%D %M %d %Y%&#123;= Kc&#125; %C%A%&#123;-&#125;'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># change command character from ctrl-a to ctrl-b (emacs users may want this)</span></span><br><span class="line"><span class="comment">#escape ^Bb</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Hide hardstatus: ctrl-a f </span></span><br><span class="line"><span class="built_in">bind</span> f <span class="built_in">eval</span> <span class="string">"hardstatus ignore"</span></span><br><span class="line"><span class="comment"># Show hardstatus: ctrl-a F</span></span><br><span class="line"><span class="built_in">bind</span> F <span class="built_in">eval</span> <span class="string">"hardstatus alwayslastline"</span></span><br></pre></td></tr></table></figure>

<h2 id="bashrc"><a href="#bashrc" class="headerlink" title="====================.bashrc =========="></a>====================.bashrc ==========</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># .bashrc</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Source global definitions</span></span><br><span class="line"><span class="keyword">if</span> [ -f /etc/bashrc ]; <span class="keyword">then</span></span><br><span class="line">        . /etc/bashrc</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"><span class="comment"># source ./prompt</span></span><br><span class="line"><span class="built_in">export</span> PS1=<span class="string">''</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> PS1=<span class="string">'[\e[104mLight blue \u \A\]$ '</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> PS1=<span class="string">"\[\e[32m\]\u@\h \d \t \w \[\e[m\] \\$"</span></span><br><span class="line"></span><br><span class="line">\e[104mLight blue</span><br><span class="line"></span><br><span class="line"><span class="comment"># Welcome message</span></span><br><span class="line"><span class="built_in">echo</span> -ne <span class="string">"Good Morning ! It's "</span>; date <span class="string">'+%A, %B %-d %Y'</span></span><br><span class="line"><span class="built_in">echo</span> -e <span class="string">"And now your moment of Zen:"</span>; fortune</span><br><span class="line"><span class="built_in">echo</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"Hardware Information:"</span></span><br><span class="line">sensors  <span class="comment"># Needs: 'sudo apt-get install lm-sensors'</span></span><br><span class="line">uptime   <span class="comment"># Needs: 'sudo apt-get install lsscsi'</span></span><br><span class="line">free -m</span><br><span class="line"></span><br><span class="line"><span class="comment"># User specific aliases and functions</span></span><br><span class="line"></span><br><span class="line">PS1=<span class="string">'\[`[ $? = 0 ] &amp;&amp; X=2 || X=1; tput setaf $X`\]\h\[`tput sgr0`\]:$PWD\n\$ '</span></span><br><span class="line"></span><br><span class="line">============vimrc =========</span><br><span class="line"><span class="built_in">set</span> number</span><br><span class="line"><span class="built_in">set</span> incsearch</span><br><span class="line"><span class="built_in">set</span> hlsearch</span><br><span class="line"></span><br><span class="line">~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span><br><span class="line"></span><br><span class="line">grep -v <span class="string">"unwanted_word"</span> file | grep XXXXXXXX</span><br><span class="line"></span><br><span class="line">// find <span class="built_in">command</span> exclude “permission denied”</span><br><span class="line">$ find . -name <span class="string">"java"</span> 2&gt;/dev/null</span><br></pre></td></tr></table></figure>

<h1 id="Passwordless-connection-in-putty"><a href="#Passwordless-connection-in-putty" class="headerlink" title="Passwordless connection in putty"></a>Passwordless connection in putty</h1><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1. Generate Public &amp; private key pair by keygen</span><br><span class="line">2. Log into Linux, nano .ssh/authorized_keys and paste the public key</span><br><span class="line">3. Save the private key <span class="keyword">in</span> putty and load it <span class="keyword">in</span> Putty session</span><br></pre></td></tr></table></figure>

<h1 id="find-java-files-older-than-3-days"><a href="#find-java-files-older-than-3-days" class="headerlink" title="find java files older than 3 days"></a>find java files older than 3 days</h1><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">find . -name <span class="string">"*.java"</span> -atime -3d</span><br></pre></td></tr></table></figure>

<h1 id="Remove-capitalization"><a href="#Remove-capitalization" class="headerlink" title="Remove capitalization"></a>Remove capitalization</h1><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed -ie <span class="string">'s/Return /return /g'</span> ReverseString.java</span><br></pre></td></tr></table></figure>

<h1 id="replace-string-in-files"><a href="#replace-string-in-files" class="headerlink" title="replace string in files"></a>replace string in files</h1><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#grep -r "pack.*me" .</span></span><br><span class="line">sed -ie <span class="string">'s/package.*me.*;/package com.todzhang;/g'</span> *.java</span><br><span class="line">sed -ie <span class="string">'s/package me.todzhang;/package com.todzhang;/g'</span> ~/dev/git/algo/algoWS/src/main/java/com/todzhang/*.java</span><br></pre></td></tr></table></figure>

<h1 id="create-directory-hierarchy-via-path"><a href="#create-directory-hierarchy-via-path" class="headerlink" title="create directory hierarchy via path"></a>create directory hierarchy via path</h1><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p ~/abc/def/egg</span><br></pre></td></tr></table></figure>

<p><code>-p</code> means create intermediary folders, if not exist. Those intermediary folders with permission 777</p>
<h1 id="lsof-to-locate-whether-who-allocated-port-8080"><a href="#lsof-to-locate-whether-who-allocated-port-8080" class="headerlink" title="lsof to locate whether/who allocated port 8080"></a>lsof to locate whether/who allocated port 8080</h1><p><code>lsof</code> means list open files.</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lsof -n -P -i | grep 8080</span><br></pre></td></tr></table></figure>

<h1 id="To-get-rid-of-‘’"><a href="#To-get-rid-of-‘’" class="headerlink" title="To get rid of ‘’"></a>To get rid of ‘’</h1><p>Sometimes got “403 Forbidden” error when trying to downalod file via wget, e.g.</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ wget http://www.xmind.net/xmind/downloads/xmind-7.5-update1-macosx.dmg</span><br><span class="line">--2016-09-09 23:27:29--  http://www.xmind.net/xmind/downloads/xmind-7.5-update1-macosx.dmg</span><br><span class="line">Resolving www.xmind.net... 23.23.188.223</span><br><span class="line">Connecting to www.xmind.net|23.23.188.223|:80... connected.</span><br><span class="line">HTTP request sent, awaiting response... 403 Forbidden</span><br><span class="line">2016-09-09 23:27:29 ERROR 403: Forbidden.</span><br></pre></td></tr></table></figure>

<p>To solve this problem, using following syntax, adding <code>-U xx</code></p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget -U <span class="string">'Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.8.1.6) Gecko/20070802 SeaMonkey/1.1.4'</span> http://www.xmind.net/xmind/downloads/xmind-7.5-update1-macosx.dmg</span><br></pre></td></tr></table></figure>

<h1 id="case-insensitive-ls-command-in-bash"><a href="#case-insensitive-ls-command-in-bash" class="headerlink" title="case insensitive ls command in bash"></a>case insensitive ls command in bash</h1><p>Update .bashrc or current active window</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">shopt</span> -s nocaseglob</span><br></pre></td></tr></table></figure>

<h1 id="one-line-command-to-download-and-extract-files"><a href="#one-line-command-to-download-and-extract-files" class="headerlink" title="one line command to download and extract files"></a>one line command to download and extract files</h1><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$cd</span> /tmp;curl https://www.kernel.org/pub/linux/utils/util-linux/v2.24/util-linux-2.24.tar.gz	| tar -zxf-;<span class="built_in">cd</span>	util-linux-2.24;</span><br></pre></td></tr></table></figure>

<h1 id="Redirect-request-for-HTTP-3xxx-code"><a href="#Redirect-request-for-HTTP-3xxx-code" class="headerlink" title="Redirect request for HTTP 3xxx code"></a>Redirect request for HTTP 3xxx code</h1><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -LSso ~/.vim/<span class="built_in">autoload</span>/pathogen.vim https://tpo.pe/pathogen.vim</span><br></pre></td></tr></table></figure>

<p><code>-L</code> means redirect upon HTTP code 3xxx<br><code>-Ss</code> work together to make the curl show errors if there are<br><code>-o</code> means output to a specified file, rather than stdout</p>
<h1 id="search-files-contains-keyword"><a href="#search-files-contains-keyword" class="headerlink" title="search files contains keyword"></a>search files contains keyword</h1><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grep -ri <span class="string">'architect'</span> . | awk -F <span class="string">':'</span> <span class="string">'&#123;print $1&#125;'</span></span><br></pre></td></tr></table></figure>

<h1 id="Show-Linux-kernel-and-name"><a href="#Show-Linux-kernel-and-name" class="headerlink" title="Show Linux kernel and name"></a>Show Linux kernel and name</h1><h2 id="lsb-release"><a href="#lsb-release" class="headerlink" title="lsb_release"></a>lsb_release</h2><p><code>lsb</code> means Linux Standard Base , <code>-a</code> means print all information</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lsb_release -a -u</span><br></pre></td></tr></table></figure>

<p>will output</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">phray@phray-VirtualBox ~ $ lsb_release -a -u</span><br><span class="line">No LSB modules are available.</span><br><span class="line">Distributor ID:	Ubuntu</span><br><span class="line">Description:	Ubuntu 14.04 LTS</span><br><span class="line">Release:	14.04</span><br><span class="line">Codename:	trusty</span><br></pre></td></tr></table></figure>

<h2 id="etc-os-release"><a href="#etc-os-release" class="headerlink" title="/etc/os-release"></a>/etc/os-release</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /etc/os-release</span><br></pre></td></tr></table></figure>

<p>will output Following</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">phray@phray-VirtualBox ~ $ cat /etc/os-release</span><br><span class="line">NAME=<span class="string">"Ubuntu"</span></span><br><span class="line">VERSION=<span class="string">"14.04.2 LTS, Trusty Tahr"</span></span><br><span class="line">ID=ubuntu</span><br><span class="line">ID_LIKE=debian</span><br><span class="line">PRETTY_NAME=<span class="string">"Ubuntu 14.04.2 LTS"</span></span><br><span class="line">VERSION_ID=<span class="string">"14.04"</span></span><br><span class="line">HOME_URL=<span class="string">"http://www.ubuntu.com/"</span></span><br><span class="line">SUPPORT_URL=<span class="string">"http://help.ubuntu.com/"</span></span><br><span class="line">BUG_REPORT_URL=<span class="string">"http://bugs.launchpad.net/ubuntu/"</span></span><br></pre></td></tr></table></figure>

<p>Following is the command found in docker.sh</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lsb_dist=$(lsb_release -a -u 2&gt;&amp;1 | tr <span class="string">'[:upper:]'</span> <span class="string">'[:lower:]'</span> | grep -E <span class="string">'id'</span> | cut -d <span class="string">':'</span> -f 2 | tr -d <span class="string">'[[:space:]]'</span>)</span><br></pre></td></tr></table></figure>

<p>show the 2nd column</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lsb_release --Codename | cut -f2</span><br></pre></td></tr></table></figure>

<h2 id="gpsswd"><a href="#gpsswd" class="headerlink" title="gpsswd"></a>gpsswd</h2><p>DESCRIPTION<br>gpasswd is used to administer the /etc/group file (and /etc/gshadow file if compiled with SHADOWGRP defined). Every group can have administrators, members and a password. System administrator can use -A option to define group administrator(s) and -M option to define members and has all rights of group administrators and members.</p>
<p>Notes about group passwords<br>Group passwords are an inherent security problem since more than one person is permitted to know the password. However, groups are a useful tool for permitting co-operation between different users.</p>
<p>OPTIONS<br>Group administrator can add and delete users using -a and -d options respectively. Administrators can use -r option to remove group password. When no password is set only group members can use newgrp to join the group. Option -R disables access via a password to the group through newgrp command (however members will still be able to switch to this group).</p>
<p>gpasswd called by a group administrator with group name only prompts for the group password. If password is set the members can still newgrp(1) without a password, non-members must supply the password.</p>
<p>FILES<br>Tag    Description<br>/etc/group<br>     Group account information.<br>/etc/gshadow<br>     Secure group account information.</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo gpasswd -a USER docker</span><br></pre></td></tr></table></figure>

<h1 id="Compare-files-difference-in-two-folders"><a href="#Compare-files-difference-in-two-folders" class="headerlink" title="Compare files difference in two folders"></a>Compare files difference in two folders</h1><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">diff -rq ~/dev/pa ~/dev/hexo/myblog/<span class="built_in">source</span>/_posts</span><br></pre></td></tr></table></figure>

<p>This used option <code>-r</code> (recursive) and <code>-q</code> quite, means only show differences</p>
<h1 id="To-execut-shell-unix-command-within-vim"><a href="#To-execut-shell-unix-command-within-vim" class="headerlink" title="To execut shell/unix command within vim"></a>To execut shell/unix command within vim</h1><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">:~ls -lt</span><br></pre></td></tr></table></figure>

<h1 id="To-open-find-result-with-sublime"><a href="#To-open-find-result-with-sublime" class="headerlink" title="To open find result with sublime"></a>To open find result with sublime</h1><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">find . -name <span class="string">"*Linux*.md"</span> | xargs sublime </span><br><span class="line">find . -name <span class="string">"*Linux*.md"</span> | xargs sublime ~ <span class="comment"># open in new Sublime window</span></span><br></pre></td></tr></table></figure>

<h1 id="To-vim-vim-edit-directly-on-file-output-by-find-command"><a href="#To-vim-vim-edit-directly-on-file-output-by-find-command" class="headerlink" title="To vim/vim edit directly on file output by find command"></a>To vim/vim edit directly on file output by find command</h1><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">find . -name <span class="string">"*tmux*"</span> -<span class="built_in">exec</span> vim &#123;&#125; \;</span><br></pre></td></tr></table></figure>

<p>Be advised you may experience following error message</p>
<blockquote>
<p>find: missing argument to `-exec’</p>
</blockquote>
<p>actually you should add a slash in front of semi colon</p>
<h1 id="quite-mode-in-apt-get"><a href="#quite-mode-in-apt-get" class="headerlink" title="quite mode in apt-get"></a>quite mode in apt-get</h1><ul>
<li>apt-get will in verbose mode</li>
<li>apt-get <code>-q</code> will be in less verbose , a.k.a quite mode</li>
<li>apt-get <code>-qq</code> in extreme least verbose mode</li>
</ul>

        </div>
    

</div>
            
        </section>
    </div>
</div>



    <div class="row">
        <div class="col-sm-12">
            <div class="wrap-pagination">
                <a class="disabled" href="/">
                    <i class="fa fa-chevron-left" aria-hidden="true"></i>
                </a>
                <a class="" href="/page/2/">
                    <i class="fa fa-chevron-right" aria-hidden="true"></i>
                </a>
            </div>
        </div>
    </div>




</div>

<!-- Footer -->
<div class="push"></div>

<footer class="footer-content">
    <div class="container">
        <div class="row">
            <div class="col-xs-12 col-sm-12 col-md-6 col-lg-6 footer-about">
                <h2>About</h2>
                <p>
                    This theme was developed by <a href="https://github.com/klugjo">Jonathan Klughertz</a>. The source code is available on Github. Create Websites. Make Magic.
                </p>
            </div>
            
    <div class="col-xs-6 col-sm-6 col-md-3 col-lg-3 recent-posts">
        <h2>Recent Posts</h2>
        <ul>
            
            <li>
                <a class="footer-post" href="/2020-03-03-Algorithm-Leecode-1/">Algorithm notes from Leec</a>
            </li>
            
            <li>
                <a class="footer-post" href="/2020-05-10-Java-Deep-Notes/">Java Deep Notes</a>
            </li>
            
            <li>
                <a class="footer-post" href="/2019-08-25-AWS-Certificate/">AWS certification</a>
            </li>
            
            <li>
                <a class="footer-post" href="/2017-01-21-Algorithm/">Algorithm</a>
            </li>
            
        </ul>
    </div>



            
        </div>
        <div class="row">
            <div class="col-xs-12 col-sm-12 col-md-12 col-lg-12">
                <ul class="list-inline footer-social-icons">
                    
                    <li class="list-inline-item">
                        <a href="https://github.com/klugjo/hexo-theme-alpha-dust">
                            <span class="footer-icon-container">
                                <i class="fa fa-github"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="https://twitter.com/?lang=en">
                            <span class="footer-icon-container">
                                <i class="fa fa-twitter"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="https://www.facebook.com/">
                            <span class="footer-icon-container">
                                <i class="fa fa-facebook"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="https://www.instagram.com/">
                            <span class="footer-icon-container">
                                <i class="fa fa-instagram"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="https://dribbble.com/">
                            <span class="footer-icon-container">
                                <i class="fa fa-dribbble"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="https://plus.google.com/">
                            <span class="footer-icon-container">
                                <i class="fa fa-google-plus"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="https://www.behance.net/">
                            <span class="footer-icon-container">
                                <i class="fa fa-behance"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="https://500px.com/">
                            <span class="footer-icon-container">
                                <i class="fa fa-500px"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="mailto:test@example.com">
                            <span class="footer-icon-container">
                                <i class="fa fa-envelope-o"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="\#">
                            <span class="footer-icon-container">
                                <i class="fa fa-rss"></i>
                            </span>
                        </a>
                    </li>
                    
                </ul>
            </div>
        </div>
        <div class="row">
            <div class="col-xs-12 col-sm-12 col-md-12 col-lg-12">
                <div class="footer-copyright">
                    @Untitled. All right reserved | Design & Hexo <a href="http://www.codeblocq.com/">Jonathan Klughertz</a>
                </div>
            </div>
        </div>
    </div>
</footer>

<!-- After footer scripts -->

<!-- jQuery -->
<script src="//code.jquery.com/jquery-2.1.4.min.js"></script>

<!-- Tween Max -->
<script src="//cdnjs.cloudflare.com/ajax/libs/gsap/1.18.5/TweenMax.min.js"></script>

<!-- Gallery -->
<script src="//cdnjs.cloudflare.com/ajax/libs/featherlight/1.3.5/featherlight.min.js" type="text/javascript" charset="utf-8"></script>

<!-- Custom JavaScript -->
<script src="/js/main.js"></script>

<!-- Disqus Comments -->



</body>

</html>