<!DOCTYPE html>





<html class="theme-next pisces use-motion" lang="en">
<head>
  <meta charset="UTF-8">
<meta name="generator" content="Hexo 3.8.0">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.3.0">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.3.0">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.3.0">
  <link rel="mask-icon" href="/images/logo.svg?v=7.3.0" color="#222">

<link rel="stylesheet" href="/css/main.css?v=7.3.0">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '7.3.0',
    exturl: false,
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    save_scroll: false,
    copycode: {"enable":false,"show_result":false,"style":null},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: 'search.xml',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: 'Copy',
      copy_success: 'Copied',
      copy_failure: 'Copy failed'
    }
  };
</script>

  <meta name="description" content="Contact me via phray.zhang@gmail.com or wechat at helloworld_2000">
<meta property="og:type" content="website">
<meta property="og:title" content="Clouds &amp; Docker">
<meta property="og:url" content="http://www.todzhang.com/index.html">
<meta property="og:site_name" content="Clouds &amp; Docker">
<meta property="og:description" content="Contact me via phray.zhang@gmail.com or wechat at helloworld_2000">
<meta property="og:locale" content="en">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Clouds &amp; Docker">
<meta name="twitter:description" content="Contact me via phray.zhang@gmail.com or wechat at helloworld_2000">
  <link rel="canonical" href="http://www.todzhang.com/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: true,
    isPost: false,
    isPage: false,
    isArchive: false
  };
</script>

  <title>Clouds & Docker</title>
  








  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  <div class="container sidebar-position-left">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Clouds & Docker</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
      
      
      
        
        <li class="menu-item menu-item-home">
      
    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>Home</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-archives">
      
    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>Archives</a>

  </li>
  </ul>

</nav>
</div>
    </header>

    

  <a href="https://github.com/CloudsDocker" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewbox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"/><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"/><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"/></svg></a>


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
            

          <div id="content" class="content page-home">
            
  <section id="posts" class="posts-expand">
      

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.todzhang.com/2019-08-25-AWS-Certificate/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Todd Zhang">
      <meta itemprop="description" content="Contact me via phray.zhang@gmail.com or wechat at helloworld_2000">
      <meta itemprop="image" content="/images/globe.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Clouds & Docker">
    </span>
      <header class="post-header">

        
          <h1 class="post-title" itemprop="name headline">
              
              <a href="/2019-08-25-AWS-Certificate/" class="post-title-link" itemprop="url">AWS certification</a>
            
          </h1>
        

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              
                
              

              <time title="Created: 2019-12-15 19:18:06" itemprop="dateCreated datePublished" datetime="2019-12-15T19:18:06+11:00">2019-12-15</time>
            </span>
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
            <h1 id="Concepting"><a href="#Concepting" class="headerlink" title="Concepting"></a>Concepting</h1><p>Cloud computing is the on-demand delivery of IT resources and applications via the Internet with pay-as-you-go pricing. Whether you run applications that share photos to millions of mobile users or deliver services that support the critical operations of your business, the cloud provides rapid access to flexible and low-cost IT resources.</p>
<p>In its simplest form, cloud computing <code>provides an easy way to access servers, storage</code>, databases, and a broad set of application services <code>over the Internet</code>.</p>
<h1 id="Benefits-of-AWS"><a href="#Benefits-of-AWS" class="headerlink" title="Benefits of AWS"></a>Benefits of AWS</h1><p>There are six advantages for AWS clouding</p>
<ol>
<li>Global in minutes</li>
<li>Variable vs capital expense</li>
<li>Economies of scale</li>
<li>Stop guessing capacity</li>
<li>Focus on business differentaiors</li>
<li>Increate speed and agility </li>
</ol>
<h2 id="Cost-saving"><a href="#Cost-saving" class="headerlink" title="Cost saving"></a>Cost saving</h2><p>One of the key benefits of cloud computing is the opportunity to <code>replace up-front capital infrastructure expenses with low variable costs</code> that scale with your business. With the cloud, businesses no longer need to plan for and procure servers and other IT infrastructure weeks or months in advance. Instead, they can instantly spin up hundres or thousands of servers in minutes and deliver results faster.</p>
<p>With pay-per-use billing, AWS clouding services <code>become an operational expense instead of a capital expense</code>.</p>
<h1 id="Metadata"><a href="#Metadata" class="headerlink" title="Metadata"></a>Metadata</h1><p>Metadata, known as tags, that you can create and assign to your Amazon EC2 resources</p>
<p>Amazon Web Services. Amazon Elastic Compute Cloud (Kindle Location 152). Amazon Web Services. Kindle Edition. </p>
<h1 id="AZ-Available-Zones"><a href="#AZ-Available-Zones" class="headerlink" title="AZ (Available Zones)"></a>AZ (Available Zones)</h1><ul>
<li>Each availability zone is a physical data center in the region, but separate from the other ones (so that they’re isolated from disasters)</li>
<li>AWS Consoles are region scoped (except IAM, S3 &amp; Route53)</li>
</ul>
<h1 id="EC2"><a href="#EC2" class="headerlink" title="EC2"></a>EC2</h1><p>Here you need to create an AMI, but because AMI are bounded in the regions they are created, they need to be copied across regions for disaster recovery purposes</p>
<h2 id="Placement-group"><a href="#Placement-group" class="headerlink" title="Placement group"></a>Placement group</h2><p> Placements groups are the answer here, where “cluster” guarantees high network performance (correct answer), whereas “spread” would guarantee independent failures between instances.</p>
<p> When you launch a new EC2 instance, the EC2 service attempts to place the instance in such a way that all of your instances are spread out across underlying hardware to minimize correlated failures. You can use placement groups to influence the placement of a group of interdependent instances to meet the needs of your workload. Depending on the type of workload, you can create a placement group using one of the following placement strategies:</p>
<p> Cluster – packs instances close together inside an Availability Zone. This strategy enables workloads to achieve the low-latency network performance necessary for tightly-coupled node-to-node communication that is typical of HPC applications.</p>
<p> Partition – spreads your instances across logical partitions such that groups of instances in one partition do not share the underlying hardware with groups of instances in different partitions. This strategy is typically used by large distributed and replicated workloads, such as Hadoop, Cassandra, and Kafka.</p>
<p> Spread – strictly places a small group of instances across distinct underlying hardware to reduce correlated failures.</p>
<p> There is no charge for creating a placement group.</p>
<h3 id="Cluster"><a href="#Cluster" class="headerlink" title="Cluster"></a>Cluster</h3><p> Cluster Placement Groups</p>
<p>A cluster placement group is a logical grouping of instances within a single Availability Zone. A placement group can span peered VPCs in the same Region. The chief benefit of a cluster placement group, in addition to a 10 Gbps flow limit, is the non-blocking, non-oversubscribed, fully bi-sectional nature of the connectivity. In other words, all nodes within the placement group can talk to all other nodes within the placement group at the full line rate of 10 Gbps flows and 100 Gbps aggregate without any slowing due to over-subscription.</p>
<h2 id="ASG"><a href="#ASG" class="headerlink" title="ASG"></a>ASG</h2><h3 id="ASG-Lauch-configuration"><a href="#ASG-Lauch-configuration" class="headerlink" title="ASG Lauch configuration"></a>ASG Lauch configuration</h3><p>Launch configurations are immutable meaning they cannot be updated. You have to create a new launch configuration, attach it to the ASG and then terminate old instances / launch new instances</p>
<h3 id="ASG-termination"><a href="#ASG-termination" class="headerlink" title="ASG termination"></a>ASG termination</h3><p>AZs will be balanced first, then the instance with the oldest launch configuration within that AZ will be terminated. For a reference to the default termination policy logic, have a look at this link: <a href="https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-termination.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-termination.html</a></p>
<h1 id="IAM"><a href="#IAM" class="headerlink" title="IAM"></a>IAM</h1><p>Your whole AWS security is there:<br>• Users<br>• Groups<br>• Roles</p>
<p>Policies are written in JSON (JavaScript Object Notation)</p>
<p>IAM has a <code>global</code> view</p>
<p>Permissions let you specify access to AWS resources. Permissions are granted to IAM entities (users, groups, and roles) and by default these entities start with no permissions. In other words, IAM entities can do nothing in AWS until you grant them your desired permissions. To give entities permissions, you can attach a policy that specifies the type of access, the actions that can be performed, and the resources on which the actions can be performed. In addition, you can specify any conditions that must be set for access to be allowed or denied.</p>
<h2 id="IAM-policies"><a href="#IAM-policies" class="headerlink" title="IAM policies"></a>IAM policies</h2><p>A permissions policy describes who has access to what. Policies attached to an IAM identity are identity-based policies (IAM policies) and policies attached to a resource are resource-based policies. Amazon RDS supports only identity-based policies (IAM policies).</p>
<h2 id="DB-authenticatioin-via-IAM"><a href="#DB-authenticatioin-via-IAM" class="headerlink" title="DB authenticatioin via IAM"></a>DB authenticatioin via IAM</h2><p>MySQL and PostgreSQL both support IAM database authentication.</p>
<p>To protect the confidential data of your customers, you have to ensure that your RDS database can only be accessed using the profile credentials specific to your EC2 instances via an authentication token.   </p>
<p>You can authenticate to your DB instance using AWS Identity and Access Management (IAM) database authentication. IAM database authentication works with MySQL and PostgreSQL. With this authentication method, you don’t need to use a password when you connect to a DB instance. Instead, you use an authentication token.</p>
<p>An authentication token is a unique string of characters that Amazon RDS generates on request. Authentication tokens are generated using AWS Signature Version 4. Each token has a lifetime of 15 minutes. You don’t need to store user credentials in the database, because authentication is managed externally using IAM. You can also still use standard database authentication.</p>
<h2 id="IAM-Federation"><a href="#IAM-Federation" class="headerlink" title="IAM Federation"></a>IAM Federation</h2><p>• Big enterprises usually integrate their own repository of users with IAM<br>• This way, one can login into AWS using their company credentials<br>• Identity Federation uses the SAML standard (Active Directory)</p>
<p>• One IAM User per PHYSICAL PERSON<br>• One IAM Role per Application</p>
<h2 id="STS"><a href="#STS" class="headerlink" title="STS"></a>STS</h2><p>Temporary Security Credentials<br>You can use the AWS Security Token Service (AWS STS) to create and provide trusted users with temporary security credentials that can control access to your AWS resources. Temporary security credentials work almost identically to the long-term access key credentials that your IAM users can use, with the following differences:</p>
<p>Temporary security credentials are short-term, as the name implies. They can be configured to last for anywhere from a few minutes to several hours. After the credentials expire, AWS no longer recognizes them or allows any kind of access from API requests made with them.</p>
<p>Temporary security credentials are not stored with the user but are generated dynamically and provided to the user when requested. When (or even before) the temporary security credentials expire, the user can request new credentials, as long as the user requesting them still has permissions to do so.</p>
<h1 id="Storage"><a href="#Storage" class="headerlink" title="Storage"></a>Storage</h1><h3 id="Performance"><a href="#Performance" class="headerlink" title="Performance"></a>Performance</h3><p>Instance Store will have the highest disk performance but comes with the storage being wiped if the instance is terminated, which is acceptable in this case. EBS volumes would provide good performance as far as disk goes, but not as good as Instance Store. EBS data survives instance termination or reboots. EFS is a network drive, and finally S3 cannot be mounted as a local disk (natively).</p>
<p>Need to define two terms:<br>• RPO: Recovery Point Objective<br>• RTO: Recovery Time Objective</p>
<h2 id="S3"><a href="#S3" class="headerlink" title="S3"></a>S3</h2><p>Generating S3 pre-signed URLs would bypass CloudFront, therefore we should use CloudFront signed URL. To generate that URL we must code, and Lambda is the perfect tool for running that code on the fly. </p>
<p>As the file is greater than 5GB in size, you must use Multi Part upload to upload that file to S3.</p>
<h3 id="OAI"><a href="#OAI" class="headerlink" title="OAI"></a>OAI</h3><p>Don’t make the S3 bucket public. You cannot attach IAM roles to the CloudFront distribution. S3 buckets don’t have security groups. Here you need to use an OAI. Read more here: <a href="https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html</a></p>
<p>Restricting Access to Amazon S3 Content by Using an Origin Access Identity<br>To restrict access to content that you serve from Amazon S3 buckets, you create CloudFront signed URLs or signed cookies to limit access to files in your Amazon S3 bucket, and then you create a special CloudFront user called an origin access identity (OAI) and associate it with your distribution. Then you configure permissions so that CloudFront can use the OAI to access and serve files to your users, but users can’t use a direct URL to the S3 bucket to access a file there. Taking these steps help you maintain secure access to the files that you serve through CloudFront.</p>
<p>In general, if you’re using an Amazon S3 bucket as the origin for a CloudFront distribution, you can either allow everyone to have access to the files there, or you can restrict access. If you limit access by using, for example, CloudFront signed URLs or signed cookies, you also won’t want people to be able to view files by simply using the direct URL for the file. Instead, you want them to only access the files by using the CloudFront URL, so your protections work. For more information about using signed URLs and signed cookies, see Serving Private Content with Signed URLs and Signed Cookies</p>
<h3 id="Encryption"><a href="#Encryption" class="headerlink" title="Encryption"></a>Encryption</h3><p>With SSE-C, your company can still provide the encryption key but let AWS do the encryption</p>
<h2 id="EBS-Elastic-Block-Storage"><a href="#EBS-Elastic-Block-Storage" class="headerlink" title="EBS (Elastic Block Storage)"></a>EBS (Elastic Block Storage)</h2><p>EBS is already redundant storage (replicated within an AZ)<br>But what if you want to increase IOPS to say 100 000 IOPS?</p>
<h3 id="RAID"><a href="#RAID" class="headerlink" title="RAID"></a>RAID</h3><h4 id="RAID-0-increase-performance"><a href="#RAID-0-increase-performance" class="headerlink" title="RAID 0 (increase performance)"></a>RAID 0 (increase performance)</h4><p> EC2 instance<br>One logical volume<br>either<br>EBS Volume 1<br>• Combining 2 or more volumes and getting the total disk space and I/O<br>• But one disk fails, all the data is failed</p>
<h4 id="RAID-1-increase-fault-tolerance"><a href="#RAID-1-increase-fault-tolerance" class="headerlink" title="RAID 1 (increase fault tolerance)"></a>RAID 1 (increase fault tolerance)</h4><p> EC2 instance<br>One logical volume<br>both<br>• RAID 1 = Mirroring a volume to another<br>• If one disk fails, our logical volume is still working<br>• We have to send the data to two EBS volume at the same time (2x network)</p>
<h3 id="EBS-types"><a href="#EBS-types" class="headerlink" title="EBS types"></a>EBS types</h3><p> keeping as io1 but reducing the iops may interfere with the burst of performance we need. The EC2 instance type changes won’t affect the 90% of the costs that are incurred to us. CloudFormation is a free service to use. Therefore, gp2 is the right choice, allowing us to save on cost while keeping a burst in performance when needed</p>
<p>You can now choose between three Amazon EBS volume types to best meet the needs of your workloads: General Purpose (SSD), Provisioned IOPS (SSD), and Magnetic volumes. </p>
<h4 id="General-Purpose-SSD"><a href="#General-Purpose-SSD" class="headerlink" title="General Purpose (SSD)"></a>General Purpose (SSD)</h4><p>GP2 volumes are suitable for a broad range of workloads, including small to medium-sized databases, development and test environments, and boot volumes. </p>
<h4 id="Provisioned-IOPS-SSD"><a href="#Provisioned-IOPS-SSD" class="headerlink" title="Provisioned IOPS (SSD)"></a>Provisioned IOPS (SSD)</h4><p>Such volumes offer storage with consistent and low-latency performance, are designed for I/O-intensive applications such as large relational or NoSQL databases, and allow you to choose the level of performance you need. </p>
<h4 id="Magnetic-volumes"><a href="#Magnetic-volumes" class="headerlink" title="Magnetic volumes"></a>Magnetic volumes</h4><p>formerly known as Standard volumes, provide the lowest cost per gigabyte of all Amazon EBS volume types and are ideal for workloads where data is accessed infrequently and applications where the lowest storage cost is important.</p>
<p>Backed by Solid-State Drives (SSDs), General Purpose (SSD) volumes provide the ability to burst to 3,000 IOPS per volume, independent of volume size, to meet the performance needs of most applications and also deliver a consistent baseline of 3 IOPS/GB. General Purpose (SSD) volumes offer the same five nines of availability and durable snapshot capabilities as other volume types. Pricing and performance for General Purpose (SSD) volumes are simple and predictable. You pay for each GB of storage you provision, and there are no additional charges for I/O performed on a volume. Prices start as low as $0.10/GB.</p>
<h4 id="EBS-snapshot"><a href="#EBS-snapshot" class="headerlink" title="EBS snapshot"></a>EBS snapshot</h4><p>While it is completing, an in-progress snapshot is not affected by ongoing reads and writes to the volume.</p>
<p>You can take a snapshot of an attached volume that is in use. However, snapshots only capture data that has been written to your Amazon EBS volume at the time the snapshot command is issued. This might exclude any data that has been cached by any applications or the operating system. If you can pause any file writes to the volume long enough to take a snapshot, your snapshot should be complete. However, if you can’t pause all file writes to the volume, you should unmount the volume from within the instance, issue the snapshot command, and then remount the volume to ensure a consistent and complete snapshot. You can remount and use your volume while the snapshot status is pending.</p>
<h4 id="Save-network-cost"><a href="#Save-network-cost" class="headerlink" title="Save network cost"></a>Save network cost</h4><p> S3 would imply changing the application code, Glacier is not applicable as the files are frequently requested, Storage Gateway isn’t for distributing files to end users. CloudFront is the right answer, because we can put it in front of our ASG and leverage a Global Caching feature that will help us distribute the content reliably with dramatically reduced costs (the ASG won’t need to scale as much)</p>
<h2 id="EFS"><a href="#EFS" class="headerlink" title="EFS"></a>EFS</h2><p>Instance Stores or EBS volumes are local disks and cannot be shared across instances. Here, we need a network file system (NFS), which is exactly what EFS is designed for.</p>
<h2 id="Redshift"><a href="#Redshift" class="headerlink" title="Redshift"></a>Redshift</h2><p>Creating a smaller cluster with the cold data would not decrease the storage cost of Redshift, which will increase as we keep on creating data. Moving the data to S3 glacier will prevent us from being able to query it. Redshift’s internal storage does not have “tiers”. Therefore, we should migrate the data to S3 IA and use Athena (serverless SQL query engine on top of S3) to analyze the cold data.</p>
<h1 id="CloundFront"><a href="#CloundFront" class="headerlink" title="CloundFront"></a>CloundFront</h1><h2 id="Origin"><a href="#Origin" class="headerlink" title="Origin"></a>Origin</h2><p>Until now, CloudFront could serve up content from Amazon S3. In content-distribution lingo, S3 was the only supported origin server. You would store your web objects (web pages, style sheets, images, JavaScript, and so forth) in S3, and then create a CloudFront distribution. Here is the basic flow:</p>
<p>Effective today we are opening up CloudFront and giving you the ability to use the origin server of your choice.</p>
<p>You can now create a CloudFront distribution using a custom origin. Each distribution will can point to an S3 or to a custom origin. This could be another storage service, or it could be something more interesting and more dynamic, such as an EC2 instance or even an Elastic Load Balancer:</p>
<h1 id="CloudFormation"><a href="#CloudFormation" class="headerlink" title="CloudFormation"></a>CloudFormation</h1><h2 id="CloudFormation-vs-Elastic-Beanstalk"><a href="#CloudFormation-vs-Elastic-Beanstalk" class="headerlink" title="CloudFormation vs Elastic Beanstalk"></a>CloudFormation vs Elastic Beanstalk</h2><p>Elastic Beanstalk provides an environment to easily deploy and run applications in the cloud.<br>CloudFormation is a convenient provisioning mechanism for a broad range of AWS resources.</p>
<h1 id="VPC"><a href="#VPC" class="headerlink" title="VPC"></a>VPC</h1><p>you can optionally connect to your own network, known as virtual private clouds (VPCs)</p>
<p>Amazon Web Services. Amazon Elastic Compute Cloud (Kindle Locations 153-154). Amazon Web Services. Kindle Edition. </p>
<p>Amazon VPC lets you provision a logically isolated section of the Amazon Web Services (AWS) cloud where you can launch AWS resources in a virtual network that you define. You have complete control over your virtual networking environment, including selection of your own IP address ranges, creation of subnets, and configuration of route tables and network gateways. You can also create a hardware Virtual Private Network (VPN) connection between your corporate datacenter and your VPC and leverage the AWS cloud as an extension of your corporate datacenter.</p>
<h2 id="Subnet"><a href="#Subnet" class="headerlink" title="Subnet"></a>Subnet</h2><p>A subnet is a range of IP addresses in your VPC. You can launch AWS resources into a specified subnet. Use a public subnet for resources that must be connected to the internet, and a private subnet for resources that won’t be connected to the internet.<br>To protect the AWS resources in each subnet, use security groups and network access control lists (ACL).</p>
<h2 id="VPC-Endpoint"><a href="#VPC-Endpoint" class="headerlink" title="VPC Endpoint"></a>VPC Endpoint</h2><p>You must remember that the two services that use a VPC Endpoint Gateway are Amazon S3 and DynamoDB. The rest are VPC Endpoint Interface</p>
<h3 id="NACL-Network-ACL"><a href="#NACL-Network-ACL" class="headerlink" title="NACL (Network ACL)"></a>NACL (Network ACL)</h3><p>NACL is stateless.</p>
<p>• NACL are like a firewall which control traffic from and to subnet<br>• Default NACL allows everything outbound and everything inbound<br>• One NACL per Subnet, new Subnets are assigned the Default NACL<br>• Define NACL rules:<br>• Rules have a number (1-32766) and higher precedence with a lower number<br>• E.g. If you define #100 ALLOW <ip> and #200 DENY <ip> , IP will be allowed • Last rule is an asterisk (*) and denies a request in case of no rule match<br>• AWS recommends adding rules by increment of 100<br>• Newly created NACL will deny everything<br>• NACL are a great way of blocking a specific IP at the subnet level</ip></ip></p>
<h3 id="NACL-noteworhty-points"><a href="#NACL-noteworhty-points" class="headerlink" title="NACL noteworhty points"></a>NACL noteworhty points</h3><p>Network ACL Basics</p>
<ul>
<li>Your VPC automatically comes with a modifiable default network ACL. By default, it allows all inbound and outbound IPv4 traffic and, if applicable, IPv6 traffic.</li>
</ul>
<h3 id="IGW-Internet-GateWay"><a href="#IGW-Internet-GateWay" class="headerlink" title="IGW (Internet GateWay)"></a>IGW (Internet GateWay)</h3><p>After creating an IGW, make sure the route tables are updated. Additionally, ensure the security group allow the ICMP protocol for ping requests</p>
<h3 id="NAT"><a href="#NAT" class="headerlink" title="NAT"></a>NAT</h3><p>NAT Instances would work but won’t scale and you would have to manage them (as they’re EC2 instances). Egress-Only Internet Gateways are for IPv6, not IPv4. Internet Gateways must be deployed in a public subnet. Therefore you must use a NAT Gateway in your public subnet in order to provide internet access to your instances in your private subnets.</p>
<h2 id="How-to-prevent-DDoS"><a href="#How-to-prevent-DDoS" class="headerlink" title="How to prevent DDoS"></a>How to prevent DDoS</h2><p>AWS provides flexible infrastructure and services that help customers implement strong DDoS mitigations and create highly available application architectures that follow AWS Best Practices for DDoS Resiliency. These include services such as Amazon Route 53, Amazon CloudFront, Elastic Load Balancing, and AWS WAF to control and absorb traffic, and deflect unwanted requests. These services integrate with AWS Shield, a managed DDoS protection service that provides always-on detection and automatic inline mitigations to safeguard web applications running on AWS.</p>
<h3 id="AWS-Shield"><a href="#AWS-Shield" class="headerlink" title="AWS Shield"></a>AWS Shield</h3><p>AWS Shield is a managed DDoS protection service that is available in two tiers: Standard and Advanced. AWS Shield Standard applies always-on detection and inline mitigation techniques, such as deterministic packet filtering and priority-based traffic shaping, to minimize application downtime and latency. </p>
<h4 id="AWS-Shield-Standard"><a href="#AWS-Shield-Standard" class="headerlink" title="AWS Shield Standard"></a>AWS Shield Standard</h4><p>It is included automatically and transparently to your Elastic Load Balancing load balancers, Amazon CloudFront distributions, and Amazon Route 53 resources at no additional cost. When you use these services that include AWS Shield Standard, you receive comprehensive availability protection against all known infrastructure layer attacks. Customers who have the technical expertise to manage their own monitoring and mitigation of application layer attacks can use AWS Shield together with AWS WAF rules to create a comprehensive DDoS attack mitigation strategy.</p>
<h4 id="AWS-Shield-Advanced"><a href="#AWS-Shield-Advanced" class="headerlink" title="AWS Shield Advanced"></a>AWS Shield Advanced</h4><p>It provides enhanced DDoS attack detection and monitoring for application-layer traffic to your Elastic Load Balancing load balancers, CloudFront distributions, Amazon Route 53 hosted zones and resources attached to an Elastic IP address, such Amazon EC2 instances. AWS Shield Advanced uses additional techniques to provide granular detection of DDoS attacks, such as resource-specific traffic monitoring to detect HTTP floods or DNS query floods. AWS Shield Advanced includes 24x7 access to the AWS DDoS Response Team (DRT), support experts who apply manual mitigations for more complex and sophisticated DDoS attacks, directly create or update AWS WAF rules, and can recommend improvements to your AWS architectures. AWS WAF is included at no additional cost for resources that you protect with AWS Shield Advanced.</p>
<p>AWS Shield Advanced includes access to near real-time metrics and reports, for extensive visibility into infrastructure layer and application layer DDoS attacks. You can combine AWS Shield Advanced metrics with additional, fine-tuned AWS WAF metrics for a more comprehensive CloudWatch monitoring and alarming strategy. Customers subscribed to AWS Shield Advanced can also apply for a credit for charges that result from scaling during a DDoS attack on protected Amazon EC2, Amazon CloudFront, Elastic Load Balancing, or Amazon Route 53 resources. See the AWS Shield Developer Guide for a detailed comparison of the two AWS Shield offerings.</p>
<h2 id="CIDR"><a href="#CIDR" class="headerlink" title="CIDR"></a>CIDR</h2><p>To add a CIDR block to your VPC, the following rules apply:</p>
<p>-The allowed block size is between a /28 netmask and /16 netmask.<br>-The CIDR block must not overlap with any existing CIDR block that’s associated with the VPC.<br>-You cannot increase or decrease the size of an existing CIDR block.<br>-You have a limit on the number of CIDR blocks you can associate with a VPC and the number of routes you can add to a route table. You cannot associate a CIDR block if this results in you exceeding your limits.<br>-The CIDR block must not be the same or larger than the CIDR range of a route in any of the VPC route tables. For example, if you have a route with a destination of 10.0.0.0/24 to a virtual private gateway, you cannot associate a CIDR block of the same range or larger. However, you can associate a CIDR block of 10.0.0.0/25 or smaller.<br>-The first four IP addresses and the last IP address in each subnet CIDR block are not available for you to use, and cannot be assigned to an instance.</p>
<h1 id="AWS-WAF"><a href="#AWS-WAF" class="headerlink" title="AWS WAF"></a>AWS WAF</h1><p>AWS WAF is a web application firewall that helps protect web applications from common web exploits that could affect application availability, compromise security, or consume excessive resources. You can use AWS WAF to define customizable web security rules that control which traffic accesses your web applications. If you use AWS Shield Advanced, you can use AWS WAF at no extra cost for those protected resources and can engage the DRT to create WAF rules.</p>
<p>AWS WAF rules use conditions to target specific requests and trigger an action, allowing you to identify and block common DDoS request patterns and effectively mitigate a DDoS attack. These include size constraint conditions to block a web request based on the length of its query string or request body, and geographic match conditions to implement geo restriction (also known as geoblocking) on requests that originate from specific countries. </p>
<h1 id="AWS-SSM-Simple-System-Manager"><a href="#AWS-SSM-Simple-System-Manager" class="headerlink" title="AWS SSM (Simple System Manager)"></a>AWS SSM (Simple System Manager)</h1><p>AWS SSM is parameter store.</p>
<h1 id="ELB-Elastic-Load-Balancing"><a href="#ELB-Elastic-Load-Balancing" class="headerlink" title="ELB: Elastic Load Balancing"></a>ELB: Elastic Load Balancing</h1><p>To automatically distribute incoming application traffic across multiple instances, use Elastic Load Balancing. </p>
<p>For HA, even though our ASG is deployed across 3 AZ, the minimum capacity to be highly available is 2. Finally, we can save costs by reserving these two instances as we know they’ll be up and running at any time</p>
<h2 id="Application-Load-Balancer-vs-Network-load-balancer"><a href="#Application-Load-Balancer-vs-Network-load-balancer" class="headerlink" title="Application Load Balancer vs Network load balancer"></a>Application Load Balancer vs Network load balancer</h2><p>Path based routing and host based routing are only available for the Application Load Balancer (ALB). Deploying an NGINX load balancer on EC2 would work but would suffer management and scaling issues. Read more here: <a href="https://aws.amazon.com/blogs/aws/new-host-based-routing-support-for-aws-application-load-balancers/" target="_blank" rel="noopener">https://aws.amazon.com/blogs/aws/new-host-based-routing-support-for-aws-application-load-balancers/</a></p>
<h3 id="ALB-amp-ASG"><a href="#ALB-amp-ASG" class="headerlink" title="ALB &amp; ASG"></a>ALB &amp; ASG</h3><p>Adding the entire CIDR of the ALB would work, but wouldn’t guarantee that only the ALB can access the EC2 instances that are part of the ASG. Here, the right solution is to add a rule on the ASG security group to allow incoming traffic from the security group configured for the ALB.</p>
<h3 id="SNI"><a href="#SNI" class="headerlink" title="SNI"></a>SNI</h3><p>support for multiple TLS/SSL certificates on Application Load Balancers (ALB) using Server Name Indication (SNI). You can now host multiple TLS secured applications, each with its own TLS certificate, behind a single load balancer. In order to use SNI, all you need to do is bind multiple certificates to the same secure listener on your load balancer. ALB will automatically choose the optimal TLS certificate for each client. These new features are provided at no additional charge.</p>
<p>One of our most frequent requests on forums, reddit, and in my e-mail inbox has been to use the Server Name Indication (SNI) extension of TLS to choose a certificate for a client. Since TLS operates at the transport layer, below HTTP, it doesn’t see the hostname requested by a client. SNI works by having the client tell the server “This is the domain I expect to get a certificate for” when it first connects. The server can then choose the correct certificate to respond to the client. All modern web browsers and a large majority of other clients support SNI. In fact, today we see SNI supported by over 99.5% of clients connecting to CloudFront.</p>
<h1 id="RDS"><a href="#RDS" class="headerlink" title="RDS"></a>RDS</h1><p>RDS stands for Relational Database Service</p>
<h2 id="Read-Replics"><a href="#Read-Replics" class="headerlink" title="Read Replics"></a>Read Replics</h2><p>RDS Read Replicas for read scalability</p>
<ul>
<li>Up to 5 Read Replicas</li>
<li>Within AZ, Cross AZ or Cross Region</li>
<li>Replication is ASYNC, so reads are eventually consistent</li>
<li>Replicas can be promoted to their own DB</li>
<li>Applications must update the connection string to leverage read replicas</li>
</ul>
<h2 id="Amazon-RDS-Multi-AZ-Deployments"><a href="#Amazon-RDS-Multi-AZ-Deployments" class="headerlink" title="Amazon RDS Multi-AZ Deployments"></a>Amazon RDS Multi-AZ Deployments</h2><p>Amazon RDS Multi-AZ deployments provide enhanced availability and durability for Database (DB) Instances, making them a natural fit for production database workloads. When you provision a Multi-AZ DB Instance, Amazon RDS automatically creates a primary DB Instance and synchronously replicates the data to a standby instance in a different Availability Zone (AZ). Each AZ runs on its own physically distinct, independent infrastructure, and is engineered to be highly reliable. In case of an infrastructure failure, Amazon RDS performs an automatic failover to the standby (or to a read replica in the case of Amazon Aurora), so that you can resume database operations as soon as the failover is complete. Since the endpoint for your DB Instance remains the same after a failover, your application can resume database operation without the need for manual administrative intervention.</p>
<h2 id="Security"><a href="#Security" class="headerlink" title="Security"></a>Security</h2><p>Use security groups to control what IP addresses or Amazon EC2 instances can connect to your databases on a DB instance.<br>Run your DB instance in an Amazon Virtual Private Cloud (VPC) for the greatest possible network access control.</p>
<h2 id="Working-with-a-DB-Instance-in-a-VPC"><a href="#Working-with-a-DB-Instance-in-a-VPC" class="headerlink" title="Working with a DB Instance in a VPC"></a>Working with a DB Instance in a VPC</h2><p>Your VPC must have at least two subnets. These subnets must be in two different Availability Zones in the region where you want to deploy your DB instance.<br>If you want your DB instance in the VPC to be publicly accessible, you must enable the VPC attributes DNS hostnames and DNS resolution.</p>
<h1 id="Elastic-Cache"><a href="#Elastic-Cache" class="headerlink" title="Elastic Cache"></a>Elastic Cache</h1><p>IAM Auth is not supported by ElastiCache</p>
<h1 id="Amazon-CloudWatch"><a href="#Amazon-CloudWatch" class="headerlink" title="Amazon CloudWatch"></a>Amazon CloudWatch</h1><p>To monitor basic statistics for your instances and Amazon EBS volumes, use Amazon CloudWatch. </p>
<p>Amazon Web Services. Amazon Elastic Compute Cloud (Kindle Locations 180-184). Amazon Web Services. Kindle Edition. </p>
<p>Disabling the Termination from the ASG would prevent our ASG to be Elastic and impact our costs. Making a snapshot of the EC2 instance before it gets terminated <em>could</em> work but it’s tedious, not elastic and very expensive, as all we’re interested about are log files. Using AWS Lambda would be extremely hard to use for this task. Here, the natural and by far easiest solution would be to use the CloudWatch Logs agents on the EC2 instances to automatically send log files into CloudWatch, so we can analyze them in the future easily should any problems arise.</p>
<h1 id="API-Gateway"><a href="#API-Gateway" class="headerlink" title="API Gateway"></a>API Gateway</h1><p>Q: What API types are supported by Amazon API Gateway?</p>
<p>Amazon API Gateway offers two options to create RESTful APIs, HTTP APIs (Preview) and REST APIs, as well as an option to create WebSocket APIs.</p>
<p>HTTP API: HTTP APIs, currently available in Preview, are optimized for building APIs that proxy to AWS Lambda functions or HTTP backends, making them ideal for serverless workloads. They do not currently offer API management functionality.</p>
<p>REST API: REST APIs offer API proxy functionality and API management features in a single solution. REST APIs offer API management features such as usage plans, API keys, publishing, and monetizing APIs.</p>
<p>WebSocket API: WebSocket APIs maintain a persistent connection between connected clients to enable real-time message communication. With WebSocket APIs in API Gateway, you can define backend integrations with AWS Lambda functions, Amazon Kinesis, or any HTTP endpoint to be invoked when messages are received from the connected clients.</p>
<h1 id="CloudTrail"><a href="#CloudTrail" class="headerlink" title="CloudTrail"></a>CloudTrail</h1><p>To monitor the calls made to the Amazon EC2 API for your account, including calls made by the AWS Management Console, command line tools, and other services, use AWS CloudTrail.</p>
<p>In general, to analyze any API calls made within your AWS account, you should use CloudTrail</p>
<p>​
Set up a new CloudTrail trail in a new S3 bucket using the AWS CLI and also pass both the –is-multi-region-trail and –include-global-service-events parameters then encrypt log files using KMS encryption. Apply Multi Factor Authentication (MFA) Delete on the S3 bucket and ensure that only authorized users can access the logs by configuring the bucket policies.</p>
<h2 id="Charge"><a href="#Charge" class="headerlink" title="Charge"></a>Charge</h2><p> the first copy of management events is free.</p>
<h2 id="Cloud-Trail-retention-days"><a href="#Cloud-Trail-retention-days" class="headerlink" title="Cloud Trail retention days"></a>Cloud Trail retention days</h2><p>Management event activity is recorded by AWS CloudTrail for the last 90 days, and can be viewed and searched free of charge from the AWS CloudTrail console, or by using the AWS CLI.</p>
<h1 id="AWS-ECS"><a href="#AWS-ECS" class="headerlink" title="AWS ECS"></a>AWS ECS</h1><h2 id="Summary-AWS-ECS-–-Elastic-Container-Service"><a href="#Summary-AWS-ECS-–-Elastic-Container-Service" class="headerlink" title="Summary: AWS ECS – Elastic Container Service"></a>Summary: AWS ECS – Elastic Container Service</h2><p>• ECS is a container orchestration service<br>• ECS helps you run Docker containers on EC2 machines<br>• ECS is complicated, and made of:<br>• “ECS Core”: Running ECS on user-provisioned EC2 instances<br>• Fargate: Running ECS tasks on AWS-provisioned compute (serverless)<br>• EKS: Running ECS on AWS-powered Kubernetes (running on EC2)<br>• ECR: Docker Container Registry hosted by AWS<br>• ECS &amp; Docker are very popular for microservices<br>• For now, for the exam, only “ECS Core” &amp; ECR is in scope<br>• IAM security and roles at the ECS task level</p>
<h2 id="AWS-ECS-–-Concepts"><a href="#AWS-ECS-–-Concepts" class="headerlink" title="AWS ECS – Concepts"></a>AWS ECS – Concepts</h2><p>• ECS cluster: set of EC2 instances<br>• ECS service: applications definitions running on ECS cluster<br>• ECS tasks + definition: containers running to create the application<br>• ECS IAM roles: roles assigned to tasks to interact with AWS</p>
<h2 id="AWS-ECS-–-ALB-integration"><a href="#AWS-ECS-–-ALB-integration" class="headerlink" title="AWS ECS – ALB integration"></a>AWS ECS – ALB integration</h2><p>• Application Load Balancer (ALB) has a direct integration feature with ECS called “port mapping”, This allows you to run multiple instances of the same application on the same EC2 machine</p>
<h3 id="Dynamic-mapping"><a href="#Dynamic-mapping" class="headerlink" title="Dynamic mapping"></a>Dynamic mapping</h3><p>Dynamic Port Mapping is available for the Application Load Balancer. A reverse proxy solution would work but would be too much work to manage. Here the ALB has a feature that provides a direct dynamic port mapping feature and integration with the ECS service so we will leverage that. Read more here: <a href="https://aws.amazon.com/premiumsupport/knowledge-center/dynamic-port-mapping-ecs/" target="_blank" rel="noopener">https://aws.amazon.com/premiumsupport/knowledge-center/dynamic-port-mapping-ecs/</a></p>
<h2 id="AWS-ECS-–-ECS-Setup-amp-Config-file"><a href="#AWS-ECS-–-ECS-Setup-amp-Config-file" class="headerlink" title="AWS ECS – ECS Setup &amp; Config file"></a>AWS ECS – ECS Setup &amp; Config file</h2><p>• Run an EC2 instance, install the ECS agent with ECS config file<br>• Or use an ECS-ready Linux AMI (still need to modify config file) • ECS Config file is at /etc/ecs/ecs.config</p>
<h2 id="AWS-ECR-–-Elastic-Container-Registry"><a href="#AWS-ECR-–-Elastic-Container-Registry" class="headerlink" title="AWS ECR – Elastic Container Registry"></a>AWS ECR – Elastic Container Registry</h2><p>• Store, managed and deploy your containers on AWS<br>• Fully integrated with IAM &amp; ECS<br>• Sent over HTTPS (encryption in flight) and encrypted at rest</p>
<h2 id="Specifying-Sensitive-Data"><a href="#Specifying-Sensitive-Data" class="headerlink" title="Specifying Sensitive Data"></a>Specifying Sensitive Data</h2><p>Amazon ECS enables you to inject sensitive data into your containers by storing your sensitive data in either AWS Secrets Manager secrets or AWS Systems Manager Parameter Store parameters and then referencing them in your container definition. This feature is supported by tasks using both the EC2 and Fargate launch types.</p>
<h3 id="Required-IAM-Permissions-for-Amazon-ECS-Secrets"><a href="#Required-IAM-Permissions-for-Amazon-ECS-Secrets" class="headerlink" title="Required IAM Permissions for Amazon ECS Secrets"></a>Required IAM Permissions for Amazon ECS Secrets</h3><p>To use this feature, you must have the Amazon ECS task execution role and reference it in your task definition. This allows the container agent to pull the necessary AWS Systems Manager or Secrets Manager resources. For more information, see Amazon ECS Task Execution IAM Role.</p>
<p>To provide access to the AWS Systems Manager Parameter Store parameters that you create, manually add the following permissions as an inline policy to the task execution role. For more information, see Adding and Removing IAM Policies.</p>
<p>ssm:GetParameters—Required if you are referencing a Systems Manager Parameter Store parameter in a task definition.</p>
<p>secretsmanager:GetSecretValue—Required if you are referencing a Secrets Manager secret either directly or if your Systems Manager Parameter Store parameter is referencing a Secrets Manager secret in a task definition.</p>
<p>kms:Decrypt—Required only if your secret uses a custom KMS key and not the default key. The ARN for your custom key should be added as a resource.</p>
<h1 id="Lambda"><a href="#Lambda" class="headerlink" title="Lambda"></a>Lambda</h1><p>AWS Lambda functions time out after 15 minutes, and are not usually meant for long running jobs.</p>
<h2 id="Lambda-parameters-Encryption"><a href="#Lambda-parameters-Encryption" class="headerlink" title="Lambda parameters Encryption"></a>Lambda parameters Encryption</h2><p>Although Lambda encrypts the environment variables in your function by default, the sensitive information would still be visible to other users who have access to the Lambda console. This is because Lambda uses a default KMS key to encrypt the variables, which is usually accessible by other users. The best option in this scenario is to use encryption helpers to secure your environment variables.</p>
<p>Enabling SSL would encrypt data only when in-transit. Your other teams would still be able to view the plaintext at-rest. Use AWS KMS instead.</p>
<p>Option 3 is incorrect since, as mentioned, Lambda does provide encryption functionality of environment variables.    </p>
<h2 id="Lambda-functions"><a href="#Lambda-functions" class="headerlink" title="Lambda functions"></a>Lambda functions</h2><p>You upload your application code in the form of one or more Lambda functions. Lambda stores code in Amazon S3 and encrypts it at rest.</p>
<h3 id="layer"><a href="#layer" class="headerlink" title="layer"></a>layer</h3><p>A layer is a ZIP archive that contains libraries, a custom runtime, or other dependencies. Use layers to manage your function’s dependencies independently and keep your deployment package small.</p>
<h3 id="Invoking-Functions"><a href="#Invoking-Functions" class="headerlink" title="Invoking Functions"></a>Invoking Functions</h3><p>Lambda supports synchronous and asynchronous invocation of a Lambda function. You can control the invocation type only when you invoke a Lambda function (referred to as on-demand invocation).<br>An event source is the entity that publishes events, and a Lambda function is the custom code that processes the events.<br>Event source mapping maps an event source to a Lambda function. It enables automatic invocation of your Lambda function when events occur.</p>
<h1 id="Disaster-Recovery"><a href="#Disaster-Recovery" class="headerlink" title="Disaster Recovery"></a>Disaster Recovery</h1><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>Need to define two terms:<br>• RPO: Recovery Point Objective<br>• RTO: Recover y Time Objective</p>
<p>Disaster Recovery Strategies<br>• Backup and Restore<br>• Pilot Light<br>• Warm Standby<br>• Hot Site / Multi Site Approach</p>
<h1 id="Route53"><a href="#Route53" class="headerlink" title="Route53"></a>Route53</h1><p>Simple Records do not have health checks, here the most likely issue is that the TTL is still in effect so you have to wait until it expires for the new users to perform another DNS query and get the value for your new Load Balancer.</p>
<h1 id="Database"><a href="#Database" class="headerlink" title="Database"></a>Database</h1><p>ElastiCache / RDS / Neptune are not serverless databases. DynamoDB is serverless, single digit latency and horizontally scales.</p>
<h2 id="DynamoDB"><a href="#DynamoDB" class="headerlink" title="DynamoDB"></a>DynamoDB</h2><p>DynamoDB Streams will contain a stream of all the changes that happen to a DynamoDB table. It can be chained with a Lambda function that will be triggered to react to these changes, one of which being a developer’s milestone. DAX is a caching layer </p>
<h3 id="DAX"><a href="#DAX" class="headerlink" title="DAX"></a>DAX</h3><p>DAX will be transparent and won’t require an application refactoring, and will cache the “hot keys”. ElastiCache could also be a solution, but it will require a lot of refactoring work on the AWS Lambda side.</p>
<p>DynamoDB is horizontally scalable, has a DynamoDB streams capability and is multi AZ by default. On top of it, we can adjust the RCU and WCU automatically using Auto Scaling.</p>
<h2 id="DynamoDB’s-partition-key"><a href="#DynamoDB’s-partition-key" class="headerlink" title="DynamoDB’s partition key"></a>DynamoDB’s partition key</h2><p>DynamoDB supports two types of primary keys:</p>
<p>Partition key: A simple primary key, composed of one attribute known as the partition key. Attributes in DynamoDB are similar in many ways to fields or columns in other database systems.<br>Partition key and sort key: Referred to as a composite primary key, this type of key is composed of two attributes. The first attribute is the partition key, and the second attribute is the sort key. Following is an example.</p>
<h3 id="Recommendations-for-partition-keys"><a href="#Recommendations-for-partition-keys" class="headerlink" title="Recommendations for partition keys"></a>Recommendations for partition keys</h3><p>Use high-cardinality attributes. These are attributes that have distinct values for each item, like e-mailid, employee_no, customerid, sessionid, orderid, and so on.</p>
<p>Use composite attributes. Try to combine more than one attribute to form a unique key, if that meets your access pattern. For example, consider an orders table with customerid+productid+countrycode as the partition key and order_date as the sort key.</p>
<p>Cache the popular items when there is a high volume of read traffic using Amazon DynamoDB Accelerator (DAX). The cache acts as a low-pass filter, preventing reads of unusually popular items from swamping partitions. For example, consider a table that has deals information for products. Some deals are expected to be more popular than others during major sale events like Black Friday or Cyber Monday. DAX is a fully managed, in-memory cache for DynamoDB that doesn’t require developers to manage cache invalidation, data population, or cluster management. DAX also is compatible with DynamoDB API calls, so developers can incorporate it more easily into existing applications.</p>
<p>Add random numbers or digits from a predetermined range for write-heavy use cases. Suppose that you expect a large volume of writes for a partition key (for example, greater than 1000 1 K writes per second). In this case, use an additional prefix or suffix (a fixed number from predetermined range, say 1–10) and add it to the partition key.</p>
<h2 id="Aurora"><a href="#Aurora" class="headerlink" title="Aurora"></a>Aurora</h2><p>Aurora Read Replicas can be deployed globally</p>
<h3 id="Aurora-endpoints"><a href="#Aurora-endpoints" class="headerlink" title="Aurora endpoints"></a>Aurora endpoints</h3><p>Amazon Aurora typically involves a cluster of DB instances instead of a single instance. Each connection is handled by a specific DB instance. When you connect to an Aurora cluster, the host name and port that you specify point to an intermediate handler called an endpoint. Aurora uses the endpoint mechanism to abstract these connections. Thus, you don’t have to hardcode all the hostnames or write your own logic for load-balancing and rerouting connections when some DB instances aren’t available.</p>
<p>For certain Aurora tasks, different instances or groups of instances perform different roles. For example, the primary instance handles all data definition language (DDL) and data manipulation language (DML) statements. Up to 15 Aurora Replicas handle read-only query traffic.</p>
<p>Using endpoints, you can map each connection to the appropriate instance or group of instances based on your use case. For example, to perform DDL statements you can connect to whichever instance is the primary instance. To perform queries, you can connect to the reader endpoint, with Aurora automatically performing load-balancing among all the Aurora Replicas. For clusters with DB instances of different capacities or configurations, you can connect to custom endpoints associated with different subsets of DB instances. For diagnosis or tuning, you can connect to a specific instance endpoint to examine details about a specific DB instance.</p>
<h4 id="Types-of-Aurora-Endpoints"><a href="#Types-of-Aurora-Endpoints" class="headerlink" title="Types of Aurora Endpoints"></a>Types of Aurora Endpoints</h4><p>An endpoint is represented as an Aurora-specific URL that contains a host address and a port. The following types of endpoints are available from an Aurora DB cluster.</p>
<h5 id="Cluster-endpoint"><a href="#Cluster-endpoint" class="headerlink" title="Cluster endpoint"></a>Cluster endpoint</h5><p>A cluster endpoint for an Aurora DB cluster that connects to the current primary DB instance for that DB cluster. This endpoint is the only one that can perform write operations such as DDL statements. Because of this, the cluster endpoint is the one that you connect to when you first set up a cluster or when your cluster only contains a single DB instance.<br>Each Aurora DB cluster has one cluster endpoint and one primary DB instance.</p>
<p>You use the cluster endpoint for all write operations on the DB cluster, including inserts, updates, deletes, and DDL changes. You can also use the cluster endpoint for read operations, such as queries.</p>
<p>The cluster endpoint provides failover support for read/write connections to the DB cluster. If the current primary DB instance of a DB cluster fails, Aurora automatically fails over to a new primary DB instance. During a failover, the DB cluster continues to serve connection requests to the cluster endpoint from the new primary DB instance, with minimal interruption of service.</p>
<h5 id="Reader-endpoint"><a href="#Reader-endpoint" class="headerlink" title="Reader endpoint"></a>Reader endpoint</h5><p>A reader endpoint for an Aurora DB cluster connects to one of the available Aurora Replicas for that DB cluster. Each Aurora DB cluster has one reader endpoint. If there is more than one Aurora Replica, the reader endpoint directs each connection request to one of the Aurora Replicas.</p>
<p>The reader endpoint provides load-balancing support for read-only connections to the DB cluster. Use the reader endpoint for read operations, such as queries. You can’t use the reader endpoint for write operations.</p>
<h5 id="Custom-endpoint"><a href="#Custom-endpoint" class="headerlink" title="Custom endpoint"></a>Custom endpoint</h5><p>A custom endpoint for an Aurora cluster represents a set of DB instances that you choose. When you connect to the endpoint, Aurora performs load balancing and chooses one of the instances in the group to handle the connection. You define which instances this endpoint refers to, and you decide what purpose the endpoint serves.</p>
<p>An Aurora DB cluster has no custom endpoints until you create one. You can create up to five custom endpoints for each provisioned Aurora cluster. You can’t use custom endpoints for Aurora Serverless clusters.</p>
<p>The custom endpoint provides load-balanced database connections based on criteria other than the read-only or read-write capability of the DB instances. For example, you might define a custom endpoint to connect to instances that use a particular AWS instance class or a particular DB parameter group. Then you might tell particular groups of users about this custom endpoint. For example, you might direct internal users to low-capacity instances for report generation or ad hoc (one-time) querying, and direct production traffic to high-capacity instances.</p>
<p>Instead of using one DB instance for each specialized purpose and connecting to its instance endpoint, you can have multiple groups of specialized DB instances. In this case, each group has its own custom endpoint. This way, Aurora can perform load balancing among all the instances dedicated to tasks such as reporting or handling production or internal queries. The custom endpoints provide load balancing and high availability for each group of DB instances within your cluster. If one of the DB instances within a group becomes unavailable, Aurora directs subsequent custom endpoint connections to one of the other DB instances associated with the same endpoint.</p>
<h5 id="Instance-endpoint"><a href="#Instance-endpoint" class="headerlink" title="Instance endpoint"></a>Instance endpoint</h5><p>An instance endpoint connects to a specific DB instance within an Aurora cluster. Each DB instance in a DB cluster has its own unique instance endpoint. So there is one instance endpoint for the current primary DB instance of the DB cluster, and there is one instance endpoint for each of the Aurora Replicas in the DB cluster.</p>
<p>The instance endpoint provides direct control over connections to the DB cluster, for scenarios where using the cluster endpoint or reader endpoint might not be appropriate. For example, your client application might require more fine-grained load balancing based on workload type. In this case, you can configure multiple clients to connect to different Aurora Replicas in a DB cluster to distribute read workloads. </p>
<h1 id="Kinesis"><a href="#Kinesis" class="headerlink" title="Kinesis"></a>Kinesis</h1><p>SQS FIFO will not work here as they cannot sustain thousands of messages per second. SNS cannot be used for data streaming. Lambda isn’t meant to retain data. Kinesis is the right answer here, with providing a partition key in our message we can guarantee ordering for a specific sensor, even if our stream is sharded</p>
<h1 id="BeanStalk"><a href="#BeanStalk" class="headerlink" title="BeanStalk"></a>BeanStalk</h1><p>When you create an AWS Elastic Beanstalk environment, you can specify an Amazon Machine Image (AMI) to use instead of the standard Elastic Beanstalk AMI included in your platform version. A custom AMI can improve provisioning times when instances are launched in your environment if you need to install a lot of software that isn’t included in the standard AMIs. Read more here: <a href="https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.customenv.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.customenv.html</a></p>
<h1 id="MQ"><a href="#MQ" class="headerlink" title="MQ"></a>MQ</h1><p>SNS, SQS and Kinesis are AWS’ proprietary technologies and do not come with MQTT compatibility. Here the only possible answer is Amazon MQ</p>
<h1 id="X-Ray"><a href="#X-Ray" class="headerlink" title="X Ray"></a>X Ray</h1><p> AWS X-Ray<br>Analyze and debug production, distributed applications</p>
<p>AWS X-Ray helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture. With X-Ray, you can understand how your application and its underlying services are performing to identify and troubleshoot the root cause of performance issues and errors. X-Ray provides an end-to-end view of requests as they travel through your application, and shows a map of your application’s underlying components. You can use X-Ray to analyze both applications in development and in production, from simple three-tier applications to complex microservices applications consisting of thousands of services.</p>
<h1 id="ElasticCache"><a href="#ElasticCache" class="headerlink" title="ElasticCache"></a>ElasticCache</h1><h2 id="Redis"><a href="#Redis" class="headerlink" title="Redis"></a>Redis</h2><h3 id="What-are-Amazon-ElastiCache-for-Redis-nodes-clusters-and-replications-groups"><a href="#What-are-Amazon-ElastiCache-for-Redis-nodes-clusters-and-replications-groups" class="headerlink" title="What are Amazon ElastiCache for Redis nodes, clusters, and replications groups?"></a>What are Amazon ElastiCache for Redis nodes, clusters, and replications groups?</h3><p>An ElastiCache for Redis node is the smallest building block of an Amazon ElastiCache for Redis deployment. Each ElastiCache for Redis node supports the Redis protocol and has its own DNS name and port. Multiple types of ElastiCache for Redis nodes are supported, each with varying amount of CPU capability, and associated memory. An ElastiCache for Redis node may take on a primary or a read replica role. A primary node can be replicated to multiple read replica nodes. An ElastiCache for Redis cluster is a collection of one or more ElastiCache for Redis nodes of the same role; the primary node will be in the primary cluster and the read replica node will be in a read replica cluster. At this time a cluster can only have one node. In the future, we will increase this limit. A cluster manages a logical key space, where each node is responsible for a part of the key space. Most of your management operations will be performed at the cluster level. An ElastiCache for Redis replication group encapsulates the primary and read replica clusters for a Redis installation. A replication group will have only one primary cluster and zero or many read replica clusters. All nodes within a replication group (and consequently cluster) will be of the same node type, and have the same parameter and security group settings.</p>
<h3 id="Does-Amazon-ElastiCache-for-Redis-support-Multi-AZ-operation"><a href="#Does-Amazon-ElastiCache-for-Redis-support-Multi-AZ-operation" class="headerlink" title="Does Amazon ElastiCache for Redis support Multi-AZ operation?"></a>Does Amazon ElastiCache for Redis support Multi-AZ operation?</h3><p>Yes, with Amazon ElastiCache for Redis you can create a read replica in another AWS Availability Zone. Upon a failure of the primary node, we will provision a new primary node. In scenarios where the primary node cannot be provisioned, you can decide which read replica to promote to be the new primary. </p>
<h3 id="What-is-Multi-AZ-for-an-ElastiCache-for-Redis-replication-group"><a href="#What-is-Multi-AZ-for-an-ElastiCache-for-Redis-replication-group" class="headerlink" title="What is Multi-AZ for an ElastiCache for Redis replication group?"></a>What is Multi-AZ for an ElastiCache for Redis replication group?</h3><p>An ElastiCache for Redis replication group consists of a primary and up to five read replicas. Redis asynchronously replicates the data from the primary to the read replicas. During certain types of planned maintenance, or in the unlikely event of ElastiCache node failure or Availability Zone failure, Amazon ElastiCache will automatically detect the failure of a primary, select a read replica, and promote it to become the new primary. ElastiCache also propagates the DNS changes of the promoted read replica, so if your application is writing to the primary node endpoint, no endpoint change will be needed.</p>
<h3 id="How-can-I-use-encryption-in-transit-at-rest-and-Redis-AUTH"><a href="#How-can-I-use-encryption-in-transit-at-rest-and-Redis-AUTH" class="headerlink" title="How can I use encryption in-transit, at-rest, and Redis AUTH?"></a>How can I use encryption in-transit, at-rest, and Redis AUTH?</h3><p>Encryption in-transit, encryption at-rest, and Redis AUTH are all opt-in features. At the time of Redis cluster creation via the console or command line interface, you can specify if you want to enable encryption and Redis AUTH and can proceed to provide an authentication token for communication with the Redis cluster. Once the cluster is setup with encryption enabled, ElastiCache seamlessly manages certificate expiration and renewal without requiring any additional action from the application. Additionally, the Redis clients need to support TLS to avail of the encrypted in-transit traffic.</p>
<h3 id="Is-Redis-password-functionality-supported-in-Amazon-ElastiCache-for-Redis"><a href="#Is-Redis-password-functionality-supported-in-Amazon-ElastiCache-for-Redis" class="headerlink" title="Is Redis password functionality supported in Amazon ElastiCache for Redis?"></a>Is Redis password functionality supported in Amazon ElastiCache for Redis?</h3><p>Yes, Amazon ElastiCache for Redis supports Redis passwords via Redis AUTH feature. It is an opt-in feature available in ElastiCache for Redis version 3.2.6 onwards. You must enable encryption in-transit to use Redis AUTH on your ElastiCache for Redis cluster.</p>
<h3 id="encryption"><a href="#encryption" class="headerlink" title="encryption"></a>encryption</h3><p>ElastiCache for Redis at-rest encryption is an optional feature to increase data security by encrypting on-disk data. When enabled on a replication group, it encrypts the following aspects:</p>
<p>Disk during sync, backup and swap operations</p>
<p>Backups stored in Amazon S3</p>
<p>ElastiCache for Redis offers default (service managed) encryption at rest, as well as ability to use your own symetric customer managed customer master keys in AWS Key Management Service (KMS).</p>
<h4 id="KMS"><a href="#KMS" class="headerlink" title="KMS"></a>KMS</h4><p>Manage keys used for encrypted DB instances using the AWS KMS. KMS encryption keys are specific to the region that they are created in.</p>
<h1 id="Q-amp-A"><a href="#Q-amp-A" class="headerlink" title="Q&amp;A"></a>Q&amp;A</h1><ul>
<li>Lambda would time out after 15 minutes (2000*3=6000 seconds = 100 minutes). Glue is for performing ETL, but cannot run custom Python scripts. Kinesis Streams is for real time data (here we are in a batch setup), RDS could be used to run SQL queries on the data, but no Python script. The correct answer is EC2</li>
<li>Elastic Beanstalk cannot manage AWS Lambda functions, OpsWorks is for Chef / Puppet, and Trusted Advisor is to get recommendations regarding the 5 pillars of the well architected framework.</li>
<li>Kinesis cannot scale infinitely and we may have the same throughput issues. SNS won’t keep our data if it cannot be delivered, and DAX is used for caching reads, not to help with writes. Here, using SQS as a middleware will help us sustain the write throughput during write peaks</li>
<li>CloudFront is not a good solution here as the content is highly dynamic, and CloudFront will cache things. Dynamic applications cannot be deployed to S3, and Route53 does not help in scaling your application. ASG is the correct answer here</li>
<li>Network Load Balancers expose a fixed IP to the public web, therefore allowing your application to be predictably reached using these IPs, while allowing you to scale your application behind the Network Load Balancer using an ASG. Application and Classic Load Balancers expose a fixed DNS (=URL). Finally, the ASG does not have a dynamic Elastic IPs attachment feature</li>
<li>Hosting the master pack into S3 will require an application code refactor. Upgrading the EC2 instances won’t help save network cost and ELB does not have any caching capability. Here you need to create a CloudFront distribution to add a caching layer in front of your ELB. That caching layer will be very effective as the image pack is a static file, and tremendously save in network cost.</li>
<li>Adding Aurora Read Replicas would greatly increase the cost, switching to a Load Balancer would not improve the problems, and AWS Lambda has no native in memory caching capability. Here, using API Gateway Caching feature is the answer, as we can accept to serve stale data to our users</li>
<li>SQS will allow you to buffer the image compression requests and process them asynchronously. It also has a direct built-in mechanism for retries and scales seamlessly. To process these jobs, due to the unpredictable nature of their volume, and the desire to save on costs, Spot Instances are recommended.</li>
<li>Distributing the static content through S3 allows to offload most of the network usage to S3 and free up our applications running on ECS. EFS will not change anything as static content on EFS would still have to be distributed by our ECS instances</li>
<li>S3 does not work as overwrites are eventually consistent so the latest data will not always be read. Neptune is a graph database so it’s not a good fit, ElastiCache could work but it’s a better fit as a caching technology to enhance reads. Here, the best fit is RDS.</li>
<li>RDS Multi AZ helps with disaster recovery in case of an AZ failure. ElastiCache would definitely help with the read load, but would require a refactor of the application’s core logic. DynamoDB with DAX would also probably help with the read load, but once again it would require a refactor of the application’s core logic. Here, our only option to scale reads is to use RDS Read Replicas</li>
</ul>
<h1 id="Resources"><a href="#Resources" class="headerlink" title="Resources"></a>Resources</h1><ul>
<li>RDS cheatsheet: <a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-relational-database-service-amazon-rds/" target="_blank" rel="noopener">https://tutorialsdojo.com/aws-cheat-sheet-amazon-relational-database-service-amazon-rds/</a></li>
<li>VPC cheatsheet: <a href="https://tutorialsdojo.com/aws-cheat-sheet-amazon-vpc/" target="_blank" rel="noopener">https://tutorialsdojo.com/aws-cheat-sheet-amazon-vpc/</a></li>
</ul>

          
        
      
    </div>

    
    
    
      <footer class="post-footer">
          <div class="post-eof"></div>
        
      </footer>
  </div>
  
  
  
  </article>

    
      

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.todzhang.com/2019-09-12-Conversations-with-God/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Todd Zhang">
      <meta itemprop="description" content="Contact me via phray.zhang@gmail.com or wechat at helloworld_2000">
      <meta itemprop="image" content="/images/globe.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Clouds & Docker">
    </span>
      <header class="post-header">

        
          <h1 class="post-title" itemprop="name headline">
              
              <a href="/2019-09-12-Conversations-with-God/" class="post-title-link" itemprop="url">Conversations with God</a>
            
          </h1>
        

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              
                
              

              <time title="Created: 2019-09-12 08:43:10" itemprop="dateCreated datePublished" datetime="2019-09-12T08:43:10+10:00">2019-09-12</time>
            </span>
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
            <p>Feelings is the language of the soul.<br>If you want to know what’s true for you about something, look to how your’re feeling about.</p>

          
        
      
    </div>

    
    
    
      <footer class="post-footer">
          <div class="post-eof"></div>
        
      </footer>
  </div>
  
  
  
  </article>

    
      

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.todzhang.com/2019-07-07-Kafka/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Todd Zhang">
      <meta itemprop="description" content="Contact me via phray.zhang@gmail.com or wechat at helloworld_2000">
      <meta itemprop="image" content="/images/globe.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Clouds & Docker">
    </span>
      <header class="post-header">

        
          <h1 class="post-title" itemprop="name headline">
              
              <a href="/2019-07-07-Kafka/" class="post-title-link" itemprop="url">Kafka</a>
            
          </h1>
        

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              
                
              

              <time title="Created: 2019-09-11 20:37:44" itemprop="dateCreated datePublished" datetime="2019-09-11T20:37:44+10:00">2019-09-11</time>
            </span>
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
            <h1 id="Kafka"><a href="#Kafka" class="headerlink" title="Kafka"></a>Kafka</h1><p>Kafka is fast. A single node can handle hundreds of read/writes from thousands of clients in real time. Kafka is also distributed and scalable. It creates and takes down nodes in an elastic manner, without incurring any downtime. Data streams are split into partitions and spread over different brokers for capability and redundancy.</p>
<h1 id="History-of-Kafka"><a href="#History-of-Kafka" class="headerlink" title="History of Kafka"></a>History of Kafka</h1><p>The result was a publish/subscribe messaging system that had an interface typical of messaging systems but a storage layer more like a log-aggregation system. Combined with the adoption of Apache Avro for message serialization, Kafka was effective for handling both metrics and user-activity tracking at a scale of billions of messages per day.</p>
<h2 id="Kafka-features"><a href="#Kafka-features" class="headerlink" title="Kafka features"></a>Kafka features</h2><ul>
<li><p>Language Agnostic<br>Producers and consumers use binary protocol to talk to a Kafka cluster.</p>
</li>
<li><p>Durability<br>Kafka does not track which messages were read by each consumer. Kafka keeps all messages for a finite amount of time, and it is consumers’ responsibility to track their location per topic, i.e. offsets.</p>
</li>
</ul>
<h2 id="Terminology"><a href="#Terminology" class="headerlink" title="Terminology:"></a>Terminology:</h2><p>Topic: a feed of messages or packages<br>Partition: group of topics split for scalability and redundancy<br>Producer: process that introduces messages into the queue<br>Consumer: process that subscribes to various topics and processes from a feed of published messages<br>Broker: a node that is part of the Kafka cluster</p>
<h2 id="Topics-and-Partitions"><a href="#Topics-and-Partitions" class="headerlink" title="Topics and Partitions"></a>Topics and Partitions</h2><p>Messages in Kafka are categorized into topics. The closest analogies for a topic are a database table or a folder in a filesystem. Topics are additionally broken down into a number of partitions. Going back to the “commit log” description, a partition is a sin‐ gle log. Messages are written to it in an append-only fashion, and are read in order from beginning to end. Note that as a topic typically has multiple partitions, there is no guarantee of message time-ordering across the entire topic, just within a single partition. </p>
<p>Partitions are also the way that Kafka provides redundancy and scalability. Each partition can be hosted on a different server, which means that a single topic can be scaled horizontally across multiple servers to provide performance far beyond the ability of a single server.</p>
<h1 id="Producers-and-consumers"><a href="#Producers-and-consumers" class="headerlink" title="Producers and consumers"></a>Producers and consumers</h1><p>Kafka clients are users of the system, and there are two basic types: producers and consumers. There are also advanced client APIs—Kafka Connect API for data inte‐ gration and Kafka Streams for stream processing. The advanced clients use producers and consumers as building blocks and provide higher-level functionality on top.</p>
<h2 id="producers"><a href="#producers" class="headerlink" title="producers"></a>producers</h2><p>Producers create new messages. In other publish/subscribe systems, these may be called publishers or writers. In general, a message will be produced to a specific topic. By default, the producer does not care what partition a specific message is written to and will balance messages over all partitions of a topic evenly. In some cases, the pro‐ ducer will direct messages to specific partitions. This is typically done using the mes‐ sage key and a partitioner that will generate a hash of the key and map it to a specific partition. This assures that all messages produced with a given key will get written to the same partition. The producer could also use a custom partitioner that follows other business rules for mapping messages to partitions.</p>
<h2 id="Consumers"><a href="#Consumers" class="headerlink" title="Consumers"></a>Consumers</h2><p>Consumers read messages. In other publish/subscribe systems, these clients may be called subscribers or readers. The consumer subscribes to one or more topics and reads the messages in the order in which they were produced. The consumer keeps track of which messages it has already consumed by keeping track of the offset of messages. The offset is another bit of metadata—an integer value that continually increases—that Kafka adds to each message as it is produced. Each message in a given partition has a unique offset. By storing the offset of the last consumed message for each partition, either in Zookeeper or in Kafka itself, a consumer can stop and restart without losing its place.</p>
<p>Consumers work as part of a consumer group, which is one or more consumers that work together to consume a topic. The group assures that each partition is only con‐ sumed by one member. there are three consumers in a single group consuming a topic. Two of the consumers are working from one partition each, while the third consumer is working from two partitions. The mapping of a consumer to a partition is often called ownership of the partition by the consumer.</p>
<h3 id="Consumer-group"><a href="#Consumer-group" class="headerlink" title="Consumer group"></a>Consumer group</h3><p>Consumers may be grouped in a consumer group with multiple consumers. Each consumer in a consumer group will read messages from a unique subset of partitions in each topic they subscribe to. Each message is delivered to one consumer in the group, and all messages with the same key arrive at the same consumer.</p>
<h2 id="Brokers-and-Clusters"><a href="#Brokers-and-Clusters" class="headerlink" title="Brokers and Clusters"></a>Brokers and Clusters</h2><p>A single Kafka server is called a broker. The broker receives messages from producers, assigns offsets to them, and commits the messages to storage on disk. It also services consumers, responding to fetch requests for partitions and responding with the mes‐ sages that have been committed to disk. Depending on the specific hardware and its performance characteristics, a single broker can easily handle thousands of partitions and millions of messages per second.<br>Kafka brokers are designed to operate as part of a cluster. Within a cluster of brokers, one broker will also function as the cluster controller (elected automatically from the live members of the cluster). The controller is responsible for administrative operations, including assigning partitions to brokers and monitoring for broker failures. A partition is owned by a single broker in the cluster, and that broker is called the leader of the partition. A partition may be assigned to multiple brokers, which will result in the partition being replicated This provides redundancy of messages in the partition, such that another broker can take over leadership if there is a broker failure. However, all consumers and producers operating on that partition must connect to the leader. </p>
<h2 id="retentions"><a href="#retentions" class="headerlink" title="retentions"></a>retentions</h2><p>A key feature of Apache Kafka is that of retention, which is the durable storage of messages for some period of time. Kafka brokers are configured with a default reten‐ tion setting for topics, either retaining messages for some period of time (e.g., 7 days) or until the topic reaches a certain size in bytes (e.g., 1 GB). Once these limits are reached, messages are expired and deleted so that the retention configuration is a minimum amount of data available at any time. Individual topics can also be config‐ ured with their own retention settings so that messages are stored for only as long as they are useful. For example, a tracking topic might be retained for several days, whereas application metrics might be retained for only a few hours. Topics can also be configured as log compacted, which means that Kafka will retain only the last mes‐ sage produced with a specific key. This can be useful for changelog-type data, where only the last update is interesting.</p>
<h2 id="mirror-maker"><a href="#mirror-maker" class="headerlink" title="mirror maker"></a>mirror maker</h2><p>The Kafka project includes a tool called MirrorMaker, used for this purpose. At its core, MirrorMaker is simply a Kafka consumer and producer, linked together with a queue. Messages are consumed from one Kafka cluster and produced for another.</p>
<h1 id="Why-Kafka"><a href="#Why-Kafka" class="headerlink" title="Why Kafka?"></a>Why Kafka?</h1><h2 id="Multiple-Producers"><a href="#Multiple-Producers" class="headerlink" title="Multiple Producers"></a>Multiple Producers</h2><p>Kafka is able to seamlessly handle multiple producers, whether those clients are using many topics or the same topic. </p>
<h2 id="Multiple-Consumers"><a href="#Multiple-Consumers" class="headerlink" title="Multiple Consumers"></a>Multiple Consumers</h2><p>In addition to multiple producers, Kafka is designed for multiple consumers to read any single stream of messages without interfering with each other. This is in contrast to many queuing systems where once a message is consumed by one client, it is not available to any other. Multiple Kafka consumers can choose to operate as part of a group and share a stream, assuring that the entire group processes a given message only once.</p>
<p>##Disk-Based Retention<br>Not only can Kafka handle multiple consumers, but durable message retention means that consumers do not always need to work in real time. Messages are committed to disk, and will be stored with configurable retention rules. </p>
<h2 id="Scalable"><a href="#Scalable" class="headerlink" title="Scalable"></a>Scalable</h2><p>Kafka’s flexible scalability makes it easy to handle any amount of data. Users can start with a single broker as a proof of concept, expand to a small development cluster of three brokers, and move into production with a larger cluster of tens or even hun‐ dreds of brokers that grows over time as the data scales up.</p>
<h2 id="High-Performance"><a href="#High-Performance" class="headerlink" title="High Performance"></a>High Performance</h2><p>All of these features come together to make Apache Kafka a publish/subscribe mes‐ saging system with excellent performance under high load. Producers, consumers, and brokers can all be scaled out to handle very large message streams with ease. This can be done while still providing subsecond message latency from producing a mes‐ sage to availability to consumers.</p>
<h1 id="Process-of-producing-message"><a href="#Process-of-producing-message" class="headerlink" title="Process of producing message"></a>Process of producing message</h1><p>We start producing messages to Kafka by creating a ProducerRecord, which must include the topic we want to send the record to and a value. Optionally, we can also specify a key and/or a partition. Once we send the ProducerRecord, the first thing the producer will do is serialize the key and value objects to ByteArrays so they can be sent over the network.<br>Next, the data is sent to a partitioner. If we specified a partition in the ProducerRecord, the partitioner doesn’t do anything and simply returns the partition we specified. If we didn’t, the partitioner will choose a partition for us, usually based on the ProducerRecord key. Once a partition is selected, the producer knows which topic and partition the record will go to. It then adds the record to a batch of records that will also be sent to the same topic and partition. A separate thread is responsible for sending those batches of records to the appropriate Kafka brokers.<br>When the broker receives the messages, it sends back a response. If the messages were successfully written to Kafka, it will return a RecordMetadata object with the topic, partition, and the offset of the record within the partition. If the broker failed to write the messages, it will return an error. When the producer receives an error, it may retry sending the message a few more times before giving up and returning an error.</p>
<h1 id="Constructing-a-Kafka-Producer"><a href="#Constructing-a-Kafka-Producer" class="headerlink" title="Constructing a Kafka Producer"></a>Constructing a Kafka Producer</h1><p>The first step in writing messages to Kafka is to create a producer object with the properties you want to pass to the producer. A Kafka producer has three mandatory properties:</p>
<h2 id="bootstrap-servers"><a href="#bootstrap-servers" class="headerlink" title="bootstrap.servers"></a>bootstrap.servers</h2><p>List of host:port pairs of brokers that the producer will use to establish initial connection to the Kafka cluster. This list doesn’t need to include all brokers, since the producer will get more information after the initial connection. But it is rec‐ ommended to include at least two, so in case one broker goes down, the producer will still be able to connect to the cluster.</p>
<h2 id="key-serializer"><a href="#key-serializer" class="headerlink" title="key.serializer"></a>key.serializer</h2><p>Name of a class that will be used to serialize the keys of the records we will pro‐ duce to Kafka. Kafka brokers expect byte arrays as keys and values of messages. However, the producer interface allows, using parameterized types, any Java object to be sent as a key and value. This makes for very readable code, but it also means that the producer has to know how to convert these objects to byte arrays. key.serializer should be set to a name of a class that implements the org.apache.kafka.common.serialization.Serializer interface. The producer will use this class to serialize the key object to a byte array. The Kafka client pack‐ age includes ByteArraySerializer (which doesn’t do much), StringSerializer, and IntegerSerializer, so if you use common types, there is no need to implement your own serializers. Setting key.serializer is required even if you intend to send only values.</p>
<h2 id="value-serializer"><a href="#value-serializer" class="headerlink" title="value.serializer"></a>value.serializer</h2><p>Name of a class that will be used to serialize the values of the records we will pro‐ duce to Kafka. The same way you set key.serializer to a name of a class that will serialize the message key object to a byte array, you set value.serializer to a class that will serialize the message value object.</p>
<h2 id="Sample-code-to-generate-producer-record"><a href="#Sample-code-to-generate-producer-record" class="headerlink" title="Sample code to generate producer record"></a>Sample code to generate producer record</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> Properties kafkaProps = <span class="keyword">new</span> Properties();</span><br><span class="line">    kafkaProps.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"broker1:9092,broker2:9092"</span>);</span><br><span class="line">    kafkaProps.put(<span class="string">"key.serializer"</span>,       <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">    kafkaProps.put(<span class="string">"value.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">    producer = <span class="keyword">new</span> KafkaProducer&lt;String, String&gt;(kafkaProps);</span><br></pre></td></tr></table></figure>

<h2 id="Deliver-message"><a href="#Deliver-message" class="headerlink" title="Deliver message"></a>Deliver message</h2><p>Once we instantiate a producer, it is time to start sending messages. There are three primary methods of sending messages:</p>
<h3 id="Fire-and-forget"><a href="#Fire-and-forget" class="headerlink" title="Fire-and-forget"></a>Fire-and-forget</h3><p>We send a message to the server and don’t really care if it arrives succesfully or not. Most of the time, it will arrive successfully, since Kafka is highly available and the producer will retry sending messages automatically. However, some mes‐ sages will get lost using this method.</p>
<h3 id="Synchronous-send"><a href="#Synchronous-send" class="headerlink" title="Synchronous send"></a>Synchronous send</h3><p>We send a message, the send() method returns a Future object, and we use get() to wait on the future and see if the send() was successful or not.</p>
<h3 id="Asynchronous-send"><a href="#Asynchronous-send" class="headerlink" title="Asynchronous send"></a>Asynchronous send</h3><p>We call the send() method with a callback function, which gets triggered when it receives a response from the Kafka broker.</p>
<p>Sample code</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">ProducerRecord&lt;String, String&gt; record =</span><br><span class="line">            <span class="keyword">new</span> ProducerRecord&lt;&gt;(<span class="string">"CustomerCountry"</span>, <span class="string">"Precision Products"</span>,</span><br><span class="line"><span class="string">"France"</span>);</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      producer.send(record);  <span class="comment">//fire and forget</span></span><br><span class="line">      producer.send(record).get(); <span class="comment">// synchronously, calling Future.get()</span></span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>We use the producer object send() method to send the ProducerRecord. As we’ve seen in the producer architecture diagram in Figure 3-1, the message will be placed in a buffer and will be sent to the broker in a separate thread. The send() method returns a Java Future object with RecordMetadata</p>
<h3 id="Samle-code-of-asynchronous"><a href="#Samle-code-of-asynchronous" class="headerlink" title="Samle code of asynchronous"></a>Samle code of asynchronous</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="class"><span class="keyword">class</span> <span class="title">DemoProducerCallback</span> <span class="keyword">implements</span> <span class="title">Callback</span> </span>&#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onCompletion</span><span class="params">(RecordMetadata recordMetadata, Exception e)</span> </span>&#123;</span><br><span class="line">         <span class="keyword">if</span> (e != <span class="keyword">null</span>) &#123;</span><br><span class="line">             e.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">&#125; &#125;</span><br><span class="line">    ProducerRecord&lt;String, String&gt; record =</span><br><span class="line">            <span class="keyword">new</span> ProducerRecord&lt;&gt;(<span class="string">"CustomerCountry"</span>, <span class="string">"Biomedical Materials"</span>, <span class="string">"USA"</span>);</span><br><span class="line">    producer.send(record, <span class="keyword">new</span> DemoProducerCallback());</span><br></pre></td></tr></table></figure>

<h2 id="Rebalancing"><a href="#Rebalancing" class="headerlink" title="Rebalancing"></a>Rebalancing</h2><p>Moving partition ownership from one consumer to another is called a rebalance. Rebalances are important because they provide the consumer group with high availa‐ bility and scalability (allowing us to easily and safely add and remove consumers), but in the normal course of events they are fairly undesirable. During a rebalance, con‐ sumers can’t consume messages, so a rebalance is basically a short window of unavail‐ ability of the entire consumer group. In addition, when partitions are moved from one consumer to another, the consumer loses its current state; if it was caching any data, it will need to refresh its caches—slowing down the application until the con‐ sumer sets up its state again. Throughout this chapter we will discuss how to safely handle rebalances and how to avoid unnecessary ones.</p>
<h2 id="cosumber"><a href="#cosumber" class="headerlink" title="cosumber"></a>cosumber</h2><h3 id="Subscribing-to-Topics"><a href="#Subscribing-to-Topics" class="headerlink" title="Subscribing to Topics"></a>Subscribing to Topics</h3><p>Once we create a consumer, the next step is to subscribe to one or more topics. The subcribe() method takes a list of topics as a parameter, so it’s pretty simple to use:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">consumer.subscribe(Collections.singletonList(<span class="string">"customerCountries"</span>));</span><br></pre></td></tr></table></figure>

<p>Here we simply create a list with a single element: the topic name customerCountries.</p>
<h3 id="Sample-consumer-code"><a href="#Sample-consumer-code" class="headerlink" title="Sample consumer code"></a>Sample consumer code</h3><p>The Poll Loop<br>At the heart of the consumer API is a simple loop for polling the server for more data. Once the consumer subscribes to topics, the poll loop handles all details of coordina‐ tion, partition rebalances, heartbeats, and data fetching, leaving the developer with a clean API that simply returns available data from the assigned partitions. The main body of a consumer will look as follows:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">          ConsumerRecords&lt;String, String&gt; records = consumer.poll(<span class="number">100</span>);</span><br><span class="line">          <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records)</span><br><span class="line">          &#123;</span><br><span class="line">              log.debug(<span class="string">"topic = %s, partition = %s, offset = %d,</span></span><br><span class="line"><span class="string">                 customer = %s, country = %s\n"</span>,</span><br><span class="line">                 record.topic(), record.partition(), record.offset(),</span><br><span class="line">                 record.key(), record.value());</span><br><span class="line">              <span class="keyword">int</span> updatedCount = <span class="number">1</span>;</span><br><span class="line">              <span class="keyword">if</span> (custCountryMap.countainsValue(record.value())) &#123;</span><br><span class="line">                  updatedCount = custCountryMap.get(record.value()) + <span class="number">1</span>;</span><br><span class="line">              &#125;</span><br><span class="line">              custCountryMap.put(record.value(), updatedCount)</span><br><span class="line">              JSONObject json = <span class="keyword">new</span> JSONObject(custCountryMap);</span><br><span class="line">              System.out.println(json.toString(<span class="number">4</span>))</span><br><span class="line">          &#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      consumer.close();</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<h3 id="Thread-safety"><a href="#Thread-safety" class="headerlink" title="Thread safety"></a>Thread safety</h3><p>Thread Safety<br>You can’t have multiple consumers that belong to the same group in one thread and you can’t have multiple threads safely use the same consumer. One consumer per thread is the rule. To run mul‐ tiple consumers in the same group in one application, you will need to run each in its own thread. It is useful to wrap the con‐ sumer logic in its own object and then use Java’s ExecutorService to start multiple threads each with its own consumer. </p>
<h2 id="Commit"><a href="#Commit" class="headerlink" title="Commit"></a>Commit</h2><p>As discussed before, one of Kafka’s unique characteristics is that it does not track acknowledgments from consumers the way many JMS queues do. Instead, it allows consumers to use Kafka to track their posi‐ tion (offset) in each partition.<br>We call the action of updating the current position in the partition a commit.</p>
<p>How does a consumer commit an offset? It produces a message to Kafka, to a special <code>__consumer_offsets</code> topic, with the committed offset for each partition. As long as all your consumers are up, running, and churning away, this will have no impact. However, if a consumer crashes or a new consumer joins the consumer group, this will trigger a rebalance. After a rebalance, each consumer may be assigned a new set of partitions than the one it processed before. In order to know where to pick up the work, the consumer will read the latest committed offset of each partition and con‐ tinue from there.</p>
<h3 id="Automatic-Commit"><a href="#Automatic-Commit" class="headerlink" title="Automatic Commit"></a>Automatic Commit</h3><p>With autocommit enabled, a call to poll will always commit the last offset returned by the previous poll. It doesn’t know which events were actually processed, so it is critical to always process all the events returned by poll() before calling poll() again. (Just like poll(), close() also commits offsets automatically.) This is usually not an issue, but pay attention when you handle exceptions or exit the poll loop prematurely.</p>
<h3 id="Manual-commit"><a href="#Manual-commit" class="headerlink" title="Manual commit"></a>Manual commit</h3><p>It is important to remember that commitSync() will commit the latest offset returned by poll(), so make sure you call commitSync() after you are done processing all the records in the collection, or you risk missing messages as described previously. When rebalance is triggered, all the messages from the beginning of the most recent batch until the time of the rebalance will be processed twice.<br>Here is how we would use commitSync to commit offsets after we finished processing the latest batch of messages:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">        ConsumerRecords&lt;String, String&gt; records = consumer.poll(<span class="number">100</span>);</span><br><span class="line">        <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records)</span><br><span class="line">        &#123;</span><br><span class="line">            System.out.printf(<span class="string">"topic = %s, partition = %s, offset =</span></span><br><span class="line"><span class="string">              %d, customer = %s, country = %s\n"</span>,</span><br><span class="line">                 record.topic(), record.partition(),</span><br><span class="line">                 record.offset(), record.key(), record.value());</span><br><span class="line">&#125; <span class="keyword">try</span> &#123;</span><br><span class="line">          consumer.commitSync();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (CommitFailedException e) &#123;</span><br><span class="line">            log.error(<span class="string">"commit failed"</span>, e)</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="Combining-Synchronous-and-Asynchronous-Commits"><a href="#Combining-Synchronous-and-Asynchronous-Commits" class="headerlink" title="Combining Synchronous and Asynchronous Commits"></a>Combining Synchronous and Asynchronous Commits</h2><p>Normally, occasional failures to commit without retrying are not a huge problem because if the problem is temporary, the following commit will be successful. But if we know that this is the last commit before we close the consumer, or before a reba‐ lance, we want to make extra sure that the commit succeeds.<br>Therefore, a common pattern is to combine commitAsync() with commitSync() just before shutdown. Here is how it works (we will discuss how to commit just before rebalance when we get to the section about rebalance listeners):</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">            ConsumerRecords&lt;String, String&gt; records = consumer.poll(<span class="number">100</span>);</span><br><span class="line">            <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line">                System.out.printf(<span class="string">"topic = %s, partition = %s, offset = %d,</span></span><br><span class="line"><span class="string">                customer = %s, country = %s\n"</span>,</span><br><span class="line">                record.topic(), record.partition(),</span><br><span class="line">                record.offset(), record.key(), record.value());</span><br><span class="line">&#125;</span><br><span class="line">            consumer.commitAsync();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">        log.error(<span class="string">"Unexpected error"</span>, e);</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            consumer.commitSync();</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">            consumer.close();</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="Exit"><a href="#Exit" class="headerlink" title="Exit"></a>Exit</h2><p>When you decide to exit the poll loop, you will need another thread to call con sumer.wakeup(). If you are running the consumer loop in the main thread, this can be done from ShutdownHook. <code>Note that consumer.wakeup() is the only consumer method that is safe to call from a different thread.</code> Calling wakeup will cause poll() to exit with WakeupException, or if consumer.wakeup() was called while the thread was not waiting on poll, the exception will be thrown on the next iteration when poll() is called. </p>
<h1 id="The-Controller"><a href="#The-Controller" class="headerlink" title="The Controller"></a>The Controller</h1><p>The controller is one of the Kafka brokers that, in addition to the usual broker func‐ tionality, is responsible for electing partition leaders (we’ll discuss partition leaders and what they do in the next section). The first broker that starts in the cluster becomes the controller by creating an ephemeral node in ZooKeeper called /control ler. When other brokers start, they also try to create this node, but receive a “node already exists” exception, which causes them to “realize” that the controller node already exists and that the cluster already has a controller. </p>
<h1 id="Replication"><a href="#Replication" class="headerlink" title="Replication"></a>Replication</h1><p>Replication is at the heart of Kafka’s architecture. The very first sentence in Kafka’s documentation describes it as “a distributed, partitioned, replicated commit log ser‐ vice.” Replication is critical because it is the way Kafka guarantees availability and durability when individual nodes inevitably fail.</p>
<h2 id="Memeroy"><a href="#Memeroy" class="headerlink" title="Memeroy"></a>Memeroy</h2><p>Kafka should run entirely on RAM. JVM heap size shouldn’t be bigger than your available RAM. That is to avoid swapping.</p>
<h3 id="Swap-usage"><a href="#Swap-usage" class="headerlink" title="Swap usage"></a>Swap usage</h3><p>Watch for swap usage, as it will degrade performance on Kafka and lead to operations timing out (set vm.swappiness = 0).    When used swap is &gt; 128MB.</p>
<h1 id="Kafka-Monitoring-Tools"><a href="#Kafka-Monitoring-Tools" class="headerlink" title="Kafka Monitoring Tools"></a>Kafka Monitoring Tools</h1><p>Any monitoring tools with JMX support should be able to monitor a Kafka cluster. Here are 3 monitoring tools we liked:</p>
<p>First one is check_kafka.pl from Hari Sekhon. It performs a complete end to end test, i.e. it inserts a message in Kafka as a producer and then extracts it as a consumer. This makes our life easier when measuring service times.</p>
<p>Another useful tool is KafkaOffsetMonitor for monitoring Kafka consumers and their position (offset) in the queue. It aids our understanding of how our queue grows and which consumers groups are lagging behind.</p>
<p>Last but not least, the LinkedIn folks have developed what we think is the smartest tool out there: Burrow. It analyzes consumer offsets and lags over a window of time and determines the consumer status. You can retrieve this status over an HTTP endpoint and then plug it into your favourite monitoring tool (Server Density for example).</p>
<p>Oh, and we would be amiss if we didn’t mention Yahoo’s Kafka-Manager. While it does include some basic monitoring, it is more of a management tool. If you are just looking for a Kafka management tool, check out AirBnb’s kafkat.</p>
<h1 id="commands"><a href="#commands" class="headerlink" title="commands"></a>commands</h1><h2 id="Start-zookeeper"><a href="#Start-zookeeper" class="headerlink" title="Start zookeeper"></a>Start zookeeper</h2><p>bin/zookeeper-server-start.sh config/zookeeper.properties</p>
<p>bin/kafka-server-start.sh config/server.properties</p>
<p>~/dev/git/kafka-demo/kafka_2.11-2.0.0/bin/kafka-topics.sh –create –zookeeper localhost:2181 –replication-factor 1 –partitions 1 –topic todtest<br> bin/kafka-topics.sh –list –zookeeper localhost:2181<br>bin/kafka-console-producer.sh –broker-list localhost:9092 –topic todtest<br>bin/kafka-console-consumer.sh –bootstrap-server localhost:9092 –topic todtest –from-beginning</p>
<p>bin/kafka-topics.sh –describe –zookeeper localhost:2181 –topic test</p>
<h2 id="list-topics"><a href="#list-topics" class="headerlink" title="list topics"></a>list topics</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./kafka-topics.sh --list --zookeeper localhost:2181</span><br></pre></td></tr></table></figure>

<h2 id="describe-topics"><a href="#describe-topics" class="headerlink" title="describe topics"></a>describe topics</h2><p>./kafka-topics.sh –describe –zookeeper localhost:2181</p>
<h3 id="using-connector"><a href="#using-connector" class="headerlink" title="using connector"></a>using connector</h3><p>bin/connect-standalone.sh config/connect-standalone.properties config/connect-file-source.properties config/connect-file-sink.properties</p>
<p>mvn archetype:generate <br>    -DarchetypeGroupId=org.apache.kafka <br>    -DarchetypeArtifactId=streams-quickstart-java <br>    -DarchetypeVersion=2.0.0 <br>    -DgroupId=io <br>    -DartifactId=todzhang <br>    -Dversion=0.1 <br>    -Dpackage=todzhangapp</p>
<h1 id="Security"><a href="#Security" class="headerlink" title="Security"></a>Security</h1><p>The keystore stores each machine’s own identity. The truststore stores all the certificates that the machine should trust. Importing a certificate into one’s truststore also means trusting all certificates that are signed by that certificate. As the analogy above, trusting the government (CA) also means trusting all passports (certificates) that it has issued. This attribute is called the chain of trust, and it is particularly useful when deploying SSL on a large Kafka cluster. You can sign all certificates in the cluster with a single CA, and have all machines share the same truststore that trusts the CA. That way all machines can authenticate all other machines.</p>
<p>To deploy SSL, the general steps are:</p>
<ul>
<li>Generate the keys and certificates</li>
<li>Create your own Certificate Authority (CA)</li>
<li>Sign the certificate</li>
</ul>
<p>Generate the key and the certificate for each Kafka broker in the cluster. Generate the key into a keystore called kafka.server.keystore so that you can export and sign it later with CA. The keystore file contains the private key of the certificate; therefore, it needs to be kept safely.</p>
<h2 id="With-user-prompts"><a href="#With-user-prompts" class="headerlink" title="With user prompts"></a>With user prompts</h2><p>keytool -keystore kafka.server.keystore.jks -alias localhost -genkey</p>
<h2 id="Without-user-prompts-pass-command-line-arguments"><a href="#Without-user-prompts-pass-command-line-arguments" class="headerlink" title="Without user prompts, pass command line arguments"></a>Without user prompts, pass command line arguments</h2><p>keytool -keystore kafka.server.keystore.jks -alias localhost -validity {validity} -genkey -storepass {keystore-pass} -keypass {key-pass} -dname {distinguished-name} -ext SAN=DNS:{hostname}<br>Ensure that the common name (CN) exactly matches the fully qualified domain name (FQDN) of the server. The client compares the CN with the DNS domain name to ensure that it is indeed connecting to the desired server, not a malicious one. The hostname of the server can also be specified in the Subject Alternative Name (SAN). Since the distinguished name is used as the server principal when SSL is used as the inter-broker security protocol, it is useful to have hostname as a SAN rather than the CN.</p>
<h2 id="Create-your-own-Certificate-Authority-CA"><a href="#Create-your-own-Certificate-Authority-CA" class="headerlink" title="Create your own Certificate Authority (CA)"></a>Create your own Certificate Authority (CA)</h2><p>Generate a CA that is simply a public-private key pair and certificate, and it is intended to sign other certificates.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openssl req -new -x509 -keyout ca-key -out ca-cert -days &#123;validity&#125;</span><br></pre></td></tr></table></figure>

<p>Add the generated CA to the clients’ truststore so that the clients can trust this CA:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keytool -keystore kafka.client.truststore.jks -<span class="built_in">alias</span> CARoot -import -file ca-cert</span><br></pre></td></tr></table></figure>

<p>Add the generated CA to the brokers’ truststore so that the brokers can trust this CA.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keytool -keystore kafka.server.truststore.jks -<span class="built_in">alias</span> CARoot -import -file ca-cert</span><br></pre></td></tr></table></figure>

<h2 id="Sign-the-certificate"><a href="#Sign-the-certificate" class="headerlink" title="Sign the certificate"></a>Sign the certificate</h2><p>To sign all certificates in the keystore with the CA that you generated:</p>
<p>Export the certificate from the keystore:</p>
<p>keytool -keystore kafka.server.keystore.jks -alias localhost -certreq -file cert-file<br>Sign it with the CA:</p>
<p>openssl x509 -req -CA ca-cert -CAkey ca-key -in cert-file -out cert-signed -days {validity} -CAcreateserial -passin pass:{ca-password}<br>Import both the certificate of the CA and the signed certificate into the broker keystore:</p>
<p>keytool -keystore kafka.server.keystore.jks -alias CARoot -import -file ca-cert<br>keytool -keystore kafka.server.keystore.jks -alias localhost -import -file cert-signed</p>
<h2 id="SASL"><a href="#SASL" class="headerlink" title="SASL"></a>SASL</h2><p>Simple Authentication and Security Layer (SASL) is a framework for authentication and data security in Internet protocols. It decouples authentication mechanisms from application protocols, in theory allowing any authentication mechanism supported by SASL to be used in any application protocol that uses SASL. Authentication mechanisms can also support proxy authorization, a facility allowing one user to assume the identity of another. They can also provide a data security layer offering data integrity and data confidentiality services. DIGEST-MD5 provides an example of mechanisms which can provide a data-security layer. Application protocols that support SASL typically also support Transport Layer Security (TLS) to complement the services offered by SASL.</p>
<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul>
<li><a href="https://blog.serverdensity.com/how-to-monitor-kafka/" target="_blank" rel="noopener">https://blog.serverdensity.com/how-to-monitor-kafka/</a></li>
</ul>

          
        
      
    </div>

    
    
    
      <footer class="post-footer">
          <div class="post-eof"></div>
        
      </footer>
  </div>
  
  
  
  </article>

    
      

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.todzhang.com/2019-09-02-Kafka-In-Spring/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Todd Zhang">
      <meta itemprop="description" content="Contact me via phray.zhang@gmail.com or wechat at helloworld_2000">
      <meta itemprop="image" content="/images/globe.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Clouds & Docker">
    </span>
      <header class="post-header">

        
          <h1 class="post-title" itemprop="name headline">
              
              <a href="/2019-09-02-Kafka-In-Spring/" class="post-title-link" itemprop="url">Kafka In Spring</a>
            
          </h1>
        

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              
                
              

              <time title="Created: 2019-09-03 20:40:06" itemprop="dateCreated datePublished" datetime="2019-09-03T20:40:06+10:00">2019-09-03</time>
            </span>
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
            <p>Enable Kafka listener annotated endpoints that are created under the covers by a AbstractListenerContainerFactory. To be used on Configuration classes as follows:<br>   @Configuration<br>   @EnableKafka<br>   public class AppConfig {<br>       @Bean<br>       public ConcurrentKafkaListenerContainerFactory myKafkaListenerContainerFactory() {<br>           ConcurrentKafkaListenerContainerFactory factory = new ConcurrentKafkaListenerContainerFactory();<br>           factory.setConsumerFactory(consumerFactory());<br>           factory.setConcurrency(4);<br>           return factory;<br>       }<br>       // other @Bean definitions<br>   }</p>
<p>The KafkaListenerContainerFactory is responsible to create the listener container for a particular endpoint. Typical implementations, as the ConcurrentKafkaListenerContainerFactory used in the sample above, provides the necessary configuration options that are supported by the underlying MessageListenerContainer.<br>@EnableKafka enables detection of KafkaListener annotations on any Spring-managed bean in the container. For example, given a class MyService:<br>   package com.acme.foo;</p>
<p>   public class MyService {<br>       @KafkaListener(containerFactory = “myKafkaListenerContainerFactory”, topics = “myTopic”)<br>       public void process(String msg) {<br>           // process incoming message<br>       }<br>   }</p>
<p>The container factory to use is identified by the containerFactory attribute defining the name of the KafkaListenerContainerFactory bean to use. When none is set a KafkaListenerContainerFactory bean with name kafkaListenerContainerFactory is assumed to be present.<br>the following configuration would ensure that every time a message is received from topic “myQueue”, MyService.process() is called with the content of the message:<br>   @Configuration<br>   @EnableKafka<br>   public class AppConfig {<br>       @Bean<br>       public MyService myService() {<br>           return new MyService();<br>       }</p>
<pre><code>// Kafka infrastructure setup</code></pre><p>   }</p>
<p>Alternatively, if MyService were annotated with @Component, the following configuration would ensure that its @KafkaListener annotated method is invoked with a matching incoming message:<br>   @Configuration<br>   @EnableKafka<br>   @ComponentScan(basePackages = “com.acme.foo”)<br>   public class AppConfig {<br>   }</p>
<p>Note that the created containers are not registered with the application context but can be easily located for management purposes using the KafkaListenerEndpointRegistry.<br>Annotated methods can use a flexible signature; in particular, it is possible to use the Message abstraction and related annotations, see KafkaListener Javadoc for more details. For instance, the following would inject the content of the message and the kafka partition header:<br>   @KafkaListener(containerFactory = “myKafkaListenerContainerFactory”, topics = “myTopic”)<br>   public void process(String msg, @Header(“kafka_partition”) int partition) {<br>       // process incoming message<br>   }</p>
<p>These features are abstracted by the MessageHandlerMethodFactory that is responsible to build the necessary invoker to process the annotated method. By default, DefaultMessageHandlerMethodFactory is used.<br>When more control is desired, a @Configuration class may implement KafkaListenerConfigurer. This allows access to the underlying KafkaListenerEndpointRegistrar instance. The following example demonstrates how to specify an explicit default KafkaListenerContainerFactory<br>   {<br>       @code<br>       @Configuration<br>       @EnableKafka<br>       public class AppConfig implements KafkaListenerConfigurer {<br>           @Override<br>           public void configureKafkaListeners(KafkaListenerEndpointRegistrar registrar) {<br>               registrar.setContainerFactory(myKafkaListenerContainerFactory());<br>           }</p>
<pre><code>    @Bean
    public KafkaListenerContainerFactory&lt;?, ?&gt; myKafkaListenerContainerFactory() {
        // factory settings
    }

    @Bean
    public MyService myService() {
        return new MyService();
    }
}</code></pre><p>   }</p>
<p>It is also possible to specify a custom KafkaListenerEndpointRegistry in case you need more control on the way the containers are created and managed. The example below also demonstrates how to customize the org.springframework.messaging.handler.annotation.support.DefaultMessageHandlerMethodFactory as well as how to supply a custom Validator so that payloads annotated with Validated are first validated against a custom Validator.<br>   {<br>       @code<br>       @Configuration<br>       @EnableKafka<br>       public class AppConfig implements KafkaListenerConfigurer {<br>           @Override<br>           public void configureKafkaListeners(KafkaListenerEndpointRegistrar registrar) {<br>               registrar.setEndpointRegistry(myKafkaListenerEndpointRegistry());<br>               registrar.setMessageHandlerMethodFactory(myMessageHandlerMethodFactory);<br>               registrar.setValidator(new MyValidator());<br>           }</p>
<pre><code>    @Bean
    public KafkaListenerEndpointRegistry myKafkaListenerEndpointRegistry() {
        // registry configuration
    }

    @Bean
    public MessageHandlerMethodFactory myMessageHandlerMethodFactory() {
        DefaultMessageHandlerMethodFactory factory = new DefaultMessageHandlerMethodFactory();
        // factory configuration
        return factory;
    }

    @Bean
    public MyService myService() {
        return new MyService();
    }
}</code></pre><p>   }</p>
<p>Implementing KafkaListenerConfigurer also allows for fine-grained control over endpoints registration via the KafkaListenerEndpointRegistrar. For example, the following configures an extra endpoint:<br>   {<br>       @code<br>       @Configuration<br>       @EnableKafka<br>       public class AppConfig implements KafkaListenerConfigurer {<br>           @Override<br>           public void configureKafkaListeners(KafkaListenerEndpointRegistrar registrar) {<br>               SimpleKafkaListenerEndpoint myEndpoint = new SimpleKafkaListenerEndpoint();<br>               // … configure the endpoint<br>               registrar.registerEndpoint(endpoint, anotherKafkaListenerContainerFactory());<br>           }</p>
<pre><code>    @Bean
    public MyService myService() {
        return new MyService();
    }

    @Bean
    public KafkaListenerContainerFactory&lt;?, ?&gt; anotherKafkaListenerContainerFactory() {
        // ...
    }

    // Kafka infrastructure setup
}</code></pre><p>   }</p>
<p>Note that all beans implementing KafkaListenerConfigurer will be detected and invoked in a similar fashion. The example above can be translated in a regular bean definition registered in the context in case you use the XML configuration.<br>See Also:<br>KafkaListener, KafkaListenerAnnotationBeanPostProcessor, org.springframework.kafka.config.KafkaListenerEndpointRegistrar, org.springframework.kafka.config.KafkaListenerEndpointRegistry<br>  spring-kafka-dist.spring-kafka.main</p>
<h1 id="flush"><a href="#flush" class="headerlink" title="flush"></a>flush</h1><p>If you wish to block the sending thread, to await the result, you can invoke the future’s get() method. You may wish to invoke flush() before waiting or, for convenience, the template has a constructor with an autoFlush parameter which will cause the template to flush() on each send. Note, however that flushing will likely significantly reduce performance.</p>
<h2 id="Non-Blocking-Async"><a href="#Non-Blocking-Async" class="headerlink" title="Non Blocking (Async)."></a>Non Blocking (Async).</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">sendToKafka</span><span class="params">(<span class="keyword">final</span> MyOutputData data)</span> </span>&#123;</span><br><span class="line"><span class="keyword">final</span> ProducerRecord&lt;String, String&gt; record = createRecord(data);</span><br><span class="line">ListenableFuture&lt;SendResult&lt;Integer, String&gt;&gt; future = template.send(record); future.addCallback(<span class="keyword">new</span> ListenableFutureCallback&lt;SendResult&lt;Integer, String&gt;&gt;() &#123;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onSuccess</span><span class="params">(SendResult&lt;Integer, String&gt; result)</span> </span>&#123; handleSuccess(data);</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onFailure</span><span class="params">(Throwable ex)</span> </span>&#123; handleFailure(data, record, ex);</span><br><span class="line">&#125;</span><br><span class="line">&#125;); &#125;</span><br></pre></td></tr></table></figure>

<h2 id="Blocking-Sync"><a href="#Blocking-Sync" class="headerlink" title="Blocking (Sync)."></a>Blocking (Sync).</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">sendToKafka</span><span class="params">(<span class="keyword">final</span> MyOutputData data)</span> </span>&#123;</span><br><span class="line"><span class="keyword">final</span> ProducerRecord&lt;String, String&gt; record = createRecord(data);</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">template.send(record).get(<span class="number">10</span>, TimeUnit.SECONDS); handleSuccess(data);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">catch</span> (ExecutionException e) &#123;</span><br><span class="line">        handleFailure(data, record, e.getCause());</span><br><span class="line">    &#125;</span><br><span class="line"><span class="keyword">catch</span> (TimeoutException | InterruptedException e) &#123; handleFailure(data, record, e);</span><br><span class="line">&#125; &#125;</span><br></pre></td></tr></table></figure>

<h2 id="KafkaTransactionManager"><a href="#KafkaTransactionManager" class="headerlink" title="KafkaTransactionManager"></a>KafkaTransactionManager</h2><p>The <code>KafkaTransactionManager</code> is an implementation of Spring Framework’s <code>PlatformTransactionManager</code>;</p>

          
        
      
    </div>

    
    
    
      <footer class="post-footer">
          <div class="post-eof"></div>
        
      </footer>
  </div>
  
  
  
  </article>

    
      

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.todzhang.com/2019-02-26-TLS-SSL-HTTPS/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Todd Zhang">
      <meta itemprop="description" content="Contact me via phray.zhang@gmail.com or wechat at helloworld_2000">
      <meta itemprop="image" content="/images/globe.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Clouds & Docker">
    </span>
      <header class="post-header">

        
          <h1 class="post-title" itemprop="name headline">
              
              <a href="/2019-02-26-TLS-SSL-HTTPS/" class="post-title-link" itemprop="url">SSL certificates</a>
            
          </h1>
        

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              
                
              

              <time title="Created: 2019-08-15 20:37:12" itemprop="dateCreated datePublished" datetime="2019-08-15T20:37:12+10:00">2019-08-15</time>
            </span>
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
            <h1 id="What’s-TLS"><a href="#What’s-TLS" class="headerlink" title="What’s TLS"></a>What’s TLS</h1><p>TLS (Transport Layer Security) and its predecessor, SSL (Secure Sockets Layer), are security protocols designed to secure the communication between a server and a client, for example, a web server and a browser. Both protocols are frequently referred to as SSL.</p>
<p>A TLS/SSL certificate (simply called SSL certificate) is required to enable SSL/TLS on your site and serve your website using the secure HTTPS protocol.</p>
<p>We offer different types of domain-validated SSL certificates signed by globally recognized certificate authorities.</p>
<h1 id="CA"><a href="#CA" class="headerlink" title="CA"></a>CA</h1><p>A Certificate Authority (CA) (or Certification Authority) is an entity that issues digital certificates.</p>
<p>The digital certificate <em>certifies the ownership</em> of a public key by the named subject of the certificate. This allows others (relying parties) to rely upon signatures or assertions made by the private key that corresponds to the public key that is certified.</p>
<h1 id="Root-certificate"><a href="#Root-certificate" class="headerlink" title="Root certificate"></a>Root certificate</h1><p>In the SSL ecosystem, anyone can generate a signing key and sign a new certificate with that signature. However, that certificate is not considered valid unless it has been directly or indirectly signed by a trusted CA.</p>
<p>A trusted certificate authority is an entity that has been entitled to verify that someone is effectively who it declares to be. In order for this model to work, all the participants on the game must agree on a set of CA which they trust. All operating systems and most of web browsers ship with a set of trusted CAs.</p>
<p>The SSL ecosystem is based on a * model of trust relationship<em>, also called *</em> “chain of trust” **. When a device validates a certificate, it compares the certificate issuer with the list of trusted CAs. If a match is not found, the client will then check to see if the certificate of the issuing CA was issued by a trusted CA, and so on until the end of the certificate chain. The top of the chain, the root certificate, must be issued by a trusted Certificate Authority.</p>
<h2 id="Tips"><a href="#Tips" class="headerlink" title="Tips"></a>Tips</h2><p>The root certificate is generally embedded in your connected device. In the case of web browsers, root certificates are packaged with the browser software.</p>
<h3 id="To-install-the-Intermediate-SSL-certificates"><a href="#To-install-the-Intermediate-SSL-certificates" class="headerlink" title="To  install the Intermediate SSL certificates?"></a>To  install the Intermediate SSL certificates?</h3><p>The procedure to install the Intermediate SSL certificates depends on the web server and the environment where you install the certificate.</p>
<p>For instance, Apache requires you to bundle the intermediate SSL certificates and assign the location of the bundle to the SSLCertificateChainFile configuration. Conversely, NGINX requires you to package the intermediate SSL certificates in a single bundle with the end-user certificate.</p>
<h1 id="SSL-certificate-chain"><a href="#SSL-certificate-chain" class="headerlink" title="SSL certificate chain"></a>SSL certificate chain</h1><p>There are two types of certificate authorities (CAs): root CAs and intermediate CAs. In order for an SSL certificate to be trusted, that certificate must have been issued by a CA that is included in the trusted store of the device that is connecting.</p>
<p>In this model of trust relationships, a CA is a trusted third party that is trusted by both the subject (owner) of the certificate and the party relying upon the certificate.</p>
<p>In the context of a website, when we use the term digital certificate we often refer to SSL certificates. The CA is the authority responsible for issuing SSL certificates publicly trusted by web browsers.</p>
<p>Anyone can issue SSL certificates, but those certificates would not be trusted automatically by web browsers. Certificates such as these are called self-signed. The CA has the responsibility to validate the entity behind an SSL certificate request and, upon successful validation, the ability to issue publicly trusted SSL certificates that will be accepted by web browsers. Essentially, the browser vendors rely on CAs to validate the entity behind a web site.</p>
<h1 id="How-SSL-work-in-browser"><a href="#How-SSL-work-in-browser" class="headerlink" title="How SSL work in browser"></a>How SSL work in browser</h1><p>There are 3 essential elements at work in the process described above: a protocol for communications (SSL), credentials for establishing identity (the SSL certificate), and a third party that vouches for the credentials (the certificate authority). </p>
<pre><code>Computers use protocols to allow different systems to work together. Web servers and web browsers rely on the Secure Sockets Layer (SSL) protocol to enable encrypted communications. The browser’s request that the server identify itself is a function of the SSL protocol.
Credentials for establishing identity are common to our everyday lives: a driver’s license, a passport, a company badge. An SSL certificate is a type of digital certificate that serves as a credential in the online world. Each SSL certificate uniquely identifies a specific domain (such as thawte.com) and a web server.
Our trust of a credential depends on our confidence in the organization that issued it. Certificate authorities have a variety of methods to verify information provided by individuals or organizations. Established certificate authorities, such as Thawte, are well known and trusted by browser vendors. Browsers extend that trust to digital certificates that are verified by the certificate authority.</code></pre><h2 id="PKI"><a href="#PKI" class="headerlink" title="PKI"></a>PKI</h2><p>You are correct that SSL uses an asymmetric key pair. One public and one private key is generated which also known as public key infrastructure (PKI). The public key is what is distributed to the world, and is used to encrypt the data. Only the private key can actually decrypt the data though.</p>
<blockquote>
<p>Say we both go to walmart.com and buy stuff. Each of us get a copy of Walmart’s public key to sign our transaction with. Once the transaction is signed by Walmart’s public key, only Walmart’s private key can decrypt the transaction. If I use my copy of Walmart’s public key, it will not decrypt your transaction. Walmart must keep their private key very private and secure, else anyone who gets it can decrypt transactions to Walmart. This is why the DigiNotar breach was such a big deal</p>
</blockquote>
<h1 id="A-sample-of-how-browser-get-SSL-certificate"><a href="#A-sample-of-how-browser-get-SSL-certificate" class="headerlink" title="A sample of how browser get SSL certificate"></a>A sample of how browser get SSL certificate</h1><p>If I get an SSL certificate from a well-known provider, what does that prove about my site and how?</p>
<p>Here’s what I know:</p>
<pre><code>Assume Alice and Bob both have public and private keys
If Alice encrypts something with Bob&apos;s public key, she ensures that only Bob can decrypt it (using his private key)
If Alice encrypts something with her own private key, anyone can decrypt it (using her public key), but they will know that it was encrypted by her
Therefore, if Alice encrypts a message first with her own private key, then with Bob&apos;s public key, she will ensure that only Bob can decrypt it and that Bob will know the message is from her.</code></pre><p>Regarding certificates, here’s what I think happens (updated):</p>
<pre><code>I generate a request for a certificate. In that request, I put my public key and a bunch of information about myself.
The certificate issuer (in theory) checks me out to make sure it knows who I am: talks to me in person, sees my driver&apos;s license, retina scan, or whatever.
If they&apos;re satisfied, the certificate issuer then encrypts my request with their private key. Anyone who decrypts it with their public key knows that they vouch for the information it contains: they agree that the public key is mine and that the information stated is true about me. This encrypted endorsement is the certificate that they issue to me.
When you connect to my site via https, I send you the certificate.
Your browser already knows the issuer&apos;s public key because your browser came installed with that information.
Your browser uses the issuer&apos;s public key to decrypt what I sent you. The fact that the issuer&apos;s public key works to decrypt it proves that the issuer&apos;s private key was used to encrypt it, and therefore, that the issuer really did create this certificate.
Inside the decrypted information is my public key, which you now know has been vouched for. You use that to encrypt some data to send to me.</code></pre><p>Your key theory: basically right, but authentication is usually done by encrypting a cryptographically secure hash of the data rather than the data itself.</p>
<p>A CA’s signature on an SSL certificate should indicate that the CA has done a certain amount of diligence to ensure that the credentials on the certificate match the owner. That diligence varies, but the ultimate point is that they’re saying that the certificate they signed belongs to the entity named on it.</p>
<p>See <a href="http://en.wikipedia.org/wiki/Digital_signature#Definition" target="_blank" rel="noopener">http://en.wikipedia.org/wiki/Digital_signature#Definition</a></p>
<p>A public key certificate is the signed combination between a public key, identifiers, and possibly other attributes. Those who sign this document effectively assert the authenticity of the binding between the public key and the identifier and these attributes, in the same way as a passport issuing authority asserts the binding between the picture and the name in a passport, as various other pieces of information (nationality, date of birth, …).</p>
<pre><code>The private key is used for signing and deciphering/decrypting.
The public key is used for verifying signatures and enciphering/encrypting.</code></pre><p>public key cryptography: A class of cryptographic techniques employing two-key ciphers. Messages encrypted with the public key can only be decrypted with the associated private key. Conversely, messages signed with the private key can be verified with the public key.</p>
<p>It should be pointed out, along with all the other answers, that your private key is not always just one key that is used for both decrypting and signing messages. These should be two separate keys. This would create 4 keys for each person:</p>
<p>Public Encryption Key - Used to encrypt data to send to me.</p>
<p>Private Decryption Key - Used to decrypt messages that were encrypted using my Public Encryption Key.</p>
<p>Private Signing Key - Used to sign messages that I send to other people.</p>
<p>Public Verify Key - Used to verify that a message was, in fact, signed by me.</p>
<p><a href="https://en.wikipedia.org/wiki/Savvis" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Savvis</a></p>
<p>Savvis - Wikipedia</p>
<p>Savvis, formerly SVVS on Nasdaq and formerly known as Savvis Communications Corporation, and, later, Savvis Inc., is a subsidiary of CenturyLink, a company headquartered in Monroe, Louisiana.[1] The company sells managed hosting and colocation services with more than 50 data centers[2] (over 2 million square feet) in North America, Europe, and Asia, automated management and provisioning systems, and information technology consulting. Savvis has approximately 2,500 unique business and government customers.[3][4] </p>
<p>The file extensions .CRT and .CER are interchangeable.  If your server requires that you use the .CER file extension, you can change the extension by following the steps below:</p>
<pre><code>Double-click on the yourwebsite.crt file to open it into the certificate display.
Select the Details tab, then select the Copy to file button.
Hit Next on the Certificate Wizard.
Select Base-64 encoded X.509 (.CER), then Next.
Select Browse (to locate a destination) and type in the filename yourwebsite.
Hit Save. You now have the file yourwebsite.cer


File extensions for cryptographic certificates aren&apos;t really as standardized as you&apos;d expect. Windows by default treats double-clicking a .crt file as a request to import the certificate into the Windows Root Certificate store, but treats a .cer file as a request just to view the certificate. So, they&apos;re different in that sense, at least, that Windows has some inherent different meaning for what happens when you double click each type of file.</code></pre><p>But the way that Windows handles them when you double-click them is about the only difference between the two. Both extensions just represent that it contains a public certificate. You can rename a file or use one in place of the other in any system or configuration file that I’ve seen. And on non-Windows platforms (and even on Windows), people aren’t particularly careful about which extension they use, and treat them both interchangeably, as there’s no difference between them as long as the contents of the file are correct.</p>
<p>*.pem, *.crt, *.ca-bundle, *.cer, *.p7b, *.p7s files contain one or more X.509 digital certificate files that use base64 (ASCII) encoding. </p>
<p>.DER = The DER extension is used for binary DER encoded certificates. These files may also bear the CER or the CRT extension.   Proper English usage would be “I have a DER encoded certificate” not “I have a DER certificate”.</p>
<p>.PEM = The PEM extension is used for different types of X.509v3 files which contain ASCII (Base64) armored data prefixed with a “—– BEGIN …” line.</p>
<p>.CRT = The CRT extension is used for certificates. The certificates may be encoded as binary DER or as ASCII PEM. The CER and CRT extensions are nearly synonymous.  Most common among *nix systems</p>
<p>CER = alternate form of .crt (Microsoft Convention) You can use MS to convert .crt to .cer (.both DER encoded .cer, or base64[PEM] encoded .cer)  The .cer file extension is also recognized by IE as a command to run a MS cryptoAPI command (specifically rundll32.exe cryptext.dll,CryptExtOpenCER) which displays a dialogue for importing and/or viewing certificate contents.</p>
<p>.KEY = The KEY extension is used both for public and private PKCS#8 keys. The keys may be encoded as binary DER or as ASCII PEM.</p>
<p>The only time CRT and CER can safely be interchanged is when the encoding type can be identical.  (ie  PEM encoded CRT = PEM encoded CER)</p>
<p>What is the SSL Certificate Chain?</p>
<p>There are two types of certificate authorities (CAs): root CAs and intermediate CAs. In order for an SSL certificate to be trusted, that certificate must have been issued by a CA that is included in the trusted store of the device that is connecting.</p>
<p>Good. I see you want to access this particular page.  I need to send the page to you in a secure way. If I<br>encrypt it using my public key, you won’t be able to decrypt it because you don’t have my private key. And since you don’t have any public key of your own that I can use to encrypt the page for you here’s what I propose<br>Since you can send me encrypted messages that only me can read (you have my public key), send me an encrypted message with an encryption key in it. Just make up a random encryption key that we’ll both use to encrypt and decrypt the messages between us during this session . </p>
<p>A simple symmetric key is enought. We’ll use the same key to encrypt and decrypt the messages.</p>
<ul>
<li>So there’s no way that anybody with your public<br>key can trick others to believe that he is you ? </li>
<li>Nope. That’s the beauty of the assymetric encryption. </li>
</ul>
<p>When you send the public key to the victim’s contain your public key + a certificate that this public key belongs to you. If you are a website, then the certificate will contain the domain name of the website. Basically, a certificate says something like:  the following public key “XYZ123” belongs to example.com. </p>
<p>that’s why we have “Certificate Authorities” like Verisign, Digicert or even Symantec. It is believed that these companies have the necessary trustworthiness to deliver certificates to different •entities.<br>Think of a CA like a registrar for public keys. Just like registrars assert that a domain name belongs to a certain person or company, CAS assert that a public key belongs to a certain domain name (or IP address) . </p>
<p>The certificate will contain the CA that delivered it, but you don’t even have to check with them because the certificate is signed by them. That signature alone is enough proof that the certificate comes from them. </p>
<p>A signature is simply a small message that is encrypted with their private key. Since private keys are asymetric, that means that only the associated public key can decrypt it.</p>
<p>Asymmetric encryption works in both way. public -&gt; private and private -&gt; public.<br>What the public key encrypts only the private key can decrypt, and what the private key can encrypt only the public key can decrypt.</p>
<p>for PKI, we’re not looking for secrecy here, we only want to prove that we’ re the real authors of the message. Suppose I send you the message “HELLO WORLD”, encrypted with my private key. The encrypted message would be, for example, “XYZ1234”. So you receive “XYZ1234” . If I give you my public key, you would be able to decrypt “XYZ1234” into “HELLO WORLD” . And by doing so, you would have proof that that message was sent by me, because the public key you used decrypts messages that were encrypted by my private key only. And since I am the only person in the universe who has that private key, that proves that I am the author of that message. </p>
<p>Really nice. So I don’t have to contact the CA to check the validity of the certificate, all I have to do is use their public key to decrypt the signature that’s in it. If it’s the same as err, wait, what should I compare the decrypted signature to again ? </p>
<p>You have to find the same hash as the one you have calculated. They are sending a small hash of the whole certificate. So what you have to do is to calculate the hash of the certificate yourself, then compare it to the hash you get when you decrypt the signature. If the two are the same that means two things </p>
<ol>
<li>The CA’s public key worked, so the signature was encrypted by the associated private key, which means the certificate was really issued by the CA. </li>
<li>Since the hash is the same, it also means that you are seeing the exact same certificate that the CA delivered to the website you are visiting. The information contained inside hasn’t been tampered with. </li>
</ol>
<p>That’s really good. So, let me recap one more time . </p>
<ol>
<li>I contact you for an HTTPS page. </li>
<li>You send me an SSL certificate that contains your public key and a signature from the CA that delivered </li>
<li>I make sure the certificate is valid by using the CA’s public key to decrypt the signature. In parallel, I also calculate the hash of the certificate. </li>
</ol>
<p>If my hash and the one I got from decrypting the signature are equal, that means that the certificate was really issued by the CA and that I can be sure that the public key you sent me is really yours. </p>
<p>Because you implicitly trust the CA.</p>
<p>Let’s continue: </p>
<ol start="4">
<li>I generate a random key that we’ll both use as a symmetric key to encrypt and decrypt the messages we’ll be sending each other. </li>
<li>I encrypt this symmetric key with your public key and send it to you. </li>
<li>You decrypt my message with your private key and find my secret key. </li>
<li>Every request or response between us will be encrypted with this shared secret symmetric key. </li>
</ol>
<h2 id="CN"><a href="#CN" class="headerlink" title="CN"></a>CN</h2><p>The Common Name (AKA CN) represents the server name protected by the SSL certificate.</p>
<p>The certificate is valid only if the request hostname matches the certificate common name.</p>
<p>To check the status, such as </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo openssl x509 -noout -<span class="keyword">in</span> xxx.com.cer -text</span><br></pre></td></tr></table></figure>

<p> Subject: C=UK, ST=London, L=London, O=AAA Bank, OU=Product and Markets, CN=*.xxxtest.com<br>        Subject Public Key Info:</p>
<h3 id="commonName-format"><a href="#commonName-format" class="headerlink" title="commonName format"></a>commonName format</h3><p>The common name is not a URL. It doesn’t include any protocol (e.g. http:// or https://), port number, or pathname. For instance, <a href="https://example.com" target="_blank" rel="noopener">https://example.com</a> or example.com/path are incorrect. In both cases, the common name should be example.com</p>
<h4 id="Common-Name-vs-Subject-Alternative-Name"><a href="#Common-Name-vs-Subject-Alternative-Name" class="headerlink" title="Common Name vs Subject Alternative Name"></a>Common Name vs Subject Alternative Name</h4><p>The common name can only contain up to one entry: either a wildcard or non-wildcard name. It’s not possible to specify a list of names covered by an SSL certificate in the common name field.</p>
<p>The Subject Alternative Name extension (also called Subject Alternate Name or SAN) was introduced to solve this limitation. The SAN allows issuance of multi-name SSL certificates.</p>
<h1 id="SHA-2-SSL-Certificates"><a href="#SHA-2-SSL-Certificates" class="headerlink" title="SHA-2 SSL Certificates"></a>SHA-2 SSL Certificates</h1><p>Almost all certificates are currently signed with the SHA-2 hash algorithm.</p>
<p>This article provides a simple overview of the SHA-1 to SHA-2 transition plans, as well additional informations on the SHA-2 hash algorithm and SSL certificates purchased with DNSimple previous than September 2014.</p>
<p>The SHA family of hashing algorithms were developed by the National Institute of Standards and Technology (NIST) and are used by certificate authorities (CAs) when digitally signing issued certificates.</p>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul>
<li><a href="https://support.dnsimple.com/articles/what-is-ssl-certificate-chain/" target="_blank" rel="noopener">https://support.dnsimple.com/articles/what-is-ssl-certificate-chain/</a></li>
<li><a href="https://www.thawte.com/resources/getting-started/how-ssl-works/" target="_blank" rel="noopener">https://www.thawte.com/resources/getting-started/how-ssl-works/</a></li>
</ul>

          
        
      
    </div>

    
    
    
      <footer class="post-footer">
          <div class="post-eof"></div>
        
      </footer>
  </div>
  
  
  
  </article>

    
      

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.todzhang.com/2019-07-26-Terraform/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Todd Zhang">
      <meta itemprop="description" content="Contact me via phray.zhang@gmail.com or wechat at helloworld_2000">
      <meta itemprop="image" content="/images/globe.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Clouds & Docker">
    </span>
      <header class="post-header">

        
          <h1 class="post-title" itemprop="name headline">
              
              <a href="/2019-07-26-Terraform/" class="post-title-link" itemprop="url">Terraform</a>
            
          </h1>
        

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              
                
              

              <time title="Created: 2019-07-26 08:44:14" itemprop="dateCreated datePublished" datetime="2019-07-26T08:44:14+10:00">2019-07-26</time>
            </span>
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
            <h1 id="Why-Terraform"><a href="#Why-Terraform" class="headerlink" title="Why Terraform"></a>Why Terraform</h1><p>Software isn’t done when the code is working on your computer. It’s not done when the tests pass. And it’s not done when someone gives you a “ship it” on a code review. Software isn’t done until you deliver it to the user.</p>

          
        
      
    </div>

    
    
    
      <footer class="post-footer">
          <div class="post-eof"></div>
        
      </footer>
  </div>
  
  
  
  </article>

    
      

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.todzhang.com/2016-06-10-Linux-Tips/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Todd Zhang">
      <meta itemprop="description" content="Contact me via phray.zhang@gmail.com or wechat at helloworld_2000">
      <meta itemprop="image" content="/images/globe.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Clouds & Docker">
    </span>
      <header class="post-header">

        
          <h1 class="post-title" itemprop="name headline">
              
              <a href="/2016-06-10-Linux-Tips/" class="post-title-link" itemprop="url">Linux Tips</a>
            
          </h1>
        

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              
                
              

              <time title="Created: 2019-07-07 15:47:04" itemprop="dateCreated datePublished" datetime="2019-07-07T15:47:04+10:00">2019-07-07</time>
            </span>
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
            <h1 id="Get-permission-denied-error-when-sudo-su-or-hyphen-in-sudo-command"><a href="#Get-permission-denied-error-when-sudo-su-or-hyphen-in-sudo-command" class="headerlink" title="Get permission denied error when sudo su (or hyphen in sudo command)"></a>Get permission denied error when sudo su (or hyphen in sudo command)</h1><p>bash: /home/YOURNAME/.bashrc: Permission denied<br>That’s because you didn’t add “-“ hyphen in your sudo command.</p>
<p>The difference between “-“ and “no hyphen” is that the latter keeps your existing environment (variables, etc); the former creates a new environment (with the settings of the actual user, not your own).</p>
<p>The hyphen has two effects:</p>
<p>1) switches from the current directory to the home directory of the new user (e.g., to /root in the case of the root user) by logging in as that user</p>
<p>2) changes the environmental variables to those of the new user as dictated by their ~/.bashrc. That is, if the first argument to su is a hyphen, the current directory and environment will be changed to what would be expected if the new user had actually logged on to a new session (rather than just taking over an existing session).</p>
<h1 id="To-delete-lines-in-files-contain-pattern"><a href="#To-delete-lines-in-files-contain-pattern" class="headerlink" title="To delete lines in files contain pattern"></a>To delete lines in files contain pattern</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed -i <span class="string">'/.*167\=OPT.*/d'</span> testdata.txt</span><br></pre></td></tr></table></figure>

<h1 id="to-select-only-only-one-element-value-of-XML-file"><a href="#to-select-only-only-one-element-value-of-XML-file" class="headerlink" title="to select only only one element value of XML file :"></a>to select only only one element value of XML file :</h1><p>grep -oPm1 “(?&lt;=<theuniqid>)[^&lt;]+” </theuniqid></p>
<h1 id="To-check-Linux-release-name"><a href="#To-check-Linux-release-name" class="headerlink" title="To check Linux release name"></a>To check Linux release name</h1><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /etc/os-release</span><br></pre></td></tr></table></figure>

<h1 id="How-to-check-whether-your-linux-is-32bit-or-64-bit"><a href="#How-to-check-whether-your-linux-is-32bit-or-64-bit" class="headerlink" title="How to check whether your linux is 32bit or 64 bit"></a>How to check whether your linux is 32bit or 64 bit</h1><p>To run “arch” command,  this is similar to “uname -m” , it prints to the screen whether your system is running 32-bit (“i686”) or 64-bit (“x86_64”).</p>
<h1 id="convert-line-ending-to-unix-sometimes-git-submit-is-dos-format"><a href="#convert-line-ending-to-unix-sometimes-git-submit-is-dos-format" class="headerlink" title="convert line ending to unix (sometimes git submit is dos format)"></a>convert line ending to unix (sometimes git submit is dos format)</h1><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dos2unix the_script_file_name</span><br></pre></td></tr></table></figure>

<h1 id="To-check-redhat-Linux-version"><a href="#To-check-redhat-Linux-version" class="headerlink" title="To check redhat Linux version"></a>To check redhat Linux version</h1><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat  /etc/redhat-release</span><br></pre></td></tr></table></figure>

<h1 id="To-list-all-users-in-linux"><a href="#To-list-all-users-in-linux" class="headerlink" title="To list all users in linux"></a>To list all users in linux</h1><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /etc/passwd</span><br></pre></td></tr></table></figure>

<h1 id="Show-IP-address-in-Linux"><a href="#Show-IP-address-in-Linux" class="headerlink" title="Show IP address in Linux"></a>Show IP address in Linux</h1><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$ifconfig</span></span><br><span class="line">eth0      Link encap:Ethernet  HWaddr 00:50:56:9B:19:81</span><br><span class="line">          inet addr:133.14.16.5  Bcast:133.14.16.255  Mask:255.255.255.0</span><br></pre></td></tr></table></figure>

<h1 id="Check-system-resource"><a href="#Check-system-resource" class="headerlink" title="Check system resource"></a>Check system resource</h1><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">execute `cat /proc/cpuinfo` and `free -m` to gain information about the server’s CPU and memory.</span><br></pre></td></tr></table></figure>

<h1 id="chmod-command"><a href="#chmod-command" class="headerlink" title="chmod command"></a>chmod command</h1><p>From one to four octal digits<br>Any omitted digits are assumed to be leading zeros. </p>
<p>The first digit = selects attributes for the set user ID (4) and set group ID (2) and save text image (1)S<br>The second digit = permissions for the user who owns the file: read (4), write (2), and execute (1)<br>The third digit = permissions for other users in the file’s group: read (4), write (2), and execute (1)<br>The fourth digit = permissions for other users NOT in the file’s group: read (4), write (2), and execute (1)</p>
<p>The octal (0-7) value is calculated by adding up the values for each digit<br>User (rwx) = 4+2+1 = 7<br>Group(rx) = 4+1 = 5<br>World (rx) = 4+1 = 5<br>chmode mode = 0755</p>
<p>Examples</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">chmod 400 file - Read by owner</span><br><span class="line">chmod 040 file - Read by group</span><br><span class="line">chmod 004 file - Read by world </span><br><span class="line"></span><br><span class="line">chmod 200 file - Write by owner</span><br><span class="line">chmod 020 file - Write by group</span><br><span class="line">chmod 002 file - Write by world</span><br></pre></td></tr></table></figure>

<h1 id="top"><a href="#top" class="headerlink" title="top"></a>top</h1><ul>
<li>enter u, then user id to show only user process</li>
<li>Z: global color scheme, e.g. red or green</li>
<li>B: global bold for all</li>
<li>z: show color, then b to hightlight, and x highlight sort fidl, y highlight running tasks</li>
<li>#3: show only 3 threads</li>
<li>c: show command line</li>
<li>F: sort, e.g. Fk sort by CPU%</li>
<li>R: reverse order</li>
</ul>
<h1 id="Sample-config-files"><a href="#Sample-config-files" class="headerlink" title="Sample config files"></a>Sample config files</h1><h2 id="vimrc"><a href="#vimrc" class="headerlink" title=".vimrc"></a>.vimrc</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="built_in">set</span> number</span><br><span class="line"><span class="built_in">set</span> incsearch</span><br><span class="line"><span class="built_in">set</span> hlsearch</span><br><span class="line">syntax on</span><br><span class="line">colorscheme desert</span><br></pre></td></tr></table></figure>

<h2 id="screenrc"><a href="#screenrc" class="headerlink" title="==== screenrc  ="></a>==== screenrc  =</h2><p><a href="https://gist.githubusercontent.com/ChrisWills/1337178/raw/8275b66c3ea86a562cdaa16f1cc6d9931d521e1b/.screenrc-main-example" target="_blank" rel="noopener">https://gist.githubusercontent.com/ChrisWills/1337178/raw/8275b66c3ea86a562cdaa16f1cc6d9931d521e1b/.screenrc-main-example</a></p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GNU Screen - main configuration file </span></span><br><span class="line"><span class="comment"># All other .screenrc files will source this file to inherit settings.</span></span><br><span class="line"><span class="comment"># Author: Christian Wills - cwills.sys@gmail.com</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Allow bold colors - necessary for some reason</span></span><br><span class="line">attrcolor b <span class="string">".I"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Tell screen how to set colors. AB = background, AF=foreground</span></span><br><span class="line">termcapinfo xterm <span class="string">'Co#256:AB=\E[48;5;%dm:AF=\E[38;5;%dm'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Enables use of shift-PgUp and shift-PgDn</span></span><br><span class="line">termcapinfo xterm|xterms|xs|rxvt ti@:te@</span><br><span class="line"></span><br><span class="line"><span class="comment"># Erase background with current bg color</span></span><br><span class="line">defbce <span class="string">"on"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Enable 256 color term</span></span><br><span class="line">term xterm-256color</span><br><span class="line"></span><br><span class="line"><span class="comment"># Cache 30000 lines for scroll back</span></span><br><span class="line">defscrollback 30000</span><br><span class="line"></span><br><span class="line"><span class="comment"># New mail notification</span></span><br><span class="line"><span class="comment"># backtick 101 30 15 $HOME/bin/mailstatus.sh</span></span><br><span class="line"></span><br><span class="line">hardstatus alwayslastline </span><br><span class="line"><span class="comment"># Very nice tabbed colored hardstatus line</span></span><br><span class="line">hardstatus string <span class="string">'%&#123;= Kd&#125; %&#123;= Kd&#125;%-w%&#123;= Kr&#125;[%&#123;= KW&#125;%n %t%&#123;= Kr&#125;]%&#123;= Kd&#125;%+w %-= %&#123;KG&#125; %H%&#123;KW&#125;|%&#123;KY&#125;%101`%&#123;KW&#125;|%D %M %d %Y%&#123;= Kc&#125; %C%A%&#123;-&#125;'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># change command character from ctrl-a to ctrl-b (emacs users may want this)</span></span><br><span class="line"><span class="comment">#escape ^Bb</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Hide hardstatus: ctrl-a f </span></span><br><span class="line"><span class="built_in">bind</span> f <span class="built_in">eval</span> <span class="string">"hardstatus ignore"</span></span><br><span class="line"><span class="comment"># Show hardstatus: ctrl-a F</span></span><br><span class="line"><span class="built_in">bind</span> F <span class="built_in">eval</span> <span class="string">"hardstatus alwayslastline"</span></span><br></pre></td></tr></table></figure>

<h2 id="bashrc"><a href="#bashrc" class="headerlink" title="====================.bashrc =========="></a>====================.bashrc ==========</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># .bashrc</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Source global definitions</span></span><br><span class="line"><span class="keyword">if</span> [ -f /etc/bashrc ]; <span class="keyword">then</span></span><br><span class="line">        . /etc/bashrc</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"><span class="comment"># source ./prompt</span></span><br><span class="line"><span class="built_in">export</span> PS1=<span class="string">''</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> PS1=<span class="string">'[\e[104mLight blue \u \A\]$ '</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> PS1=<span class="string">"\[\e[32m\]\u@\h \d \t \w \[\e[m\] \\$"</span></span><br><span class="line"></span><br><span class="line">\e[104mLight blue</span><br><span class="line"></span><br><span class="line"><span class="comment"># Welcome message</span></span><br><span class="line"><span class="built_in">echo</span> -ne <span class="string">"Good Morning ! It's "</span>; date <span class="string">'+%A, %B %-d %Y'</span></span><br><span class="line"><span class="built_in">echo</span> -e <span class="string">"And now your moment of Zen:"</span>; fortune</span><br><span class="line"><span class="built_in">echo</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"Hardware Information:"</span></span><br><span class="line">sensors  <span class="comment"># Needs: 'sudo apt-get install lm-sensors'</span></span><br><span class="line">uptime   <span class="comment"># Needs: 'sudo apt-get install lsscsi'</span></span><br><span class="line">free -m</span><br><span class="line"></span><br><span class="line"><span class="comment"># User specific aliases and functions</span></span><br><span class="line"></span><br><span class="line">PS1=<span class="string">'\[`[ $? = 0 ] &amp;&amp; X=2 || X=1; tput setaf $X`\]\h\[`tput sgr0`\]:$PWD\n\$ '</span></span><br><span class="line"></span><br><span class="line">============vimrc =========</span><br><span class="line"><span class="built_in">set</span> number</span><br><span class="line"><span class="built_in">set</span> incsearch</span><br><span class="line"><span class="built_in">set</span> hlsearch</span><br><span class="line"></span><br><span class="line">~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span><br><span class="line"></span><br><span class="line">grep -v <span class="string">"unwanted_word"</span> file | grep XXXXXXXX</span><br><span class="line"></span><br><span class="line">// find <span class="built_in">command</span> exclude “permission denied”</span><br><span class="line">$ find . -name <span class="string">"java"</span> 2&gt;/dev/null</span><br></pre></td></tr></table></figure>

<h1 id="Passwordless-connection-in-putty"><a href="#Passwordless-connection-in-putty" class="headerlink" title="Passwordless connection in putty"></a>Passwordless connection in putty</h1><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1. Generate Public &amp; private key pair by keygen</span><br><span class="line">2. Log into Linux, nano .ssh/authorized_keys and paste the public key</span><br><span class="line">3. Save the private key <span class="keyword">in</span> putty and load it <span class="keyword">in</span> Putty session</span><br></pre></td></tr></table></figure>

<h1 id="find-java-files-older-than-3-days"><a href="#find-java-files-older-than-3-days" class="headerlink" title="find java files older than 3 days"></a>find java files older than 3 days</h1><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">find . -name <span class="string">"*.java"</span> -atime -3d</span><br></pre></td></tr></table></figure>

<h1 id="Remove-capitalization"><a href="#Remove-capitalization" class="headerlink" title="Remove capitalization"></a>Remove capitalization</h1><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed -ie <span class="string">'s/Return /return /g'</span> ReverseString.java</span><br></pre></td></tr></table></figure>

<h1 id="replace-string-in-files"><a href="#replace-string-in-files" class="headerlink" title="replace string in files"></a>replace string in files</h1><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#grep -r "pack.*me" .</span></span><br><span class="line">sed -ie <span class="string">'s/package.*me.*;/package com.todzhang;/g'</span> *.java</span><br><span class="line">sed -ie <span class="string">'s/package me.todzhang;/package com.todzhang;/g'</span> ~/dev/git/algo/algoWS/src/main/java/com/todzhang/*.java</span><br></pre></td></tr></table></figure>

<h1 id="create-directory-hierarchy-via-path"><a href="#create-directory-hierarchy-via-path" class="headerlink" title="create directory hierarchy via path"></a>create directory hierarchy via path</h1><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p ~/abc/def/egg</span><br></pre></td></tr></table></figure>

<p><code>-p</code> means create intermediary folders, if not exist. Those intermediary folders with permission 777</p>
<h1 id="lsof-to-locate-whether-who-allocated-port-8080"><a href="#lsof-to-locate-whether-who-allocated-port-8080" class="headerlink" title="lsof to locate whether/who allocated port 8080"></a>lsof to locate whether/who allocated port 8080</h1><p><code>lsof</code> means list open files.</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lsof -n -P -i | grep 8080</span><br></pre></td></tr></table></figure>

<h1 id="To-get-rid-of-‘’"><a href="#To-get-rid-of-‘’" class="headerlink" title="To get rid of ‘’"></a>To get rid of ‘’</h1><p>Sometimes got “403 Forbidden” error when trying to downalod file via wget, e.g.</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ wget http://www.xmind.net/xmind/downloads/xmind-7.5-update1-macosx.dmg</span><br><span class="line">--2016-09-09 23:27:29--  http://www.xmind.net/xmind/downloads/xmind-7.5-update1-macosx.dmg</span><br><span class="line">Resolving www.xmind.net... 23.23.188.223</span><br><span class="line">Connecting to www.xmind.net|23.23.188.223|:80... connected.</span><br><span class="line">HTTP request sent, awaiting response... 403 Forbidden</span><br><span class="line">2016-09-09 23:27:29 ERROR 403: Forbidden.</span><br></pre></td></tr></table></figure>

<p>To solve this problem, using following syntax, adding <code>-U xx</code></p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget -U <span class="string">'Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.8.1.6) Gecko/20070802 SeaMonkey/1.1.4'</span> http://www.xmind.net/xmind/downloads/xmind-7.5-update1-macosx.dmg</span><br></pre></td></tr></table></figure>

<h1 id="case-insensitive-ls-command-in-bash"><a href="#case-insensitive-ls-command-in-bash" class="headerlink" title="case insensitive ls command in bash"></a>case insensitive ls command in bash</h1><p>Update .bashrc or current active window</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">shopt</span> -s nocaseglob</span><br></pre></td></tr></table></figure>

<h1 id="one-line-command-to-download-and-extract-files"><a href="#one-line-command-to-download-and-extract-files" class="headerlink" title="one line command to download and extract files"></a>one line command to download and extract files</h1><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$cd</span> /tmp;curl https://www.kernel.org/pub/linux/utils/util-linux/v2.24/util-linux-2.24.tar.gz	| tar -zxf-;<span class="built_in">cd</span>	util-linux-2.24;</span><br></pre></td></tr></table></figure>

<h1 id="Redirect-request-for-HTTP-3xxx-code"><a href="#Redirect-request-for-HTTP-3xxx-code" class="headerlink" title="Redirect request for HTTP 3xxx code"></a>Redirect request for HTTP 3xxx code</h1><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -LSso ~/.vim/<span class="built_in">autoload</span>/pathogen.vim https://tpo.pe/pathogen.vim</span><br></pre></td></tr></table></figure>

<p><code>-L</code> means redirect upon HTTP code 3xxx<br><code>-Ss</code> work together to make the curl show errors if there are<br><code>-o</code> means output to a specified file, rather than stdout</p>
<h1 id="search-files-contains-keyword"><a href="#search-files-contains-keyword" class="headerlink" title="search files contains keyword"></a>search files contains keyword</h1><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grep -ri <span class="string">'architect'</span> . | awk -F <span class="string">':'</span> <span class="string">'&#123;print $1&#125;'</span></span><br></pre></td></tr></table></figure>

<h1 id="Show-Linux-kernel-and-name"><a href="#Show-Linux-kernel-and-name" class="headerlink" title="Show Linux kernel and name"></a>Show Linux kernel and name</h1><h2 id="lsb-release"><a href="#lsb-release" class="headerlink" title="lsb_release"></a>lsb_release</h2><p><code>lsb</code> means Linux Standard Base , <code>-a</code> means print all information</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lsb_release -a -u</span><br></pre></td></tr></table></figure>

<p>will output</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">phray@phray-VirtualBox ~ $ lsb_release -a -u</span><br><span class="line">No LSB modules are available.</span><br><span class="line">Distributor ID:	Ubuntu</span><br><span class="line">Description:	Ubuntu 14.04 LTS</span><br><span class="line">Release:	14.04</span><br><span class="line">Codename:	trusty</span><br></pre></td></tr></table></figure>

<h2 id="etc-os-release"><a href="#etc-os-release" class="headerlink" title="/etc/os-release"></a>/etc/os-release</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /etc/os-release</span><br></pre></td></tr></table></figure>

<p>will output Following</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">phray@phray-VirtualBox ~ $ cat /etc/os-release</span><br><span class="line">NAME=<span class="string">"Ubuntu"</span></span><br><span class="line">VERSION=<span class="string">"14.04.2 LTS, Trusty Tahr"</span></span><br><span class="line">ID=ubuntu</span><br><span class="line">ID_LIKE=debian</span><br><span class="line">PRETTY_NAME=<span class="string">"Ubuntu 14.04.2 LTS"</span></span><br><span class="line">VERSION_ID=<span class="string">"14.04"</span></span><br><span class="line">HOME_URL=<span class="string">"http://www.ubuntu.com/"</span></span><br><span class="line">SUPPORT_URL=<span class="string">"http://help.ubuntu.com/"</span></span><br><span class="line">BUG_REPORT_URL=<span class="string">"http://bugs.launchpad.net/ubuntu/"</span></span><br></pre></td></tr></table></figure>

<p>Following is the command found in docker.sh</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lsb_dist=$(lsb_release -a -u 2&gt;&amp;1 | tr <span class="string">'[:upper:]'</span> <span class="string">'[:lower:]'</span> | grep -E <span class="string">'id'</span> | cut -d <span class="string">':'</span> -f 2 | tr -d <span class="string">'[[:space:]]'</span>)</span><br></pre></td></tr></table></figure>

<p>show the 2nd column</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lsb_release --Codename | cut -f2</span><br></pre></td></tr></table></figure>

<h2 id="gpsswd"><a href="#gpsswd" class="headerlink" title="gpsswd"></a>gpsswd</h2><p>DESCRIPTION<br>gpasswd is used to administer the /etc/group file (and /etc/gshadow file if compiled with SHADOWGRP defined). Every group can have administrators, members and a password. System administrator can use -A option to define group administrator(s) and -M option to define members and has all rights of group administrators and members.</p>
<p>Notes about group passwords<br>Group passwords are an inherent security problem since more than one person is permitted to know the password. However, groups are a useful tool for permitting co-operation between different users.</p>
<p>OPTIONS<br>Group administrator can add and delete users using -a and -d options respectively. Administrators can use -r option to remove group password. When no password is set only group members can use newgrp to join the group. Option -R disables access via a password to the group through newgrp command (however members will still be able to switch to this group).</p>
<p>gpasswd called by a group administrator with group name only prompts for the group password. If password is set the members can still newgrp(1) without a password, non-members must supply the password.</p>
<p>FILES<br>Tag    Description<br>/etc/group<br>     Group account information.<br>/etc/gshadow<br>     Secure group account information.</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo gpasswd -a USER docker</span><br></pre></td></tr></table></figure>

<h1 id="Compare-files-difference-in-two-folders"><a href="#Compare-files-difference-in-two-folders" class="headerlink" title="Compare files difference in two folders"></a>Compare files difference in two folders</h1><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">diff -rq ~/dev/pa ~/dev/hexo/myblog/<span class="built_in">source</span>/_posts</span><br></pre></td></tr></table></figure>

<p>This used option <code>-r</code> (recursive) and <code>-q</code> quite, means only show differences</p>
<h1 id="To-execut-shell-unix-command-within-vim"><a href="#To-execut-shell-unix-command-within-vim" class="headerlink" title="To execut shell/unix command within vim"></a>To execut shell/unix command within vim</h1><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">:~ls -lt</span><br></pre></td></tr></table></figure>

<h1 id="To-open-find-result-with-sublime"><a href="#To-open-find-result-with-sublime" class="headerlink" title="To open find result with sublime"></a>To open find result with sublime</h1><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">find . -name <span class="string">"*Linux*.md"</span> | xargs sublime </span><br><span class="line">find . -name <span class="string">"*Linux*.md"</span> | xargs sublime ~ <span class="comment"># open in new Sublime window</span></span><br></pre></td></tr></table></figure>

<h1 id="To-vim-vim-edit-directly-on-file-output-by-find-command"><a href="#To-vim-vim-edit-directly-on-file-output-by-find-command" class="headerlink" title="To vim/vim edit directly on file output by find command"></a>To vim/vim edit directly on file output by find command</h1><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">find . -name <span class="string">"*tmux*"</span> -<span class="built_in">exec</span> vim &#123;&#125; \;</span><br></pre></td></tr></table></figure>

<p>Be advised you may experience following error message</p>
<blockquote>
<p>find: missing argument to `-exec’</p>
</blockquote>
<p>actually you should add a slash in front of semi colon</p>
<h1 id="quite-mode-in-apt-get"><a href="#quite-mode-in-apt-get" class="headerlink" title="quite mode in apt-get"></a>quite mode in apt-get</h1><ul>
<li>apt-get will in verbose mode</li>
<li>apt-get <code>-q</code> will be in less verbose , a.k.a quite mode</li>
<li>apt-get <code>-qq</code> in extreme least verbose mode</li>
</ul>

          
        
      
    </div>

    
    
    
      <footer class="post-footer">
          <div class="post-eof"></div>
        
      </footer>
  </div>
  
  
  
  </article>

    
      

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.todzhang.com/2018-06-12-KDB/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Todd Zhang">
      <meta itemprop="description" content="Contact me via phray.zhang@gmail.com or wechat at helloworld_2000">
      <meta itemprop="image" content="/images/globe.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Clouds & Docker">
    </span>
      <header class="post-header">

        
          <h1 class="post-title" itemprop="name headline">
              
              <a href="/2018-06-12-KDB/" class="post-title-link" itemprop="url">KDB</a>
            
          </h1>
        

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              
                
              

              <time title="Created: 2019-06-18 22:39:42" itemprop="dateCreated datePublished" datetime="2019-06-18T22:39:42+10:00">2019-06-18</time>
            </span>
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
            <h1 id="KDB"><a href="#KDB" class="headerlink" title="KDB"></a>KDB</h1><p>However kdb+ evaluates expressions right-to-left. There are no precedence rules. The reason commonly given for this behaviour is that it is a much simpler to understand system, many q-gods would agree, beginners however may not.</p>
<p>The kdb+ built-in commands are mostly a single letter. If a system command that is not built-in is entered, that command will be passed to the underlying operating system.</p>
<p>Created with financial institutions in mind, the database was developed as a central repository <em>to store time series data</em> that <em>supports real-time analysis of billions of records</em>.</p>
<p>Columnar databases return answers to some queries in a more efficient way than row-based database management systems.</p>
<p>kdb+ dictionaries, tables and nanosecond time stamps are native data types and are used to store time series data.</p>
<p>In 1998, Kx Systems released kdb, a database built on the language K written by Arthur Whitney. In 2003, kdb+ was released as a 64-bit version of kdb.</p>
<p>kdb/q/kdb+ is both a database (kdb) and a vector language (q). It’s used by almost every major financial institution</p>
<p><strong>Kdb+ is an in-memory column-oriented database based on the concept of ordered lists</strong>. In-memory means it primarily stores its data in RAM. This makes it extremely fast with a much simplified database engine but it requires a lot of RAM (Which no longer poses a problem as servers with massive amounts of RAM are now inexpensive). Column oriented database means that each column of data is stored sequentially in memory</p>
<h1 id="Column-DB"><a href="#Column-DB" class="headerlink" title="Column DB"></a>Column DB</h1><p>The main reason why indexes dramatically improve performance on large datasets is that database indexes on one or more columns are typically sorted by value, which makes range queries operations (like the above “find all records with salaries between 40,000 and 50,000” example) very fast (lower time-complexity).</p>
<p>Kdb+ the Database - Column Oriented DB allowing fast timeseries analysis</p>
<h2 id="Column-oriented-systems"><a href="#Column-oriented-systems" class="headerlink" title="Column-oriented systems"></a>Column-oriented systems</h2><p>A column-oriented database serializes all of the values of a column together, then the values of the next column, and so on. For our example table, the data would be stored in this fashion:<br>10:001,12:002,11:003,22:004;<br>Smith:001,Jones:002,Johnson:003,Jones:004;<br>Joe:001,Mary:002,Cathy:003,Bob:004;<br>40000:001,50000:002,44000:003,55000:004;<br>In this layout, any one of the columns more closely matches the structure of an index in a row-based system. This may cause confusion that can lead to the mistaken belief a column-oriented store “is really just” a row-store with an index on every column. However, it is the mapping of the data that differs dramatically. In a row-oriented indexed system, the primary key is the rowid that is mapped from indexed data. In the column-oriented system, the primary key is the data, which is mapped from rowids.[2] This may seem subtle, but the difference can be seen in this common modification to the same store:</p>
<blockquote>
<p>…;Smith:001;Jones:002,004;Johnson:003;…</p>
</blockquote>
<p>Whether or not a column-oriented system will be more efficient in operation depends heavily on the workload being automated. Operations that retrieve all the data for a given object (the entire row) are slower. A row-based system can retrieve the row in a single disk read, whereas numerous disk operations to collect data from multiple columns are required from a columnar database. However, these whole-row operations are generally rare. In the majority of cases, only a limited subset of data is retrieved. In a rolodex application, for instance, collecting the first and last names from many rows to build a list of contacts is far more common than reading all data for any single address. This is even more true for writing data into the database, especially if the data tends to be “sparse” with many optional columns. For this reason, column stores have demonstrated excellent real-world performance in spite of many theoretical disadvantages.</p>
<h2 id="Why-are-most-databases-row-oriented"><a href="#Why-are-most-databases-row-oriented" class="headerlink" title="Why are most databases row-oriented?"></a>Why are most databases row-oriented?</h2><p>Imagine we want to add one row somewhere in the middle of our data for 2011-02-26, on the row oriented database no problem, column oriented we will have to move almost all the data! Lucky since we <code>mostly deal with time series new data only appends to the end</code> of our table.</p>
<h2 id="Difference-vs-row-based-DB"><a href="#Difference-vs-row-based-DB" class="headerlink" title="Difference vs row based DB"></a>Difference vs row based DB</h2><p> a subtle point is that unlike most standard <code>SQL which is based on set theory</code>, <code>kdb+ is based on vectors of ordered lists</code>. Where standard SQL has struggled with queries like find the top 3 stocks by price, find the bottom 3 by market cap because it has no concept of order, kdb’s ordering significantly simplifies many queries. This ordered concept allows kdb+ to provide unique timeseries joins that would be be extremely difficult in other variations of SQL and require the use of slow cursors.</p>
<h1 id="Language-Q"><a href="#Language-Q" class="headerlink" title="Language Q"></a>Language Q</h1><p>the primary design objectives of q are expressiveness, speed and efficiency. In these, it is beyond compare. The design trade-off is a terseness that can be disconcerting to programmers coming from verbose traditional database programming environments – e.g., C++, Java, C# or Python – and a relational DBMS</p>
<p>Q evolved from APL (A Programming Language), which was first invented as a mathematical notation by Kenneth Iverson at Harvard University in the 1950s. APL was introduced in the 1960s by IBM as a vector programming language, meaning that it processes a list of numbers in a single operation. It was successful in finance and other industries that required heavy number crunching.</p>
<p>q is Kx’s proprietary language. It’s a powerful, concise and elegant array language, which means that a production system could just be a single page of code, not pages and pages of code and a nightmare to maintain. Clearly there is an initial investment in learning it, but the power it gives you to manipulate streaming, real-time and historical data makes that initial investment really worthwhile. </p>
<p>q language - fast, interpreted vector based language</p>
<p>Q is a interpreted vector based dynamically typed language built for speed and expressiveness.</p>
<p>Since q is interpreted you can enter commands straight into the console there is no waiting for compilation, feedback is instantaneous. </p>
<h2 id="Key-features-of-Q"><a href="#Key-features-of-Q" class="headerlink" title="Key features of Q"></a>Key features of Q</h2><p>Interpreted Q is interpreted, not compiled. During execution, data and functions live in an in-memory workspace. Iterations of the development cycle tend to be quick because all run-time information needed to test and debug is immediately available in the workspace. Q programs are stored and executed as simple text files called scripts. The interpreter’s eval and parse routines are exposed so that you can dynamically generate code in a controlled manner.</p>
<p>Types Q is a dynamically typed language, in which type checking is mostly unobtrusive. Each variable has the type of its currently assigned value and type promotion is automatic for most numeric operations. Types are checked on operations to homogenous lists.</p>
<p>Evaluation Order While q is entered left-to-right, expressions are evaluated right-to-left or, as the q gods prefer, left of right – meaning that a function is applied to the argument on its right. There is no operator precedence and function application can be written without brackets. Punctuation noise is significantly reduced.</p>
<p>Null and Infinity Values In classical SQL, the value NULL represents missing data for a field of any type and takes no storage space. In q, null values are typed and take the same space as non-nulls. Numeric types also have infinity values. Infinite and null values can participate in arithmetic and other operations with (mostly) predictable results.</p>
<p>Integrated I/O I/O is done through function handles that act as windows to the outside world. Once such a handle is initialized, passing a value to the handle is a write.</p>
<p>Table Oriented Give up objects, ye who enter here. In contrast to traditional languages, you’ll find no classes, objects, inheritance and virtual methods in q. Instead, q has tables as first class entities. The lack of objects is not as severe as might first appear. Objects are essentially glorified records (i.e., entities with named fields), which are modeled by q dictionaries. A table can be viewed as a list of record dictionaries.</p>
<p>Ordered Lists Because classical SQL is the algebra of sets – which are unordered with no duplicates – row order and column order are not defined, making time series processing cumbersome and slow. In q, data structures are based on ordered lists, so time series maintain the order in which they are created. Moreover, simple lists occupy contiguous storage, so processing big data is fast. Very fast.</p>
<p>Column Oriented SQL tables are organized as rows distributed across storage and operations apply to fields within a row. Q tables are column lists in contiguous storage and operations apply on entire columns.</p>
<p>In-Memory Database One can think of kdb+ as an in-memory database with persistent backing. Since data manipulation is performed with q, there is no separate stored procedure language. In fact, kdb+ comprises serialized q column lists written to the file system and then mapped into memory.</p>
<p>In q, data structures are based on ordered lists, so time series maintain the order in which they are created. Moreover, simple lists occupy contiguous storage, so processing big data is fast. Very fast.</p>
<p>Q tables are column lists in contiguous storage and operations apply on entire columns.</p>
<h2 id="Sample-of-Q-code"><a href="#Sample-of-Q-code" class="headerlink" title="Sample of Q code"></a>Sample of Q code</h2><figure class="highlight q"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">q)l:<span class="number">10</span> <span class="number">12</span> <span class="number">14</span> <span class="number">16</span> <span class="number">18</span> <span class="number">22</span> <span class="number">32</span> <span class="number">45</span></span><br><span class="line">q)<span class="built_in">sum</span> l</span><br><span class="line"><span class="number">169</span></span><br><span class="line">q)<span class="built_in">avg</span> l</span><br><span class="line"><span class="number">21.125</span></span><br><span class="line">q)l*<span class="number">10</span></span><br><span class="line"><span class="number">100</span> <span class="number">120</span> <span class="number">140</span> <span class="number">160</span> <span class="number">180</span> <span class="number">220</span> <span class="number">320</span> <span class="number">450</span></span><br><span class="line"></span><br><span class="line">q)k:<span class="built_in">til</span> <span class="number">8</span></span><br><span class="line">q)k</span><br><span class="line"><span class="number">0</span> <span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span> <span class="number">5</span> <span class="number">6</span> <span class="number">7</span></span><br><span class="line">q)l+k</span><br><span class="line"><span class="number">10</span> <span class="number">13</span> <span class="number">16</span> <span class="number">19</span> <span class="number">22</span> <span class="number">27</span> <span class="number">38</span> <span class="number">52</span></span><br></pre></td></tr></table></figure>

<p>Notice in the example code above the absence of loops, no for/while/do yet we could easily express adding one array to another. This is because the vector/list is the primary unit of data in kdb+. Operations are intended to be performed and expressed as being on an entire set of data. Dictionaries can be defined using lists, they provide a hashmap datastructure for quick lookups. Tables are constructed from dictionaries and lists. This brevity of data structures is actually one of the attributes that gives q its ability to express concisely what would take many lines in other languages.</p>
<h1 id="Some-of-the-practical-applications-of-being-able-to-combine-both-streaming-and-real-time-data-together-with-historical-all-in-the-same-database"><a href="#Some-of-the-practical-applications-of-being-able-to-combine-both-streaming-and-real-time-data-together-with-historical-all-in-the-same-database" class="headerlink" title="Some of the practical applications of being able to combine both streaming and real-time data together with historical, all in the same database?"></a>Some of the practical applications of being able to combine both streaming and real-time data together with historical, all in the same database?</h1><p>The most important thing is the simplicity, which translates into speed and ease of doing analysis. For example, it allows you to do complicated, time-critical analysis, such as pre-trade risk. This means that you are likely to see interesting trading opportunities before those who are using the same off-the-shelf solution as everybody else. So you’re there first and you’re there so early, you can afford to do comprehensive pre-trade risk analysis, and you’re able to look for patterns you’ve seen in the past. </p>
<p>In addition to capturing market data, firms are using kdb+ for order-book management, algorithmic trading, and risk assessment. “They are using kdb+/q for queries being performed on both streaming or historical data — the latter easily accommodating research and back-testing,”</p>
<p>DeltaFlow, a platform from First Derivatives that’s based on kdb+, is used by traders for high volume, low-latency algorithmic trading and by regulators for real-time detection of market abuse and unauthorized trading activity across multiple asset classes.</p>
<h1 id="Combined-power-of-kdb-q"><a href="#Combined-power-of-kdb-q" class="headerlink" title="Combined power of kdb+/q"></a>Combined power of kdb+/q</h1><p>What’s beautiful about kdb+ is that since tables are columns of vectors, all the power of the q language can be used as easily on table data as it was on lists. Where we had sum[l],avg[l],weightedAvg[l1;l2] of lists we can write similar qSQL:<br>select avg price, sum volume, weightedAvg[time;price] from trade<br>Want to apply a function to a timeseries, simply place it inline:<br>select {a:avg x; sqrt avg (x<em>x)-a</em>a} price from trade</p>
<h1 id="Q-commadns"><a href="#Q-commadns" class="headerlink" title="Q commadns"></a>Q commadns</h1><p>To obtain official console display of any q value, apply the built-in function show to it.</p>
<p>q)show a:42</p>
<h3 id="comments"><a href="#comments" class="headerlink" title="comments"></a>comments</h3><p>At least one whitespace character must separate / intended to begin a comment from any text to the left of it on a line.</p>
<h3 id="boolean"><a href="#boolean" class="headerlink" title="boolean"></a>boolean</h3><p>Boolean values in q are stored in a single byte and are denoted as the binary values they really are with an explicit type suffix b. One way to generate boolean values is to test for equality.</p>
<p>q)42=40+2<br>1b</p>
<h3 id="date"><a href="#date" class="headerlink" title="date"></a>date</h3><p>One interesting and useful feature of q temporal values is that, as integral values under the covers, they naturally participate in arithmetic. For example, to advance a date five days, add 5.</p>
<p>q)2000.01.01+5<br>_
Or to advance a time by one microsecond (i.e., 1000 nanoseconds) add 1000.</p>
<p>q)12:00:00.000000000+1000<br>_
Or to verify that temporal values are indeed their underlying values, test for equality.</p>
<p>q)2000.01.01=0</p>
<h3 id="Symbols"><a href="#Symbols" class="headerlink" title="Symbols"></a>Symbols</h3><p>Symbols are denoted by a leading back-quote (called “back tick” in q-speak) followed by characters. Symbols without embedded blanks or other special characters can be entered literally into the console.<br>q)`aapl<br>_</p>
<p>Since symbols are atoms, any two can be tested for equality.</p>
<p>q)<code>aapl=</code>apl<br>_</p>
<h2 id="list"><a href="#list" class="headerlink" title="list"></a>list</h2><p>The fundamental q data structure is a list, which is an ordered collection of items sequenced from left to right. The notation for a general list encloses items with ( and ) and uses ; as separator. Spaces after the semi-colons are optional but can improve readability.</p>
<p>q)(1; 1.2; `one)<br>_</p>
<p>In the case of a homogenous list of atoms, called a simple list, q adopts a simplified format for both storage and display. The parentheses and semicolons are dropped. For example, a list of underlying numeric type separates its items with a space.</p>
<p>q)(1; 2; 3)<br>1 2 3</p>
<p>A simple list of booleans is juxtaposed with no spaces and has a trailing b type indicator.</p>
<p>q)(1b; 0b; 1b)<br>101b<br>A simple list of symbols is displayed with no separating spaces.</p>
<p>q)(<code>one;</code>two; <code>three)</code>one<code>two</code>three</p>
<h3 id="basic-operations"><a href="#basic-operations" class="headerlink" title="basic operations"></a>basic operations</h3><p> to construct and manipulate lists. The most fundamental is til, which takes a non-negative integer n and returns the first n integers starting at 0 (n itself is not included in the result).</p>
<p>q)til 10<br>0 1 2 3 4 5 6 7 8 9</p>
<h4 id="til-list-tips"><a href="#til-list-tips" class="headerlink" title="til list tips"></a>til list tips</h4><p>Be mindful that q always evaluates expressions from right to left and that operations work on vectors whenever possible.</p>
<p>q)1+til 10<br>1 2 3 4 5 6 7 8 9 10</p>
<p>Similarly, we obtain the first 10 even numbers and the first ten odd numbers.</p>
<p>q)2<em>til 10<br>_
q)1+2</em>til 10<br>_
Finally, we obtain the first 10 even numbers starting at 42.</p>
<p>q)42+2*til 10<br>_</p>
<p>Another frequently used list primitive is join , that returns the list obtained by concatenating its right operand to its left operand.</p>
<p>q)1 2 3,4 5<br>1 2 3 4 5</p>
<h3 id="extract"><a href="#extract" class="headerlink" title="extract"></a>extract</h3><p>To extract items from the front or back of a list, use the take operator #. Positive argument means take from the front, negative from the back.</p>
<p>q)2#til 10<br>0 1<br>q)-2#til 10 </p>
<blockquote>
<p>Applying # always results in a list.</p>
</blockquote>
<p>In particular, the idiom 0# returns an empty list of the same type as the first item in its argument. Using an atom argument is a succinct way to create a typed empty list of the type of the atom.</p>
<p>q)0#1 2 3<br>`long$()</p>
<p>Should you extract more items than there are in the list, # restarts at the beginning and continues extracting. It does this until the specified number of items is reached.</p>
<p>q)5#1 2 3<br>1 2 3 1 2</p>
<p>As with atoms, a list can be assigned to a variable.</p>
<p>q)L:10 20 30<br>The items of a list can be accessed via indexing, which uses square brackets and is relative to 0.</p>
<p>q)L[0]<br>10</p>
<h3 id="Function"><a href="#Function" class="headerlink" title="Function"></a>Function</h3><p>Conceptually, a q function is a sequence of steps that produces an output result from an input value. Since q is not purely functional, these rules can interact with the world by reaching outside the context of the function. Such actions are called side effects and should be carefully controlled.</p>
<p>Function definition is delimited by matching curly braces { and }. Immediately after the opening brace, the formal parameters are names enclosed in square brackets [ and ] and separated by semi-colons. These parameters presumably appear in the body of the function, which follows the formal parameters and is a succession of expressions sequenced by semi-colons.</p>
<p>Following is a simple function that returns the square of its input. On the next line we assign the same function to the variable sq. The whitespace is optional.</p>
<p>q){[x] x<em>x}<br>_
q)sq:{[x] x</em>x}<br>_</p>
<p>Here is a function that takes two input values and returns the sum of their squares.</p>
<p>q){[x;y] a:x<em>x; b:y</em>y; a+b}<br>_
q)pyth:{[x;y] a:x<em>x; b:y</em>y; a+b}<br>_</p>
<p>Here are the previous functions applied to arguments.</p>
<p>q){[x] x<em>x}[5]<br>25<br>q)sq[5]<br>_
q){[x;y] a:x</em>x; b:y*y; a+b}[3;4]<br>25<br>q)pyth[3;4]<br>_</p>
<h3 id="monadic-function"><a href="#monadic-function" class="headerlink" title="monadic function"></a>monadic function</h3><p>In q, as in most functional languages, we don’t need no stinkin’ brackets for application of a monadic function – i.e., with one parameter. Simply separate the function from its argument by whitespace. This is called function juxtaposition.</p>
<p>q){x<em>x} 5<br>_
q)f:{x</em>x}<br>q)f 5<br>_</p>
<h3 id="x-y-z"><a href="#x-y-z" class="headerlink" title="x,y,z"></a>x,y,z</h3><p>It is common in mathematics to use function parameters x, y, or z. If you are content with these names (in the belief that descriptive names provide no useful information to the poor soul reading your code), you can omit their declaration and q will understand that you mean the implicit parameters x, y, and z in that order.</p>
<p>q){x<em>x}[5]<br>25<br>q){a:x</em>x; b:y*y; a+b}[3;4]<br>25</p>
<h3 id="verbs"><a href="#verbs" class="headerlink" title="verbs"></a>verbs</h3><p>higher order functions, or as they are called in q, adverbs. </p>
<p>In words, we tell q to start with the initial value of 0 in the accumulator and then modify + with the adverb / so that it adds across the list.</p>
<p>q)0 +/ 1 2 3 4 5<br>15</p>
<p>In this situation we don’t really need the flexibility to specify the initial value of the accumulator. It suffices to start with the first item of the list and proceed across the rest of the list. There is an even simpler form for this case.</p>
<p>q)(+/) 1 2 3 4 5</p>
<h4 id="for-loop"><a href="#for-loop" class="headerlink" title="for loop"></a>for loop</h4><p>If you are new to functional programming, you may think, “Big deal, I write for loops in my sleep.” </p>
<p>More importantly, you can focus on what you want done without the irrelevant scaffolding of how to set up control structures. This is called declarative programming.</p>
<p>What else can we do with our newfound adverb? Change addition to multiplication for factorial.</p>
<p>q)(*/) 1+til 10<br>3628800</p>
<h3 id="larger-vs-smaller"><a href="#larger-vs-smaller" class="headerlink" title="larger vs smaller"></a>larger vs smaller</h3><p>The fun isn’t limited to arithmetic primitives. We introduce |, which returns the larger of its operands and &amp;, which returns the smaller of its operands.</p>
<p>q)42|98<br>98<br>q)42&amp;98</p>
<p>Use | or &amp; with over and you have maximum or minimum.</p>
<p>q)(|/) 20 10 40 30<br>40<br>q)(&amp;/) 20 10 40 30</p>
<h3 id="command-‘over’-adverb"><a href="#command-‘over’-adverb" class="headerlink" title="command ‘over’ adverb"></a>command ‘over’ adverb</h3><p>Some applications of / are so common that they have their own names.</p>
<p>q)sum 1+til 10<br>55<br>q)prd 1+til 10 “o”<br>_
q)max 20 10 40 30<br>_
q)min 20 10 40 30 </p>
<p>At this point the / pattern should be clear: it takes a given function and produces a new function that accumulates across the original list, producing a single result. In particular, / converts a dyadic function to a monadic aggregate function – i.e., one that collapses a list to an atom.</p>
<h1 id="data-type"><a href="#data-type" class="headerlink" title="data type"></a>data type</h1><p>long¶</p>
<p>In q versions 3.0 and later, the basic integer type is a signed eight-byte integer, called long. A literal is identified as a long by the fact that it contains only numeric digits, with an optional leading minus sign, and no decimal point. It may also have an optional trailing type indicator j indicating it is a long and not another integer type. Here is a typical long integer value.</p>
<p>q)42<br>42<br>Observe that the type indicator j is accepted but redundant.</p>
<p>q)42j<br>42</p>
<p>The short type represents a two-byte signed integer and requires the trailing type indicator h. For example,</p>
<p>q)-123h<br>_
Similarly, the int type represents a four-byte signed integer and requires the trailing type indicator i.</p>
<p>The float type represents an IEEE standard eight-byte floating-point number, often called “double” in traditional languages. A float can hold (at least) 15 decimal digits of precision. It is denoted by optionally signed numeric digits with either a decimal point or an optional trailing type indicator f. Observe that the console shortens the display of floats with no significant digits to the right of the decimal.</p>
<p>You can change this by using the \P command (note upper case) to specify a display width up to 16 digits. If you issue \P 0 the console will display all 17 decimal digits of the underlying binary representation, although the last digit is unreliable.</p>
<p>boolean¶</p>
<p>The boolean type uses one byte to store a bit and is denoted by the bit value with the trailing type indicator b. There are no keywords for ‘true’ or ‘false’, nor are there separate logical operators for booleans.</p>
<p>q)0b<br>_
q)1b</p>
<p>Text Data¶</p>
<p>There are two atomic text types in q. They are more akin to the SQL types CHAR and VARCHAR than the character types of traditional languages.</p>
<p>2.4.1 char¶</p>
<p>A char holds an individual ASCII or 8-bit Unicode character that is stored in one byte. It corresponds to a SQL CHAR. It is denoted by a single character enclosed in double quotes.</p>
<p>q)”q”</p>
<p>Some keyboard characters – e.g., the double-quote – cannot be entered directly into a char since they have special meaning in the q console. As in C, special characters are escaped with a preceding back-slash . The console display somewhat confusingly displays the escape, but the following are all actually single characters.</p>
<p>q)”&quot;“ 
“&quot;“
q)”\“ 
_
q)”\n”<br>_
q)”\r”<br>_
q)”\t”<br>_
Also as in C, you can escape any ASCII character by specifying its underlying numeric value as three octal digits.</p>
<p>q)”\142”<br>“b”</p>
<p>symbol¶</p>
<p>A symbol is an atom holding text. It is denoted by a leading back-quote, read “back tick” in q-speak.</p>
<p>q)<code>q
_
q)</code>zaphod<br>_</p>
<p>a symbol is not a collection of char. The symbol `a and the char “a” are not the same, as we can see by asking q if they are identical.</p>
<p>q)`a~”a”<br>0b</p>
<h1 id="list-1"><a href="#list-1" class="headerlink" title="list"></a>list</h1><h2 id="Index-Notation¶"><a href="#Index-Notation¶" class="headerlink" title="Index Notation¶"></a>Index Notation¶</h2><p>To access the item at index i in a list, follow the list immediately with [i]. This is called item indexing. For example,</p>
<p>q)(100; 200; 300)[0]</p>
<p> Indexed Assignment¶</p>
<p>Items in a list can also be assigned via item indexing. Thus,</p>
<p>q)L:1 2 3<br>q)L[1]:42<br>q)L<br>1 42 3</p>
<p>An omitted index returns the entire list.</p>
<p>q)L:10 20 30 40<br>q)L[]<br>10 20 30 40</p>

          
        
      
    </div>

    
    
    
      <footer class="post-footer">
          <div class="post-eof"></div>
        
      </footer>
  </div>
  
  
  
  </article>

    
      

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.todzhang.com/2016-09-01-Mac-Tips/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Todd Zhang">
      <meta itemprop="description" content="Contact me via phray.zhang@gmail.com or wechat at helloworld_2000">
      <meta itemprop="image" content="/images/globe.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Clouds & Docker">
    </span>
      <header class="post-header">

        
          <h1 class="post-title" itemprop="name headline">
              
              <a href="/2016-09-01-Mac-Tips/" class="post-title-link" itemprop="url">Mac tips</a>
            
          </h1>
        

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              
                
              

              <time title="Created: 2019-06-09 13:53:39" itemprop="dateCreated datePublished" datetime="2019-06-09T13:53:39+10:00">2019-06-09</time>
            </span>
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
            <h1 id="how-to-switch-differnt-input"><a href="#how-to-switch-differnt-input" class="headerlink" title="how to switch differnt input"></a>how to switch differnt input</h1><p>This can be configured in system proferenece-&gt;keyboard shortcuts-&gt;input sources-&gt;select next source in input<br>Ctrl+Alt+Space</p>
<h1 id="How-to-force-an-app-to-quit-on-your-Mac"><a href="#How-to-force-an-app-to-quit-on-your-Mac" class="headerlink" title="How to force an app to quit on your Mac"></a>How to force an app to quit on your Mac</h1><p>Press these three keys together: Option, Command, and Esc (Escape). This is similar to pressing Control-Alt-Delete on a PC. Or choose Force Quit from the Apple () menu in the upper-left corner of your screen.</p>
<h1 id="To-capture-screenshot-of-Mac"><a href="#To-capture-screenshot-of-Mac" class="headerlink" title="To capture screenshot of Mac"></a>To capture screenshot of Mac</h1><p>Press Shift + Command + 5, then click an option, like Selection button to capture a still selection or Whole screen button to record your whole screen.</p>
<p>The screenshot tools appear in a small palette, which you can drag to reposition. The palette includes options for where to save the screenshot, whether to show the pointer, and more.</p>
<p>Save, edit, and share your shots<br> After you take a screenshot or video, a thumbnail appears in the corner of your screen. Drag it int</p>
<h1 id="Forward-delete-delete-char-next-to-cursor"><a href="#Forward-delete-delete-char-next-to-cursor" class="headerlink" title="Forward delete (delete char next to cursor)"></a>Forward delete (delete char next to cursor)</h1><p>Fn+Delete</p>
<h1 id="To-hide-and-show-dock"><a href="#To-hide-and-show-dock" class="headerlink" title="To hide and show dock"></a>To hide and show dock</h1><p>Just hit Command+Option+D at any time, and the dock will glide away (or back again).</p>
<h1 id="Windows"><a href="#Windows" class="headerlink" title="Windows"></a>Windows</h1><h2 id="Open-a-file"><a href="#Open-a-file" class="headerlink" title="Open a file"></a>Open a file</h2><ul>
<li>Cmd+Down arrow</li>
<li>Cmd+O</li>
</ul>
<h2 id="Switch-between-windows"><a href="#Switch-between-windows" class="headerlink" title="Switch between windows"></a>Switch between windows</h2><p>While you have two or more documents open in your favorite word-processing software, simply press and hold the Command ⌘ key and then strike the ~ (Tilde) key on your keyboard.</p>
<h2 id="navigate-on-page"><a href="#navigate-on-page" class="headerlink" title="navigate on page"></a>navigate on page</h2><p>fn + ←  Jump to top of document<br>fn + →  Jump to bottom of document<br>fn + ↓  Advance down one page<br>fn + ↑  Advance up one page</p>
<h2 id="Enter-path-in-open-save-window"><a href="#Enter-path-in-open-save-window" class="headerlink" title="Enter path in open/save window"></a>Enter path in open/save window</h2><p>Press <code>Ctrl+Shift+G</code> will open GoTo window</p>
<h2 id="Switch-running-applications"><a href="#Switch-running-applications" class="headerlink" title="Switch running applications"></a>Switch running applications</h2><p>Besides Command + Tab, you can swipe up on the touchpad with three fingers to view the windows of open apps, allowing you to quickly switch between programs. This view is called Mission Control, which also has its own dedicated keyboard shortcut (F3).</p>
<h2 id="Capture-screen-shot"><a href="#Capture-screen-shot" class="headerlink" title="Capture screen shot"></a>Capture screen shot</h2><h3 id="To-capture-the-entire-screen-press-Command-Shift-3"><a href="#To-capture-the-entire-screen-press-Command-Shift-3" class="headerlink" title="To capture the entire screen, press Command-Shift-3"></a>To capture the entire screen, press Command-Shift-3</h3><p> The screen shot will be automatically saved as a PNG file on your desktop with the filename starting with “Picture” followed by a number, example Picture 1, Picture 2, and so on.</p>
<h3 id="To-capture-a-portion-of-the-screen-press-Command-Shift-4"><a href="#To-capture-a-portion-of-the-screen-press-Command-Shift-4" class="headerlink" title="To capture a portion of the screen, press Command-Shift-4"></a>To capture a portion of the screen, press Command-Shift-4</h3><h1 id="Terminal"><a href="#Terminal" class="headerlink" title="Terminal"></a>Terminal</h1><h2 id="Page-up-Down"><a href="#Page-up-Down" class="headerlink" title="Page up /Down"></a>Page up /Down</h2><ul>
<li>Page up/down: Fn + up/down arrow, or Ctrl+f/b<h2 id="Setup-command-line-for-sublime"><a href="#Setup-command-line-for-sublime" class="headerlink" title="Setup command line for sublime"></a>Setup command line for sublime</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ln -s <span class="string">"/Applications/Sublime Text.app/Contents/SharedSupport/bin/subl"</span> /usr/<span class="built_in">local</span>/bin/sublime</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h2 id="Open-files-from-terminal"><a href="#Open-files-from-terminal" class="headerlink" title="Open files from terminal"></a>Open files from terminal</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">open abc.jpg</span><br></pre></td></tr></table></figure>

<h2 id="To-make-changed-profile-settings-take-effect-right-away"><a href="#To-make-changed-profile-settings-take-effect-right-away" class="headerlink" title="To make changed profile settings take effect right away"></a>To make changed profile settings take effect right away</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> .bash_profile</span><br></pre></td></tr></table></figure>

<h1 id="Applications"><a href="#Applications" class="headerlink" title="Applications"></a>Applications</h1><h2 id="Safari"><a href="#Safari" class="headerlink" title="Safari"></a>Safari</h2><h2 id="Sublime"><a href="#Sublime" class="headerlink" title="Sublime"></a>Sublime</h2><h3 id="Switch-different-sublime-windows"><a href="#Switch-different-sublime-windows" class="headerlink" title="Switch different sublime windows"></a>Switch different sublime windows</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'command + ~'</span></span><br></pre></td></tr></table></figure>

<h3 id="Path-of-customize-build-system"><a href="#Path-of-customize-build-system" class="headerlink" title="Path of customize build system"></a>Path of customize build system</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">~/Library/Application Support/Sublime Text 3/Packages/User/Nodejs.sublime-build</span><br></pre></td></tr></table></figure>

<h3 id="Open-folders-in-Sublime"><a href="#Open-folders-in-Sublime" class="headerlink" title="Open folders in Sublime"></a>Open folders in Sublime</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sublime .</span><br></pre></td></tr></table></figure>

<h3 id="Open-and-edit-file-with-sublime"><a href="#Open-and-edit-file-with-sublime" class="headerlink" title="Open and edit file with sublime"></a>Open and edit file with sublime</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">open -a <span class="string">'Sublime Text'</span> .bash_profile</span><br></pre></td></tr></table></figure>

<h3 id="Show-full-path-in-Sublime-text-3"><a href="#Show-full-path-in-Sublime-text-3" class="headerlink" title="Show full path in Sublime text 3"></a>Show full path in Sublime text 3</h3><p>With Sublime Text 3, all that’s necessary is to edit your Sublime user preferences (Preferences -&gt; Settings - User) to include:</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  // ... other settings</span><br><span class="line">  <span class="string">"show_full_path"</span>: <span class="literal">true</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="Open-files-go-to-anything-in-sublime"><a href="#Open-files-go-to-anything-in-sublime" class="headerlink" title="Open files (go to anything) in sublime"></a>Open files (go to anything) in sublime</h3><p>Command + P</p>
<h3 id="Go-to-matching-brace"><a href="#Go-to-matching-brace" class="headerlink" title="Go to matching brace"></a>Go to matching brace</h3><p>Ctrl + M</p>
<h3 id="Go-to-previous-next-position"><a href="#Go-to-previous-next-position" class="headerlink" title="Go to previous/next position"></a>Go to previous/next position</h3><ul>
<li>Ctrl + “-“</li>
<li>Ctrl + Shift + “-“</li>
</ul>
<h3 id="Preview-markdown"><a href="#Preview-markdown" class="headerlink" title="Preview markdown"></a>Preview markdown</h3><p>Command + Shift + P -&gt; Preview</p>
<h1 id="Gestures"><a href="#Gestures" class="headerlink" title="Gestures"></a>Gestures</h1><h2 id="Three-3-fingures-for-windows-switch"><a href="#Three-3-fingures-for-windows-switch" class="headerlink" title="Three (3) fingures for windows switch"></a>Three (3) fingures for windows switch</h2><ul>
<li>Up: Mission control, show all running applications</li>
<li>Left/Right: Swtich desktops</li>
</ul>
<h2 id="Thumb-three-figures"><a href="#Thumb-three-figures" class="headerlink" title="Thumb + three figures"></a>Thumb + three figures</h2><ul>
<li>Center to outside: show desktop</li>
<li>Outside to Center: show launch pad</li>
</ul>

          
        
      
    </div>

    
    
    
      <footer class="post-footer">
          <div class="post-eof"></div>
        
      </footer>
  </div>
  
  
  
  </article>

    
      

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.todzhang.com/2018-11-01-seconds/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Todd Zhang">
      <meta itemprop="description" content="Contact me via phray.zhang@gmail.com or wechat at helloworld_2000">
      <meta itemprop="image" content="/images/globe.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Clouds & Docker">
    </span>
      <header class="post-header">

        
          <h1 class="post-title" itemprop="name headline">
              
              <a href="/2018-11-01-seconds/" class="post-title-link" itemprop="url">Seconds</a>
            
          </h1>
        

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              
                
              

              <time title="Created: 2019-05-31 23:07:34" itemprop="dateCreated datePublished" datetime="2019-05-31T23:07:34+10:00">2019-05-31</time>
            </span>
          

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
            <h1 id="nano-seconds"><a href="#nano-seconds" class="headerlink" title="nano seconds"></a>nano seconds</h1><p>ns: 1/1,000,000,000 second, i.e. 10(-9) seconds<br>1 ns = 1/1,000 micro second<br>1 ns = 1/1,000,000 milis second</p>
<p>used in telecommunications</p>
<h1 id="micro-seconds"><a href="#micro-seconds" class="headerlink" title="micro seconds"></a>micro seconds</h1><p>Its symbol is μs. 微秒<br>1 μs = 1000 ns<br>1 μs = 1/1,000 milli Seconds</p>
<p>8.01 μs： light took the time to travel 1 mile in vaccum</p>
<p>The average human eye blink takes 350,000 microseconds (just over 1/3 of one second).<br>The average human finger snap takes 150,000 microseconds (just over 1/7 of one second).<br>A camera flash illuminates for 1000 microseconds.</p>
<h1 id="milli-second"><a href="#milli-second" class="headerlink" title="milli second"></a>milli second</h1><p>ms 毫秒<br>1 ms = 1/1,000 second<br>1 ms = 1,000 μs = 1,000,000 ns</p>
<p>3 ms: fly flgp its wing<br>5 ms: bee flap wing<br>300-400 ms: human eye to blink</p>

          
        
      
    </div>

    
    
    
      <footer class="post-footer">
          <div class="post-eof"></div>
        
      </footer>
  </div>
  
  
  
  </article>

    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/16/">16</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>


          </div>
          

        </div>
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      <div class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">

          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" src="/images/globe.gif" alt="Todd Zhang">
  <p class="site-author-name" itemprop="name">Todd Zhang</p>
  <div class="site-description motion-element" itemprop="description">Contact me via phray.zhang@gmail.com or wechat at helloworld_2000</div>
</div>
  <nav class="site-state motion-element">
      <div class="site-state-item site-state-posts">
        
          <a href="/archives/">
        
          <span class="site-state-item-count">151</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-tags">
        
          
            <a href="/tags/">
          
        
        
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
        <span class="site-state-item-count">127</span>
        <span class="site-state-item-name">tags</span>
        </a>
      </div>
    
  </nav>



        </div>
      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Todd Zhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> v3.8.0</div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">Theme – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v7.3.0</div>

        








        
      </div>
    </footer>
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
      </div>

    

  </div>

  
  <script src="/lib/jquery/index.js?v=3.4.1"></script>
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

<script src="/js/utils.js?v=7.3.0"></script>
  <script src="/js/motion.js?v=7.3.0"></script>

<script src="/js/schemes/pisces.js?v=7.3.0"></script>


<script src="/js/next-boot.js?v=7.3.0"></script>




  




























  

  

  


  
</body>
</html>
